## Correlations Application

We are going to jump straight into this one! To get you used to running a correlation we will use the examples you read about in **Miller and Haden (2013)**, Chapter 11, looking at the **relationship** between four variables: reading ability, intelligence, the number of minutes per week spent reading at home, and the number of minutes per week spent watching TV at home. This is again a great example of where a correlation works best because it would be unethical to manipulate these variables so measuring them as they exist in the environment is most appropriate.

<a href="./data/09-s02/inclass/ch09-inclass-data.zip" target = "_blank">Click here to download the data for today</a>.

### Task 1 - The Data {#Ch10InClassQueT1}

After downloading the data folder, unzip the folder, set your working directory appropriately, open a new R Markdown, and load in the Miller and Haden data (`MillerHadenData.csv`), storing it in a tibble called `mh`. As always, the solutions are at the end of the chapter.

* **Note 1:** Remember that in reality you could store the data under any name you like, but to make it easier for a demonstrator to debug with you it is handy if we all use the same names.
* **Note 2:** You will find that the instructions for tasks are sparser from now on, compared to earlier in the book. We want you to really push yourself and develop the skills you learnt previously. Don't worry though, we are always here to help, so if you get stuck, ask!
* **Note 3:** You will see an additional data set on vaping. We will use that later on in this chapter, so you can ignore it for now.

`r hide("Task 1: Hints for loading in Data")`
```{block, type ="info"}
* Hint 1: We are going to need the following libraries: tidyverse, broom
* Hint 2: mh <- read_csv()
* Remember that, in this book, we will ALWAYS ask you to use `read_csv()` to load in data. Avoid using other functions such as `read.csv()`. Whilst these functions are similar, they are not the same.
```
`r unhide()`  
<br>
Let's look at your data in `mh` - we showed you a number of ways to do this before. As in Miller and Haden, we have five columns: 

* Participant (`Participant`), 
* Reading Ability (`Abil`), 
* Intelligence (`IQ`), 
* Number of minutes spent reading at home per week (`Home`), 
* Number of minutes spent watching TV per week (`TV`). 

For the exercises we will focus on the relationship between Reading Ability and IQ, but for further practice you can look at other relationships on your own.  

A probable hypothesis for today could be that as Reading Ability increases so does Intelligence (*but do you see the issue with causality and direction?*). Or phrasing the alternative hypothesis ($H_1$) more formally, we hypothesise that the reading ability of school children, as measured through a standardized test, and intelligence, again measured through a standardized test, show a linear positive relationship. This is the hypothesis we will test today, but remember that we could always state the null hypothesis ($H_0$) that there is no relationship between reading ability and IQ. 

First, however, we must check some assumptions of the correlation tests. As we have stated that we hypothesise a linear relationship, this means that we are going to be looking at the **Pearson's product-moment correlation** which is often just shortened to **Pearson's correlation** and is symbolised by the letter **r**. 

The main assumptions we need to check for the Pearson's correlation, and which we will look at in turn, are:

* Is the data interval, ratio, or ordinal?
* Is there a data point for each participant on both variables?
* Is the data normally distributed in both variables?
* Does the relationship between variables appear linear?
* Does the spread have homoscedasticity?



### Task 2 - Interval or Ordinal {#Ch10InClassQueT2}

**Assumption 1: Level of Measurement**

<span style="font-size: 22px; font-weight: bold; color: var(--pink);">Thinking Cap Point</span>

If we are going to run a Pearson correlation then we need interval or ratio data. An alternative correlation, a non-parametric equivalent of the Pearson correlation is the Spearman correlation and it can run with ordinal, interval or ratio data. What type of data do we have?

* **Check your thinking:** the type of data in this analysis is most probably `r mcq(c("ratio", answer = "interval", answer = "ordinal", "nominal"))` as the data is `r mcq(c(answer = "continuous","discrete"))` and there is unlikely to be a true zero

`r hide("Hints on data type")`
```{block, type ="info"}
* are the variables continuous? 
* is the difference between 1 and 2 on the scale equal to the difference between 2 and 3?
```
`r unhide()`  
<br>



### Task 3 - Missing Data {#Ch10InClassQueT3}

**Assumption 2: Pairs of Data**

All correlations must have a data point for each participant on the two variables being correlated. This should make sense as to why - you can't correlate against an empty cell! Check that you have a data point in both columns for each participant. After that, try answering the question in Task 3. A discussion is in the solutions at the end of the chapter.

**Note:** You can check for missing data by visual inspection - literally using your eyes. A missing data point will show as a **NA**, which is short for not applicable, not available, or no answer. An alternative would be to use the `is.na()` function. This can be really handy when you have lots of data and visual inspection would just take too long.  If for example you ran the following code:

```{r isna-fake, eval=FALSE}
is.na(mh)
```

If you look at the output from that function, each `FALSE` tells you that there is a data-point in that cell. That is because `is.na()` asks is that cell a NA; is it empty. If the cell was empty then it would come back as `TRUE`. As all cells have data in them, they are all showing as `FALSE`. If you wanted to ask the opposite question, is their data in this cell, then you would write `!is.na()` which is read as "is not NA". The exclamation mark `!` turns the question into the opposite.


It looks like everyone has data in all the columns but let's test our skills a little whilst we are here. Answer the following questions:

1. How is missing data represented in a cell of a tibble? `r mcq(c("an empty cell", answer = "NA","a large number", "don't know"))`
2. Which code would leave you with just the participants who were missing Reading Ability data in mh: 
`r mcq(c("filter(mh, is.na(Ability)", answer = "filter(mh, is.na(Abil)", "filter(mh, !is.na(Ability)", "filter(mh, !is.na(Abil)"))`
3. Which code would leave you with just the participants who were not missing Reading Ability data in mh: `r mcq(c("filter(mh, is.na(Ability)", "filter(mh, is.na(Abil)", "filter(mh, !is.na(Ability)", answer = "filter(mh, !is.na(Abil)"))`

`r hide("Hints on removing missing data points")`
```{block, type ="info"}
* filter(dat, is.na(variable)) versus filter(dat, !is.na(variable))
```
`r unhide()`  
<br>


### Task 4 - Normality {#Ch10InClassQueT4}

**Assumption 3: The shape of my data**

The remaining assumptions are all best checked through visualisations. You could use histograms to check that the data (Abil and IQ) are both normally distributed, and you could use a scatterplot (scattergrams) of IQ as a function of Abil to check whether the relationship is linear, with homoscedasticity, and without outliers! 

An alternative would be to use **z-scores** to check for outliers with the cut-off usually being set at around $\pm2.5SD$. You could do this using the mutate function (e.g. `mutate(z = (X - mean(X))/SD(X))`), but today we will just use visual checks.

Create the following figures and discuss the outputs with your group. A discussion is in the solutions at the end of the chapter. 

* A histogram for Ability and a histogram for IQ. 
    - Are they both normally distributed? 
* A scatterplot of IQ (IQ) as a function of Ability (Abil).
    - Do you see any outliers? 
    - Does the relationship appear linear? 
    - Does the spread appear ok in terms of homoscedasticity?

`r hide("Hints to create figures")`
```{block, type ="info"}
* ggplot(mh, aes(x = )) + geom_histogram()
* ggplot(mh, aes(x = , y = )) + geom_point()
* Normality: something to keep in mind is that there are only 25 participants, so how 'normal' do we expect relationships to be
* homoscedasticity is that the spread of data points around the (imaginary) line of best fit is equal on both sides along the line; as opposed to very narrow at one point and very wide at others.
* Remember these are all judgement calls!
```
`r unhide()`  
<br>
 
 
### Task 5 - Descriptives {#Ch10InClassQueT5}

**Descriptives of a correlation**

A key thing to keep in mind is that the scatterplot is actually the descriptive of the correlation. Meaning that in an article, or in a report, you would not only use the scatterplot to determine which type of correlation to use, but also to describe the potential relationship in regards to your hypothesis. So you would always expect to see a scatterplot in the write-up of this type of analysis

<span style="font-size: 22px; font-weight: bold; color: var(--pink);">Thinking Cap Point</span>

Looking at the scatterplot you created in Task 4, spend a couple of minutes describing the relationship between Ability and IQ in terms of your hypothesis. Remember this is a descriptive analysis at this stage, so nothing is confirmed. Does the relationship appear to be as we predicted in our hypothesis? A discussion is in the solutions at the end of the chapter.

`r hide("Hints on discussing descriptives")`
```{block, type ="info"}
* Hint 1: We hypothesised that reading ability and intelligence were positively correlated. Is that what you see in the scatterplot?
* Hint 2: Keep in mind it is subjective at this stage.
* Hint 3: Remember to only talk about a relationship and not a prediction. This is correlational work, not regression.
* Hint 4: Can you say something about both the strength (weak, medium, strong) and the direction (positive, negative)?
```
`r unhide()`  
<br>


### Task 6 - Pearson or Spearman? {#Ch10InClassQueT6}

**The correlation**

Finally we will run the correlation using the `cor.test()` function.  Remember that for help on any function you can type `?cor.test` in the console window.  The `cor.test()` function requires:

* the name of the tibble and the column name of Variable 1
* the name of the tibble and the column name of Variable 2
* the type of correlation you want to run: e.g. "pearson", "spearman"
* the type of NHST tail you want to run: e.g. "one.sided", "two.sided"

For example, if your data is stored in `dat` and you are wanting a two-sided pearson correlation of the variables (columns) `X` and `Y`, then you would do:

```
cor.test(dat$X, dat$Y, method = "pearson", alternative = "two.sided")
```

* where `dat$X` means the column `X` in the tibble `dat`.  The dollar sign (`$`) is a way of indexing, or calling out, a specific column.

Based on your answers to Task 5, spend a couple of minutes deciding which correlation method to use (e.g., Pearson or Spearman) and the type of NHST tail to set (e.g., two.sided or one.sided). Now, run the correlation between IQ and Ability and save it in a tibble called `results` (hint: `broom::tidy()`). The solution is at the end of the chapter.

`r hide("Hints to correlation")`
```{block, type ="info"}
* Hint 1: the data looked reasonably normal and linear so the method would be?
* Hint 2: results <- cor.test(mh$Abil......, method = ....., alternative....) %>% tidy()
```
`r unhide()`  
<br>


### Task 7 - Interpretation {#Ch10InClassQueT7}

**Interpreting the Correlation**

You should now have a tibble called `results` that gives you the output of the correlation between Reading Ability and IQ for the school children measured in Miller and Haden (2013) Chapter 11. All that is left to do now is interpret the output of the correlation. 

Look at `results`. Locate your correlation value, e.g. `results %>% pull(estimate)`, and then answer the following questions. Again a discussion can be found at the end of the chapter.

* The direction of the relationship between Ability and IQ is: `r mcq(c(answer = "positive", "negative","no relationship"))`
* The strength of the relationship between Ability and IQ is: `r mcq(c("strong", answer = "medium", "weak"))`
* Based on $\alpha = .05$ the relationship between Ability and IQ is: `r mcq(c(answer = "significant", "not significant"))`
* Based on the output, given the hypothesis that the reading ability of school children, as measured through a standardized test, and intelligence, again through a standardized test, are positively correlated, we can say that the hypothesis: `r mcq(c(answer = "is supported", "is not supported", "is proven", "is not proven"))` 

`r hide("Hints to interpretation")`
```{block, type ="info"}
* Hint1: If Y increases as X increases then the relationship is positive. If Y increases as X decreases then the relationship is negative. If there is no change in Y as X changes then there is no relationship
* Hint2: Depending on the field most correlation values greater than .5 would be strong; .3 to .5 as medium, and .1 to .3 as small.
* Hint3: The field standard says less than .05 is significant.
* Hint4: Hypotheses can only be supported or not supported, never proven.
```
`r unhide()`  
<br>

### Task 8 - The Matrix {#Ch10InClassQueT8}

Great, so far we have set a hypothesis for a correlation, checked the assumptions, run the correlation and interpreted it appropriately. So as you can see running the correlation is the easy bit. As in a lot of analyses it is getting your data in order, checking assumptions, and interpreting your output that is the hard part. 

We have now walked you through one analysis but you can always go run more with the Miller and Haden dataset. There are six in total that could be run but watch out for Multiple Comparisons - where your **False Positive Error rate** (Type 1 error rate) is inflated and as such **the chance of finding a significant effect is inflated by simply running numerous tests.** Alternatively, we have another data set below that we want you to run a correlation on yourself but first we want to show you something that can be very handy when you want to view lots of correlations at once.

**Advanced 1: Matrix of Scatterplots**

Above we ran one correlation and if we wanted to do a different correlation then we would have to edit the `cor.test()` line and run it again. However, when you have lots of variables in a dataset, to get a quick overview of patterns, one thing you might want to do is run all the correlations at the same time or create a matrix of scatterplots at the one time. You can do this with functions from the `Hmisc` library - already installed in the Boyd Orr Labs. We will use the Miller and Haden data here again which you should still have in a tibble called `mh`. 

First, we need to get rid of the Participant column as we don't want to correlate that with anything. It won't tell us anything. Copy and run the below line of code

```{r ch10-advance-load-hide, message = FALSE, warning = FALSE, echo = FALSE}
library("Hmisc")
library("tidyverse")
mh <- read_csv("data/09-s02/inclass/MillerHadenData.csv") %>% select(-Participant)
```

```{r ch10-advance-load-show, message = FALSE, warning = FALSE, eval = FALSE}
library("Hmisc")
library("tidyverse")
mh <- read_csv("MillerHadenData.csv") %>% select(-Participant)
```

Now run the following line. The `pairs()` function from the `Hmisc` library creates a matrix of scatterplots which you can then use to view all the relationships at the one time.

```{r ch10-advance-pairs, fig.cap='Matrix of Correlation plots of Miller and Haden (2013) data'}
pairs(mh)
```

And the `rcorr()` function creates a matrix of correlations and p-values. But watch out, it only accepts the data in matrix format. Run the following two lines of code.

```{r ch10-advance-matrix}
mh_mx <- as.matrix(mh)
rcorr(mh_mx, type = "pearson")
```


After running the above lines, spend a few minutes answering the following questions. The first table outputted is the correlation values and the second table is the p-values.

1. Why do the tables look symmetrical around a blank diagonal?
2. What is the strongest positive correlation?
3. What is the strongest negative correlation?

`r hide("Hints to Matrix of correlations")`
```{block, type ="info"}
* Hint1: There is no hint, this is just a cheeky test to make sure you have read the correlation chapter in Miller and Haden, like we asked you to! :-)  If you are unsure of these answers, the solutions are at the end of the chapter.
```
`r unhide()`  

### Task 9 - Power Calculations {#Ch10InClassQueT9}

In the previous chapter (Chapter 8), we looked at power calculations in relation to t-tests. Now, let’s look at power calculations in the context of correlations. 

For correlations, we are going to take a similar approach. However, instead of using the `pwr.t.test()` function, we will use the `pwr.r.test()` function. The structure of this function is very similar to `pwr.t.test()` and works on the same leave-one-out principle:

* **n** - Number of observations/participants
* **r** - Correlation coefficient (the effect size of interest)
* **sig.level**	or $\alpha$
* **power** or $1-\beta$
* **alternative** - a character string specifying the alternative hypothesis, must be one of `two.sided` (default), `greater` (a positive correlation) or `less` (a negative correlation).

**1. Sample size for a correlation**

* Assuming you are interested in detecting a minimum correlation of **r = .4** (in either direction), what would be the minimum number of participants you would need for a correlation analysis, assuming **power = .8**, $\alpha$ **= .05**? 

Using a pipeline, store the answer as a single, rounded value called `sample_size_r` (i.e. use `pluck() %>% ceiling()`).

* Enter the minimum number of participants you would need in this correlation: `r fitb("46")`

**2. Effect size for a correlation analysis**

* You run a correlation analysis with 50 participants and the standard power and alpha levels and you have hypothesised a positive correlation, what would be the minimum effect size you can reliably detect? 
Answer the following questions to check your answers. The solutions are at the bottom if you need them:

* Based on the information given, what will you set `alternative` as in the function? `r mcq(c("two.sided", answer = "greater", "less"))`
* Based on the output, enter the minimum effect size you can reliably detect in this test, rounded to two decimal places: `r fitb(c(".34", "0.34"),ignore_ws = TRUE)`
* According to Cohen (1988), the effect size for this correlation is `r mcq(c("small to medium", answer = "medium to large", "large"))`
* Say you run the study and find that the effect size determined is r = 0.24. Given what you know about power, select the statement that is true: `r longmcq(c("the study is sufficiently powered as the analysis indicates you can detect only effect sizes smaller than r = 0.24",answer = "the study is underpowered as the analysis indicates you can detect only effect sizes larger than r = 0.34"))`

### Task 10 - Attitudes to Vaping {#Ch10InClassQueT10}

**Advanced 2: Attitudes towards Vaping**

Great work so far! Now we really want to see what you can do yourself. As we mentioned earlier, in the data folder there is another file called `VapingData.csv`. This data comes from a data set looking at implicit and explicit attitudes towards vaping.

* **Explicit attitudes** were measured via a questionnaire where higher scores indicated a positive attitude towards vaping. 
* **Implicit attitudes** were measured through an Implicit Association Test (IAT) using images of Vaping and Kitchen utensils and associating them with positive and negative words. 

The IAT works on the principle that associations that go together (that are congruent, e.g. warm and sun) should be quicker to respond to than associations that do not go together (that are incongruent, e.g. warm and ice). You can read up more on the procedure at a later date on the Noba Project <a href="https://nobaproject.com/modules/research-methods-in-social-psychology" target = "_blank">https://nobaproject.com/modules/research-methods-in-social-psychology</a> which has a good description of the procedure under the section "Subtle/Nonsconscious Research Methods".  

For this exercise, you need to know that "Block 3" in the experiment tested reaction times and accuracy towards congruent associations, pairing positive words with Kitchen utensils and negative words with Vaping. "Block 5" in the experiment tested reaction times and accuracy towards incongruent associations, pairing positive words with Vaping and negative words with Kitchen Utensils. As such, if reaction times were longer in "Block 5" than in "Block 3" then people are considered to hold the view that Vaping is negative (i.e., congruent associations are quicker than incongruent associations). However, if reaction times were quicker in "Block 5" than in "Block 3" then people are considered to hold the view that Vaping is positive (i.e., incongruent associations were quicker than congruent associations). The difference between reaction times in "Block 5" and "Block 3" is called the participants IAT score.

Load in the data in `VapingData.csv` and analyse it to test the hypothesis that Implicit and Explicit attitudes towards Vaping are positively related. Here are some pointers, hints and tips. Again, the solutions walk you through each step, but it will help you to try the steps yourself first. Think through each step. Sometimes you will need to think backwards, what do I want my tibble to look like. You can do all of these steps. This is nothing new.

1. After loading in the data, start by looking at the data. You have 8 columns. Reaction times and Accuracy scores for Blocks 3 and 5 as well as the Explicit Vaping Questionnaire scores, Gender and Age, for each participant.
2. Accuracy was calculated as proportion and as such can't go above 1. Participants entered their own data so some might have made a mistake. Remove participants who had an accuracy greater than 1 in either Block 3 or Block 5 as we are unclear on the accuracy of these values.
3. We also only want participants that were paying attention so best remove anybody whose average accuracy score across Blocks 3 and 5 was less than 80%. Note - this value is arbitrary and if you wanted, in your own experiment, you could use a more relaxed or strict cut-off based on other studies or guidance. Note that these decisions should be set out at the start of your research as part of your pre-registration or as part of your Registered Report. Finally, in this instance, remember, the values are in proportions not percentages (so 80% will be .8).
4. Now create an IAT score for participants by subtracting Block 3 reaction times (RT) away from Block 5 reaction times e.g $Block5 - Block3$. Use the information above to understand how the scores relate to attitudes.
5. Create a descriptives summary of the number of people, the mean RT and Vaping Questionnaire Score. Why might these averages be useful? Why are averages not always useful in correlations?
6. Check your assumptions of correlations as we did above and descriptives, thinking about how it compares to the hypothesis.
7. Run the appropriate correlation based on your assumptions and interpret the output.

`r hide("Hints for Vaping")`
```{block, type ="info"}
* libraries might include tidyverse and broom

* Hint Step 1: read_csv()
* Hint Step 2: filter(Accuracy < 1 OR Accuracy <= 1 OR Accuracy > 1 OR Accuracy >= 1) 
* Hint Step 3: average accuracy: mutate(data, name = (1 + 2)/2) %>% filter(name > ...)
* Hint Step 4: RT: mutate(data, nom = 1 - 2)
* Hint Step 5: descriptives <- summarise()
* Hint Step 6: assumptions would be type of data, normality, linear relationship, homoscedasicity, data points for everyone!
* Hint Step 7: results <- cor.test(method = "pearson")?
```
`r unhide()`  
<br>

Excellent work, who would have thought that about Explicit and Implicit attitudes towards Vaping?
