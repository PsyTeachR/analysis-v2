
```{r ch6-preclass-setup, echo = FALSE, message=FALSE, warning=FALSE, results='asis'}
# all data in here to make inline code work 
# solutions are shown for students at the bottom
# key values
n_new <- 22
orig_mean <- 590
orig_sd <- 94
#Task 1
ns_data <- tibble(participant = 1:n_new,
                  valid_rt = c(631.2,800.8,595.4,502.6,604.5,
                               516.9,658.0,502.0,496.7,600.3,
                               714.6,623.7,634.5,724.9,815.7,
                               456.9,703.4,647.5,657.9,613.2,
                               585.4,674.1))

#Task 2
woods_mean <- 590
n_participants <- ns_data %>%
  filter(valid_rt > woods_mean) %>%
  nrow()
#Task 3
pval_dbinom <- sum(dbinom(n_participants:nrow(ns_data), nrow(ns_data), .5))
#Task 4
ns_data_mean <- ns_data %>% summarise(m = mean(valid_rt)) %>% pull(m)  
ns_data_sd <- ns_data %>% summarise(sd = sd(valid_rt)) %>% pull(sd) 
#Task 5
t_obs <- (ns_data_mean - woods_mean) / (ns_data_sd / sqrt(nrow(ns_data)))
#Task 6
pval <- pt(abs(t_obs), nrow(ns_data) - 1L, lower.tail = FALSE) * 2L
#Task 7
ttest <- t.test(pull(ns_data, valid_rt), mu = woods_mean)
```


## The One-Sample t-test

The binomial test of the null hypothesis testing suggested that there was a significant difference in mean reaction times to valid trials on the modified Posner task between the participants in the pilot study and the participants in the original study by Woods et al. However, the binomial test did not use all the available information in the data because each participant was simply classified as being above or below the mean of the original paper, i.e. yes or no. Information about the **magnitude** of the discrepancy from the mean was discarded. This information is really interesting and important however and if we wanted to maintain that information then we would need to use a **One-sample $t$-test**.

In a One-sample $t$-test, you test the null hypothesis $H_0: \mu = \mu_0$ where: 

* $H_0$ is the symbol for the null hypothesis,
* $\mu$ (pronounced mu - like few with an m) is the unobserved population mean. It is the unobserved true mean of all possible participants. We don't know this value. Our best guess is the mean of the sample of `r n_new` participants so we will use that mean here. As such will substitute this value into our formula, which we call $\bar{X}$ (pronounced X-bar), instead of $\mu$.
* and $\mu_0$ (mu-zero) is some other mean to compare against (which could be an alternative population or sample mean or a constant). For us this is the mean of the original paper which we observed to be `r woods_mean` ms.  

And we will do this by calculating the test statistic $t$ which comes from the $t$-distribution - more on that distribution below and in the lectures. The formula to calculate the observed test statistic $t$ for the one-sample $t$-test is:

$$t = \frac{\mu - \mu_0}{s\ / \sqrt(n)}$$

* $s$ is the standard deviation of the sample collected, 
* and $n$ is the number of participants in the sample.

<br>

So, we are testing the null hypothesis that $H_0: \bar{X} =$ `r woods_mean`. As such the formula for our one-sample $t$-test becomes:

$$t = \frac{\bar{X} - \mu_0}{s\ / \sqrt(n)}$$

Now we just need to fill in the numbers.

### Task 4: Calculating the Mean and Standard Deviation {#Ch6PreClassQueT4}

* Calculate the mean and standard deviation of `valid_rt` for our `r n_new`  participants (i.e., for **all** participant data). 
* Store the mean in `ns_data_mean` and store the standard deviation in `ns_data_sd`. Make sure to store them both as single values!

`r hide("Helpful Hint")`
```{block, type ="info"}
In the below code, replace NULL with the code that would find the mean, `m`, of `ns_data`.

`ns_data_mean <- summarise(NULL) %>% pull(NULL)` 

Replace NULL with the code that would find the standard deviation, `sd`, of `ns_data`.

`ns_data_sd <- summarise(NULL) %>% pull(NULL)`
```
`r unhide()`

### Task 5: Calculating the Observed Test Statistic {#Ch6PreClassQueT5}

From Task 4, you found out that $\bar{X}$, the sample mean, was `r round(ns_data_mean, 3)` ms, and $s$, the sample standard deviation, was `r round(ns_data_sd, 3)` ms. Now, keeping in mind that $n$ is the number of observations/participants in the sample, and $\mu_0$ is the mean from Woods et al. (2009):

* Use the One-sample t-test formula above to compute your observed test statistic. Store the answer in `t_obs` .
* `t_obs <- (x - y)/(s/sqrt(n))`

<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>  

Answering this question will help you in this task as you'll also need these numbers to substitute into the formula:

* The mean from Woods et al. (2009) was `r mcq(c("595",answer = "590","580","585"))`, and the number of participants in our sample is: (type in numbers) `r fitb(22, ignore_ws = TRUE, width = 3)`.
* Remember the solutions at the end of the chapter if you are stuck. To check that you are correct without looking at the solutions though - the observed $t$-value in `t_obs`, to two decimal places, is `r mcq(c("1.66",answer = "1.76","1.86","1.96"))`

`r hide("Helpful Hint")`
```{block, type ="info"}
Remember BODMAS and/or PEDMAS when given more than one operation to calculate.
(i.e. Brackets/Parenthesis, Orders/Exponents, Division, Multiplication, Addition, Subtraction)

`t_obs <- (sample mean - woods mean) / (sample standard deviation / square root of n)`
```
`r unhide()`

### Task 6: Comparing the Observed Test Statistic to the t-distribution using **`pt()`** {#Ch6PreClassQueT6}

Now you need to compare `t_obs` to the t-distribution to determine how likely the observation (i.e. your test statistic) is under the null hypothesis of no difference. To do this you need to use the `pt()` function. 

* Use the `pt()` function to get the $p$-value for a `r glossary("two-tailed")` test with $\alpha$ level set to .05. The test has $n - 1$ degrees of freedom, where $n$ is the number of observations contributing to the sample mean $\bar{X}$. Store the $p$ value in the variable `pval`.  
* Do you reject the null?
* **Hint:** The `pt()` function works similar to `pbinom()` and `pnorm()`.
* **Hint:** Because we want the p-value for a **two-tailed** test, **multiply `pt()` by two**.

`r hide("Helpful Hint")`
```{block, type ="info"}
Remember to get help you can enter `?pt` in the console.

The `pt()` function works similar to `pbinom()` and `pnorm()`: 

* `pval <- pt(test statistic, df, lower.tail = FALSE) * 2`
* Use the **absolute value** of the test statistic; i.e. ignore minus signs.
* Remember, `df` is equal to `n-1`.
* Use `lower.tail = FALSE` because we are wanting to know the probability of obtaining a value higher than the one we got.
* Reject the null at the field standard of  p < .05
```
`r unhide()`
<br>

### Task 7: Comparing the Observed Test Statistic to the t-distribution using **`t.test()`** {#Ch6PreClassQueT7}

Now that you have done this by hand, try using the `t.test()` function to get the same result. Take a moment to read the documentation for this function by typing `?t.test` in the console window. No need to store the t-test output in a dataframe, but do check that the p-value matches the `pval` in Task 6.

* The structure of the `t.test()` function is `t.test(column_of_data, mu = mean_to_compare_against)`

`r hide("Helpful Hint")`
```{block, type ="info"}
The function requires a vector, not a table, as the first argument. You can use the `pull()` function to pull out the `valid_rt` column from the tibble `ns_data` with `pull(ns_data, valid_rt)`.

You also need to include `mu` in the `t.test()`, where `mu` is equal to the mean you are comparing to.
```
`r unhide()`

<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>  

To make sure you are understanding the output of the t-test, try to answer the following questions.

1. To three decimal places, type in the p-value for the t-test in Task 7 `r fitb(c("0.092",".092"), ignore_ws = TRUE, width = 6)`

2. As such this One-sample t-test is `r mcq(c("significant",answer = "not significant"))`

3. The outcome of the binomial test and the one sample t-test produce `r mcq(c("the same",answer = "a different"))` answer  

### Task 8: Drawing Conclusions about the new data {#Ch6PreClassQueT8}

Given these results, what do you conclude about how similar these `r n_new` participants are to the original participants in <a href = "https://www.sciencedirect.com/science/article/pii/S0005796708002738" target = "_blank">Woods et al (2009)</a> and whether or not you have managed to recruit sleepers similar to that study? 

* Think about which test used more of the available information? 
* Also, how reliable is the finding if the two tests give different answers?

We have given some of our thoughts at the end of the chapter.
