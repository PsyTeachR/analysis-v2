## Correlations Basics

The majority of the basics activities in the first part of the chapters from now on will involve reading a chapter or two from <a href="https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view" target = "_blank">Miller and Haden (2013)</a> and trying out a couple of tasks. This is an excellent free textbook that we will use to introduce you to the General Linear Model, a model that underlies all the majority of analyses you have seen and will see this year - e.g., t-tests, correlations, ANOVAs (Analysis of Variance), Regression are all related and are all part of the General Linear Model (GLMs). 

We will start off this chapter with some introduction to correlations.

### Read

**Chapter**

* Read Chapters 10 and 11 of <a href="https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view" target = "_blank">Miller and Haden (2013)</a>. 

Both chapters are really short, but give a good basis to understanding correlational analysis. Please note, in Chapter 10 you might not know some of the the terminology yet, e.g. ANOVA means Analysis of Variance and GLM means General Linear Model (Reading Chapter 1 of Miller and Haden might help). We will go into depth on these terms in the coming chapters.

### Watch

**Visualisation**

* Have a look at this visualisation of correlations by Kristoffer Magnusson: <a href = "https://rpsychologist.com/d3/correlation/" target = "_blank">https://rpsychologist.com/d3/correlation/</a>. 

1. After having read Miller and Haden Chapter 11, use this visualisation page to visually replicate the scatterplots in Figures 11.3 and 11.4 - use a sample of 100. After that, visually replicate the scatterplots in Figure 11.5. Each time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of relationship between the two variables. The greater the shared variance, the stronger the relationship. 
2. Also, try setting the correlation to r = .5 and then moving a single dot to see how one data point, a potential outlier, can change the stated correlation value between two variables

### Play

**Guess the correlation**

* Now that you are well versed in interpreting scatterplots (scattergrams) have a go at this online app on guessing the correlation: <a href = "https://www.rossmanchance.com/applets/GuessCorrelation.html" target = "_blank">https://www.rossmanchance.com/applets/GuessCorrelation.html</a>. 

This is a very basic application that allows you to see how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the "Track Performance" tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation.

*Is this all just a bit of fun?* Well, yes because stats is actually fun, and no, because it serves a purpose of helping you determine if the correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will know from the above examples one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. One only needs to mention Anscombe's Quartet to be reminded of the importance of visualising your data.


```{r corr-anscombe, echo = FALSE, message=FALSE, warning=FALSE}
dat <- read_csv("data/09-s02/preclass/AnscombeData.csv")

ans_data<- group_by(dat, Group) %>% 
  summarise(meanX = mean(X),
            sdX = sd(X),
            meanY = mean(Y),
            sdY = sd(Y), 
            corrs = cor(X,Y, method = "pearson")) %>%
  mutate_each(funs(rnd = round(.,3)), meanX:corrs) %>% 
  select(Group, meanX_rnd:corrs_rnd)

ans_plot <- ggplot(dat, aes(x = X, y = Y)) +
  geom_point(size = 2, color = "red") +
  geom_smooth(method = lm, se = FALSE, color = "black") +
  facet_wrap(~Group)  +
  labs(title = "Anscombe's Quartet") +
  scale_y_continuous(breaks = seq(0, 14, 2)) +
  scale_x_continuous(breaks = seq(0, 21, 3)) +
  coord_cartesian(ylim = c(0,14), xlim = c(0,20), expand = FALSE) +
  theme_bw()
```
<br>

Anscombe (1973) showed that four sets of bivariate data (X,Y) that have the exact same means, medians, and relationships:

```{r corr-anscombe-table, echo=FALSE}
knitr::kable(ans_data, align = "c", caption = "Four bivariate datasets that have the same means, medians, standard deviations, and relationships")
```
<br>

Can look very different when plotted:

```{r corr-anscombe-figure, echo = FALSE, fig.cap = "Though all datasets have a correlation of r = .82, when plotted the four datasets look very different. Grp I is a standard linear relationship where a pearson correlation would be suitable. Grp II would appear to be a non-linear relationship and a non-parametric analysis would be appropriate. Grp III again shows a linear relationship (approaching r = 1) where an outlier has lowered the correlation coefficient. Grp IV shows no relationship between the two variables (X, Y) but an oultier has inflated the correlation higher."}
ans_plot
```
<br>

All in this is a clear example of where you should visualise your data and not to rely on just the numbers. You can read more on Anscombe's Quartet in your own time, with wikipedia (<a href = "https://en.wikipedia.org/wiki/Anscombe%27s_quartet" target = "_blank">https://en.wikipedia.org/wiki/Anscombe%27s_quartet</a>) offering a good primer and the data used for the above example.
