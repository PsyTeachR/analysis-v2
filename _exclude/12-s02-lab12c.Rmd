## InClass Activity

**One-factor ANOVA: Worked example**

Just like we did in Chapter 11, let's start with some simulated data corresponding to a between-subjects design with three groups (conditions/levels) on one factor (variable). In this hypothetical study, you're investigating the effects of ambient noise on concentration. You have participants transcribe a handwritten document onto a laptop and count the number of typing errors (DV = typos) each participant makes under their respective different conditions:

* while hearing ambient conversation such as you would find in a busy cafe (**cafe** condition); 
* while listening to mellow jazz music (**jazz** condition); 
* or in silence (**silence** condition).  

Again for practice we will only use small, highly under-powered groups. You have three different participants in each condition. As such, your data are as follows:

- **cafe**: 111, 102, 111 
- **jazz**: 89, 127, 90
- **silence**: 97, 85, 88

Below is the decomposition matrix for this data set, based on the GLM: $$Y_{ij} = \mu + A_i + S(A)_{ij}$$ 

Here we are copying what we did in the last chapter in both the inclass and assignment and what you did for the homework activity - with just less participants. You can have a go at creating the decomposition matrix yourself from scratch if you like, as good practice, or, in the interests of time, feel free to reveal the code and run that code to create the dmx.

**Note** that we have also included a column called `sub_id` with a unique identifier for each participant. This is not that important for the decomposition matrix but we will definitely need it later for running the ANOVA using the `afex::aov_ez()` function, so let's just include it now so we don't forget.

`r hide("Reveal DMX code")`
```{r dmx}
dmx <- tibble(sub_id = 1:9,
              i = rep(1:3, each = 3),
              j = rep(1:3, times = 3),
              typos = c(111, 102, 111, 
                    89, 127, 90,
                    97, 85, 88),
              sound = rep(c("cafe", "jazz", "silence"), each = 3)) %>%
  mutate(mu = mean(typos)) %>%
  group_by(i) %>%
  mutate(Ai = mean(typos) - mu) %>%
  ungroup() %>%
  mutate(err = typos - (mu + Ai))
```
`r unhide()`


```{r dmx-show, echo = FALSE, results='asis'}
knitr::kable(dmx, 
             align = "c",
             digits = 3,
             caption = "Decomposition Matrix for Typos Example") %>%
  kable_styling() %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```


We finished off last week by calculating the Sums of Squares for the different columns - a measure of the variance attributable to that part of the model (or that column). Remember that to calculate the Sums of Squares (or often shortend to $SS$) we square all the values in a column and sum them up. So for example:

$$SS_{\mu} = \sum\mu^2 = (\mu_{11} \times \mu_{11}) + (\mu_{12} \times \mu_{12}) + \space ... \space (\mu_{33} \times \mu_{33})$$

We also said at the end of the last Chapter that the Sums of Squares of the different columns were all linked through the following relationship:

$$SS_{total} = SS_{\mu} + SS_{A} + SS_{error}$$

* Have a go at calculating the SS of the above decomposition matrix (`dmx`) using the code we showed you towards the end of the inclass activity in the last chapter. If unsure, then the code can be revealed below:

`r hide("Show how to calculate Sums of Squares")`

```{r dat_ss}
dat_ss <- dmx %>% 
  summarise(total = sum(typos^2),
            ss_mu = sum(mu^2),
            ss_sound = sum(Ai^2),
            ss_err = sum(err^2))
```

* **Which would give:**

```{r dat-ss-show, echo=FALSE, results='asis'}
knitr::kable(dat_ss, 
             align = "c",
             digits = 3,
             caption = "Sums of Squares for Typos Example") %>%
  kable_styling() %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```


We can then check that we have calculated everything correctly by using the following relationship:

$$SS_{total} = SS_{\mu} + SS_{A} + SS_{error}$$

then:

$$`r as.integer(dat_ss[["total"]])` = `r as.integer(dat_ss[["ss_mu"]])` + `r dat_ss[["ss_sound"]]` + `r dat_ss[["ss_err"]]`$$


`r unhide()`



### Task 1 - Quick Checks {#Ch12InClassQueT1}

<span style="font-size: 22px; font-weight: bold; color: var(--green);">QuickFire Questions</span>

Answer the following questions. The solutions are at the end of the chapter.

1. If the **corrected total Sum of Squares** is the $SS_{total}$ minus the part of the total attributable to the intercept (i.e., the grand mean, $SS_{\mu}$), calculate the corrected total Sum of Squares for the above example.

    * **hint**: $SS_{corrected} = SS_{total} - SS_{\mu}$

2. What proportion of the **corrected total sum of squares** is attributable to the main effect of `sound`? 

    * **hint**: $SS_{sound} = SS_{A}$
    * **hint**: $\frac{SS_{sound}}{SS_{corrected}}$

3. What proportion of the **corrected total sum of squares** is attributable to residual error? 

    * **hint**: $SS_{error}$

### Task 2 - Mean squares and degrees of freedom {#Ch12InClassQueT2}

Great, so now we know how to create our decomposition matrix and how to calculate our sums of squares. The only thing left to do is to calculate the **F-ratio** (the test statistic for the ANOVA, otherwise called the **F-value**) to determine if there is a significant effect between our groups. But, as it is always good to have a view of the whole picture, let's not forget that the whole purpose here is to show you where the numbers come from in our quest to determine if there is a significant difference between our groups, or in other words, is there an effect of listening condition on concentration!

You will remember from your lectures and from your reading of Miller & Haden (2013) that the F-value is a ratio of two estimates of population variance:

$$F = \frac{MS_{between}}{MS_{within}}  = \frac{MS_{treatment}}{MS_{error}} = \frac{MS_{A}}{MS_{S(A)}}= \frac{MS_{A}}{MS_{error}}$$

And you will also remember that the Mean Square (MS) is the Sums of Squares (SS) divided by its degrees of freedom (df). If you don't remember what degrees of freedom are, go back to pages 21-23 of <a href="https://drive.google.com/file/d/0B1fyuTuvj3YoaFdUR3FZaXNuNXc/view" target = "_blank">Miller and Haden (2013)</a>. They have a good explanation for it, however, these things are easy to forget, so make sure to qucikly skim back through the book. So the general formula for the Mean Square is: 

$$MS = \frac{SS}{df}$$

Let's start trying to put all this information together!  If we know the SS of our group/treatment ($SS_{A} = `r dat_ss[["ss_sound"]]`$ - also called the **between variance**) and we know the SS of our error/residuals ($SS_{error} = `r dat_ss[["ss_err"]]`$ - also called the **within variance**), then we can convert both of the $SS$s to Mean Squares (MS) (i.e. the average variance for that condition) by dividing them by their respective degrees of freedom (df). 

We can then calculate F-value (also called F-observed) by: $$F = \frac{MS_{A}}{MS_{error}}$$  

If the $MS_{error}$ is larger than $MS_{A}$ (the group effect) then F-value will be small and there will be no significant effect of group - any difference in groups is purely due to individual differences (another way of thinking about error). On the other hand, if $MS_{A}$ (the group effect) is larger than $MS_{error}$ then F-value will be large, and depending on how large the F-value is, there may be a significant difference caused by your group variable.

With all that in mind, and it may take a couple of readings, try to answer the following questions (consulting Miller & Haden Ch. 3 and your lecture slides where needed). The solutions are at the end of the chapter.

<span style="font-size: 22px; font-weight: bold; color: var(--green);">QuickFire Questions</span>

1. Stated in terms of $\mu_{jazz}$, $\mu_{cafe}$, and $\mu_{silence}$, what is the null hypothesis for this specific study of the effects of sound on typographic errors? You may need to look back to Chapters 6 and 7 to think about this.

**Hint:** the alternative hypothesis for a two condition experiment would be $\mu_{1} \ne \mu_{2}$

2. How many degrees of freedom are there for $A_{i}$, the main effect of **sound**, if $dfA_{i}$ = k - 1?

3. How many degrees of freedom are there for $S(A)_{ij}$, the error term, if $dfS(A)_{ij}$ = N - $dfA_{i}$ - 1?

**Hint:** You can also refer to $dfS(A)_{ij}$ as $df_{error}$

4. Calculate $MS_{A}$, where $A$ is the factor **sound**.  

**Note:** You can access individual columns in a table using double square brackets `[[]]`; for instance dat_ss[["ss_mu"]] gives you the column `ss_mu` from `dat_ss`. This is an alternative to `$` that some may know; e.g. `dat_ss$mu`.

5. Calculate $MS_{S(A)}$. 

**Hint:** You can also refer to $MS_{S(A)}$ as $MS_{error}$

`r hide("Hints for Task 2")`
```{block, type ="info"}
1. Remember that the null says that there are no differences between conditions.
2. Sound, our factor, has three levels.
3. N is the total number of participants
4. $MS_{A} = \frac{SS_{A}}{dfA_{i}}$
5. $MS_{S(A)} = \frac{SS_{error}}{dfS(A)_{ij}}$
```
`r unhide()`
    
### Task 3 - F-ratios {#Ch12InClassQueT3}

Last step, calculating the F-value.  As above, if the null hypothesis is true ($\mu_{1} = \mu_{2} = \mu_{3}$), then both estimates of the population variance ($MS_{between}$ and $MS_{within}$) should be equal, and the $F$-ratio should approach 1 (because $\frac{x}{x} = 1$). Now, we can't expect these two estimates to be **exactly** equal because of sampling bias, so to see how **un**likely our observed F-ratio is under the null hypothesis, we have to compare it to the F-distribution.

To learn a bit about the F-distribution we have created a shiny app to play with, which you can see below.  Shiny Apps are interactive webpages and applications made through R.  

```{r, echo = FALSE, fig.cap="F-distribution app"}
knitr::include_url("https://shiny.psy.gla.ac.uk/Dale/fdist/",
                   height = "1050px")
```

* The F distribution is a representation of the probability of various values of F under the null hypothesis. It depends upon two parameters: $df_{numerator}$ and $df_{denominator}$. Play around with the sliders corresponding to these two parameters and observe how the shape of the distribution changes.

* There is also a slider that lets you specify an observed $F$ ratio (to one digit of precision).  It is represented on the <span style = "color:blue">blue line</span> of the graph.  Move this slider around and watch how the p-values change.  The p-value is the total area under the curve to the right of the blue line.

* The <span style = "color:red">red line</span> on the plot denotes the critical value of F required for a significant difference, given the $\alpha$ (type 1 error rate) and the $df_{numerator}$ and $df_{denominator}$ .

<span style="font-size: 22px; font-weight: bold; color: var(--green);">QuickFire Questions</span>

Try using your data and the app to answer the following questions. The solutions are at the end of the chapter.

1. From your data, calculate the observed F-value (called `f_obs`) for the effect of `sound` on `typos` (concentration).

    * **hint**: $MS_{between} / MS_{within}$

Using the app shown above, set $\alpha = .05$, set the degrees of freedom to correspond to those in your study, and set the observed F ratio as close as you can to the value you got in the above question.  

2. Now, according to the app, what is the critical value for $F$ (**hint**: red line)?

3. According to the app, what is the approximate $p$ value associated with your observed $F$ ratio?

4. Based on these values, do you reject or retain the null hypothesis?

**Tricky question:** Note that you can use the distribution functions for $F$ in the same way you did in Chapter 4 for the normal distribution (`pnorm()`, `dnorm()`, `qnorm()`) or for the binomial distribution (`pbinom()`, `dbinom()`, `qbinom()`), keeping in mind however that the F distribution, being continuous, is more analogous to the normal distribution.  See `?df` for the distribution functions associated with $F$.

5. Using the appropriate distribution function, calculate the $p$ value associated with $F_{obs}$. This will be more precise than the app.

`r hide("Hints for Question 5")`
```{block, type ="info"}
* look at inputs for the function - ?pf
* ignore ncp
* f_obs = q
* lower.tail? What is the probability of obtaining an F_obs higher than your value.
```
`r unhide()`

### Task 4 - Using afex::aov_ez() {#Ch12InClassQueT4}

Great, so we have calculated the F-value for this test and made a judgement about whether it is significant or not. But that was quite a long way of doing it, and whilst it is always great to understand where the data comes from, you don't want to have to do that each time you run a test.  So now we are going to re-analyse the same dataset but this time we are going to have the computer do all the computational work for us. 

There are various options for running ANOVAs in R, but the function we will be using for this course is `aov_ez()` function in the **`afex`** add-on package.  Note that to use `aov_ez()` you either have to load in the package using `library(afex)`, or you can call it directly without loading using `afex::aov_ez()` (the `package_name::function` syntax).  If you're just using the function once, the latter often makes more sense.  The **`afex`** package is already installed in the Boyd Orr machines so it only needs called to the library. On your own machines you will need to install the package if you haven't already done so.

* Have a quick read through the documentation for `aov_ez` (type `?aov_ez` in the console) and then look at the code below which shows how to run the ANOVA and try to understand it. Pay specific attention to how you stipulate the datafile, the dv, the factor, the participants, etc. 

```{r afex-example, warning = FALSE, message=FALSE}
library(afex)

dat <- select(dmx, sub_id, typos, sound)

results <- afex::aov_ez(data = dat,
                         dv = "typos",
                         id = "sub_id",
                         type = 3,
                         between = "sound")

```

You will have seen a couple of messages pop up about converting `sound` to factor. That is fine. You can see these in the solutions if you want to confirm yours are the same. If we then look at the output of the analysis through the following code:

```{r afex-print-hide, results = 'hide'}
print(results$Anova)
```

We see:

```{r afex-print-show, echo=FALSE}
print(results$Anova)
```

And if you ran the following code:

```{r afex-print-hide2, results = 'hide'}
print(results$anova_table)
```

Then you would be able to see the effect size for this ANOVA in the form of generalised eta-squared (ges) - $\eta_G^2 = .32$:

```{r afex-print-show2, echo=FALSE}
print(results$anova_table)
```

* Looking at the ANOVA ouput, do you see how the numbers match up with what we calculated above? You can ignore the intercept for now (though the Sums of Squares will be familiar). This table doesn't show the Mean Squares but it does show the Sums of Squares and the F-value.
* What do you conclude about the effects of ambient noise on concentration?

**Conclusion**

Not much to be honest with you! The study returns a non-significant finding suggesting that there is no significant effect of ambient noise on concentration, F(2, 6) = 1.413, p = .31, ges = `r pluck(results$anova_table, "ges") %>% round(2)`. However, before you go off and publish this highly underpowered study we should probably look to replicate it with a larger sample (which you could calculate using your skills from Chapter 8).

<span style="font-size: 22px; font-weight: bold; color: var(--blue);">Job Done - Activity Complete!</span>

Excellent work today! And super interesting as well, huh? Quick, everyone to the cafe and don't worry about the typos!!!! Only joking, we are all going to the cafe to replicate!

**One last thing:**

Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become.

