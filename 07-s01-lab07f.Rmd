## Additional Material

Below is some additional material that might help you understand the tests in this Chapter a bit more as well as some additional ideas.

```{r lab7-solutions-setup, echo = FALSE, message=FALSE, warning=FALSE}
library(broom)
library(tidyverse)
ratings <- read_csv("data/07-s01/inclass/GuiltJudgements.csv")
```

### Non-Parametric tests {-}

In this chapter we have really been focussing on between-subjects and within-subjects t-tests which fall under the category of **parametric tests**. One of the main things you will know about these tests is that they have a fair number of assumptions that you need to check first to make sure that your results are valid. We looked at how to do this in the main body of the chapter, and you will get more practice at this as we progress, but one question you might have is, what do you do if the assumptions aren't met (or are "violated"" as it is termed)? 

So what options are there? Well actually you have already seen one - in Chapter 5. We could use permutation tests and bootstrapping (replication) techniques to compare two conditions. This is a nice approach as it has very few assumptions about the data - merely that the shape of the sample distribution is the shape of the population distribution. However permutation tests are a relatively new approach and require really thorough analytical skills to make sure you are doing them correctly when designs get complicated. 

Alternatively, there are tests known as **non-parametric tests** that have fewer assumptions than the parametric tests and can be run quite quickly using the same approach as we have seen with the t-tests. The non-parametric "t-tests" generally don't require any assumption of normality and tend to work on either the medians of the data (as opposed to the mean values) or the rank order of the data - i.e. what was the highest value, the second highest, the lowest - as opposed to the actual value.

And just like there is slightly different versions of t-tests there are different non-parametric tests for between-subjects designs and within-subjects designs as such:

* The Mann-Whitney U-test is the non-parametric equivalent of the between-subjects t-test
* The Wilcoxon Signed-Ranks Test is the non-parametric equivalent of the within-subjects t-test.
* **Note:** Bootstrapping and permutation tests would also be considered non-parametric tests. However if you were to ask someone about a non-parametric version of a t-test they would likely think of the Mann-Whitney or the Wilcoxon Signed-Ranks Test depending on your design. 
* **Note:** The Mann-Whitney and the Wilcoxon Signed-Ranks tests are now a bit antiquated as they were designed to be done by hand when computer processing power was limited. However they are still used in Psychology and you will still see them in older papers, so it is worth seeing one in action at least.

So for example, if you were concerned that your data was really far from being normally distributed, and weren't quite sure about permutation tests, you might use the Mann-Whitney or the Wilcoxon Signed-Ranks Test depending on your design. Here we will run through a Mann-Whitney U-test and then you can try out a Wilcoxon Signed-Ranks Test in your own time as it uses the same function - it is again just a matter of saying `paired = TRUE`.

**Our Scenario**

* **Aim:** To examine the influence of perceived reward on problem solving.
* **Procedure:** 14 Participants in 2 groups (7 per group) are asked to solve a difficult lateral thinking puzzle. One group is offered a monetary reward for completing it as quick as possible. One group is offered nothing; just the internal joy of getting the task completed and correct.
* **Task:** The participants are asked to solve the following puzzle. "Man walks into a bar and asks for a glass of water. The barman shoots at him with a gun. The man smiles, says thanks, and leaves. Why?"
* **IV:** Reward group vs. No Reward group
* **DV:** Time taken to solve puzzle measured in minutes.
* **Hypothesis:** We hypothesise that participants who are given a monetary incentive for solving a puzzle will solve the puzzle significantly faster, as measured in minutes to solve the puzzle, than those that are given no incentive. 

**Our Analysis**

Here is our data and a boxplot of the data to try and visualise what is happening in the data.
<br>
<br>

```{r chpt7-additional-data, echo=FALSE}
scores <- tibble(`No Reward` = c(1.32, 3.56, 7.55, 8.05, 8.54, 10.18, 15.55),
                 Reward = c(3.25, 5.54, 7.66, 9.02, 10.45, 13.21, 21.37)) %>% 
  pivot_longer(cols = Reward:`No Reward`, names_to = "Group", values_to = "Time") %>%
  arrange(Group) %>%
  mutate(Participant = 1:14) %>%
  select(Participant, Group, Time)
```

```{r chpt7-additional-table, echo=FALSE}
knitr::kable(scores, align = "c", caption = "Table showing the time taken to complete the puzzle for the Reward and No Reward groups")
```

<br>

```{r chpt7-additional-figure, echo = FALSE, fig.cap="Boxplots showing the time taken to solve the puzzle for the two conditions, Reward vs No Reward. Outliers are represented by solid blue dots."}
ggplot(data = scores, aes(x = Group, y = Time, fill = Group)) + 
  geom_boxplot(position = position_dodge(1),
               outlier.colour = "blue",
               outlier.size = 3) +
  scale_x_discrete(limits=c("Reward","No Reward")) +
  labs(y = "Time taken to complete problem (mins)") +
  guides(fill=FALSE) + 
  theme_classic()
```

Looking at the boxplots there is potentially some issues with skew in the data (see the No Reward group in particular) and both conditions are showing as having at least one outlier. As such we are not convinced our assumption of normality is held so we will run the Mann-Whitney U-test - the non-parametric equivalent of the between-subjects t-test (i.e. independent groups) - as it does not require the assumption of normal data.

**The descriptives**

Next, as always, we should look at the descriptives as well to make some subjective, descriptive inference about the pattern of the results. One thing to note is that the Mann-Whitney analysis is based on the "rank order" of the data regardless of group. In this code and table below we have put the data in order from lowest to highest and added on a rank order column. We have used the `rank()` function to create the ranks and setting the `ties.method = "average"`. We won't go into why that is the case here but you can read about it through ?rank.

```{r chpt7-additional-grouping-rnks}
scores_rnk <- scores %>%
  arrange(Time) %>%
  mutate(ranks = rank(Time, ties.method = "average"))
```

```{r chpt7-additional-table-rnks, echo=FALSE}
knitr::kable(scores_rnk, align = "c", caption = "Table showing the time taken to complete the puzzle for the Reward and No Reward groups and the rank order of these times.")
```

Here is a table of descriptives for this dataset an the code we used to create it.

```{r chpt7-additional-grouping}
ByGrp <- group_by(scores_rnk, Group) %>%
  summarise(n_Pp = n(),
            MedianTime = median(Time),
            MeanRank = mean(ranks))
```

```{r chpt7-additional-descriptives, echo=FALSE}
knitr::kable(ByGrp, align = "c", caption = "Descriptive (N, Medians and Mean Ranks) for the two groups (Reward vs No Reward) in time taken to solve the puzzle.")
```

Based on figure and descriptive data we can suggest that there appears to be no real difference between the two groups in terms of time taken to solve the puzzle. The group that was offered a reward have a slightly higher spread of data than the no reward group. However the medians and mean ranks are very comparable.

**The inferential test**

We will now run the Mann-Whitney U-test to see if the difference between the two groups is significant or not. To do this, somewhat confusingly, we us the `wilcox.test()` function. The code to do the analysis on the current data (with the tibble `scores`, the DV in the column `Time`, and the IV in the column `Group`) is shown below. It works just like the `t-test()` functio in that you can use either vectors or the fomula approach. 

* **Note:** There are a couple of additional calls in this function that you can read about using the `?wilcox.test()` approach.
* **Note:** We could just have easily used `scores_rnk` as our tibble in the `wilcox.test()` as opposed to `scores`. We are using `scores` to show you that you don't need to put the ranks into the `wilcox.test()` function, it will create them itself when it runs the analysis. We only created them to run some descriptives.

```{r chpt7-additional-code-test}
result <- wilcox.test(Time ~ Group, 
                      data = scores, alternative = "two.sided", 
            exact = TRUE, correct = FALSE) %>%
  tidy()
```

And here is the output of the test after is has been tidied into a tibble using `tidy()`

```{r chpt7-additional-code-output, echo=FALSE}
knitr::kable(result, align = "c", caption = "Output of the Mann-Whitney U-test")
```
<br>

The main statistic (the test-value) of the Mann-Whitney test is called the U-value and is shown in the above table as `statistic`; i.e. U = `r result %>% pull(statistic)` and you can see from the results that the difference was non-significant as p = `r result %>% pull(p.value) %>% round(3)`. 

* **Note:** The eagle-eyed of you will spot that the test actually says it is a Wilcoxon rank sum test. That is fine. The Mann-Whitney U is calculated from the sum of the ranks (shown in the table above). The Wilcoxon rank sum test is just that, a sum of the ranks. The U-value is then created from the summed ranks.  
* **Note:** Also, don't mistake the Wilcoxon rank sum test mentioned here - for between-subjects - with the Wilcoxon Signed-Ranks test for within-subjects mentioned above. They are different tests

However, one thing to note about the U is that it is an unstandardised value - meaning that it is dependent on the values sampled and it can't be compared to other U values to look at the magnitude of one effect versus another. The second thing to note about the U-value is that `wilcox.test()` will return a diferent U-value depending on which condition is stated as Group 1 or Group 2. 

Compare the outputs of these two tests where we have switched the order of the conditions:

**Version 1:**

```{r chpt7-additional-code-test-v1}
result_v1 <- wilcox.test(scores %>% filter(Group == "Reward") %>% pull(Time),
                         scores %>% filter(Group == "No Reward") %>% pull(Time),
                         data = scores, alternative = "two.sided",
                         exact = TRUE, correct = FALSE) %>%
  tidy()
```

**Version 2:**

```{r chpt7-additional-code-test-v2}
result_v2 <- wilcox.test(scores %>% filter(Group == "No Reward") %>% pull(Time),
                         scores %>% filter(Group == "Reward") %>% pull(Time),
                         data = scores, alternative = "two.sided",
                         exact = TRUE, correct = FALSE) %>%
  tidy()
```

The U-value for these two tests are, for Version 1, U = `r result_v1$statistic` and for Versoin 2, U = `r result_v2$statistic`. This may seem odd but actually both those test are correct. However, strictly speaking the U-value is the smaller of the two-values given by the different outputs. It is to do with how the U-value is calculated. Both groups have a U-value and the one that is checked for significance is the smaller of the two.

So for the reasons mentioned above, when we present the Mann-Whitney U-test we usually also give a Z-statistic, which is the standardised version of the U-value. We also present an effect size, commonly r. 

Z and r can be calculated as follows:

* Z = $\frac{U - \frac{N1 \times N2}{2}}{\sqrt\frac{N1 \times N2 \times (N1 + N2 + 1)}{12}}$

* r = $\frac{Z}{\sqrt(N1 + N2)}$

Putting these formulas into a coded format would look like this:

```{r chpt7-additional-calculater}
U <- result$statistic
N1 <- ByGrp %>% filter(Group == "Reward") %>% pull(n_Pp)
N2 <- ByGrp %>% filter(Group == "No Reward") %>% pull(n_Pp)
Z <- (U - ((N1*N2)/2))/ sqrt((N1*N2*(N1+N2+1))/12)
r <- Z/sqrt(N1+N2)
```

And as such the write-up could be written as: 

The time taken to solve the problem for the Reward group (n = `r ByGrp %>% filter(Group == "Reward") %>% pull(n_Pp)`, Mdn Time = `r ByGrp %>% filter(Group == "Reward") %>% pull(MedianTime)`, Mean Rank = `r ByGrp %>% filter(Group == "Reward") %>% pull(MeanRank) %>% round(2)`) and the no reward group (n = `r ByGrp %>% filter(Group == "No Reward") %>% pull(n_Pp)`, Mdn Time = `r ByGrp %>% filter(Group == "No Reward") %>% pull(MedianTime)`, Mean Rank = `r ByGrp %>% filter(Group == "No Reward") %>% pull(MeanRank) %>% round(2)`) were compared using a Mann-Whitney U-test. No significance difference was found, U = `r U`, Z = `r round(Z, 3)`, p = `r round(result$p.value,3)`, r = `r round(r, 3)`

**Last point on calculating U**

In the final write-up there we know, because of our codes, that U = `r U` is the smallest U-value of the test. However had you put the alternative U, U = `r result_v1$statistic`, when you calculated Z you would have got Z = `r round(Z, 3)*-1`, as opposed to Z = `r round(Z, 3)`. So your standardised statistic will have the same value but just the opposite polarity (either positive or negative). That is fine though as you can look at the medians and mean ranks to make sure you are interpreting the data correctly.

However, what you do have to watch out for when writing up this test is that you are presenting the correct U-value - remembering that technically you should present the smallest of the two U-values (refer back to Version 1 and Version 2 of using the `wilcox.test()`) above.  Fortunately you don't have to run both analyses to figure out which is the smaller U (though you could if you wanted). There is a quicker way using the below formula:

$U1 + U2 = N1 \times N2$

* where U1 is the U-value from your `wilcox.test()` function
* N1 is the number of people in one group (technically doesnt matter which group) and N2 is the number of people in the other group

We actually know both our U-values as we ran both tests; they are U1 = `r result_v1$statistic` and U2 = `r result_v2$statistic`, and we know our two groups are N1 = `r N1` and N2 = `r N2`. And if we put those numbers in the formula we get

$U1 + U2 = N1 \times N2$

=> $`r result_v1 %>% pull(statistic)` + `r result_v2 %>% pull(statistic)` = `r N1` \times `r N2`$

=> $`r result_v1 %>% pull(statistic) + result_v2 %>% pull(statistic)` = `r N1 * N2`$

So both sides equal `r N1 * N2`.  But say you only know one of the U-values; you of course will know both Ns.  Well you can quickly figure out the other U-value based on:

$U2 = (N1 \times N2) - U1$

for example, if you know U1 = `r U`, N1 = `r N1` and N = `r N2` then:

$U2 = (`r N1` \times `r N2`) - `r U`$

=> $U2 = (`r N1 * N2`) - `r U`$

=> $U2 = `r (N1 * N2) - U`$

And then you just have to present the smallest of the two U-values, in this case U = `r U`.  

That is it for this additional materials and hopefully you now have a decent understanding of the Mann-Whitney test. Remember you have the skills to simulate data to run some new examples. You could also try running a Wilcoxon Signed-Ranks Test as well though you might have to read a little on how to present those. It is similar to the Mann-Whitney though and you should be able to get there. However, if stuck, do ask!

Oh, and last last point, how to remember which test is which? Is the Mann-Whitney for between-subjects or within-subjects, and what is the Wilcoxon Signed-Ranks test for? You know they do different designs but which is which? Well, as silly as this memory aid is... The other name of between-subjects designs, as you know, is independent designs.  Add to that the fact that the late great singer Whitney Houston once starred in "The Bodyguard" which was about maintaining your right to freedom and independence. So whenever you get stuck on knowing which test is which, remember Whitney wanted independence in The Bodyguard and you should be ok. We did not say this was a very good memory aid!

<span style="font-size: 22px; font-weight: bold; color: var(--purple);">End of Additional Material!</span>