## Additional Material

Below is some additional material that might help you understand APES a bit more and some additional ideas.

### A different power function - `power.t.test()` {-}

First thing we wanted to mention was that you can still do this chapter if you don't have the `pwr` library installed. You could instead use the `power.t.test()` function which is a function available in base R, meaning that it is included when you install R, so you do not need to install any additional package.  This is handy to know when you are using a computer that you can't install libraries on to. The **`pwr`** library offers more functions and is easier to follow but but for now let's just use the base function `power.t.test()`.

Again, remember that for more information on this function, simply do `?power.t.test` in the console. On doing this you will see that `power.t.test()` takes a series of inputs:

* **n** - observations/participants, **per group** for the independent samples version, or the **number of subjects or matched pairs** for the paired and one-sample designs.
* **delta** - the difference between means
* **sd** - standard deviation; **note: if sd = 1 then delta = Cohen's d**
* **sig.level** or $\alpha$
* **power** or $1-\beta$
* **type** - the type of t-test; e.g. `"two.sample", "one.sample", "paired"`
* **alternative** - the type of hypothesis; `"two.sided", "one.sided"`

And it works on a leave one out principle, just like in the main chapter - except there is the added inclusion of `sd` and `delta` instead of `d`. But other than those differences it is very similar to what we did in the main activities of the chapter. You give it all the info you have and it returns the element you are missing.  So, returning to the example from the main chapter, say you needed to know how many people per group you would need to detect an effect size as low as `d = .4` with `power = .8`, `alpha = .05` in a `two.sample (between-subjects) t-test` on a `two.sided` hypothesis test.  You would do:

```{r pwr_example_out-1, echo=FALSE, message=FALSE}
n_test1 <- power.t.test(delta = .4, 
                       sd = 1,
                       power = .8,
                       sig.level = .05,
                       alternative = "two.sided",
                       type = "two.sample") %>% 
  pluck("n")
```

```{r pwr_example-1, eval=FALSE}
power.t.test(delta = .4,
             sd = 1,
             power = .8,
             sig.level = .05,
             alternative = "two.sided",
             type = "two.sample")
```

Which gives the following output:

```{r pwr_example-2, echo=FALSE}
power.t.test(delta = .4,
             sd = 1,
             power = .8,
             sig.level = .05,
             alternative = "two.sided",
             type = "two.sample")
```

And it would tell you that you would need `r n_test1` people per condition. Which matches the `r ceiling(n_test1)` **per condition** that we saw in the chapter. That really is a lot of people!!!

And just in case you were wondering, you can also use `pluck()` to pull out individual values as follows:

```{r pwr_example_out-2, echo=TRUE}
n_test <- power.t.test(delta = .4, 
                       sd = 1,
                       power = .8,
                       sig.level = .05,
                       alternative = "two.sided",
                       type = "two.sample") %>% 
  pluck("n")
```

And when you call

```{r pwr-example-out-2, eval = FALSE}
n_test
```

You again see:

```{r pwr-example-out-2a, echo= FALSE}
n_test
```

So hopefully that shows you an alternative if there is an issue with your `pwr` library. We would recommend using the `pwr` library where possible.

### Cohen's d to r {-}

As we said in the chapter there are actually a lot of different effect sizes that you could calculate and that Cohen's d is only one of them. An alternative is $r$. You will see $r$ a lot more as we progress through the book, and in particular around correlations, but it is fair to say that it is becoming more of a standard effect size for t-test as well. One thing that people do not like about Cohen's d is that it can actually be a very large number - well above 1 - and that can be difficult to compare across studies. $r$ on the other hand can't go above 1 and is therefore is considered easier to compare to across studies. The good news is that $r$ and Cohen's d can be calculated from each other using the below formulas:

$r = \frac{d}{\sqrt(d^2 + 4)}$

and

$d = \frac{2 \times r}{\sqrt(1-r^2)}$

You can present either for a t-test in the format of:

* t(df) = t-value, p = p-value, d = d-value 

or 

* t(df) = t-value, p = p-value, r = r-value 

### How to choose an effect size {-}

A really quick analogy from Ian Walker's "Research Methods and statistics", is say your test is not a stats test but a telescope. And say you have a telescope that is specifically designed only for spotting animals that are the size of elephants or larger (similar to saying a cohens d of .8 or greater for example - very big effect). If your telescope can only reliably detect something down to the size of an elephant but when you look through it you see something smaller that you think might be a mouse, you can't say that the "object"" is definitely is a mouse as you don't have enough power in your telescope - it is too blurry. But likewise you can't rule out that it isn't a mouse as that would be something you don't know for sure  - both of these are true because your telescope was only designed to spot things the size of an elephant or larger. You only bought a telescope that was able to spot elephants because that was all your were interested in. Had you been interested in spotting mice you would have had to have bought a more powerful telescope. And that is the point of Lakens' SESOI - you power to the minimum effect size (minimum object size) you would be interested in. This is why it is imperative that you decide before your study what effect you are interested in - and you can base this on previous literature or theory.

### Interpreting and writing up power {-}

A few points on interpreting power to consolidate things a bit.  Firstly, it is great that you are now thinking about power and effect sizes in the first place. It is important that this becomes as second nature as thinking about the design of your study and in future years and future studies the first question you should ask yourself when designing your study/secondary analysis is what size are my APES - Alpha, Power, Effect Size and Sample. And remember that **a priori power analysis** is the way ahead. The power and alpha are determined in advance of the study and you are using them to determine the effect size or the sample size.

Power is stated more and more commonly again in papers now and you will start to notice it in the Methods or Results sections. You will see something along the lines of "Based on a power =..... and alpha =...., given we had X voices in our sample, a power analysis (pwr package citation) revealed d = ...... as the minimum effect sizes we could reliably determine."

But how do you interpret a study in terms of power? Well, lets say you run a power analysis for a t-test (or for a correlation), and you set the smallest effect size of interest as d = .4 (or the equivalent r-value).  If you then run your analysis and find d = .6 and the effect is significant, then your study had enough power to determine this effect. The effect that you found was bigger than the effect you could have found. You can have some confidence that you have a reliable effect at that given power and alpha values. However, say that instead of d = .6 you found a significant effect but with an effect size just below .4, say d = .3 - the effect size you found is smaller than the smallest effect you could reliably find. In this case you have to be cautious as you are still unclear as to whether there actually is an effect or whether you have found an effect by chance due to your study not having enough power to reliably detect an effect size that small. You can't say for sure that there is an effect or that there isn't an effect.  You need to consider both stances in your write up. Remember though that you have sampled a population, so how representative that sample is of your population will also influence the validity of your power. Each sample will give a slightly different effect size.

Alternatively, and probably quite likely in many undergraduate projects due to time constraints, say you find a non-significant effect at an effect size smaller than what you predicted; say you find a non-significant effect with an effect size of d = .2 and your power analysis said you could only reliably detect an effect as small as d = .4. The issue you have here is that you can't determine solely based on this study if you a) have a non-significant effect because you are under powered or b) that you have a non-significant effect because there is actually no effect in the first place.  Again in your discussion you would need to consider both stances. What you can however say is that the effect that you were looking for is not any bigger than $d = .4$. That is still useful information. Ok you don't know how small the effect really is, but you can rule out any effect size bigger than your original d-value. In turn this helps future researchers plan their studies better and can guide them better in knowing how many participants to run. See how useful it would be if we published null findings!

Basically, when your test finds an effect size smaller than you can detect, you don't know what it but you know what it isn't - we aren't sure if it is a mouse but we know it is not an elephant. Instead you would use previous findings to support the object being a mouse or not but caveat the conclusion with the suggestion that the test isn't really sensitive to finding a mouse. Similar to a finding that has an effect size smaller than you can detect. You can use previous literature to support their not being an effect but you can't rule it out for sure. You might have actually found an effect had you had a more powerful test. Just like you might have been able to determine that it was a mouse had you had a more powerful telescope.

Taking this a bit further in some studies there really is enough power (in terms of N - say a study of 25000 participants) to find a flea on the proverbial mouse, but where nevertheless there is a non-significant finding. In this case you have the fortunate situation where you have a well-powered study and so can say with some degree of confidence that your hypothesis and design is unlikely to ever produce a replicable significant result.  That is probably about as certain as you can get in our science or as close as you can get to a "fact", a very rare and precious thing. 
However, incredibly high powered studies, with lots of participants, tend to be able to find any difference as a significant difference. A within-subjects design with 10000 participants ($power = .8, \alpha = .05$) can determine reliably detect an incredibly small effect size of d = .04. The question at that stage is whether that effect has any real world significance or meaning.

So the take-home message here is that your discussion should always consider the result in relation to the hypothesis, integrating previous research and theory, and if there is an additional issue of power, then your discussion could also consider the result in relation to whether you can truly determine the effect and how that might be resolved (e.g. re-assessing the effect size, changing the design (withins are more powerful), low sample, power to high (e.g. .9), alpha to low (e.g. .01)). This issue of power would probably be a small part in the generalisability/limitation section.

Note: In all of the above you can swap effect and relationship, d and r, and other analyses accordingly.

<span style="font-size: 22px; font-weight: bold; color: var(--purple);">End of Additional Material!</span>