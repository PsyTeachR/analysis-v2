## Additional Material

Below is some additional material that might help you understand correlations a bit more and some additional ideas.



### Checking for outliers with z-scores {-}

We briefly mentioned in the above activities that you could use z-scores to check for outliers, instead of visual inspection. We have covered this in lectures and it works for any interval or ratio dataset, and not just ones for correlations, but we will demonstrate it here using the IQ data from Miller and Haden. First, lets get just the IQ data.


```r
mh_IQ <- mh %>% 
  select(IQ)
```

A z-score is just a standardised value based on the mean ($\mu$) and standard deviation (SD or $\sigma$, proonounced "sigma") of the sample it comes from. The formula for a z-score is:
<br>

$$z = \frac{x - \mu}{\sigma}$$

Using z-scores is away of effectively converting your data onto the Normal Distribution. As such, because of known cut-offs from the Normal Distribution (e.g., 68% of data between $\pm1SD$, etc - See Chapter 4), we can use that information to determine a value as an outlier. To do it, you convert all the data within your variable into their respective z-score and if any of the values fall above or below your cut-off then they are considered an outlier.


```r
mh_IQ_z <- mh_IQ %>%
  mutate(z_scores = (IQ - mean(IQ))/sd(IQ))
```

Which will give us the following data (showing only the first 6 rows):

<table>
<caption>(\#tab:zscore-iq-table)Raw IQ data and z-scored IQ data from Miller and Haden</caption>
 <thead>
  <tr>
   <th style="text-align:center;"> IQ </th>
   <th style="text-align:center;"> z_scores </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 107 </td>
   <td style="text-align:center;"> 0.7695895 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 109 </td>
   <td style="text-align:center;"> 0.9907359 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 81 </td>
   <td style="text-align:center;"> -2.1053138 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 100 </td>
   <td style="text-align:center;"> -0.0044229 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 92 </td>
   <td style="text-align:center;"> -0.8890086 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 105 </td>
   <td style="text-align:center;"> 0.5484431 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 92 </td>
   <td style="text-align:center;"> -0.8890086 </td>
  </tr>
</tbody>
</table>

And if you run through the data you will see a whole range of z-scores ranging from -2.1053138 to 1.9858948.  During the class we said that we would use a cut-off of $\pm2.5SD$ and you can see from the range of the IQ z-scores that there is no value above that, so we have no outliers. Which you could confirm with the following code:


```r
mh_IQ_z %>% filter(abs(z_scores) > 2.5) %>% count() %>% pull(n)
```

Which if you run that code, you can see we have 0 outliers. This also demonstrates however why you must stipulate your cut-offs in advance - otherwise you might fall foul of adjusting your data to fit your own predictions and hiding your decision making in the numerous researcher degrees of freedom that exist.  For example, say you run the above analysis and see no outliers but don't find the result you want/expect/believe should be there. A questionable research practice would be now to start adjusting your z-score cut-off to different values to see what difference that makes. For instance, had we said the cut-off was more stringent at $\pm2SD$ we would have found 1 outlier.  

So the two take-aways from this are:

1. How to spot outliers using z-scores.
2. You must set your exclusion criteria in advance of running your analysis.


### A different approach to making a correlation table {-}

In the above activities, Task 8, we looked at making a table of correlation functions. It was a bit messy and there is actually an alternative approach that we can use, with some useful additional functions. This makes use of the **`corrr`** package which you may need to install onto your laptop if you want to follow along.



Here is the code below that you can try out and explore yourself.  The main functions are `correlate()`, `shave()`, and `fashion()`:

* `correlate()` runs the correlations and can be changed between pearson, spearman, etc. The `quiet` argument just removes some information reminders that we dont really need. Switch it to `FALSE` to see what happens. A nice aspect of this function is that the default is to only use complete rows but you can alter that.
* `shave()` can be used to convert one set of correlation values that are showing the same test to NAs. For example, the bottom half of the table just reflects the top half of the table so we can convert one half. The default is to convert the upper half of the table.
* `fashion()` can be used to tidy up the table in terms of removing NAs, leading zeros, and for setting the number of decimal places.

The code would look like this:


```r
library(corrr)

mh <- read_csv("MillerHadenData.csv")

mh %>% 
  select(-Participant) %>%
  correlate(method = "pearson", quiet = TRUE) %>%
  shave() %>%
  fashion(decimals = 3, leading_zeros = FALSE)
```

Which if you then look at the ouput you get this nice clean correlation matrix table shown below.

<div class="kable-table">

<table>
 <thead>
  <tr>
   <th style="text-align:left;"> term </th>
   <th style="text-align:left;"> Abil </th>
   <th style="text-align:left;"> IQ </th>
   <th style="text-align:left;"> Home </th>
   <th style="text-align:left;"> TV </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Abil </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> IQ </td>
   <td style="text-align:left;"> .451 </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Home </td>
   <td style="text-align:left;"> .744 </td>
   <td style="text-align:left;"> .202 </td>
   <td style="text-align:left;">  </td>
   <td style="text-align:left;">  </td>
  </tr>
  <tr>
   <td style="text-align:left;"> TV </td>
   <td style="text-align:left;"> -.288 </td>
   <td style="text-align:left;"> .246 </td>
   <td style="text-align:left;"> -.648 </td>
   <td style="text-align:left;">  </td>
  </tr>
</tbody>
</table>

</div>

So that is a bit of a nicer approach than the one shown in Task 8, and gives you more control over the output of your correlation matrix table. However, the downside of this approach, and the reason we didn't use it above is because this approach does not show you the p-values that easily. In fact, it doesn't show you them at all and you need to do a bit more work. You can read more about using the **`corrr`** package in this webpage by the author of the package: <a href = "https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr" target = "_blank">https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr</a>

### Comparing Correlations {-}

One step that is often overlooked when working with correlations and a data set with numerous variables, is to compare whether the correlations are significantly different from each other. For example, say you are looking at whether there is a relationship between height and attractiveness in males and in females. You run the two correlations and find one is a stronger relationship than the other. Many would try to conclude that that means there is a significantly stronger relationship in one sex than the other. However that has not been tested. It can be tested though. We will quickly show you how, but this paper will also help the discussion: Diedenhofen and Musch (2015) cocor: A Comprehensive Solution for the Statistical Comparison of Correlations. PLOS ONE 10(6): e0131499. <a href = "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121945" target = "_blank">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121945</a>

As always we need the libraries first. You will need to install the **`cocor`** package on to your laptop.


```r
library(cocor)
library(tidyverse)
```

Now we are going to walk through three different examples just to show you how it would work in different experimental designs. You will need to match the values in the code with the values in the text to make full sense of these examples. We will use examples from work on faces, voices and personality traits that you will encounter later in this book.

**Comparing correlation of between-subjects designs (i.e., independent-samples)**

Given the scenario of males voice pitch and height (r(28) = .89) and female voice pitch and height (r(28) = .75) can you say that the difference between these correlations is significant?

As these correlations come from two different groups - male voices and female voices - we used the `cocor.indep.groups()` function.

**Note:** As the df of both groups is 28, this means that the N is 30 of both groups (based on N = df-2)


```r
compare1 <- cocor.indep.groups(r1.jk = .89, 
                               r2.hm = .75, 
                               n1 = 30, 
                               n2 = 30)
```

Gives the output of:


```
## 
##   Results of a comparison of two correlations based on independent groups
## 
## Comparison between r1.jk = 0.89 and r2.hm = 0.75
## Difference: r1.jk - r2.hm = 0.14
## Group sizes: n1 = 30, n2 = 30
## Null hypothesis: r1.jk is equal to r2.hm
## Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided)
## Alpha: 0.05
## 
## fisher1925: Fisher's z (1925)
##   z = 1.6496, p-value = 0.0990
##   Null hypothesis retained
## 
## zou2007: Zou's (2007) confidence interval
##   95% confidence interval for r1.jk - r2.hm: -0.0260 0.3633
##   Null hypothesis retained (Interval includes 0)
```

You actually get two test outputs with this function: Fisher's Z and Zou's Confidence Interval. The one most commonly used is Fisher's Z so we will look at that one. As the p-value of the comparison between the two correlations, p = .1, is greater than p = .05, then this tells us that we have cannot reject the null hypothesis that there is no significant difference between these two correlations and they are of similar magnitude - i.e. the correlation is similar in both groups.

You would report this outcome along the lines of, "despite the correlation between male voice pitch and height was found to be stronger than the same relationship in female voices, Fisher's Z suggested that this was not a significant difference (Z = 1.65, p = .1)"

**Within-Subjects (i.e., dependent samples) with common variable**

What about the scenario where you have 30 participants rating faces on the scales of trust, warmth and likability, and we want to know if the relationship between trust and warmth (r = .89) is significantly different than the relationship between trust and likability (r = .8).

As this data comes from the same participants, and there is crossover/overlap in traits - i.e. one trait appears in both correlations of interest - then we will use the `cocor.dep.groups.overlap()`. To do this comparison we also need to know the relationship between the remaining correlation of warmth and likability (r = .91).

**Note:** The order of input of correlations matters. See `?cocor.dep.groups.overlap()` for help.


```r
compare2 <- cocor.dep.groups.overlap(r.jk = .89, 
                                     r.jh = .8, 
                                     r.kh = .91, 
                                     n = 30)
```

Gives the output of:


```
## 
##   Results of a comparison of two overlapping correlations based on dependent groups
## 
## Comparison between r.jk = 0.89 and r.jh = 0.8
## Difference: r.jk - r.jh = 0.09
## Related correlation: r.kh = 0.91
## Group size: n = 30
## Null hypothesis: r.jk is equal to r.jh
## Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
## Alpha: 0.05
## 
## pearson1898: Pearson and Filon's z (1898)
##   z = 1.9800, p-value = 0.0477
##   Null hypothesis rejected
## 
## hotelling1940: Hotelling's t (1940)
##   t = 2.4208, df = 27, p-value = 0.0225
##   Null hypothesis rejected
## 
## williams1959: Williams' t (1959)
##   t = 2.4126, df = 27, p-value = 0.0229
##   Null hypothesis rejected
## 
## olkin1967: Olkin's z (1967)
##   z = 1.9800, p-value = 0.0477
##   Null hypothesis rejected
## 
## dunn1969: Dunn and Clark's z (1969)
##   z = 2.3319, p-value = 0.0197
##   Null hypothesis rejected
## 
## hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)
##   t = 2.4208, df = 27, p-value = 0.0225
##   Null hypothesis rejected
## 
## steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations
##   z = 2.2476, p-value = 0.0246
##   Null hypothesis rejected
## 
## meng1992: Meng, Rosenthal, and Rubin's z (1992)
##   z = 2.2410, p-value = 0.0250
##   Null hypothesis rejected
##   95% confidence interval for r.jk - r.jh: 0.0405 0.6061
##   Null hypothesis rejected (Interval does not include 0)
## 
## hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure
##   z = 2.2137, p-value = 0.0268
##   Null hypothesis rejected
## 
## zou2007: Zou's (2007) confidence interval
##   95% confidence interval for r.jk - r.jh: 0.0135 0.2353
##   Null hypothesis rejected (Interval does not include 0)
```

So this test actually produces the output of a lot of tests; many of which we probably don't know whether to use or not. However, what you can do is run the analysis and choose just a specific test by adding the argument to the function: `test = steiger1980` or `test = pearson1898` for example.  These are probably the two most common.

Let's run the analysis again using just Pearson's Z (1898)


```r
compare3 <- cocor.dep.groups.overlap(r.jk = .89, 
                                     r.jh = .8, 
                                     r.kh = .91, 
                                     n = 30,
                                     test = "pearson1898")
```

Gives the output of:


```
## 
##   Results of a comparison of two overlapping correlations based on dependent groups
## 
## Comparison between r.jk = 0.89 and r.jh = 0.8
## Difference: r.jk - r.jh = 0.09
## Related correlation: r.kh = 0.91
## Group size: n = 30
## Null hypothesis: r.jk is equal to r.jh
## Alternative hypothesis: r.jk is not equal to r.jh (two-sided)
## Alpha: 0.05
## 
## pearson1898: Pearson and Filon's z (1898)
##   z = 1.9800, p-value = 0.0477
##   Null hypothesis rejected
```
As you can see from the output, this test is significant (p = .047) suggesting that there is a significant difference between the correlation of trust and warmth (r = .89) and the correlation of trust and likeability (r = .8).

**Within-Subjects (i.e., dependent samples) with no common variable**

Ok last scenario. You have 30 participants rating faces on the scales of trust, warmth, likability, and attractiveness, and we want to know if the relationship between trust and warmth (r = .89) is significantly different than the relationship between likability and attractiveness (r = .93).

As the correlations of interest have no crossover in variables but do come from the same participants, in this example we use the `cocor.dep.groups.nonoverlap()`.

**Note:** In order to do this we need the correlations of all other comparisons:

* trust and likability (.88)
* trust and attractiveness (.91)
* warmth and likability (.87)
* warmth and attractiveness (.92)

**Note:** The order of input of correlations matters. See `?cocor.dep.groups.nonoverlap()` for help.


```r
compare5 <- cocor.dep.groups.nonoverlap(r.jk = .89, 
                                        r.hm = .93, 
                                        r.jh = .88, 
                                        r.jm = .91, 
                                        r.kh = .87, 
                                        r.km = .92, 
                                        n = 30,
                                        test = "pearson1898")
```

Gives the output of:


```
## 
##   Results of a comparison of two nonoverlapping correlations based on dependent groups
## 
## Comparison between r.jk = 0.89 and r.hm = 0.93
## Difference: r.jk - r.hm = -0.04
## Related correlations: r.jh = 0.88, r.jm = 0.91, r.kh = 0.87, r.km = 0.92
## Group size: n = 30
## Null hypothesis: r.jk is equal to r.hm
## Alternative hypothesis: r.jk is not equal to r.hm (two-sided)
## Alpha: 0.05
## 
## pearson1898: Pearson and Filon's z (1898)
##   z = -1.1424, p-value = 0.2533
##   Null hypothesis retained
```

As you can see from the output, this test is non-significant (p = .253) suggesting that there is no significant difference between the correlation of trust and warmth (r = .89) and the correlation of likability and attractiveness (r = .8).

<span style="font-size: 22px; font-weight: bold; color: var(--purple);">End of Chapter!</span>
