
## PreClass Activity 1

**ManyLabs - an approach to reproducible science**

As you will learn from reading papers around the reproduciblity crisis, findings from experiments tend to be more reproducible when we increase participant numbers as this increases the <a class='glossary' target='_blank' title='NA' href='https://psyteachr.github.io/glossary/p#power'>power</a> of the study's design; the likelihood of an experimental design detecting an effect, of a given size, when there is an effect to detect. 


<div class='solution'><button>Portfolio Point - The power of what?</button>

<div class="info">
<p>Power is a rather tricky concept in research that essentially amounts to the probability of your design being able to detect a significant difference when there is actually a significant difference to detect.</p>
<p>Power is an interplay between three other aspects of research design:</p>
<ul>
<li>alpha - your critical p-value (normally .05);</li>
<li>the sample size (n);</li>
<li>the effect size - how big is the difference (measured in a number of ways).</li>
</ul>
<p>If you know any three of these four elements (power, alpha, effect size, n) you can calculate the fourth. We will save further discussion of power until Chapter 8 but if you want to read ahead then this blog is highly recommended: <a href="https://pigee.wordpress.com/2016/09/13/the-power-dialogues/" target ="_blank">The Power Dialogues</a>.</p>
</div>

</div>

<br>

However, running several hundred participants in your one study can be a significant time and financial investment. Fortunately, the idea of a "ManyLabs" project can solve this problem. In this scenario the same experiment is run in various locations, all using the same procedure, and then the data is collapsed together and analysed as one. You can see a nice example of a Many Labs project in the paper <a href="" target = "_blank">Investigating Variation in Replicability (Klein et al., 2014)</a>. See how many labs and researchers are involved? Perhaps this is a better approach than lots of researchers working individually? 

You think this all sounds a great idea so in your quest to be a collaborative reproducible researcher, and as a high-five to #TeamScience, you have joined a ManyLabs study replicating the findings of Woods et al. (2009). And that study is the premis for today's activities so let's start by having a quick-run through of the background of the experiment.

**The Background**

<a href = "https://www.sciencedirect.com/science/article/pii/S0005796708002738" target = "_blank">Woods and colleagues (2009)</a> were interested in how the attention of people with poor sleep (Primary Insomnia - PI) was more tuned towards sleep-related stimuli than the attention of people with normal sleep (NS). Woods et al., hypothesised that participants with poor sleep would be more attentive to images related to a lack of sleep (i.e. an alarm clock showing 2AM) than participants with normal sleep would be. To test their hypothesis, the authors used a modified Posner paradigm, shown in Figure 1 of the paper, where images of an alarm clock acted as the cue on both valid and invalid trials, with the symbol **( .. )** being the target. 

As can be seen in Figure 3 of <a href = "https://www.sciencedirect.com/science/article/pii/S0005796708002738" target = "_blank">Woods et al.,</a> the authors found that, on valid trials, whilst Primary Insomnia participants were faster in responding to the target, suggesting a slight increase in attention to the sleep related cue compared to the Normal Sleepers, there was no difference between groups. In contrast, for invalid trials, where poor sleep participants were expected to be distracted by the cue, the authors did indeed find a significant difference between groups consistent with their alternative hypothesis $H_{1}$. Woods et al., concluded that poor sleepers (Primary Insomnia participants) were slower to respond to the target on invalid trials, compared to Normal Sleepers, due to the attention of the Primary Insomnia participants being drawn to the misleading cue (the alarm clock) on the invalid trials. This increased attention to the sleep-related cue led to an overall slower reponse to the target on these invalid trials.



As we said above, your lab is now part of a ManyLabs project that is looking to replicate this finding from Woods et al., (2009). As a <a class='glossary' target='_blank' title='NA' href='https://psyteachr.github.io/glossary/p#pilot'>pilot study</a>, to test recruitment procedures, as well as the experimental paradigm and analyses pipeline, each lab gathers data from 22 Normal Sleepers. It is common to use only the control participants in a pilot (in this cas the NS participants) as they are more plentiful in the population than the experimental group (in this case PI participants) and saves using participants from the PI group which may be harder to obtain in the long run.

After gathering your data, we want to check the recruitment process and whether or not you have been able to draw a sample of normal sleepers, similar to the sample drawn by Woods et al. To keep things straightforward, allowing us to understand the analyses better, we will only look at valid trials today, in NS participants, but in effect you could perform this test on all groups and conditions.

**Are These Participants Normal Sleepers (NS)?**

Below is the data from the 22 participants you have collected in your pilot study. Their mean reaction time for valid trials (in milliseconds) is shown in the right hand column, `valid_rt`.

<table>
<caption>(\#tab:ch6-preclass-nsdata)Pilot Data for 22 Participants on a Sleep-Related Posner Paradigm. ID is shown in participant column and mean reaction time (ms) on valid trails is shown in valid_rt column.</caption>
 <thead>
  <tr>
   <th style="text-align:center;"> participant </th>
   <th style="text-align:center;"> valid_rt </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 631.2 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 800.8 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 595.4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 502.6 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 604.5 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 516.9 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 7 </td>
   <td style="text-align:center;"> 658.0 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 8 </td>
   <td style="text-align:center;"> 502.0 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 9 </td>
   <td style="text-align:center;"> 496.7 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 10 </td>
   <td style="text-align:center;"> 600.3 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 11 </td>
   <td style="text-align:center;"> 714.6 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 12 </td>
   <td style="text-align:center;"> 623.7 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 13 </td>
   <td style="text-align:center;"> 634.5 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 14 </td>
   <td style="text-align:center;"> 724.9 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 15 </td>
   <td style="text-align:center;"> 815.7 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 16 </td>
   <td style="text-align:center;"> 456.9 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 17 </td>
   <td style="text-align:center;"> 703.4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 18 </td>
   <td style="text-align:center;"> 647.5 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 19 </td>
   <td style="text-align:center;"> 657.9 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 20 </td>
   <td style="text-align:center;"> 613.2 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 21 </td>
   <td style="text-align:center;"> 585.4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 22 </td>
   <td style="text-align:center;"> 674.1 </td>
  </tr>
</tbody>
</table>

If you look at <a href="https://www.sciencedirect.com/science/article/pii/S0005796708002738" target = "_blank">Woods et al (2009)</a> Figure 3 you will see that, on valid trials, the mean reaction time for NS participants was 590 ms with a SD = 94 ms. As above, as part of our pilot study, we want to confirm that the 22 participants we have gathered are indeed Normal Sleepers. We will use the mean and SD from Woods et al., to confirm this. Essentially we are asking if the participants in the pilotare from the same population as NS participants in the original study.

You will know from Chapter 5 that when using Null Hypothesis Significance Testing (NHST) we are working with both a null hypothesis ($H_{0}$) and an alternative hypothesis ($H_{1}$). Thinking about this study it makes some logical sense to think about it in terms of the null hypothesis ($\mu1 = \mu2$). So we could phrase our hypothesis as, "we hypothesise that there is no significant difference in mean reaction times to valid trials on the modified Posner experiment between the participants in the pilot study and the participants in the original study by Woods et al."

There is actually a few analytical ways to test our null hypothesis. Today we will show you how to do two of these. In tasks 1-3 we will use a **binomial test** and in tasks 4-8 we will use a **one-sample t-test**


<div class='solution'><button>Portfolio Point - Binomial test and the one-sample t-test</button>

<div class="info">
<p>The Binomial test is a very simple test that converts all participants to either being above or below a cut-off point, e.g.Â a mean value, and looking at the probability of finding that number of participants above that cut-off.</p>
<p>The one-sample t-test is similar in that it compares participants to a cut-off but it compares the mean and standard deviation of the collected sample to an ideal mean and standard deviation. By comparing the difference in means, divided by the standard deviation of the difference (a measure of the variance), we can determine if the sample is similar or not to the ideal mean.</p>
</div>

</div>


___

### The Binomial Test

The Binomial test is one of the most "basic tests" in null hypothesis testing in that it uses very little information. The binomial test is used when a study has two possible outcomes (success or failure) and you have an idea about what the probability of success is. This will sound familiar from the work we did in Chapter 4 and the Binomial distribution. 

A binomial test tests if an observed result is different from what was expected. For example, is the number of heads in a series of coin flips different from what was expected. Or in our case for this chapter, we want to test whether our normal sleepers are giving reaction times that are the same or different from those measured by Woods et al. The following tasks will take you through the process.

### Task 1: Creating a Dataframe {#Ch6PreClassQueT1}

First we need to create a tibble with our data so that we can work with it.

* Enter the data for the 22 participants displayed above into a tibble and store it in `ns_data`.  Have one column showing the participant number (called `participant`) and another column showing the mean reaction time, called `valid_rt`.
    * We saw how to enter data into tibbles in Chapter 5 PreClass Skill 3.
    * You could type each value out or copy and paste them from the hint below.


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p>You can use this code structure and replace the NULL values:</p>
<p><code>ns_data &lt;- tibble(participant = c(NULL,NULL,...), valid_rt = c(NULL,NULL,...))</code></p>
<p>The values are: 631.2, 800.8, 595.4, 502.6, 604.5, 516.9, 658.0, 502.0, 496.7, 600.3, 714.6, 623.7, 634.5, 724.9, 815.7, 456.9, 703.4, 647.5, 657.9, 613.2, 585.4, 674.1</p>
</div>

</div>

<br>

### Task 2: Comparing Original and New Sample Reaction Times {#Ch6PreClassQueT2}

Our next step is to establish how many participants from our pilot study are above the mean in the original study by Woods et al.  

* In the original study the mean reaction time for valid trials was 590 ms.  Store this value in `woods_mean`.  
* Now write code to calculate the number of participants in the new sample (`ns_data` created in Task 1) that had a mean reaction time greater than the original paper's mean. Store this **single value** in `n_participants`. 
    * The function `nrow()` may help here.
    * `nrow()` is similar to `count()` or `n()` but `nrow()` returns the number as a single value and not in a tibble. 
    * Be sure whatever method you use you end up with a single value, not a tibble. You may need to use `pull()` or `pluck()`


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p><strong>Part 1</strong></p>
<p><code>woods_mean &lt;- value</code></p>
<p><strong>Part 2</strong></p>
<ul>
<li>A few ways to achieve this. Here are a couple you could try</li>
</ul>
<p><code>ns_data %&gt;% filter(x ? y) %&gt;% count() %&gt;% pull(?)</code></p>
<p>or</p>
<p><code>ns_data %&gt;% filter(x ? y) %&gt;% summarise(n = ?) %&gt;% pull(?)</code></p>
<p>or</p>
<p><code>ns_data %&gt;% filter(x ? y) %&gt;% nrow()</code></p>
<p>or</p>
<p><code>dim[] %&gt;% pluck()</code></p>
</div>

</div>

<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>  

* The number of participants that have a mean reaction time for valid trials greater than that of the original paper is: <select class='solveme' data-answer='["16"]'> <option></option> <option>6</option> <option>10</option> <option>16</option> <option>17</option></select>

### Task 3: Calculating Probability {#Ch6PreClassQueT3}

Our final step for the binomial test is to compare our value from Task 2, 16 participants, to our hypothetical cut-off. We will work under the assumption that the mean reaction time from the original paper, i.e. 590 ms, is a good estimate for the population of good sleepers (NS). If that is true then each new participant that we have tested should have a .5 chance of being above this mean reaction time ($p = .5$ for each participant).  

To phrase this another way, the expected number of participants above the cut-off would be $.5 \times N$, where $N$ is the number of participants, or $.5 \times 22$ = 11 participants.  

* Calculate what would be the probability of observing at least 16 participants out of your 22 participants that had a `valid_rt` greater than the Woods et al (2009) mean value.  
  * **hint:** We looked at very similar questions in Chapter 4 using `dbinom()` and `pbinom()` 
  * **hint:** The key thing is that you are asking about obtaining **X or more** successes. You will need to think back about cut-offs and `lower.tails`. 


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p>Think back to Chapter 4 where we used the binomial distribution. This question can be phrased as, what is the probability of obtaining X or more succeses out of Y trials, given the expected probability of Z.</p>
<ul>
<li>How many Xs? (see question)</li>
<li>How many Ys? (see question)</li>
<li>What is the probability of being either above or below the mean/cut-off? (see question)</li>
<li>You can use a dbinom() %&gt;% sum() for this or maybe a <code>pbinom()</code></li>
</ul>
</div>

</div>

<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>  

* Using the Psychology standard $\alpha = .05$, do you think these NS participants are responding in just the same fashion as the participants in the original paper? Select the appropriate answer: <select class='solveme' data-answer='["No"]'> <option></option> <option>No</option> <option>Yes</option></select>
* According to the Binomial test would you accept or reject the null hypothesis that we set at the start of this test? <select class='solveme' data-answer='["Reject"]'> <option></option> <option>Reject</option> <option>Accept</option></select>


<div class='solution'><button>Explain This - I don't get this answer</button>

<div class="info">
<p>The probability of obtaining 16 participants with a mean reaction time greater than the cut-off of 590 ms is p = .026. This is smaller than the field norm of p = .05. As such we can say that, using the binomial test, the new sample appears to be significantly different from the old sample as there is a significantly larger number of participants above the cut-off (M = 590ms) than would be expected if the new sample and the old sample were the same. We would therefor reject our null hypothesis!</p>
</div>

</div>


___

### The One-Sample t-test

In Task 3 you ran a **binomial test** of the null hypothesis testing that the mean reaction time for valid trials in good sleepers has remained stable across the two studies.  However, this test did not use all the available information because each participant was simply classified as being above or below the mean of the original paper, i.e. yes or no. Information about the **magnitude** of the discrepancy from the mean was discarded. This information is really interesting and important however and if we wanted to maintain that information then we would need to use a one-sample $t$-test.

In a one-sample $t$-test, you test the null hypothesis $H_0: \mu = \mu_0$ where: 

* $H_0$ is the symbol for the null hypothesis,
* $\mu$ is the unobserved population mean, 
* and $\mu_0$ is some other mean to compare against (which could be an alternative population or sample mean or a constant).  

For the current problem: 

* $\mu$ is the unobserved mean of the 22 participants so we will substitute $\bar{X}$, the mean of the sample.  
* $\mu_0$ is the mean of the original paper which we observed to be 590.  

So in other words we are testing the null hypothesis that $H_0: \bar{X} =$ 590.

And we will do this by calculating the test statistic $t$ which comes from the $t$ distribution - more on that distribution below and in the lectures. The formula to calculate the observed test statistic $t$ for the one-sample $t$-test is:

$t = \frac{\bar{X} - \mu_0}{s\ / \sqrt(n)}$

Now we just need to fill in the numbers.

### Task 4: Calculating the Mean and Standard Deviation {#Ch6PreClassQueT4}

* Calculate the mean and standard deviation of `valid_rt` for our 22  participants (i.e., for **all** participant data at the top of this lab). Store the mean in `ns_data_mean` and store the standard deviation in `ns_data_sd`. Make sure to store them both as single values!


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p>Replace NULL with the code that would find the mean, <code>m</code>, of <code>ns_data</code>.</p>
<p><code>ns_data_mean &lt;- summarise(NULL) %&gt;% pull(NULL)</code></p>
<p>Replace NULL with the code that would find the standard deviation, <code>sd</code>, of <code>ns_data</code>.</p>
<p><code>ns_data_sd &lt;- summarise(NULL) %&gt;% pull(NULL)</code></p>
</div>

</div>


### Task 5: Calculating the Observed Test Statistic {#Ch6PreClassQueT5}

From Task 4, you found out that $\bar{X}$, the sample mean, was 625.464 ms, and $s$, the sample standard deviation, was 94.307 ms. Now, keeping in mind that $n$ is the number of observations in the sample, and $\mu_0$ is the mean from Woods et al (2009):

* Substitute the values into the one-sample t-test formula above to compute your observed test statistic. Store the answer in `t_obs` .

**Answering this question will help you in this task as you'll also need these numbers to substitute into the formula:** 

* The mean from Woods et al (2009) was <select class='solveme' data-answer='["590"]'> <option></option> <option>595</option> <option>590</option> <option>580</option> <option>585</option></select>, and the number of participants in our sample is: (type in numbers) <input class='solveme nospaces' size='3' data-answer='["22"]'/>.


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p>Remember BODMAS and/or PEDMAS when given more than one operation to calculate. (i.e.Â Brackets/Parenthesis, Orders/Exponents, Division, Multiplication, Addition, Subtraction)</p>
<p><code>t_obs &lt;- (sample mean - woods mean) / (sample standard deviation / square root of n)</code></p>
</div>

</div>


### Task 6: Comparing the Observed Test Statistic to the t-distribution using **`pt()`** {#Ch6PreClassQueT6}

Now you need to compare `t_obs` to the t-distribution to determine how likely the observation (i.e. your test statistic) is under the null hypothesis of no difference.  To do this you need to use the `pt()` function. 

* Use the `pt()` function to get the $p$-value for a two-tailed test with $\alpha$ level set to .05.  The test has $n - 1$ degrees of freedom, where $n$ is the number of observations contributing to the sample mean $\bar{X}$.  Store the $p$ value in the variable `pval`.  Do you reject the null?


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p>Remember to get help you can enter <code>?pt</code> in the console.</p>
<p>The <code>pt()</code> function works similar to <code>pbinom()</code> and <code>pnorm()</code>:</p>
<ul>
<li><code>pval &lt;- pt(test statistic, df, lower.tail = FALSE) * 2</code></li>
<li>Use the <strong>absolute value</strong> of the test statistic; i.e.Â ignore minus signs.</li>
<li>Remember, <code>df</code> is equal to <code>n-1</code>.</li>
<li>Use <code>lower.tail = FALSE</code> because we are wanting to know the probability of obtaining a value higher than the one we got.</li>
<li>Because we want the p-value for a <strong>two-tailed</strong> test, <strong>multiply <code>pt()</code> by two</strong>.</li>
<li>Reject the null at the field standard of p &lt; .05</li>
</ul>
</div>

</div>

<br>

### Task 7: Comparing the Observed Test Statistic to the t-distribution using **`t.test()`** {#Ch6PreClassQueT7}

Now that you have done this by hand, try using the `t.test()` function to get the same result.  Take a moment to read the documentation for this function by typing `?t.test` in the console window.  No need to store the t-test output in a dataframe but do check that the p-value matches the `pval` in Task 6.


<div class='solution'><button>Helpful Hint</button>

<div class="info">
<p>The function requires a vector, not a table, as the first argument. You can use the <code>pull()</code> function to pull out the <code>valid_rt</code> column from the tibble <code>ns_data</code> with <code>pull(ns_data, valid_rt)</code>.</p>
<p>You also need to include <code>mu</code> in the <code>t.test()</code>, where <code>mu</code> is equal to the mean you are comparing to.</p>
</div>

</div>

<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>  

To make sure you are understanding the output of the t-test, try to answer the following questions.

1. To three decimal places, type in the p value for the t-test in Task 7 <input class='solveme nospaces' size='6' data-answer='["0.092",".092"]'/>

2. As such this one sample t-test is <select class='solveme' data-answer='["not significant"]'> <option></option> <option>significant</option> <option>not significant</option></select>

3. The outcome of the binomial test and the one sample t-test produce <select class='solveme' data-answer='["a different"]'> <option></option> <option>the same</option> <option>a different</option></select> answer  

### Task 8: Drawing Conclusions about the new data {#Ch6PreClassQueT8}

Given these results, what do you conclude about how similar these 22 participants are to the original participants in <a href = "https://www.sciencedirect.com/science/article/pii/S0005796708002738" target = "_blank">Woods et al (2009)</a> and whether or not you have managed to recruit sleepers similar to that study? 

* Think about which test used more of the available information? 
* Also, how reliable is the finding if the two tests give different answers?

We have given some of our thoughts at the end of the chapter.

<span style="font-size: 22px; font-weight: bold; color: var(--blue);">Job Done - Activity Complete!</span>

That's all! There is quite a bit in this lab in terms of theory of Null Hypothesis Significance Testing (NHST) so you might want to go back and add any informative points to your Portfolio. Post any questions on the forums

