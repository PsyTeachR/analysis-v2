[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"Authors: Phil McAleer, Carolina E. Kuepper-Tetzel, & Helena M. PatersonAim: course covers data skills R Markdown, data wrangling tidyverse, data visualisation ggplot2. also introduces statistical concepts probabilities, Null Hypothesis Significance Testing (NHST), alpha, power, effect size, sample size. common statistical analyses covered book t-test, correlations, ANOVAs, Regressions.Note: book currently updated means chapters published rolling basis.Contact: book living document regularly checked updated improvements. issues using book queries, please contact Carolina E. Kuepper-Tetzel.R Version: book written R version 4.1.0 (2021-05-18)Randomising Seed: chapters use level randomisation, remembered, seed set 1409.\nbook help learn whole host skills methods based around psychologist. completed Data Skills book PsyTeachR series (https://psyteachr.github.io/) first chapters familiar , additions. deliberate order refresh knowledge skills moving advanced topics. First, remind work R Markdown, recapping main functions use visualisation data wrangling. build understanding probability going using refreshed skills analyse variety different experiments. main idea book reproducible data analysis approach.book requires higher level self-directed learning first book; part learning trying things recognising need help. get stuck, google problem like see helps.working book remember learning software. teach R independent statistical knowledge content. Rather, teach data analytical skills knowledge within R. goal continuously improve data analysis skills!can !","code":""},{"path":"starting-with-r-markdown.html","id":"starting-with-r-markdown","chapter":"1 Starting with R Markdown","heading":"1 Starting with R Markdown","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"overview-1","chapter":"1 Starting with R Markdown","heading":"1.1 Overview","text":"key goal researcher carry experiment tell others . One main ways Psychologists publication journal articles. numerous ways people combine different software create journal article, recent innovation field want know creating reports articles R Markdown. like, can see example research team school recent PLOS article. link within article methods section (one - https://osf.io/eb9dq/) allows see one file creates whole manuscript. Obviously writing full journal articles just yet, use R Markdown throughout lab series assignments. also use subjects write reports, make portfolio hints, tips, study aids suggest throughout labs.Today, start showing skills using R Markdown efficiently.chapter learn:R Markdown?create R Markdown file knit .add code edit rules R Markdown file.format text.","code":""},{"path":"starting-with-r-markdown.html","id":"what-is-r-markdown","chapter":"1 Starting with R Markdown","heading":"1.1.1 What is R Markdown?","text":"R Markdown (abbreviated Rmd) great way create dynamic documents embedded chunks code. documents self-contained fully reproducible makes easy share. information R Markdown, feel free look main webpage sometime: R Markdown Webpage. key advantage R Markdown allows write code document, along regular text, knit using package knitr() create document either webpage (HTML), PDF, Word document (.docx).\nThroughout labs see little tabs give information, answers quick questions, helpful hints, solutions tasks, suggestions information want note somewhere. read find get less course progresses, might help stuck something.\n\nKnit say want turn R Markdown file either webpage, PDF, Word document. Often labs hear someone say, \"tried knitting ?\" \"happens knit ?\". simply means happens try turning file pdf webpage.\n\npractical data assignments, one check run submitting knit code html (webpage) file see can open file browser. check code correct. however confirm code runs critical issues stop code running. valuable check.\n","code":""},{"path":"starting-with-r-markdown.html","id":"advantages-of-using-r-markdown","chapter":"1 Starting with R Markdown","heading":"1.1.2 Advantages of using R Markdown","text":"output one file includes figures, text, citations. additional files needed easy keep work one place.output one file includes figures, text, citations. additional files needed easy keep work one place.R code can put directly R Markdown report, necessary keep writing (e.g., Word document) analysis (e.g., R script) separate.R code can put directly R Markdown report, necessary keep writing (e.g., Word document) analysis (e.g., R script) separate.Including R code directly lets others see analysis - good thing science! reproducible transparent, key components Open Science!Including R code directly lets others see analysis - good thing science! reproducible transparent, key components Open Science!write report plain text, non software-specific format easy share, necessary learn new coding language HTML, can create various outputs depending need.write report plain text, non software-specific format easy share, necessary learn new coding language HTML, can create various outputs depending need.","code":""},{"path":"starting-with-r-markdown.html","id":"creating-an-r-markdown-.rmd-file","chapter":"1 Starting with R Markdown","heading":"1.1.3 Creating an R Markdown (.Rmd) File","text":"chapter going create R Markdown document. Knowing :help navigate R Markdown.show create homework assignment documents.help create reports using .point unsure something remember think can get help, , google (R markdown cheat sheets internet). example, forget put words bold, simply go Google type \"rmarkdown bold\" doubt get lot useful hints. nothing wrong . Nobody expecting keep every function head; need reminders. find elements stick head better others. remember, Google friend!Quickfire QuestionsWe put questions throughout help test knowledge. type choose correct answer, dashed box change color become solid.following options, creating R Markdown document instead simply using R script? R Markdown can combine report writing analysisR Scripts run codeReproducible Science!\none answer question! R Markdown can combine report writing analysis, providing open access others examine data, create Reproducible Science. incorrect answer? R Scripts fact run R code may remember Level 1 labs. key difference R Scripts really used documentation creating reports easily - R Markdown used ensure code can added information research can reproduced others.\n","code":""},{"path":"starting-with-r-markdown.html","id":"one-last-thing-before-beginning","chapter":"1 Starting with R Markdown","heading":"1.1.4 One last thing before beginning!","text":"Remember: can always go back Data Skills Book Level 1 remind skills want already learned using R RStudio. first chapters book partly overlap learned previously, extend skills knowledge.","code":""},{"path":"starting-with-r-markdown.html","id":"r-markdown-basics","chapter":"1 Starting with R Markdown","heading":"1.2 R Markdown Basics","text":"read Overview chapter, reason behind using R, now going work making reproducible code. laptop, best install R Rstudio use. Appendix find reminder install R Rstudio.","code":""},{"path":"starting-with-r-markdown.html","id":"create-a-new-r-markdown-document","chapter":"1 Starting with R Markdown","heading":"1.2.1 Create a new R Markdown document","text":"Create new R Markdown file (.Rmd) opening Rstudio, top menu, selecting File >> New File >> R Markdown.... now see following dialog box:\nFigure 1.1: Starting R Markdown file\nClick Document left-hand panel give document Title.file call want make sure informative reader.Put name student ID Author field author. now focus making HTML output, make sure selected shown Figure 1.1 hit OK done . now .Rmd file open Rstudio.first thing see R Markdown file header section enclosed top bottom ---. Technically called yaml header, section lists title, author, date output format. layout header precise look like shown Figure 1.2, currently set output HTML.\nFigure 1.2: Rmd yaml header\ndefault file header includes info shown Figure 1.2 many options available. can learn spare time like links \n.html options{_target=\"_blank\"} \n.pdf options.\nWAIT!! spelt name wrong? change ?\n\nlong way close file start . shorter way just correct info header - just remember keep quotes. E.g. \"Si Cologe\" instead \"Untitled\"\n","code":""},{"path":"starting-with-r-markdown.html","id":"code-chunks","chapter":"1 Starting with R Markdown","heading":"1.2.2 Code Chunks","text":"Immediately header information see default setup code chunk shown Figure 1.3. time, lab series, edit information chunk. Instead, add information, text, code, chunks, chunk.\nFigure 1.3: defualt setup code chunk\nRMarkdown can type text want directly document just word document. However, want include code need include one code chunks similar Figure 1.3. Code chunks start line contains three backwards apostrophes ` (called grave accents - often top-left QWERTY keyboards), set curly brackets letter r inside:always need parts create code chunk:three back ticks ` part Rmd file says code inserted document.{r} part says specifically including R code.default setup code chunk provides basic options R Markdown file knits work. , now, best leave particular code chunk alone. Instead show use R Markdown editing code chunks come default chunk.next code chunk file look bit like :Within curly brackets, first line chunk, word cars included letter r. simply name label code chunk really called anything. example, called code chunk cars1 later chunk cars2 show first second chunk relating cars. Whilst always advisable name code chunks, need name . However, put names chunks use name twice cause script crash knit , e.g. use data data; instead maybe use personality-data participant-info whatever makes sense chunk. OK? Different names different chunks! individual.\nRemember knitting just means converting rendering file pdf, webpage, etc. Crashing means error code stopped knitting working finishing. can usually find problem line code error message see.\n\nsecond line code chunk R code written: summary(cars). case, just asking summary() inbuilt dataset cars. R lot inbuilt datasets practice ; cars one .third line closes code chunk, three backwards apostrophes. means whatever contained first third lines code run.\npeople first starting using R Markdown, common issue code working started code chunk correctly, forgotten close bottom three backticks. Remember, three backticks open, three backticks close, chunk bind .\nQuickfire QuestionsFrom following options name, label, default setup code chunk (.e. first code chunk R Markdown file)? includersetupFALSE\nlook default setup code chunk can see code chunk name setup. include=FALSE rule explain little bit.\n","code":"```{r}``````{r cars}\nsummary(cars)```"},{"path":"starting-with-r-markdown.html","id":"knitting-code","chapter":"1 Starting with R Markdown","heading":"1.2.3 Knitting Code","text":"Now good time try knitting file see code chunks . can using Knit button top RStudio screen:\nFigure 1.4: knit button. Clicking knit file.\nclick Knit ask save file .Rmd file. Call file L2Psych_Ch1_RMarkdownBasics.Rmd save folder keep information lab. working Psychology labs University Library need save location drive space full access can save files . best one campus M: drive. using device anywhere can save file work. However, good folder structure help navigate labs better.\nbeneficial create folder M: drive contain data skills work rest Level 2. Maybe something like Psychology_Level2_DataSkills_Work folders within lab, e.g Chapter1. clearer structure folders easier find use files ! important one thing keep telling LOOK BACK (politely) previously .\n\nCouple tips:\n\nAvoid spaces file names folder names. can make life really complicated bad habit start . Use underscores words filenames folder names.\n\nNever call folder \"R\". crash R potentially lead reinstall R Rstudio. Rstudio opens looks folder called R expects contain software libraries. now looking different folder name, things go wrong.\n\nsaving file, webpage appear. first thing notice lines code chunks disappeared: ```{r} closing ``` code chunk gone. Whenever knit R Markdown file lines disappear leaving code within. also notice output code also now showing webpage. next section show control showing output code, , adding rules.\nFigure 1.5: knitted summary output\n","code":""},{"path":"starting-with-r-markdown.html","id":"adding-code-chunk-rules-and-options","chapter":"1 Starting with R Markdown","heading":"1.2.4 Adding Code Chunk Rules and Options","text":"can often good idea even necessary show data outcome test report, example writing report wanted include table results. code displayed table 10,000 lines long? case might want show output show code. can including rule within first line code chunk - ```{r name, rule = option} line. already seen rule standard default chunk, include rule, number others. look now:First, look hide output show code. , use results = \"hide\" rule:\nFigure 1.6: results Rule\nAdd rule example code chunk, shown , knit file . happens? Note comma separating name chunk rule. now see code data. key thing note code still \"running\", just showing output. example, say code said x <- 2 + 2. results = \"hide\" rule, still running line code, x assigned 4, just see output.\n\nAlternatively, can hide code, show ouput using echo = FALSE rule:\nFigure 1.7: echo Rule\n\ntemplate Rmd file, rule echo set FALSE meaning show figure code. Change rule code echo set TRUE, knit file . happens?\nRemember Level 1 called libraries environment. \"echo = FALSE\" option useful commands like library() just calling package library necessarily want display final report final HTML file. Another example might wanted make plot want include code, just want show plot report.\n\nNext, say want hide code output still run code. can using include rule:\nFigure 1.8: include Rule\nChange rule example code chunk, shown , include = FALSE knit file . happens? Note code still runs. just show anything.Finally, can use eval rule specifies whether want code chunk written evaluated knit RMarkdown file. Evaluated means run carry code. , eval = FALSE rule stop code evaluated. code shown rule stopping output get evaluated eval rule FALSE.\nFigure 1.9: eval Rule\n\nmight useful cases want show code relating programmed stimuli experiment, necessarily want run part R Markdown file.\n\nprobably wee summary :\n\nTable 1.1: Rules! Rules! Rules!\ncan also mix match rules get code/output display want. takes little getting used first doubt, just ask.\ncan use RStudio's autocomplete (tab button) see different options different rules. example, type include = hit tab button keyboard. see options TRUE FALSE.\n\nAutocomplete also works lot functions quite remember spell well. gg-? gg-{tab button}... Ah yes, ggplot().\nQuickfire QuestionsYou've got large dataset thousands participants' personality happiness scores want analyse present RMarkdown.want show code running analysis show output much display. Note want code run. Type box (e.g. rule = set) set results rule ? want show code running analysis show output much display. Note want code run. Type box (e.g. rule = set) set results rule ? create plot happiness versus neuroticism scores want hide code show output. can ? echo = TRUEinclude = FALSEcode = HIDEecho = FALSEYou create plot happiness versus neuroticism scores want hide code show output. can ? echo = TRUEinclude = FALSEcode = HIDEecho = FALSE\nfirst answer results = \"hide\" want show code run code necessarily show output code.\n\nsecond question, include = FALSE technically hide code, also hides output! echo = FALSE allows still see plot hiding code want hidden. code = HIDE - simple!\n\nRemember, aim questions help memorise codes (one can !); help gain better understanding apply codes come across future.\nTrue False, writing echo = TRUE effect output code chunk echo rule : TRUEFALSE\ncode chunk rules default option. example, echo, include, eval usually default set TRUE. result, set echo rule, .e. specifically set echo = FALSE code chunk, setting echo = TRUE. specifying option give default setting option.\nTrue False, difference setting results = \"hide\" eval = FALSE hide output: TRUEFALSE\nsetting results = \"hide\", code evaluated results produced output hidden. setting eval = FALSE, code evaluated therefore results output produced. need output later part code might use results = \"hide\". need output just want show code example might use eval = FALSE.\n","code":""},{"path":"starting-with-r-markdown.html","id":"adding-inline-code","chapter":"1 Starting with R Markdown","heading":"1.2.5 Adding Inline Code","text":"alternative way add code report called using inline code. inline code use code chunk. Instead code appears inline text. Inline code can inserted using back-tick, letter r, followed space, code want include, finally another back-tick. example, writing `r 2 + 2` return answer 4 knit file instead showing code. Remember, inside code chunk, line text, e.g.:\n\"ran `r 2+2` people\".\n\nknitted becomes:\n\"ran 4 people\".inline coding really useful want calculations within text insert values text, say dataframe, make informative sentence. look complex examples later labs really useful tool writing manuscripts R Markdown comfortable get .Quickfire QuestionsYou need TwoOneThree back tick(s) insert code chunksYou need TwoOneThree back tick(s) insert code chunksWhy inline code, `{r} 6 * 8` , going show calculated answer knit file? Try editing code line Rmarkdown knitting get work. need space back tick codeInline code complete calcuationsCurly brackets around r needed code chunksWhy inline code, `{r} 6 * 8` , going show calculated answer knit file? Try editing code line Rmarkdown knitting get work. need space back tick codeInline code complete calcuationsCurly brackets around r needed code chunks\n\ncode chunks start end three back-ticks.\n\n\ncode chunks start end three back-ticks.\n\n\nInline coding use curly brackets around r.\n\n\nInline coding use curly brackets around r.\n\n\nneed inline coding back-tick, r, space, code, final back-tick.\n\n\nneed inline coding back-tick, r, space, code, final back-tick.\n","code":""},{"path":"starting-with-r-markdown.html","id":"formatting-the-r-markdown-file","chapter":"1 Starting with R Markdown","heading":"1.2.6 Formatting the R Markdown File","text":"last thing want show preclass activity format text.writing code chunks can format document lots different ways just like Word document (expensive license-based software). R Markdown cheatsheet provides lots information show couple things might want try .can make text bold including two ** (two asterisks) start end text want present bold font. example:\n\n\"ran **4 people**.\n\nknitted becomes:\n\n\"ran 4 people\".\nNow write text Rmd file put bold. Knit file check worked.also try using italics putting single * (asterisk) start end word sentence. Try now. example help.\n\n\"ran *4 people*.\n\nknitted becomes:\n\n\"ran 4 people\".\nNote: italics can difficult read many people tried avoid using book. find italics, necessary, please let us know claim reward packet minstrels. Yes, whole packet!Finally, might want add headings sub-headings file. example, maybe writing Psychology journal article want put header Introduction, Methods, Results, Discussion sections. using # (hashtag) symbol shown Figure 1.10.\nFigure 1.10: Inputting different Header levels using #s\nNow, type four main sections found Psychology journal article R Markdown file, typing one separate line. mentioned . Knit file. look like?Now add different number #'s heading, space heading hashtag (e.g. # Introduction) knit file . notice different number hashtags?Quickfire QuestionsIf * puts words italics, ** puts words bold, type box might put (technically ) word put italics bold? * puts words italics, ** puts words bold, type box might put (technically ) word put italics bold? True False: '#'s include, smaller header : TRUEFALSETrue False: '#'s include, smaller header : TRUEFALSEFrom options, common order headings found Psychology Journal : Discussion, Introduction, Methods, ResultsDiscussion, Results, Methods, IntroductionIntroduction, Methods, Results, DiscussionIntroduction, Results, Methods, DiscussionFrom options, common order headings found Psychology Journal : Discussion, Introduction, Methods, ResultsDiscussion, Results, Methods, IntroductionIntroduction, Methods, Results, DiscussionIntroduction, Results, Methods, Discussion\n* start end word puts italics (e.g. italics) ** puts bold (e.g. bold), putting three *** start end put italics bold (e.g. italics-bold).\n\ntrue #'s use, smaller heading . Word document writers use different headings well. , # gives biggest heading, gets smaller smaller every extra #.\n\nFinally, Psychology, vast majority journal articles written format : Introduction, Methods, Results, Discussion. format always hold journals ask authors use different format, depending much emphasis journal (erroneously) likes put results hypothesis methods. however teach order stated . question approach always important, , results! course know learning Registered Reports labs lectures.\n","code":""},{"path":"starting-with-r-markdown.html","id":"r-markdown-application","chapter":"1 Starting with R Markdown","heading":"1.3 R Markdown Application","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"r-markdown-and-the-experimental-design-portfolio","chapter":"1 Starting with R Markdown","heading":"1.3.1 R Markdown and The Experimental Design Portfolio","text":"going create R Markdown scratch. also start create Experimental Design Analysis Portfolio R Markdown. aim portfolio consolidate learning experimental design analysis, allowing reflect back learning progressed. add whenever think \"Oh good tip!\" \"something want remember!\". chapter way consolidate knowledge. portfolio assessed marked anyway. learning aid help develop understanding research methods analysis Psychology.Across following nine tasks, help structure format R Markdown files; can apply learn portfolio time. begin!\nThroughout book see Portfolio Points. just points suggest add portfolio. Ultimately, keep portfolio, examples kind things recommend include:\n\nKey points classic experiments\n\nmain goal, outcome, authors, year\n\n\ntop tip write short summary every paper read, including authors' names help consolidate information\n\n\nmain goal, outcome, authors, year\n\ntop tip write short summary every paper read, including authors' names help consolidate information\n\nAspects Reports' designs analyses\n\ndecisions made ; compare studies.\n\n\ndecisions made ; compare studies.\n\nGlossary points R code functions\n\ncodes find challenging understand function \n\n\ncodes might use frequently future activities\n\n\ndeveloping glossary can send us items include get involved . still development can see https://psyteachr.github.io/glossary/.\n\n\ncodes find challenging understand function \n\ncodes might use frequently future activities\n\ndeveloping glossary can send us items include get involved . still development can see https://psyteachr.github.io/glossary/.\n\nReflection Points learned week.\n","code":""},{"path":"starting-with-r-markdown.html","id":"the-ponzo-illusion-and-age","chapter":"1 Starting with R Markdown","heading":"1.3.2 The Ponzo Illusion and Age","text":"activities chapter make use open dataset.\nopen dataset made available everyone see stored internet researchers use. previous section, saw example start PLOS One article. Many journals now ask researchers make data available post somewhere accessible like Open Science Framework.\n\nInterestingly, art making data available standard classic older articles. data using today comes 1967. Sometime recent times, data started made unavailable - closed. believe data made available encourage coming years. Transparent science Open Science!\n\ndata use today paper looking Ponzo illusion Age:Leibowitz, H. W. & Judisch, J. M. (1967). Relation Age Magnitude Ponzo Illusion. American Journal Psychology, 80(1), 105-109. can accessed campus (University Glasgow) link. campus can sign read University Glasgow library student Glasgow.basics Ponzo illusion (Wikipedia page) two lines size viewed different length based surrounding information - like sleepers traintrack. See Figure 1 Leibowitz Judisch (1967) example (P106). authors showed people two vertical lines surrounded differing horizontal lines running angles behind main vertical lines. authors varied size one vertical lines (left line) asked participants judge two vertical lines bigger longer; left line (variable) right one (standard). paper also tested illusion influenced age. info, see paper. Operationalising dependent variable, Leibowitz & Judisch measured size left line considered size standard line right. data using can seen page 107, includes:Group participants assigned according age, group made 10 participants sexThe Sex GroupThe Mean Age GroupThe Mean Length left vertical line","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT1","chapter":"1 Starting with R Markdown","heading":"1.3.3 Task 1: Setting up Your R Markdown Portfolio","text":"overall goal make reproducible \"report\" summarising data Leibowitz Judisch (1967) paper. begin!Create new R Markdown document.Give title, e.g. Psychology Research Methods PortfolioEnter GUID name authorSet output HTML.\nThroughout labs see Helpful Hints. Usually solutions nearby end chapter prevent temptation.\n\nsetting Rmd file, followed steps correctly, probably see new R Markdown file header containing title, author, date output information shown previous section.\n\nsee document header, probably created R Script instead. Refer back R Markdown Basics activity try . Look list File options top menu.\n\ncan now remove parts generic R Markdown code need; anything setup code chunk can removed (see Figure 1.3). anything line 11 can removed. Leave first code chunk however - lines 8 10 - lines make R Markdown show code chunks unless otherwise specified - note echo =  TRUE.\nWrite reminder somewhere portfolio code chunk . Writing notes somewhere accessible mean can find easily.\n","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT2","chapter":"1 Starting with R Markdown","heading":"1.3.4 Task 2: Give your Report a Heading","text":"going start portfolio creating brief report Leibowitz Judisch (1967), give heading.setup code chunk, give report heading, e.g. Lab 1 - Magnitude Ponzo Illusion varies function Age.Using hashtags, give heading Header 1 size.\nRemember fewer number hashtags larger heading size.\n","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT3","chapter":"1 Starting with R Markdown","heading":"1.3.5 Task 3: Creating a Code Chunk","text":"going need data soon best bring start code.Set working directory: Session >> Set Working Directory >> Choose Directory\nOne common issues see people using Rstudio forget set working directory folder containing data file working . means try knit run code line work Rstudio know data . Remember set working directory start session, using Session >> Set Working Directory >> Choose Directory\n\nAvoid using code set working directory often work machine others therefore fully reproducible without editing script.\nDownload data lab zip file clicking link. Unzip save folder working .Create new code chunk R Markdown script, give code chunk name load_data.Copy paste code code chunk. Spend couple minutes partner reminding code . answer hint .Now, add change echo rule code chunk knit file, code included final document.Knit document now see output looks like. ask save file somewhere. Remember Boyd Orr Lab PCs best done M: drive, given available space.Important: good chance , webpage knitted, see either warnings messages. can suppress using message warning rules within code chunks well. Try now - PreClass Activities R-Markdown cheatsheet help.\nHints:\n\nStep 4 - echo can equal TRUE FALSE.\n\nRemember separate rules code chunk commas. E.g. {r, rule1 = FALSE, rule2 = TRUE}\n\ncode ?\n\nLine 1 loads tidyverse packages associated packages e.g. dplyr, readr ggplot2. used Level 1 Grassroots book - recap lot coming labs.\nLine 2 loads data using read_csv() function stores ponzo_data.\n\nImportant points note:\n\nponzo_data called anything best call something makes clear . rule spaces name. ponzo_data ponzo.data acceptable, different . ponzo data acceptable crash code.\n\nread_csv() actually readr package available loaded tidyverse library(tidyverse). always tell use read_csv() read data csv file. codes load data - one similar one read.csv(). work differently. ever use read_csv() Psychology labs unless otherwise instructed.\n\nremember <- essentially means assign . Assigning ponzo data table ponzo_data can actually can written way around - read_csv(\"PonzoAgeData.csv\") -> ponzo_data - convention usually puts way code.\n","code":"\nlibrary(\"tidyverse\")\nponzo_data <- read_csv(\"PonzoAgeData.csv\")"},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT4","chapter":"1 Starting with R Markdown","heading":"1.3.6 Task 4: Writing your Report","text":"start giving brief report information structure full report.Underneath code chunk entered, put new heading called Introduction give Header 2 size.Next, little research Ponzo Illusion write sentence two describing works tells us; include citation support research. link wikipedia page illusion top section might help.Finally, copy text box report finish text putting names two hypotheses behind illusion sentence ordered list style; .e. 1... 2..., etc. two hypotheses Framing hypothesis Perspective hypothesis.\nLists can tricky begin straightforward know key points.\n\nlist begins blank line text. start list without leaving blank line top work.\n\npoint starts asterisk (*) integer full-stop (e.g. 1.)\n\nmust space * 1. writing point.\n\npoint new line.\n\nstagger points list (.e. indent), leave 4 blank spaces (two tabs) put * etc.\nQuickfire QuestionHere couple questions try group remind using citations:writing report, cite:Papers five authors first mention? Author 1, Author 2, Author 3, Author 4, & Author 5Author 1, Author 2, Author 3, Author 4, & Author 5, YearAuthor 1 et al., YearPapers five authors second mention? Author 1, Author 2, Author 3, Author 4, & Author 5Author 1, Author 2, Author 3, Author 4, & Author 5, YearAuthor 1 et al., YearPapers seven authors first mention? Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, & Author 7Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, & Author 7, YearAuthor 1 et al., YearPapers two authors citation? (Author 1 & Author 2)(Author 1 et al., Year)Two papers one paretheses? Order chronologically according year, separated semi-colonOrder alphabetically according first author surname, separated semi-colonTwo papers author? Order chronologically according year, separated semi-colonOrder chronologically according year, separated commaOrder alphabetically adding letter year","code":"\"There are two underlying hypotheses that may explain the Ponzo Illusion. These are: ...\""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT5","chapter":"1 Starting with R Markdown","heading":"1.3.7 Task 5: Making Text Bold or Italicized","text":"Sometimes want add emphasis text.report, format line two underlying hypotheses... bold. Answering question might help remember .Quickfire QuestionBold text italicized text created similarly, create italicized text? * (text)** (text)* (text)** (text)good idea knit file point make sure codes working correctly.","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT6","chapter":"1 Starting with R Markdown","heading":"1.3.8 Task 6: Adding Links to the Data in your Methods","text":"Good practice Report include information got data .Create new heading list two hypotheses call Methods. Set Header 2 size.Methods write new heading called Data set Header 3 size.Underneath Methods heading, copy paste sentence turn citation internet link paper.\"data report obtained within original paper (Lebowitz & Judisch, 1967). \"Now knit document make sure formatting working. Titles bigger normal text list indented numbers start line.\ncan get web address following link paper shown towards beginning lab activity. Include https part.\n\nUse R Markdown cheatsheet see insert links. something square brackets [] circular brackets () next .\n","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT7","chapter":"1 Starting with R Markdown","heading":"1.3.9 Task 7: Adding an Image to your Methods","text":"certain studies, may want add image Methods section, either stimuli, materials, procedure. look R Markdown cheatsheet see adding image similar adding link, difference exclamation mark, !, beforehand. Surprising, know!now just add image illusion taken internet illustrate add images documents.sentence added Task 6, add new heading called Stimuli set Header 3 size.Stimuli heading, insert image following web address:\nRemember good methods section contain necessary information required another researcher replicate experiment exactly! normally split three sections including Participants, Materials, Procedure.\n\nmay sound obvious surprised many Methods sections give enough information replicating study. Articles tend word counts - just like assignments. Authors tended cut words can fit discussion results. Methods sections suffered result. !\n","code":"https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg"},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT8","chapter":"1 Starting with R Markdown","heading":"1.3.10 Task 8: Adding a Table to your Results","text":"Another benefit R Markdown can insert tables results directly report without format - though aesthetics want learn format tables eventually. now...Create new heading methods sentence, called Results format Header 2 size.Add new code chunk give name table, include code shown . first part code my_table <- group_by %>% summarise creates table stores my_table. second part code my_table calls table. Calls means display show sense.Add echo rule code included final document ouput table included.Now, knit document see produced. see code, just output table.","code":"\nmy_table <- group_by(ponzo_data, Sex) %>% \n  summarise(NofGroups=n(), mean_length = mean(ComparisonLength))\n\nmy_table"},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT9","chapter":"1 Starting with R Markdown","heading":"1.3.11 Task 9: Adding a Figure to your Results","text":"Nearly research reports figure want add one well.Underneath table code chunk, add new code chunk give name plot.Add code chunk set include rule code plot included final report.\nmay notice assigned data table my_table called my_table show . However, figure. just put code figure assign . ?\n\ngreat answer assign assign either, chop change throughout labs show difference tendency assign tables assign figures. Simply often creating figures show therefore assigning calling requires code. Tables hand often stored work later, makes sense assign .\n\nhard fast rule often assign figures just makes quicker . ever assign figure remember call , figure displayed!\n, knit document make sure working correctly. table now ggplot code followed nice scatterplot. Thinking Cap Point think figure answer following question.\"Based distribution data, shown Figure, ...\" age increases, people perceive shorter vertical line length standard vertical lineas age increases, people perceive longer vertical line length standard vertical lingeThere relationship Age Ponzo illusionThis figure tells nothing relationship Age Ponzo illusion\ndot represent Figure, pattern dots?\nlearn improve visualisations progress, now completed bones first report! Compare report one created see match, can found end chapter click download .Rmd file zip folder. Fix anything formatted template.\nreal-world scenario plotting R Markdown can save lot effort. Say carried experiment, made figure results using R Script, wrote report using Microsoft Word. realised forgot include two participants. fix , re-run R script, make new plot, save plot, transfer Word document. However, used R Markdown begin analysis report place, can simply update code within document new figure created exact place old one. Magic!\n\ncode uses ggplot2 package used . main package use plots, figures, visualisations, however like call . can called library , automatically called call tidyverse package. Later, revist ggplot2 detail. now, using make scatterplot (geom_point) Age (Mean_Age) Comparison Length (ComparisonLength), splitting data males females.\nJob Done - Activity Complete!Great work! now created rough layout report. section missing Discussion relate information previous research study showed. Feel free add one time; read short summary end actual paper help get thoughts together. Well done successfully creating R Markdown file!practice newly acquired skills really strengthen , complete exercise .","code":"\nggplot(ponzo_data, \n       aes(x = Mean_Age, y = ComparisonLength, color = Sex)) +\n  geom_point()"},{"path":"starting-with-r-markdown.html","id":"practice-your-skills","chapter":"1 Starting with R Markdown","heading":"1.4 Practice Your Skills","text":"brief exercise practice skills taught chapter. future assignments ask coding interpretation, exercise just want familiarise working .Rmd files.set task can practice 1) downloading assignment files, 2) renaming files, 3) editing .Rmd file, 4) saving edited .Rmd file.Download filesYou first need download file zip folder Moodle open R RStudio. exercise, can also download ZIP file .Simply follow instructions .Rmd document find ZIP file. Enjoy!","code":""},{"path":"starting-with-r-markdown.html","id":"solutions-to-questions","chapter":"1 Starting with R Markdown","heading":"1.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"starting-with-r-markdown.html","id":"task-2-give-your-report-a-heading","chapter":"1 Starting with R Markdown","heading":"1.5.1 Task 2: Give your Report a Heading","text":"used one hashtag give biggest heading size.# Lab 1 - magnitude Ponzo Illusion varies function AgeReturn Task","code":""},{"path":"starting-with-r-markdown.html","id":"task-3-creating-a-code-chunk","chapter":"1 Starting with R Markdown","heading":"1.5.2 Task 3: Creating a Code Chunk","text":"echo rule, warning rule message rule set FALSE. , start code chunk look like:Return Task","code":"```{r load_data, echo = FALSE, warning = FALSE, message = FALSE}```"},{"path":"starting-with-r-markdown.html","id":"task-4-writing-your-report","chapter":"1 Starting with R Markdown","heading":"1.5.3 Task 4: Writing your Report","text":"Task 4 setting title Header 2 style. done via two ## start line - word Introduction case forget space.## IntroductionWorth noting: basic R Scripts, # start line result turning line comment. , R Markdown, # sets header size much like Word document headerFor second part, create ordered list putting 1 followed . space first piece information. 2 . second, . Note lists work empty line list well:Return Task","code":"1. The Perspective Hypothesis\n2. The Framing Hypothesis"},{"path":"starting-with-r-markdown.html","id":"task-5-making-text-bold-or-italicized","chapter":"1 Starting with R Markdown","heading":"1.5.4 Task 5: Making Text Bold or Italicized","text":"turn text bold need put two ** start end word sentence want bold, e.g.Return Task","code":"**make me bold**"},{"path":"starting-with-r-markdown.html","id":"task-6-adding-links-to-the-data-in-your-methods","chapter":"1 Starting with R Markdown","heading":"1.5.5 Task 6: Adding Links to the Data in your Methods","text":"set header Header 2 style use ## start line.set header Header 3 style use ### start line.link created putting words want act link [] link immediately (). example:","code":"[Lebowitz and Judisch (2016)](https://www.jstor.org/stable/1420548?seq=1#page_scan_tab_contents)"},{"path":"starting-with-r-markdown.html","id":"task-7-adding-an-image-to-your-methods","chapter":"1 Starting with R Markdown","heading":"1.5.6 Task 7: Adding an Image to your Methods","text":"set header Header 3 style use ### start line.image created putting words want act name image [] link image immediately (). key thing start exclamation mark !. example:thereforeReturn Task","code":"![name](link)![The Ponzo Illusion](https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg)"},{"path":"starting-with-r-markdown.html","id":"task-8-adding-a-table-to-your-results","chapter":"1 Starting with R Markdown","heading":"1.5.7 Task 8: Adding a Table to your Results","text":"set header Header 2 style use ## start line.set header Header 2 style use ## start line.code chunk heading read follows:code chunk heading read follows:Return Task","code":"```{r table, echo = FALSE}```"},{"path":"starting-with-r-markdown.html","id":"task-9-adding-a-figure-to-your-results","chapter":"1 Starting with R Markdown","heading":"1.5.8 Task 9: Adding a Figure to your Results","text":"code chunk heading read follows:Return Task","code":"```{r plot, include = TRUE}```"},{"path":"starting-with-r-markdown.html","id":"example-of-output-after-completing-all-activities","chapter":"1 Starting with R Markdown","heading":"1.5.9 Example of output after completing all activities","text":"section shows output expected follow inclass activities correctly.Note: Headings comparison appear one size smaller knit Rmd due rendering. worry look bit bigger, headers key part. output match output knitting .Rmd document found .","code":""},{"path":"starting-with-r-markdown.html","id":"the-magnitude-of-the-ponzo-illusion-varies-as-a-function-of-age","chapter":"1 Starting with R Markdown","heading":"The Magnitude of the Ponzo Illusion Varies as a Function of Age","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"introduction","chapter":"1 Starting with R Markdown","heading":"Introduction","text":"Ponzo Illusion ...two underlying hypotheses may explain Ponzo Illusion. : Framing hypothesisThe Perspective hypothesis","code":""},{"path":"starting-with-r-markdown.html","id":"methods","chapter":"1 Starting with R Markdown","heading":"Methods","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"data","chapter":"1 Starting with R Markdown","heading":"Data","text":"data report obtained within original paper, Lebowitz Judisch (2016)","code":""},{"path":"starting-with-r-markdown.html","id":"stimuli","chapter":"1 Starting with R Markdown","heading":"Stimuli","text":"PonzoIllusion","code":""},{"path":"starting-with-r-markdown.html","id":"results","chapter":"1 Starting with R Markdown","heading":"Results","text":"\nFigure 1.11: caption. cover later!\nChapter Complete!","code":"\nggplot(ponzo_data, \n       aes(x = Mean_Age, y = ComparisonLength, color = Sex)) +\n  geom_point()"},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-a-key-skill","chapter":"2 Data-Wrangling: A Key Skill","heading":"2 Data-Wrangling: A Key Skill","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"overview-2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.1 Overview","text":"One key skills researcher's toolbox ability work data. run experiment get lots data various files. instance, uncommon experimental software create new file every participant run participant's file contain numerous columns rows data, important. able wrangle data, manipulate different layouts, extract parts need, summarise , one important skills.next chapters aimed refreshing consolidating skills working data. chapter focuses organizing data using tidyverse package. course activities, recap main functions use , use number real datasets give wide range exposure Psychology , reiterate skills apply across different datasets. skills change, just data!\nstyle programming teach, efficient format/layout data known Tidy Data, data format easily processed tidyverse package. can read type data layout paper: Tidy Data (Wickham, 2014). surprisingly good read.\n\nHowever, data work always formatted efficient way possible. happens first step put Tidy Data format. two fundamental principles defining Tidy Data:\n\nvariable must column.\n\nobservation must row.\n\nTidy Data (Wickham, 2014) adds following principle:\n\ntype observation unit forms table.\n\nGrolemund Wickham (2017) restate third principle :\n\nvalue must cell (.e. grouping two variables together, e.g. time/date one cell).\n\ncell specific row column meet; single data point tibble cell example. Grolemund Wickham (2017) book useful read free, browsing chapter Tidy Data help visualise want arrange data. Try keep principles mind whilst .\n\nquestions answer go along help build skills: use example code guide check answer solutions end chapter. Finally, remember pro-active learning, work together community, get stuck: google trying , use cheatsheets Data Skills R Book. key cheatsheet activity Data Transformation Cheatsheet dplyr.chapter recap :Data-Wrangling Wickham Six one-table verbsAdditional useful functions count, pivot_longer, joinsPiping making efficient codes\nRemember open portfolio created Chapter 1 can add useful information work tasks. Also summarising information give chapter, words, great way learn! read might help time time explain parts .\n\ninstance, remember get help R function RStudio? Console window, can call help function (e.g. ?mutate) view reference page function. example shows get help mutate() function within dplyr, use later labs.\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-basics","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2 Data Wrangling Basics","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"revisiting-the-wickham-six","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.1 Revisiting the Wickham Six","text":"main way teach data-wrangling skills using Wickham Six one-table verbs. part tidyverse package introduced first PsyTeachR book, specifically dplyr package contained within tidyverse. six verbs often referred Wickham Six \"one-table\" dplyr verbs perform actions single table data.look basics , try look back exercises Data Skills book see used verbs (functions) previously.Wickham Six :\nuse Wickham Six frequently wrangling data definitely something making notes - just names, work particular nuances spot. Perhaps recreate table add examples.\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"learning-to-wrangle","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.2 Learning to Wrangle","text":"Today going using data paper: Witt et al. (2017). chastity belt perception. main research question asks: ability perform action influence perception? instance, ability hit tennis ball influence fast perceive ball moving? phrase another way, expert tennis players perceive tennis ball moving slower novice tennis players?experiment use tennis players however, used Pong task: \"computerised game participants aim block moving balls various sizes paddles\". bit like classic retro arcade game. Participants tend estimate balls moving faster block smaller paddle opposed bigger paddle. can read paper get details wish hopefully gives enough idea help understand wrangling data. cleaned data little start . begin!Download data zip file link save somewhere access. lab, use M: drive.Download data zip file link save somewhere access. lab, use M: drive.Set working directory folder data. Session >> Set Working Directory >> Choose DirectorySet working directory folder data. Session >> Set Working Directory >> Choose DirectoryOpen new script copy paste two lines load tidyverse library session load data read_csv() function storing tibble called pong_data.Open new script copy paste two lines load tidyverse library session load data read_csv() function storing tibble called pong_data.\ninstall packages Boyd Orr labs; already just need called library().\n\nHowever, using computer previously installed tidyverse package , install first (install.packages(\"tidyverse\")).\n\nalready installed tidyverse, long time ago, might worth running updates packages may old version works differently. easiest way RStudio using menu top - Tools >> Check Package Updates. can update packages individually just run updates. Tends better just update packages many packages linked.\n\nthree common mistakes see :\n\nMake sure spelt data file name exactly shown. Spaces everything. change name .csv file, fix code instead. reason different name file someone else code reproducible. say avoid using spaces filenames create, one created another researcher already , leave work .\n\nRemember uploading data use read_csv underscore, whereas data file dot name, filename.csv.\n\nCheck datafile actually folder set working directory.\nlook pong_data see organized. Type View(pong_data) glimpse(pong_data) Console window (Capital V little g).dataset, see row (observation) represents one trial per participant 288 trials 16 participants. columns (variables) dataset follows:use data master skills Wickham Six verbs, taking verb turn looking briefly. develop skills setting new challenges based ones set. 6 verbs work briefly recap two functions finishing quick look pipes. Try everything let us know anything quite get.\nData research methods stored two-dimensional tables; called data-frames, tables, tibbles. ways storing data discover time, mainly using tibbles (like info, type vignette(\"tibble\") Console window). tibble table data columns rows information, within tibble can get different r glossary(\"data type\", display = \"types data\"), .e. r glossary(\"double\"), r glossary(\"integer\"), r glossary(\"character\").\n\nNote: Double Integer can referred Numeric data, see word time time. clarity, use Double term number decimal (e.g., 3.14) Integer term whole number (e.g., 3).\n","code":"\nlibrary(\"tidyverse\")\npong_data <- read_csv(\"PongBlueRedBack 1-16 Codebook.csv\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.3 select() Function - keep only specific columns","text":"select() function lets us pick variables within dataset want work . example, say pong_data wanted keep columns Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss, need BackgroundColor BlockNumber.can two ways:Tell function variables includeTell function variables exclude -ColumnName approach (.e., minus ColumnName)second example, -BackgroundColor means 'BackgroundColor', saying columns except BackgroundColor BlockNumber. minus sign crucial part!\nTask 1: Using select() functionEither inclusion exclusion, select columns Participant, PaddleLength, TrialNumber, BackgroundColor HitOrMiss pong_data.know select() can also used reorder columns?Use select() keep columns Participant, JudgedSpeed, BallSpeed, TrialNumber, HitOrMiss, display alphabetical order, left right.\n\nremembered include dataset pong_data? Pay attention upper/lower case letters spelling!\n\n\nremembered include dataset pong_data? Pay attention upper/lower case letters spelling!\n\n\nThink first entered column names appeared. happens change order enter column names?\n\n\nThink first entered column names appeared. happens change order enter column names?\n","code":"\nselect(pong_data, Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss)\nselect(pong_data, -BackgroundColor, -BlockNumber)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.4 arrange() Function - sort and arrange columns","text":"arrange() function sorts rows tibble according column tell sort .Arrange data one column, e.g., BallSpeed:Arrange data multiple columns, e.g., BallSpeed (fastest first) BackgroundColor:\ndesc() sort largest smallest, .e., descending order.\n\nCompare output two lines BallSpeed column.\n\ndesc() also work BackgroundColor?\nTask 2: Arranging Data arrange() functionArrange data pong_data two variables: HitOrMiss (putting hits - 1 - first), JudgedSpeed (fast judgement - 1 - first).\n","code":"\narrange(pong_data, BallSpeed)\narrange(pong_data, desc(BallSpeed), BackgroundColor)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.5 filter() Function - keep only parts of the data","text":"filter() function lets us parse subset data, meaning keep parts data.example, might want keep red BackgroundColoror keep BallSpeed 4 pixelsor keep trials match red BackgroundColor BallSpeed 4 pixels. trial red background color slower 5 pixels removed.last example can also written follows. Two arguments requirements separated comma equivalent & (ampersand - meaning \"\"):say want keep specific Participant IDs. Say want just data Participants 1, 3, 10, 14 16. write follows.%% called group membership means keep ParticipantsThe c() creates little container items called vector.finally, say wanted keep Participants except Participant 7:can read != (exclamation mark followed equals) 'equal'. Participant != \"7\" means keep Participants values Participant column 7.exclamation mark can sometimes used negate function follows .Task 3: Using filter() FunctionUse filter() extract Participants fast speed judgement, speeds 2, 4, 5, 7, missed ball. Store remaining data variable called pong_fast_miss\nthree parts filter best think individually combine .\n\n\nFilter fast speed judgements (JudgedSpeed)\n\n\nFilter fast speed judgements (JudgedSpeed)\n\n\nFilter speeds 2, 4, 5 7 (BallSpeed)\n\n\nFilter speeds 2, 4, 5 7 (BallSpeed)\n\n\nFilter Misses (HitOrMiss)\n\n\nFilter Misses (HitOrMiss)\n\nthree filters one uses output preceeding one, remember filter functions can take one argument - see example . Also, JudgedSpeed HitOrMiss Integer need == instead just =.\nCommon mistakes filter()filter function useful, used wrongly can give misleading findings. important always check data perform action. say working comparative psychology run study looking cats, dogs, horses perceive emotion. say data stored tibble animal_data column called animals tells type animal participant . Something like :Ok, imagine wanted data just cats:filter(animal_data, animals == \"cat\")Exactly! wanted cats dogs?filter(animal_data, animals == \"cat\", animals == \"dog\")Right? Wrong! actually says \"give everything cat dog\". nothing cat dog, weird - like dat cog! actually want everything either cat dog, stated :filter(animal_data, animals == \"cat\" | animals == \"dog\")vertical line | symbol , just & symbol ., always pay attention want importantly code produces.","code":"\nfilter(pong_data, BackgroundColor == \"red\")\nfilter(pong_data, BallSpeed > 4)\nfilter(pong_data, BackgroundColor == \"red\", BallSpeed > 4)\nfilter(pong_data, BackgroundColor == \"red\" & BallSpeed > 4)\nfilter(pong_data, Participant %in% c(\"1\", \"3\", \"10\", \"14\", \"16\")) \nfilter(pong_data, Participant != \"7\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.6 mutate() Function - add new columns","text":"mutate() function lets us create new variable dataset. example, add new column pong_data background color represented numbers, red represented 1, blue represented 2.look code detail:BackgroundColorNumeric name new column adding tibble.BackgroundColor name original column tibble one take information .1 2 new codings red blue, respectively.mutate() function also handy making calculations across columns data. example, say realise made mistake experiment participant numbers 1 higher every participant, .e. Participant 1 actually numbered Participant 2, etc. something like:Note: \"new column\" name old column, .e. Participant. resulting table, Participant column new values differ values original pong_data table. may seem like overwritten values, reality created copy table altered values, lost anything: original values still pong_data store (assign) action pong_data. save change basically.general, good practice overwrite pong_data new version pong_data, store altered table new tibble, e.g., pong_data_mutated, like :Task 4: Mutating variables mutate()realise another mistake trial numbers wrong. first trial (trial number 1) practice excluded experiment actually started trial 2. Tidy :Creating new tibble called pong_data_filt store data pong_data filtering trials number 1 (TrialNumber column).Now use mutate() function renumber remaining trial numbers, pong_data_filt, starting 1 instead 2. Store output new tibble called pong_data2.\nStep 1:\n\nfilter(TrialNumber equal 1).\n\nremember store output tibble called pong_data_filt\n\nStep 2:\n\nmutate(TrialNumber = TrialNumber minus 1)\n\nexclamation mark, equals\n","code":"\npong_data <- mutate(pong_data, \n                    BackgroundColorNumeric = recode(BackgroundColor, \n                                                    \"red\" = 1, \n                                                    \"blue\" = 2))\nmutate(pong_data, Participant = Participant + 1)\npong_data_mutated <- mutate(pong_data, Participant = Participant + 1)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.7 group_by() Function - group parts of data together","text":"group_by() function groups rows dataset according category specify, e.g., animals example , grouping cat data together, dog data together, horse data together.Looking data within pong_data2, say wanted eventually create means, etc., different background color conditions, start grouping trials BackgroundColor, grouping data red background data blue background data:can add numerous grouping variables depending want split data. group Hit Miss (HitOrMiss column) background color (Red Blue). gives four groups (.e, Hit Red, Miss Red, Hit Blue, Miss Blue):Note: Nothing actually appears change data, unlike functions, big operation taken place. Look output console run group_by(pong_data2, BackgroundColor). top output notice 2nd line output tells us grouping criteria many groups now exist: see line Groups: BackgroundColor [2]: grouped BackgroundColor [2] groups - one red one blue.Task 5: Grouping Data group_by()Group data BlockNumber BackgroundColor, order, enter number groups (.e., number) get result : \nprocedure different column names:\n\ngroup_by(pong_data2, HitOrMiss, BackgroundColor)\n\nnumber groups product (.e., multiplication) sum number background colors (red blue) number blocks (12).\ngroup_by() incredibly useful , data organised groups, can apply functions (filter, arrange, mutate,...) groups within data interested , instead entire dataset. instance, common second step group_by might summarise data.Good know: ungroup() functionThe ungroup() function undoes action group_by() function.grouping data together using group_by() function performing task , e.g., filter(), summarise(), can good practice ungroup data performing another function. Forgetting ungroup dataset always affect processing, sometimes can really mess things.just good reminder always check data getting function ) makes sense b) expect.","code":"\ngroup_by(pong_data2, BackgroundColor)\ngroup_by(pong_data2, HitOrMiss, BackgroundColor)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.8 summarise() Function - do some calculations on the data","text":"summarise() function lets calculate descriptive statistics data. example, say want count number hits different paddle lengths number hits background color red blue.First group data accordingly, storing pong_data2_groupThen summarise , storing answer total_hitsFinally, fun can filter just red, small paddle hits summarised data.leave us :\nTable 2.1: Summarising group_by() summarise()\nTip: name column within pong_data2_hits_red_small summarised data total_hits; called creating pong_data2_hits. called anything wanted, always try use something sensible. Make sure call variables something (anyone looking code) understand recognize later (.e., variable1, variable2, variable3. etc.), avoid spaces (use_underscores_never_spaces).summarise() range internal functions make life really easy, e.g., mean(), median(), n(), sum(), max, min, etc. common ones shown , see dplyr cheatsheets examples.Task 6: Summarising Data summarise()Use lines code calculate mean number hits made small paddle (50) red color background. Enter value box two decimal places (e.g., 0.12): Quickfire QuestionsWhich Wickham Six use sort columns smallest largest: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use sort columns smallest largest: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use calculate mean column: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use calculate mean column: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use remove certain observations - e.g. remove males: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use remove certain observations - e.g. remove males: selectfiltermutatearrangegroup_bysummarise","code":"\npong_data2_group <- group_by(pong_data2, BackgroundColor, PaddleLength)\npong_data2_hits <- summarise(pong_data2_group, total_hits = sum(HitOrMiss))\npong_data2_hits_red_small <- filter(pong_data2_hits, BackgroundColor == \"red\", PaddleLength == 50)## `summarise()` has grouped output by 'BackgroundColor'. You can override using the `.groups` argument.\n## `summarise()` has grouped output by 'BackgroundColor'. You can override using the `.groups` argument."},{"path":"data-wrangling-a-key-skill.html","id":"other-useful-functions-bind_rows-and-count","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.9 Other Useful Functions: bind_rows() and count()","text":"Wickham Six verbs let lot things data however thousands functions disposal. want something data sure using functions, Google search alternative function - chances someone else problem help guide.Two useful functions bind_rows() function count() function. briefly show .Binding columns bind_rows()bind_rows() function useful want combine two tibbles together one larger tibble column structure, .e. exactly columns want combine attaching one bottom :Say tibble data ball speeds 1 2:another tibble data ball speeds 6 7:Now combine tibbles together one big tibble containing extreme ball speeds:Count count() functionThe count() function shortcut can sometimes used count number rows groups data, without use group_by() summarise() functions. tally basically. sum values. just counts many observations .example, Task 6 combined group_by() summarise() calculate many hits based background color paddle length. Alternatively, done:results , just count() version get information, including misses, just counting rows. summarise() method got hits effect summed. two different methods give similar answers.","code":"\nslow_ball<- filter(pong_data2, BallSpeed < 3) \nfast_ball <- filter(pong_data2, BallSpeed >= 6) \nextreme_balls <- bind_rows(slow_ball, fast_ball) \ncount(pong_data2, BackgroundColor, PaddleLength, HitOrMiss)"},{"path":"data-wrangling-a-key-skill.html","id":"pipes---make-your-code-efficient","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.10 Pipes (%>%) - make your code efficient","text":"now noticed thattidyverse functions generally take following grammatical structure (called syntax): function_name(dataset, arg1, arg2,..., argN) dataset entire tibble data using argument (arg) operation particular column variable, column name want work . example:examples follow structure function_name(dataset, arg1, arg2, ....)first example, filtering (function) whole pong_data2 dataset particular paddle length, particular speeds (arguments). second, grouping BallSpeed Participant. Note order arguments specific performs argument1 argument2, etc. Changing order arguments may give different output. order work important, called pipeline. example, pipeline used find many hits small paddle length red background.First group data accordingly, storing pong_data2_groupThen summarise , storing answer total_hitsAnd finally filter just red, small paddle hitsPipelines allow us quickly reproducibly perform action take much longer manually. However, can make code even efficient, using less code, stringing sequence functions together using 'pipes', written %>%. Changing code one using pipes give us:chunks show exactly procedure, adding pipes can make code easier read understand piping.Compare code without pipe:function_name(dataset, arg1, arg2,...,argN)o code pipe:dataset %>% function_name(arg1, arg2,...,argN)premise can pipe (%>%) functions input function output previous function. Alternatively, can use pipe put data first function, shown directly .can think pipe (%>%) saying '' 'goes ', e.g., data goes function function function.One last point pipes can written single line code much easier see pipe function takes line. Every time add function pipeline, remember add %>% first note using separate lines function, %>% must appear end line start next line. Compare two examples . first work, second second puts pipes end line need !Example 1: work pipes (%>%) wrong place.Example 2: work pipes (%>%) correct place.\npiping becomes useful string series functions together, rather using separate steps save data time new tibble name getting confused. non-piped version create new tibble time, example, data, data_filtered, data_arranged, data_grouped, data_summarised just get final one actually want, data_summarised. creates lot tibbles environment can make everything unclear eventually slow computer. piped version however uses one tibble name, saving space environment, clear easy read. pipes, skip unnecessary steps avoid cluttering environment.\nQuickfire QuestionsWhat line code say? data %>% filter() %>% group_by() %>% summarise(): take data group filter summarise ittake data filter group summarise ittake data summarise filter group ittake data group summarise filter ","code":"\nfilter(pong_data2, PaddleLength == \"50\", BallSpeed > 4)\ngroup_by(pong_data2, BallSpeed, Participant)\npong_data2_group <- group_by(pong_data, BackgroundColor, PaddleLength)\npong_data2_hits <- summarise(pong_data2_group, total_hits = sum(HitOrMiss))\npong_data2_hits_red_small <- filter(pong_data2_hits, BackgroundColor == \"red\", PaddleLength == 50)\npong_data_hits_red_small <- pong_data2 %>% \n  group_by(BackgroundColor, PaddleLength) %>% \n  summarise(total_hits = sum(HitOrMiss)) %>%\n  filter(BackgroundColor == \"red\", PaddleLength == 50)data_arrange <- pong_data2 \n                %>% filter(PaddleLength == \"50\")\n                %>% arrange(BallSpeed) \ndata_arrange <- pong_data2 %>%\n                filter(PaddleLength == \"50\") %>%\n                arrange(BallSpeed) "},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-application","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3 Data Wrangling Application","text":"looked series functions known Wickham six one-table filter, arrange, group_by, select, mutate summarise. Now focus working data across two tables using functions come across Data Skills book. two main functions add Wickham six pivot_longer() inner_join().pivot_longer() allows us transform table wide format long format.\nuse Tidy Data really efficient works well tidyverse. However, people used use data structured long format wide format.\n\nLong format row single observation, typically single trial experiment response single item questionnaire. multiple trials per participant, multiple rows participant. identify participants, need variable kind participant id, can simple distinct integer value participant. addition participant identifier, measurements taken observation (e.g., response time) experimental condition observation taken .\n\nwide format data, row corresponds single participant, multiple observations participant spread across columns. instance, survey data, separate column survey question.\n\nTidy mix approaches functions tidyverse assume tidy format, typically first thing need get data, particularly wide-format data, reshape wrangling. teach really important skills.\ninner_join() allows us combine two tables together based common columns.Analysing Autism Spectrum Quotient (AQ)continue building data wrangling skills recap skills Data Skills book tidying data Autism Spectrum Quotient (AQ) questionnaire. completed Data Skills book may familiar AQ10; non-diagnostic short form AQ 10 questions per participant. discrete scale higher participant scores AQ10 autistic-like traits said display. Anyone scoring 7 recommended diagnosis. can see example AQ10 link: AQ10 Example.four data files work :responses.csv containing AQ survey responses 10 questions 66 participantsqformats.csv containing information question coded, .e. forward reverse codedscoring.csv containing information many points specific response get; depending whether forward reverse codedpinfo.csv containing participant information Age, Sex importantly ID number.Click download files zip file. Now unzip files folder access .\ncsv stands 'comma separated values' basic format storing data plain text file. really just stores numbers text separated commas nothing else. great thing basic can read many different systems non-proprietary, .e., need purchase commercial software open .\nNow set working directory folder saved .csv files. dropdown menus top toolbar: Session >> Set Working Directory >> Choose Directory find folder .csv files.Today work RScript instead .Rmd, want turn R Markdown report add elements Portfolio please feel free.Thinking Cap PointNow good time make sure using RStudio effectively know window .TRUE FALSE, Console best practice Script window saving: TRUEFALSETRUE FALSE, Environment holds data objects loaded created: TRUEFALSETRUE FALSE, clicking name table Environment window open Script window: TRUEFALSE\nanswer True.\n\n\nScript window write code comments going save send people. Console practice stuff - nothing saved ; like sandbox just gets wiped away.\n\n\nScript window write code comments going save send people. Console practice stuff - nothing saved ; like sandbox just gets wiped away.\n\n\ndata load create held Environment (Global Environment) window variable name gave .\n\n\ndata load create held Environment (Global Environment) window variable name gave .\n\n\nclicking name table Environment window open Script window can look make sure expect. works tables types data. learn difference go along!\n\n\nclicking name table Environment window open Script window can look make sure expect. works tables types data. learn difference go along!\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-1-open-a-script","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.1 Task 1: Open a Script","text":"Start new RScript save folder .csv files, calling RScript something informative like AQ_DataWrangling.R.Make sure environment completely empty mix one analysis . can run following code line console clear environment clicking little brush environment window.\nRemember using script can write notes remind line code . Just put hashtag start line R ignore line. clear using Script versus R Markdown file. Script, # means line ignored, Markdown # sets line header!.\n\nrun line script, simplest way click anywhere line either press Run top script window press CTRL+Enter keyboard (mac equivalent).\n","code":"\nrm(list = ls()) "},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.2 Task 2: Bring in Your Library","text":"Add line code brings tidyverse package working environment run .\nCombine function library() package tidyverse remember solutions end chapter.\n\nlab machines Psychology necessary packages already machines, just need called library. however using machine install packages first ().\n\ninstall packages Psychology machines! ?\n\nalready installed can cause package stop working student tries install package machines.\n\nalready installed bit like using apps phone. Install putting app onto phone, library just opening app. already downloaded app (package) just need open (library()) use !\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.3 Task 3: Load in the Data","text":"Now load .csv datafiles using read_csv() function save tibbles environment. example, load data responses.csv save tibble responses type:Add following lines code script complete load four .csv datafiles. Use code example name tibble original filename (minus .csv part), , e.g. responses.csv gets saved responses. Remember run lines data loaded stored environment.\nwork data functions find functions similar names, give different results. One read function csv. Make sure always use read_csv() function load csv files. Nothing else. part readr package automatically brought tidyverse.\n\nsimilarly named function called read.csv(). use function. always expect use read_csv(). Although similar name work way create differences data.\n","code":"\nresponses <- read_csv(\"responses.csv\") responses <-  read_csv()    # survey responses\nqformats <-                 # question formats\nscoring <-                  # scoring info\npinfo <-                    # participant information"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.4 Task 4: Review Your Data.","text":"Now data loaded always best look data get idea layout. showed one way , clicking name environment, can also use glimpse() View() functions Console window. Put name data brackets see arranged. add script though - just one-offs testing.look data responses see think Tidy answer following question: data responses TidyLongWide format\nreponses tibble far tidy; row represents multiple observations participant, .e., row shows responses multiple questions - wide format.\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.5 Task 5: Gathering Data with pivot_longer().","text":"order make easier us get AQ score participant, need change layout responses tibble wide format long format using pivot_longer() function.Copy code line script run .look code detail:first argument given pivot_longer() function tibble holds data want wrangle, responses.\nRemember written pipe well, e.g. rlong <- responses %>% pivot_longer(...)\nRemember written pipe well, e.g. rlong <- responses %>% pivot_longer(...)second argument names specific columns original tibble want gather together, Q1:Q10 meaning columns Q1 Q10.\nactually need write cols = makes things clearer.\n\"Gathering\" columns based position tibble. order columns tibble Q1 Q10, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q10, therefore code gathers columns Q1 Q10.\nColumn names put quotes exist already tibble responses.\nactually need write cols = makes things clearer.\"Gathering\" columns based position tibble. order columns tibble Q1 Q10, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q10, therefore code gathers columns Q1 Q10.Column names put quotes exist already tibble responses.third fourth arguments names new columns creating;\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".\nsecond store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".\nnew column names put quotes already exist tibble. always case, case function.\nNote names anything using names code makes sense.\nLastly, need write names_to = ... values_to = ... otherwise columns created correctly.\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".second store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".new column names put quotes already exist tibble. always case, case function.Note names anything using names code makes sense.Lastly, need write names_to = ... values_to = ... otherwise columns created correctly.case wondering, wanted go back way ungather data just gathered, use pivot_wider() function: e.g. rwide <- rlong %>% pivot_wider(names_from = Question, values_from = Response). want add code.Quickfire QuestionsLet's see understand pivot_longer(). Say wanted gather first three columns responses (Q1, Q2, Q3), put question numbers column called Jam, responses column called Strawberry, store everything tibble called sandwich. Fill box write: \nsandwich <- pivot_longer(responses, cols = Q1:Q3, names_to = \"Jam\", values_to = \"Strawberry\")\n\npivot_longer() wants data first, columns gather, name new column store gathered column names , finally name new column store values .\n","code":"\nrlong <- pivot_longer(responses,\n                      cols = Q1:Q10,\n                      names_to = \"Question\",\n                      values_to = \"Response\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.6 Task 6: Combining Data.","text":"now responses data tidy format, closer getting AQ score person. However, still need add information :show question reverse forward scored - found qformatsshow number points give specific response - found scoring.typical analysis situation different information different tables need join together. pieces information contained qformats scoring, respectively, want join data rlong create one informative tidy table info.can sort join function inner_join(); function combine information two tibbles using column (columns) common tibbles.Copy line code run . piece code combines rows tibble rlong rows tibble qformats, based common column \"Question\".Now look rlong2. matched question scoring format, forward reverse.\nlot questionnaires questions Forward scored questions Reverse scored. mean? Imagine situation options replying question : 1 - extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely disagree. forward-scoring question get 1 point extremely agree, 2 agree, 3 neutral, etc. reverse scoring question, get 5 extremely agree, 4 agree, 3 neutral, etc.\n\nreasoning behind shift sometimes agreeing disagreeing might favourable depending question worded. Secondly, sometimes questions used just catch people - imagine two similar questions one reverse meaning . scenario, people respond opposites. respond might paying attention.\nNow need combine information table, rlong2, scoring table know many points attribute question based answer participant gave, whether question forward reverse coded. , use inner_join() function, time common columns found rlong2 scoring QFormat Response. combine two columns just write sequence shown . Note: one common column two tibbles joining, combine columns avoid repeat columns new tibble. forget , new tibble names column_name.x column_name.y. cause confusion avoid combining common columns.Copy line code run . code combine rows rlong2 scoring based columns, QFormat Response.","code":"\nrlong2 <- inner_join(rlong, qformats, \"Question\")\nrscores <- inner_join(rlong2, scoring, c(\"QFormat\", \"Response\"))"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT7","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.7 Task 7: Calculating the AQ Scores.","text":"now created rscores information participant responded question question coded scored, within one tibble. need now sum scores participant get AQ score.Based learning previous section, copy line code complete obtain individual aq_scores participant.Save script run start make sure works!\nparticipant grouped Id.\n\nsummed value Score might get full AQ Score particpipant.\n\nYep, well spotted. Pipes friend. Think saying '' 'goes '. example take rscores group something summarise AQ scores based ...\n\ncases, pipe serves purpose putting input function taking output one function treating input another function.\n\nexample first pipe takes rscores input group_by, second pipe takes output group_by puts input summarise. See can almost read chain actions steps.\nQuickfire QuestionsThe whole purpose chapter calculate AQ scores individual participants. Try answer following questions. Try using code possible help based knowledge chapter. Remember cheatsheets well. Look dplyr one!options, choose correct citation AQ 10 question questionnaire: Allison, Auyeung, Baron-Cohen, (2011)Allison, Auyeung, Baron-Cohen, (2012)Allison Baron-Cohen, (2012)Auyeung, Allison, Baron-Cohen, (2012)options, choose correct citation AQ 10 question questionnaire: Allison, Auyeung, Baron-Cohen, (2011)Allison, Auyeung, Baron-Cohen, (2012)Allison Baron-Cohen, (2012)Auyeung, Allison, Baron-Cohen, (2012)Complete sentence, higher AQ score...less autistic-like traits displayedhas relation autistic-like traitsthe autistic-like traits displayedComplete sentence, higher AQ score...less autistic-like traits displayedhas relation autistic-like traitsthe autistic-like traits displayedType AQ score (just number) Participant ID . 87: Type AQ score (just number) Participant ID . 87: Type many participants AQ score 3 (just number): Type many participants AQ score 3 (just number): cut-AQ10 usually said around 6 meaning anyone score 6 referred diagnostic assessment. Type many participants refer sample: cut-AQ10 usually said around 6 meaning anyone score 6 referred diagnostic assessment. Type many participants refer sample: \n\nlink can see appropriate citation AQ10 (Allison, Auyeung, Baron-Cohen, (2012))\n\n\nlink can see appropriate citation AQ10 (Allison, Auyeung, Baron-Cohen, (2012))\n\n\nmentioned, higher score AQ10 autistic-like traits participant said show.\n\n\nmentioned, higher score AQ10 autistic-like traits participant said show.\n\n\ncode filter(aq_scores, Id == 87), give tibble 1x2 showing ID number score. just wanted score use pull() shown yet works follows: filter(aq_scores, Id == 87) %>% pull(AQ). answer AQ score 2.\n\n\ncode filter(aq_scores, Id == 87), give tibble 1x2 showing ID number score. just wanted score use pull() shown yet works follows: filter(aq_scores, Id == 87) %>% pull(AQ). answer AQ score 2.\n\n\nchanging argument filter. filter(aq_scores, AQ == 3) %>% count(). answer 13. Remember can counting code makes reproducible every time.\n\n\nchanging argument filter. filter(aq_scores, AQ == 3) %>% count(). answer 13. Remember can counting code makes reproducible every time.\n\n\nfilter(aq_scores, AQ > 6) %>% count() filter(aq_scores, AQ >= 7) %>% count(). answer 6.\n\n\nfilter(aq_scores, AQ > 6) %>% count() filter(aq_scores, AQ >= 7) %>% count(). answer 6.\n","code":"\naq_scores <- rscores %>% \n             group_by() %>% # how will you group individual participants?\n             summarise(AQ = sum()) # which column will you sum to obtain AQ scores?"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT8","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.8 Task 8: Practice using pipes","text":"now complete code load data, convert Tidy, combine tables calculate AQ score participant. , look , code efficient using pipes.Go back code rewrite using pipes %>% efficient possible.\npoint first argument function name variable created line, good chance used pipe! bits code piped together one chain:\n\nrlong <- pivot_longer(responses, cols = Q1:Q10, names_to = \"Question\", values_to = \"Response\")\n\nrlong2 <- inner_join(rlong, qformats, \"Question\")\n\nrscores <- inner_join(rlong2, scoring, c(\"QFormat\", \"Response\"))\n\naq_scores <- rscores %>% group_by(Id) %>% summarise(AQ = sum(Score))\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"practice-your-skills-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4 Practice Your Skills","text":"order complete tasks need download data .csv files .Rmd file, need edit, titled Ch2_PracticeSkills_Template.Rmd. can downloaded within zip file link. downloaded unzipped, create new folder use working directory; put data files .Rmd file folder set working directory folder drop-menus top. Download Exercises .zip file .Now open .Rmd file within RStudio. see code chunk task. Follow instructions edit code chunk. often entering code based covered point.chapter recapped data-wrangling using Wickham 6 verbs, looked additional functions pivot_longer() inner_join(), piping chains code efficiency using %>%. need skills complete following exercises, make sure worked chapter attempting exercise. Two useful online resources :Hadley Wickham's R Data Science book @ http://r4ds..co.nz RStudio's dplyr cheatsheet @ Rstudio.comPsyTeachR Data Skills book","code":""},{"path":"data-wrangling-a-key-skill.html","id":"the-ageing-brain","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.1 The Ageing Brain","text":"key topic current psychological research, one forms main focus research School, human ageing. research use brain imaging techniques understand changes brain function structure relate changes perception behaviour. typical 'ageing' experiment compare measure (number measures) performance cognitive perceptual task younger older adults (.e., -subjects design experiment).However, order make sure studying 'healthy' ageing, first 'screen' older participants symptoms age-related dementia (Alzheimer's Disease), cognitive function can significantly impaired. using range cognitive tests. studies also test participants' sensory acuity (ability perceive something), function age (particularly eyesight hearing).data downloaded exercise example screening data taken research investigating ageing brain processes different types sounds. tests used study detailed . Please note links provide information examples tests completed assignment wish; read complete exercises.Montreal Cognitive Assessment (MoCA) : test specifically devised stand-alone screening tool mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, abstraction skills. Example hereMontreal Cognitive Assessment (MoCA) : test specifically devised stand-alone screening tool mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, abstraction skills. Example hereWorking Memory Digit Span Test (D-SPAN): measures capacity participants' short-term (working) memory.Working Memory Digit Span Test (D-SPAN): measures capacity participants' short-term (working) memory.D2 Test Attention: measures participants' selective sustained concentration visual scanning speed.D2 Test Attention: measures participants' selective sustained concentration visual scanning speed.Better Hearing Institute Quick Hearing Check: self-report questionnaire measures participants' subjective experience hearing abilities.Better Hearing Institute Quick Hearing Check: self-report questionnaire measures participants' subjective experience hearing abilities.Data FilesYou just downloaded three .csv files containing data need. list .csv file names description variables contains:p_screen.csv contains particpants demographic information including:\nID Participant Id number - confidentiality (names identifying info)\nAGE years\nSEX M male, F female\nHANDEDNESS L left-handed, R right-handed\nEDUCATION years\nMUSICAL whether musical abilties/experience (YES )\nFLANG speak foreign languages (YES )\nMOCA Montreal Cognitive Assessment score\nD-SPAN Working Memory Digit Span test score\nD2 D2 Test Attention score\nID Participant Id number - confidentiality (names identifying info)AGE yearsSEX M male, F femaleHANDEDNESS L left-handed, R right-handedEDUCATION yearsMUSICAL whether musical abilties/experience (YES )FLANG speak foreign languages (YES )MOCA Montreal Cognitive Assessment scoreD-SPAN Working Memory Digit Span test scoreD2 D2 Test Attention scoreQHC_responses.csv contains participants' responses question \"Better Hearing Institute Quick Hearing Check (QHC)\" questionnaire.\nColumn 1 represents participants' ID (matching p_screen.csv).\ncolumn thereafter represents 15 questions questionnaire.\nrow represents participant response question.\nColumn 1 represents participants' ID (matching p_screen.csv).column thereafter represents 15 questions questionnaire.row represents participant response question.QHC_scoring.csv contains scoring key question QHC, columns:\nRESPONSE types responses participants give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE)\nSCORE points awarded response type (0 4). score participant can calculated converting categorical responses values summing values.\nRESPONSE types responses participants give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE)SCORE points awarded response type (0 4). score participant can calculated converting categorical responses values summing values.starting lets check:.csv files saved folder computer manually set folder working directory..csv files saved folder computer manually set folder working directory..Rmd file saved folder .csv files..Rmd file saved folder .csv files.","code":""},{"path":"data-wrangling-a-key-skill.html","id":"load-in-the-data","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.2 Load in the data","text":"see code chunk called libraries, similar one , top .Rmd assignment file. set-load data call tidyverse library(). Run code chunk now bring data tidyverse. can console, script, even code chunk clicking small green play symbol top right code chunk.View dataIt always good idea familiarise layout data just loaded . can using glimpse() View() Console window, must never put functions assignment file.Tasks:Now data loaded, tidyverse attached, viewed data, now try complete following 9 tasks. may want practice first get correct code format, make sure work. can console script, remember, correct code, edit necessary parts assignment .Rmd file produce reproducible .Rmd file. now assessment files practicing now really help. short, go tasks change NULL question asks make sure file knits end fully reproducible code.","code":"\nlibrary(\"tidyverse\")\n\nscreening <- read_csv(\"p_screen.csv\")\nresponses <- read_csv(\"QHC_responses.csv\")\nscoring <- read_csv(\"QHC_scoring.csv\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.3 Task 1 - Oldest Participant","text":"Replace NULL T1 code chunk Participant ID oldest participant. Store single value oldest_participant (e.g. oldest_participant <- 999).hint: look data, oldest?","code":"\noldest_participant <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.4 Task 2 - Arranging D-SPAN","text":"Replace NULL T2 code chunk code arranges participants' D-SPAN performance highest lowest using appropriate one-table dplyr (.e., Wickham) verb. Store output cogtest_sort. (e.g. cogtest_sort <- verb(data, argument))hint: arrange screening data","code":"\ncogtest_sort <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.5 Task 3 - Foreign Language Speakers","text":"Replace NULL two lines code chunk T3, descriptives column called n shows number participants speak foreign language number participants speak foreign language, another column called median_age shows median age two groups. done correctly, descriptives 3 columns 2 rows data, including header row.hint: First need group_by() foreign languagehint: Second need summarise(). need n() function. Pay attention specific column names given.","code":"\nscreen_groups <- NULL\ndescriptives <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.6 Task 4 - Creating Percentage MOCA scores","text":"Replace NULL T4 code chunk code using one dplyr verbs add new column called MOCA_Perc dataframe screening new column MOCA scores converted percentages. maximum achievable score MOCA 30 percentages calculated (participant score / max score) * 100. Store output screening.hint: mutate() something using MOCA percentage formula","code":"\nscreening <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.7 Task 5 - Remove the MOCA column","text":"Now MoCA score expressed percentage MOCA_Perc longer need raw scores held MOCA. Replace NULL T5 code chunk using one-table dplyr verb keep columns screening, order, without MOCA column. Store output screening.hint: select columnsThe remaining tasks focus merging two tables.suspect older adults musical experience might report finely-tuned hearing abilities without musical experience. therefore decide check whether trend exists data. measured participants' self reported hearing abilities using Better Hearing Institute Quick Hearing Check Questionnaire. questionnaire, participants rated extent agree disagree list statements (e.g., 'problem hearing telephone') using 5 point Likert scale (Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree).participant's response question contained responses dataframe environment. response type worth certain number points (e.g., Strongly Disagree = 0, Strongly Agree = 5) scoring key contained scoring dataframe. score participant calculated totaling number points across questions derive overall score. lower overall score, better participants' self-reported hearing ability.order score questionnaire first need perform couple steps.","code":"\nscreening <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.8 Task 6 - Gather the Responses together","text":"Replace NULL T6 code chunk using code gather responses questions QHC wide format tidy/long format. Put names Question values RESPONSE. Store output responses_long.hint: pivot_longer()hint: names \"Question\"hint: values \"RESPONSE\"","code":"\nresponses_long <- NULL "},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT7","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.9 Task 7 - Joining the data","text":"Now need join number points response scoring participants' responses responses_long.Replace NULL T7 code chunk using inner_join() combine responses_long scoring new variable called responses_points.hint: join column common scoring responses_long","code":"\nresponses_points <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT8","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.10 Task 8 - Working the Pipes","text":"given five lines code takes data current long format creates QHC score participant (group_by()...summarise()). joins screening information (inner_join()) calculating mean QHC score two groups participants - play musical instruments . final step stored tibble called musical_means.Use five lines code replace NULL T8 code chunk functioning code pipeline using pipes. Put function new line one . pipeline result mean QHC values musical non-musical people stored tibble musical_means. final tibble consist two rows two columns (.e. four cells total).hint: pipes, output previous function input subsequent function.hint: function1(...) %>% function2(...)","code":"participant_groups <- group_by(responses_points, ID)\nparticipant_scores <- summarise(participant_groups, Total_QHC = sum(SCORE))\nparticipant_screening <- inner_join(participant_scores, screening, \"ID\")\nscreening_groups_new <- group_by(participant_screening, MUSICAL)\nmusical_means <- summarise(screening_groups_new, mean_score = mean(Total_QHC))\nmusical_means <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT9","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.11 Task 9 - Difference in Musical Means","text":"Finally, replace NULL T9 code chunk value much higher QHC score people play music compared people play music. single numeric value, two decimal places, e.g. 2.93hint: look musical means enter difference two means.Well done, finished! Now go check answers solutions end chapter. looking check answers submitted exactly ones solution - example, remember Mycolumn different mycolumn one correct.","code":"\nQHC_diff <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"solutions-to-questions-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-basics-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1 Data Wrangling Basics","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.1 Task 1","text":"Using select() include stated columns:Using select() exclude certain columns:Using select() change order columns:Return Task","code":"\nselect(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss)\nselect(pong_data, -JudgedSpeed, -BallSpeed, -BlockNumber)\nselect(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber)"},{"path":"data-wrangling-a-key-skill.html","id":"task-2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.2 Task 2","text":"Return Task","code":"\narrange(pong_data, desc(HitOrMiss), desc(JudgedSpeed))"},{"path":"data-wrangling-a-key-skill.html","id":"task-3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.3 Task 3","text":"Return Task","code":"\nfilter(pong_data, \n       JudgedSpeed == 1, \n       BallSpeed %in% c(\"2\", \"4\", \"5\", \"7\"), \n       HitOrMiss == 0)"},{"path":"data-wrangling-a-key-skill.html","id":"task-4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.4 Task 4","text":"first step created filter()second step created mutate()Return Task","code":"\npong_data_filt <- filter(pong_data, TrialNumber >= 2) \npong_data2 <- mutate(pong_data_filt, TrialNumber = TrialNumber - 1)"},{"path":"data-wrangling-a-key-skill.html","id":"task-5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.5 Task 5","text":"Return Task","code":"\ngroup_by(pong_data2, BlockNumber, BackgroundColor)"},{"path":"data-wrangling-a-key-skill.html","id":"task-6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.6 Task 6","text":"find number hits made small paddle (50) red color background 0.450655Return Task","code":"\npong_data2_group <- group_by(pong_data2, BackgroundColor, PaddleLength)\npong_data2_hits <- summarise(pong_data2_group, mean_hits = mean(HitOrMiss))## `summarise()` has grouped output by 'BackgroundColor'. You can override using the `.groups` argument."},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-application-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2 Data Wrangling Application","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-2-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.1 Task 2","text":"orNote, difference library(tidyverse) library(\"tidyverse\") work.Return Task","code":"\nlibrary(tidyverse)\nlibrary(\"tidyverse\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-3-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.2 Task 3","text":"Note difference responses <- read_csv(\"responses.csv\") responses <- read_csv(responses.csv). need quotes around .csv filename shown code chunk (e.g. responses <- read_csv(\"responses.csv\")), code work.Return Task","code":"\nresponses <- read_csv(\"responses.csv\")                  \nqformats <- read_csv(\"qformats.csv\")                 \nscoring <- read_csv(\"scoring.csv\")                  \npinfo <- read_csv(\"pinfo.csv\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-7","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.3 Task 7","text":"Return Task","code":"\naq_scores <- rscores %>% \n             group_by(Id) %>% # group by the ID number in column Id\n             summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores."},{"path":"data-wrangling-a-key-skill.html","id":"task-8","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.4 Task 8","text":"Return Task","code":"\naq_scores2 <- responses %>% \n  pivot_longer(cols = Q1:Q10,\n               names_to = \"Question\",\n               values_to = \"Response\") %>%\n  inner_join(qformats, \"Question\") %>%\n  inner_join(scoring, c(\"QFormat\", \"Response\")) %>%\n             group_by(Id) %>% \n             summarise(AQ = sum(Score))"},{"path":"data-wrangling-a-key-skill.html","id":"practice-your-skills-2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3 Practice Your Skills","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-1---oldest-participant","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.1 Task 1 - Oldest Participant","text":"Whether coded answer just read data, Participant ID Number 3 oldest.also answered code. quite shown yet look like :Return Task","code":"\noldest_participant <- 3\noldest_participant_code <- arrange(screening, desc(AGE)) %>% \n  slice(1) %>% \n  pull(ID)"},{"path":"data-wrangling-a-key-skill.html","id":"task-2---arranging-d-span","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.2 Task 2 - Arranging D-SPAN","text":"arrange() main function hereYou also needed use desc() sort high lowReturn Task","code":"\ncogtest_sort <- arrange(screening, desc(DSPAN))"},{"path":"data-wrangling-a-key-skill.html","id":"task-3---foreign-language-speakers","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.3 Task 3 - Foreign Language Speakers","text":"First group screening data FLANG using group_by()Next, summarise, paying attention use variable names instructedn() function use within summarise() count many observations . works like count() use count() within summarise()median() function use within summarise() calculate median. Much like sum() mean() sd(), etc.Return Task","code":"\nscreen_groups <- group_by(screening, FLANG) \ndescriptives <- summarise(screen_groups, \n                          n = n(), \n                          median_age = median(AGE))"},{"path":"data-wrangling-a-key-skill.html","id":"task-4---creating-percentage-moca-scores","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.4 Task 4 - Creating Percentage MOCA scores","text":"mutate() function add new column dataHere mutating/adding column called MOCA_Perc shows participant's MOCA score divided 30 multiplied 100.Return Task","code":"\nscreening <- mutate(screening, MOCA_Perc = (MOCA / 30) * 100)"},{"path":"data-wrangling-a-key-skill.html","id":"task-5---remove-the-moca-column","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.5 Task 5 - Remove the MOCA column","text":"select() key function keep remove certain columns.Two options ; give dataframe.first option shows deselect column keep everything else.second option shows select columns want.Remember order important select columns order want.Option 1:Option 2:Return Task","code":"\nscreening <- select(screening, -MOCA)\nscreening <- select(screening, ID, AGE, SEX, HANDEDNESS, EDUCATION, MUSICAL, FLANG, DSPAN, D2, MOCA_Perc)"},{"path":"data-wrangling-a-key-skill.html","id":"task-6---gather-the-responses-together","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.6 Task 6 - Gather the Responses together","text":"pivot_longer() function use .People take understand function spend time looking example start make sense.first argument data. case responses.second argument name columns want gather. gathering columns Q1 column Q15 column. Remember colon (:) says \"... ...\"\nactually need write cols = makes things clearer.\n\"Gathering\" columns based position tibble. order columns tibble Q1 Q15, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q15, therefore code gathers columns Q1 Q15.\nColum names put quotes exist already tibble responses.\nactually need write cols = makes things clearer.\"Gathering\" columns based position tibble. order columns tibble Q1 Q15, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q15, therefore code gathers columns Q1 Q15.Colum names put quotes exist already tibble responses.third fourth arguments names new columns creating;\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".\nsecond store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".\nnew column names put quotes already exist tibble. always case case function.\nNote names anything using names code makes sense.\nLastly, need write names_to = ... values_to = ... otherwise columns created correctly.\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".second store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".new column names put quotes already exist tibble. always case case function.Note names anything using names code makes sense.Lastly, need write names_to = ... values_to = ... otherwise columns created correctly.Return Task","code":"\nresponses_long <- pivot_longer(responses, \n                         cols = Q1:Q15, \n                         names_to = \"Question\", \n                         values_to = \"RESPONSE\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-7---joining-the-data","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.7 Task 7 - Joining the data","text":"inner_join() combine common information two sets data common column columns.joining data responses_long data scoring common column RESPONSE.Keep mind inner_join() keeps rows data datasets. remove rows data one dataset.joining two datasets, join common columns one column common.Return Task","code":"\nresponses_points <- inner_join(responses_long, scoring, \"RESPONSE\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-8---working-the-pipes","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.8 Task 8 - Working the Pipes","text":"code started .transcribe series functions pipeline.Remember, using pipes, output previous function input subsequent functionReturn Task","code":"\nparticipant_groups <- group_by(responses_points, ID)\nparticipant_scores <- summarise(participant_groups, Total_QHC = sum(SCORE))\nparticipant_screening <- inner_join(participant_scores, screening, \"ID\")\nscreening_groups_new <- group_by(participant_screening, MUSICAL)\nmusical_means <- summarise(screening_groups_new, mean_score = mean(Total_QHC))\nmusical_means <- group_by(responses_points, ID) %>%\n                  summarise(Total_QHC = sum(SCORE)) %>%\n                  inner_join(screening, \"ID\") %>%\n                  group_by(MUSICAL) %>%\n                  summarise(mean_score = mean(Total_QHC))"},{"path":"data-wrangling-a-key-skill.html","id":"task-9---difference-in-musical-means","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.9 Task 9 - Difference in Musical Means","text":"People play music QHC score 1.53 units higher people play music.can looking musical_means, reading values, quick maths.second option code. Code always better can reduce error reproducible!Return Task","code":"\n# Option 1\nQHC_diff <- 1.53\n\n# Option 2\n# You will soon learn the functions to do this by code but here is how you could do it.\nQHC_diff_code <- pivot_wider(musical_means, \n                             names_from = \"MUSICAL\", \n                             values_from = \"mean_score\") %>% \n  mutate(diff = YES - NO) %>% \n  pull(diff) %>% \n  round(2)"},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-through-ggplot2","chapter":"3 Data Visualisation Through ggplot2","heading":"3 Data Visualisation Through ggplot2","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"overview-3","chapter":"3 Data Visualisation Through ggplot2","heading":"3.1 Overview","text":"Data visualisation important understanding data. key part data analysis exploring data, checking assumptions, displaying results. Figures plots allow get insight patterns data. example, regards seeing differences groups, also seeing things quite match think happening. great example Anscombe's Quartet, can read later date like - see - four datasets given exact means, different underlying structures visualised. key point always good visualise data visualisation common step data skills set.PsyTeachR Data Skills book introduced data visualisation using ggplot2, main visualisation package tidyverse. look back working chapter, can find additional info main page package: ggplot2.visualisaion use ggplot2 listed great online resources might want consult want fuller understanding:R Graphics Cookbookggplot2 bookggplot2 cheatsheetggplot2 Reference GuideIn chapter, revisit plotting data expand skills order make effective informative figures. become really beneficial progress data visualisation skill applies multiple careers, just Psychology.chapter :Recap data visualisationExpand skills produce new figuresLearn Mental Rotation research","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-basics","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2 Data Visualisation Basics","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"introducing-the-data-set-mental-rotation-ability","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.1 Introducing the Data Set: Mental Rotation Ability","text":"data use today comes replication classic experiment merging fields Perception Cognition. Shepard Metzler (1971) demonstrated participants shown two similar three-dimensional shapes, one just rotated version (see figure - top panel), asked whether shape , reaction time error rates responses function rotation; .e. larger difference rotation two shapes, longer took participants say \"\" \"different\", errors made.\nFigure 3.1: Mental Rotation Task shown Ganis Kievit (2016) Figure 1\nimage shown Figure 3.1 actually comes replication Ganis Kievit (2016). top panel two shapes shape right rotated vertically 150 degrees original (left shape) participants respond \"\". bottom panel, however, two shapes different; one right rotated 150 degrees, trial takes longer participants realise different shapes.can read Ganis Kievit (2016) time, basic methods ran 54 participants series images using four angles rotation (0, 50, 100, 150 degrees) asked people respond '' 'different' trial. data can downloaded . use data follow along try answer questions.Look dataDownload data folder, unzip , save folder access .Set working directory folder Session >> Set Working Directory >> Choose DirectoryOpen new Rscript .RMarkdown file save within folder contains data, giving script sensible name, e.g. Chapter3_visualisations.R. prefer work RMarkdown, just remember need embed code code chunks, shown Chapter 1.Copy three code lines script run bring tidyverse library read two data files.Note, difference library(tidyverse) library(\"tidyverse\") work.However, difference demog <- read_csv(\"demographics.csv\") demog <- read_csv(demographics.csv). need quotes around .csv file name shown code chunk (e.g. demog <- read_csv(\"demographics.csv\")), code work.\nreally great question always seem saying use dplyr readr ggplot, never actually call . Remember, however, tidyverse actually collection packages, common packages fact, use bring common packages (including ggplot2) probably need packages along codes run smoothly. try tell need call packages alongside tidyverse, keep mind codes least start tidyverse package.\n\nSmall point, looking help ggplot, package actually called ggplot2. newer version package, search ggplot2 need help.\n\nstart look data brought . can whichever way choose; mentioned three ways previous chapter.First, demog - short demographics. three columns:Participant - ID participantAge - age participantSex - sex participantSecondly, menrot - short mental rotation. 8 columns:Participant - ID participant; matches ID demogTrial - trial number experiment participantCondition - name image shown; R indicates rotated image differentTime - reaction time respond trial millisecondsDesiredResponse - participants responded trial; Different SameActualResponse - participants respond trial; Different SameAngle - angle shape right rotated compared shape left (0, 50, 100, 150)CorrectResponse - whether participant correct incorrect given trial\nGanis Kievit (2016) short paper really introduce stimuli set rather give extensive background topic mental rotation - call 'methods paper'. said, writing paper clear procedure well detailed ran actual experiment.\n\nwriting procedure, remember give much information needed allow someone exactly replicate study. read procedure time think information , also information , help develop writing reports. example, fingers participants use respond important?\nnow data want create plots visualise . show code create four types plots get practice , remember PsyTeachR Data Skills book. go plots, edit/change code give see differences can control changes can create plots. Editing altering code works see happens change something great way working.","code":"\nlibrary(\"tidyverse\")\n\nmenrot <- read_csv(\"MentalRotationBehavioralData.csv\")\ndemog <- read_csv(\"demographics.csv\")"},{"path":"data-visualisation-through-ggplot2.html","id":"basic-structure-of-ggplot","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.2 Basic Structure of ggplot()","text":"two main things know working ggplot :usual ggplot format :ggplot(data, aes(x = x_axis, y = y_axis)) + geom_type_of_plot()first thing enter dataframe/tibble; data. , within aes() say x_axis y_axis, using column names within tibble. aes stands aesthetics maps data visual features. Finally, tell code type plot want.ggplot works concept layersLayers common way graphics work. Think ggplot() function creating first layer every function adding layers top create figure want. first layer always data axis/axes, .e. `ggplot(....). second layer, added using plus symbol '+', type plot. look adding layers progress.ggplot() powerful package used whole range industries, including newspapers mainstream media outlets, can make quite sophisticated images. One beauties data skills just transferable across many fields.","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"scatterplots---geom_point","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.3 Scatterplots - geom_point()","text":"Scatterplots great way visualising continuous data - data can take value scale measured. example, current dataset, can use scatterplots explore potential relationship two continuous variables Age Reaction Time: variables increase/decrease rate (.e., positive relationship)? one variable increase decrease (.e., negative relationship)? maybe overall relationship?data, say want test overall average time respond mental rotation task related age participant highlighting sex participants. show relationship scatterplot using code , :Wrangles data create average response time participant, Mean_Time, joins information demographic data, Participant. stored tibble menrot_time_age.plots scatterplot (geom_point()) age plotted x-axis, Mean_Time y-axisFinally, uses additional aes call color Sex color point based whether male female participant responding. default coloring call two options. Later look controlling using colors .\nFigure 3.2: scatterplot Mean Time function Age\nQuickfire QuestionsLooking scatterplot Figure 3.2, can say relationship age overall response time? age increases, overall response time increasesas age increases, overall response time decreasesthere overall relationshipLooking scatterplot Figure 3.2, can say relationship age overall response time? age increases, overall response time increasesas age increases, overall response time decreasesthere overall relationshipLooking scatterplot, can say difference male female participants? males show increase overall response time age femalesfemales show increase overall response time age malesthere real difference males females terms overall response time ageLooking scatterplot, can say difference male female participants? males show increase overall response time age femalesfemales show increase overall response time age malesthere real difference males females terms overall response time age\nlook figure, appear age increases (x-axis) overall resposne time (y-axis)? age decreases overall response time? maybe even age increases, overall response time decreases? Well, actually, looking figure appears relationship two variables case one either increases decreases . relationship appears flat.\n\ncomparing sex, based color dots, appears major differences relationship looks flat sex.\n\nLater book look correlational analysis - method quantifying relationship two variables.\nNote: often case visualise data first wrangle format. using functions saw Chapter 2, make sure tasks understood wrangle verbs pipes work. Keep mind functions use format, function(data, argument)","code":"\nmenrot_time_age <- group_by(menrot, Participant) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  inner_join(demog, \"Participant\")\n\nggplot(data = menrot_time_age, \n       aes(x = Age, \n           y = Mean_Time, \n           color = Sex)) +\n  geom_point()"},{"path":"data-visualisation-through-ggplot2.html","id":"histograms---geom_histogram","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.4 Histograms - geom_histogram()","text":"Histograms great way showing overall distribution data. data look normally distributed? data skewed - positive skew negative skew? peaky? flat? terms become familiar learn statistics, try think terms concepts visualising looking data.Looking data, say wanted test overall distribution mean response times correct trials normally distributed. visualise question following code, :Wrangles data create average response time participant, Mean_Time, filters information correct trials . stored tibble menrot_hist_correct.Plots histogram (geom_histogram()) Mean_Time plotted x-axis, count value Mean_Time plotted y-axis. code creates y-axis automatically state :\nFigure 3.3: histogram distribution Mean Time counts\nQuickfire QuestionsLooking histogram Figure 3.3, can say overall shape distribution? data looks reasonably normally distributedthe data looks positively skewedthe data looks negatively skewedLooking histogram Figure 3.3, can say overall shape distribution? data looks reasonably normally distributedthe data looks positively skewedthe data looks negatively skewedLooking histogram, common average overall response time correct trials? approximately 2000 millisecondsapproximately 2500 millisecondsapproximately 3000 millisecondsLooking histogram, common average overall response time correct trials? approximately 2000 millisecondsapproximately 2500 millisecondsapproximately 3000 milliseconds\nKeep mind real data never give beautiful textbook shape see classic diagrams looking normally distributed data skewed data. decisions regarding distributions often require degree judgement.\n\nPositive skewed data means data shifted left (low numbers) tail stretching right (high numbers). Negative skew data shifted right (high numbers) tail stretching left (low numbers). Normally distributed data data middle even tails either side. Although perfect, data shown histogram reasonable representation normally distributed data real world; particularly small sample participants.\n\ny-axis count values x-axis, common overall response time can found reading highest column data. distribution, looks around 2500 milliseconds 2.5 seconds.\n","code":"\nmenrot_hist_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  filter(CorrectResponse == \"Correct\")\n\nggplot(data = menrot_hist_correct, \n       aes(x = Mean_Time)) + \n  geom_histogram()"},{"path":"data-visualisation-through-ggplot2.html","id":"boxplots---geom_boxplot","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.5 Boxplots - geom_boxplot()","text":"Boxplots great means visualising spread data highlighting outliers data. looking boxplots, consider:whether median (thick horizontal black line) middle box higher lower middle box?whether box evenly distributed around median ?box whiskers (vertical tails top bottom box) similar length sides box?outliers - usually highlighted star dot beyond whiskers?look compare distributions mean reaction times correct incorrect responses. can done using code, :Repeats first two wrangle steps created scatterplot, additionally groups CorrectResponse, stores data tibble menrot_box_correctPlots boxplot (geom_boxplot()) overall average response times y-axis, Mean_Time, based condition, CorrectResponse, x-axisUses additional aes call fill colour boxplots, two categories, based whether CorrectResponse correct incorrect. default look editing later.Turns legend using guides() call needed x-axis tells group . later though.Run code first. , run code fill = TRUE instead. difference? Notice fill name call ggplot(...) function. linked.\nFigure 3.4: boxplot spreads Mean Time Correct Incorrect Responses\nQuickfire QuestionsLooking boxplots, many outliers ? 1230Looking boxplots, many outliers ? 1230Looking boxplots Figure 3.4, condition longer median overall average response time mental rotation task? Median response time longer Correct responsesMedian response time longer Incorrect responsesBoth medians approximatelyLooking boxplots Figure 3.4, condition longer median overall average response time mental rotation task? Median response time longer Correct responsesMedian response time longer Incorrect responsesBoth medians approximately\nnumber ways determining outliers. Two methods standard deviations (usually 2.5 3 SD used cut-offs) boxplots, outlier determined \\(1.5*IQR\\) (inter-quartile range) top bottom box. Outliers shown dots whiskers boxplot. can see figure outliers see data.\n\nmedian one five values required make boxplot shown horizontal thick black line within box . Looking two conditions comparing position median y-axis (response time) can see median response time incorrect trials higher correct trials. suggest people take longer make mind give decision trials get wrong. Makes sense think ; uncertainty takes longer leads errors.\n","code":"\nmenrot_box_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  inner_join(demog, \"Participant\")\n\nggplot(data = menrot_box_correct, \n       aes(x = CorrectResponse, \n           y = Mean_Time, \n           fill = CorrectResponse)) + \n  geom_boxplot() +\n  guides(fill = FALSE)## `summarise()` has grouped output by 'Participant'. You can override using the `.groups` argument.## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead."},{"path":"data-visualisation-through-ggplot2.html","id":"barplots---geom_bar-or-geom_col","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.6 Barplots - geom_bar() or geom_col()","text":"Barplots typically show specific values condition. Sometimes really simple like count variable mean, e.g. many people replied yes. Others show something bit complex average spread values via error bars, e.g. standard error. looking barplots, main considerations whether appears difference conditions interested whether conditions .worth knowing barplots now used less frequently actually show lot information, discussed blog, One simple step improving statistical inference. However, still see used great able create interpret .Using geom_bar()Using data, say interested whether difference average percentage correct incorrect responses across male female participants. visualise using following code, :Wrangles data series steps establish overall percent average correct incorrect responses sex, stored menrot_resp_sex.Plots barplot (geom_bar()) condition Sex x-axis, Avg_Percent y-axis, created wrangle, fill bars based CorrectResponse.Finally, within geom_bar says treat data final values average , stat = \"identity\", makes columns visible moving apart position = position_dodge(.9)) - without last step bars overlap see everything. Try changing .9.Note: participant 96 trials study.\nFigure 3.5: barplot average percent Correct Incorrect responses Female Male participants - using geom_bar()\nUsing geom_col()geom_col() - short column - alternative geom_bar() require part code say anything data, .e. stat=\"identity\". shown . Notice difference codes, produce figure.\nFigure 3.6: barplot average percent Correct Incorrect responses Female Male participants - using geom_col()\nQuickfire QuestionsLooking barplot data, average, sex correct responses? femalemaleboth samecan't tellLooking barplot data, average, sex correct responses? femalemaleboth samecan't tellLooking barplot data, average, sex incorrect responses? femalemaleboth samecan't tellLooking barplot data, average, sex incorrect responses? femalemaleboth samecan't tellLooking code, happens decrease position.dodge() value? bars get apartthe bars start overlapnothing changes figureLooking code, happens decrease position.dodge() value? bars get apartthe bars start overlapnothing changes figureLooking code, happens change aes call fill color? bars stay different colorthe bars become grey outlines become different colorsnothing changes figureLooking code, happens change aes call fill color? bars stay different colorthe bars become grey outlines become different colorsnothing changes figure\nRemember barplots plotting mean, top column average value condition. actually people like barplots; though commonly used, really show one value data, average, disregard information unless indication spread given.\n\nmind, comparing two Correct columns can see females average correct responses males. Incorrect columns can see males incorrect responses females. actually makes sense response option experiment either correct incorrect, add correct incorrect percentage responses one sex together get 100%. females gave correct reponses must given less incorrect responses.\n\nlast two questions playing code. Remember said plots work concept layers. set position.dodge() 0, find one columns disappears completely overlap now. need set position.dodge() reasonable value columns separate. set 1? barplots, often find different levels (categories) variable touching. Note, however, value dodge, case 1, relative size x-axis - scale x-axis ran 0 100 dodge 1 little effect. Sometimes need little trial error. Always look output code.\n\nfinal point shows can add lot calls just x y axis change presentation figures. fill changes color columns, color changes outline color columns. see progress look difference putting inside aes() outside . play figures see happens. worth pointing though, turned legend using guides(fill = FALSE), works used fill = ... call change colours. used color = ... call change colours need use guides(colors = FALSE) turn legend. See linked? guide matches called.\n","code":"\ntotal_n_trials <- 96\n\nmenrot_resp_sex <- count(menrot, Participant, CorrectResponse) %>% \n  inner_join(demog, \"Participant\") %>%\n  mutate(PercentPerParticipant = (n/total_n_trials)*100) %>%\n  group_by(Sex, CorrectResponse) %>%\n  summarise(Avg_Percent = mean(PercentPerParticipant))\n\nggplot(data = menrot_resp_sex, \n       aes(x = Sex, \n           y = Avg_Percent, \n           fill = CorrectResponse)) + \n  geom_bar(stat = \"identity\", \n           position = position_dodge(.9))## `summarise()` has grouped output by 'Sex'. You can override using the `.groups` argument.\nggplot(data = menrot_resp_sex, \n       aes(x = Sex, \n           y = Avg_Percent, \n           fill = CorrectResponse)) +\n  geom_col(position = position_dodge(.9))"},{"path":"data-visualisation-through-ggplot2.html","id":"themes-labels-guides-and-facet_wraps","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.7 Themes, Labels, Guides, and facet_wraps()","text":"couple layers can add ggplot calls make figures look professional. show code , want run teach work changing code, removing parts within ggplot, adding figures shown .themes - changing overall presentation figure. Try running code comparing figure barplot . Remember, ?theme_bw() give information look cheatsheets different themes theme_light(), theme_classic(), theme_gray() theme_dark().\ntheme_gray() actually default equivalent stating theme function.\n_ theme_classic() close basic APA figure presentation.\nthemes - changing overall presentation figure. Try running code comparing figure barplot . Remember, ?theme_bw() give information look cheatsheets different themes theme_light(), theme_classic(), theme_gray() theme_dark().theme_gray() actually default equivalent stating theme function.\n_ theme_classic() close basic APA figure presentation.labs - putting appropriate labels figures readers understand displayed. Try changing text within quotes.labs - putting appropriate labels figures readers understand displayed. Try changing text within quotes.facet_wraps - splitting data separate figures clarity. work one conditions categorical, can really effective means displaying information.facet_wraps - splitting data separate figures clarity. work one conditions categorical, can really effective means displaying information.guides - remove see happens. understand use fill situation, perhaps others?guides - remove see happens. understand use fill situation, perhaps others?Try running editing code:","code":"\ntotal_n_trials <- 96\n\nmenrot_better_plot <- count(menrot, Participant, CorrectResponse) %>% \n  inner_join(demog, \"Participant\") %>%\n  mutate(PercentPerParticipant = n/total_n_trials) %>%\n  group_by(Sex, CorrectResponse) %>%\n  summarise(Avg_Percent = mean(PercentPerParticipant))\n\nggplot(data = menrot_better_plot, \n       aes(x = Sex,\n           y = Avg_Percent, \n           fill = CorrectResponse)) + \n  geom_col(position = position_dodge(.9)) +\n  labs(x = \"Sex of Participant\", \n       y = \"Percent Average (%)\") +\n  guides(fill = FALSE) +\n  facet_wrap(~CorrectResponse) +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"choosing-appropriate-figures","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.8 Choosing Appropriate Figures","text":"progress Psychology, come across variety different figures plots, looking slightly different giving different information. looking figures, indeed choosing one analyses, think figure appropriate data. example, scatterplots great variables continuous; boxplots histograms great viewing spreads data; barplots commonly used one variable categorical - note barplots can misleading lots new approaches display categorical information created. Violin-boxplots taught Chapter 7 Data Skills book provide complete way data visualisation. Always keep asking , plot display data correctly. Also, right number dots/conditions/groups figure? many suggest something quite right. Look data!","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-application","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3 Data Visualisation Application","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"mental-rotation-angle-and-reaction-time","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.1 Mental Rotation: Angle and Reaction Time","text":"continue using data set Ganis Kievit (2016). understanding experiment well develop skills visualisation interpretation, look mean reaction time correct trials, function angle rotation sex. idea rotated test image original image, longer take participants determine image completely different images. Fifty-four participants responded '' 'different' trial series rotated images using four angles rotation (0, 50, 100, 150 degrees rotated compared original). Link data.\ncome across phrase 'function ' quite bit dealing visualisations. means something like 'compared ' 'across'. say going look Mean Reaction Time across four different Angles Rotation written Mean Reaction Time function Angle Rotation. Usually, plotted y-axis function x-axis. similar idea functions use codes want see happens put y function x. good become familiar terms language used reports ) understand reading b) can use language writing.\n","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.2 Task 1: Loading and Viewing the Data","text":"Download data, unzip , save folder access .Set working directory folder data .Start new script .Rmd file (RMarkdown), save folder data .script, load tidyverse library.Load datasets exactly , storing experimental data menrot demographic data demog.\nlibrary()\n\nmenrot <- read_csv()\n\ndemog <- read_csv()\n\nRemember always looking data good practice make sure expect . Use either View() glimpse(), console window RStudio, never script .Rmd file. useful functions can use check data structure :str() - shows type data . Look words like table, dataframe, character, integer, double.head(), tail(), names() - show top six bottom six rows column names, just column names names().dim() - shows dimensions data.Refer previous section list columns refer sure. keep mind reproducible means careful spelling punctuation names functions, tibbles, columns, conditions, etc, times - e.g. Juggler juggler.Quickfire QuestionsTake couple minutes try functions answer following questions.options, type data variable Angle, found dataframe menrot? characterintegerdouble/numericalFrom options, type data variable Angle, found dataframe menrot? characterintegerdouble/numericalType box name dataframe contains information regarding sex participants: Type box name dataframe contains information regarding sex participants: options, column within menrot? CorrectresponseCorretcResponseCorrectResponsecorrectReponseFrom options, column within menrot? CorrectresponseCorretcResponseCorrectResponsecorrectReponseFrom options, according dim() call, many rows demog? 38545184From options, according dim() call, many rows demog? 38545184\nmaking sure loading data correctly, using instructed names - reproducible - understand data looking . completed Task 1 successfully, loading data correct dataframes, following answers work questions.\n\n\ncalling str(menrot) looking information comes see data column Angle loaded double/numerical. Technically integers (whole numbers decimal places), default load make numerical.\n\n\ncalling str(menrot) looking information comes see data column Angle loaded double/numerical. Technically integers (whole numbers decimal places), default load make numerical.\n\n\ndemog loaded information regarding demographics including sex participant.\n\n\ndemog loaded information regarding demographics including sex participant.\n\n\nnames(menrot) give column names. question making sure correct spelling: CorrectResponse. spellings work data spelling column names specific!\n\n\nnames(menrot) give column names. question making sure correct spelling: CorrectResponse. spellings work data spelling column names specific!\n\n\ndim(demog) shows number rows (54) number columns (3).\n\n\ndim(demog) shows number rows (54) number columns (3).\n","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.3 Task 2: Recreating the Figure","text":"start making representation top part Figure 2 Ganis Kievit (2016) - Mean Reaction Time function Angle Rotation.Copy lines code script.Copy lines code script.Replace NULLs order recreate figure similar Ganis Kievit (2016) Figure 2 (top).\nNote figure shows information correct responses just Ganis Kievit (2016).\nRemember ggplot case layers. first layer says data want axis. Every subsequent layer says want data displayed - points (geom_point()) connecting line (geom_line()).\nrunning tasks, come back one see can figure coord_cartesian() changing numbers ylim = () call. comment solutions.\nReplace NULLs order recreate figure similar Ganis Kievit (2016) Figure 2 (top).Note figure shows information correct responses just Ganis Kievit (2016).Remember ggplot case layers. first layer says data want axis. Every subsequent layer says want data displayed - points (geom_point()) connecting line (geom_line()).running tasks, come back one see can figure coord_cartesian() changing numbers ylim = () call. comment solutions.\nfirst four lines, create menrot_angle, functions Wickham Six verbs refer back see work.\n\n\nanswer CorrectResponse allow keep just correct answers?\n\n\nanswer CorrectResponse allow keep just correct answers?\n\n\ncommon variable allow join information demog?\n\n\ncommon variable allow join information demog?\n\n\nreally care four levels rotation, variable/column group_by?\n\n\nreally care four levels rotation, variable/column group_by?\n\n\nvariable/column want mean mean response time?\n\n\nvariable/column want mean mean response time?\n\nggplot line, think format, data, x-axis name, y-axis name.\n\nFigure 3.7: Basic Scatterplot Response Time Angle Rotation\nThinking Cap PointGreat, replicated figure! However, know means? figure tell mean reaction time angle rotation, fit overall theory introduced ? Answering question may help:options, figure suggest angle rotation increases: mean reaction time decreasesmean reaction time stays samemean reaction time increases\ncan see figure, consistent Shepard Metzler (1971), participants Ganis Kievit (2016) showed increase reaction time angle rotation increased. Therefore, Ganis Kievit (2016) replicated findings Shepard Metzler (1971).\n\nquick note though , yes, mean reaction time increase angle rotation, consistent increase. see difference mean reaction times 150 100 degrees smaller 0 50. Reaction times start plateau certain angle rotation.\n","code":"\nmenrot_angle <- filter(menrot, CorrectResponse == NULL) %>%\n  inner_join(demog, NULL) %>%\n  group_by(NULL) %>% \n  summarise(mean_Resp = mean(NULL))\n\nggplot(data = NULL, aes(x = NULL, y = NULL)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT3","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.4 Task 3: Examining Additional Variable Effects","text":"previous section, looked sex participant quite interesting. covered Ganis Kievit (2016), let us take look .pipeline Task 2, add variable Sex group_by() function group data Angle Sex.Running code creates figure.\nRemember, add grouping variables just separate comma. Everything else code stays .\n\nFigure 3.8: Separating points Sex\nHmmm, figure look informative. looks similar one created dots doubled - now 8 instead 4 - know male female, connecting line confusing. need add little code tell separate data based Sex.","code":"## `summarise()` has grouped output by 'Angle'. You can override using the `.groups` argument."},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT4","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.5 Task 4: Grouping the Figure Data","text":"previous section used fill color inside aes() change basic information figure. also one called group. Add group call inside aes(...) separate data Sex.Run code see figure looks like\nggplot(....., aes(x = , y = , group = ???))\nnow least see different lines two sex, still tell sex line, can ? just looks like two black parallel lines, one slightly higher . ideal changing color points based whether male female participants! Fortunately, geoms can also take information well.","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT5","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.6 Task 5: Identifying Groups Using aes()","text":"Add aes() call inside geom_point() function color dots Sex.\ngeom_point(aes(??? = ???)\n\nnow something like :\n\nFigure 3.9: Separate lines Sex\nGreat! can now see female line top male line bottom. start interpreting figure finish tidying tasks. example dots data point perhaps little small see, increase size. Also, color great can print color, also change shape dots help people distinguish Sex displaying color option. use additional calls shape size within geom_point().","code":"## `summarise()` has grouped output by 'Angle'. You can override using the `.groups` argument."},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT6","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.7 Task 6: Changing the Shape and Size of Data Points","text":"want Sex different shaped points, add call shape within aes() call geom_point() function, just like color.want Sex different shaped points, add call shape within aes() call geom_point() function, just like color.However, want Sex size point, add call size within geom_point() function, inside aes() call. Set appropriate number size instead naming variable. Maybe size = 3?However, want Sex size point, add call size within geom_point() function, inside aes() call. Set appropriate number size instead naming variable. Maybe size = 3?\ngeom_point(aes(color = Sex, Shape = ???), size = ???)\ngive figure like :\nFigure 3.10: Changing Shape Size Data Points\nquick point - , , aes ?Hopefully beginning spot difference setting call within aes() (stands aesthetics) setting outside aes(). Outside aes() means observations take one value color type. Inside means observation within condition takes value color type, different conditions different values/color/type.look done help us compare:size called outside aes() assign specific value. can see Task 6 figure, condition now size points. set size want, keep smallish: 3 5 ok; 50 artistic, informative. set size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) male female different shapes different sizes. can play around code see things work.size called outside aes() assign specific value. can see Task 6 figure, condition now size points. set size want, keep smallish: 3 5 ok; 50 artistic, informative. set size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) male female different shapes different sizes. can play around code see things work.contrast, called shape inside aes() set based variable, Sex. way ensures level Sex variable, male female, get different shape. instead set shape outside aes(), something like geom_point(shape = 3, size = 3) conditions shape size. Different numbers relate different shapes different sizes. example compare shape = 3 shape = 13In contrast, called shape inside aes() set based variable, Sex. way ensures level Sex variable, male female, get different shape. instead set shape outside aes(), something like geom_point(shape = 3, size = 3) conditions shape size. Different numbers relate different shapes different sizes. example compare shape = 3 shape = 13There arguments use: example, wanted points color, say red example, geom_point(color = \"red\"). Remember put quotes around color.hopefully starting make sense can think implementing figures. Note arguments separated comma. e.g. geom_point(color = \"red\", size = 3, shape = 2).","code":"## `summarise()` has grouped output by 'Angle'. You can override using the `.groups` argument."},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT7","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.8 Task 7: Adding Labels and Changing the Background","text":"figure looking really nice now. finish adding appropriate labels editing background. introduced previous section hopefully play see work.change label, use labs() function works like labs(x = \"Name\", y = \"Name\", title = \"Name\").Add function code y-axis indicates Mean Reaction Time (ms) x-axis indicates Angle Rotation (degrees). set title , can want practice. Titles Psychology figures common.Add function code y-axis indicates Mean Reaction Time (ms) x-axis indicates Angle Rotation (degrees). set title , can want practice. Titles Psychology figures common.Set figure theme_bw() - looks nice, options might want try can explore ?theme cheatsheet.Set figure theme_bw() - looks nice, options might want try can explore ?theme cheatsheet.\nlabs(x = \"...\", y = \"...\") + theme_bw()\n\nkey thing remember + layer ggplot chain. get confused pipes (%>%).\n\nNote: add (+) layers figures, pipe (%>%) functions.\n","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT8","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.9 Task 8: Separating a Variable and Removing Legends","text":"Finally, showed two functions use tidy figures: facet_wrap() guides().facet_wrap() really effective splitting figures panels based variable; works like facet_wrap(~variable) ~ can read . \"split figure variable\", example Sex. two variables split figure , : facet_wrap(~variable1 + variable2).facet_wrap() really effective splitting figures panels based variable; works like facet_wrap(~variable) ~ can read . \"split figure variable\", example Sex. two variables split figure , : facet_wrap(~variable1 + variable2).guides() handy turning legends might taking space. instance, use facet_wrap() split panels Female Male, really need legend right saying Female Male? normally guide color, shape, etc, calls pipeline within aes() calls. works like guide(call = FALSE).guides() handy turning legends might taking space. instance, use facet_wrap() split panels Female Male, really need legend right saying Female Male? normally guide color, shape, etc, calls pipeline within aes() calls. works like guide(call = FALSE).Add facet_wrap() separate panels figure based Sex.Add facet_wrap() separate panels figure based Sex.Turn guides legend figure.Turn guides legend figure.\nfacet_wrap(~variable)\n\nguides(group = FALSE, ???? = False, ....)\nThinking Cap PointIf followed tasks correctly, following figure:\nFigure 3.11: finished figure!\nTake minutes look figure try interpret terms reaction time function rotation sex. Try answering following questions:sexes, mean reaction time decreases withincreases withis unaffected angle rotation.sexes, mean reaction time decreases withincreases withis unaffected angle rotation.Angle Rotation influences female participantsmale participants female participantsmale participantsAngle Rotation influences female participantsmale participants female participantsmale participants\nlooking figure, angle rotation increases (moving right x-axis), mean reaction time increases (getting higher y-axis), indicating participants take longer respond target image rotated original. Also, male mean reaction times quicker overall female mean reaction times, differences reaction times 0 degrees 150 degrees smaller males, perhaps say males affected less females, males perform task quicker.\n\nKeep mind looking correct responses . , figure suggest difference just male participants just responding quicker overall; instead may suggest males responding correctly quicker overall.\n\nDifferences mental rotation tasks received much attention years refer reference sections two main papers activity wish follow topic .\n","code":"## `summarise()` has grouped output by 'Sex'. You can override using the `.groups` argument.## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead."},{"path":"data-visualisation-through-ggplot2.html","id":"final-considerations","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.10 Final Considerations","text":"Many options seen terms geom_point() applied geom_line() make alterations line. Try playing options. example, code line result sexes red line equal size, style line different. Give shot!geom_line(aes(linetype = Sex), size = .5, color = \"red\")Finally, look closely figure, see line points actually goes front points. looks bit messy. make tidier line run behind data points? sure? Remember figures constructed based series layers. draw one layer next, try changing Task 8 draw lines first points top. Give go!chapter looked working layers variety calls shape, color, fills, etc, create professional looking figures. Understanding figures ggplot can seem like trial error lot experience. beauty figure really like, run code get exactly figure .","code":"\nggplot(data = menrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_line() +\n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  facet_wrap(~Sex) +\n  guides(group = FALSE, color = FALSE, shape = FALSE) +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3PracticeSkills","chapter":"3 Data Visualisation Through ggplot2","heading":"3.4 Practice Your Skills","text":"order complete tasks need download data .csv files .Rmd file, need edit, titled Ch3_PracticeSkills_Visualisations.Rmd. can downloaded within zip file link. downloaded unzipped, create new folder use working directory; put data files .Rmd file folder set working directory folder drop-menus top. Download Exercises .zip file .Now open .Rmd file within RStudio. see code chunk task. Follow instructions edit code chunk. often entering code based covered point.Happy Data Visualising!","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"solutions-to-questions-2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-application-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1 Data Visualisation Application","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"task-1-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.1 Task 1","text":"Return Task","code":"\nlibrary(\"tidyverse\")\n\nmenrot <- read_csv(\"MentalRotationBehavioralData.csv\")\ndemog <- read_csv(\"demographics.csv\")"},{"path":"data-visualisation-through-ggplot2.html","id":"task-2-2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.2 Task 2","text":"coord_cartesian() function can used show certain parts figure, controlling visible X Y axes. expand = TRUE adds smaller buffer numbers set. want remove buffer set expand = FALSE.Return Task","code":"\nmenrot_angle <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_angle, aes(x = Angle, y = mean_Resp)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-3-2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.3 Task 3","text":"Return Task","code":"\nmenrot_angle_sex <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-4-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.4 Task 4","text":"Return Task","code":"\nmenrot_grouped <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_grouped, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-5-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.5 Task 5","text":"Return Task","code":"\nmenrot_grouped_color <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_grouped_color, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-6-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.6 Task 6","text":"Return Task","code":"\nmenrot_shape_size <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_shape_size, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-7-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.7 Task 7","text":"Return Task","code":"\nmenrot_lab_theme <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_lab_theme, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"task-8-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.8 Task 8","text":"\nFigure 3.12: Task 8\nRemembering layer system, use code lines behind points.\nFigure 3.13: Task 8 Alternative\nReturn Task","code":"\nmanrot_facet_guide <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))## `summarise()` has grouped output by 'Angle'. You can override using the `.groups` argument.\nggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  facet_wrap(~Sex) +\n  guides(group = FALSE, color = FALSE, shape = FALSE) +\n  theme_bw()## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead.\nmanrot_facet_guide <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))## `summarise()` has grouped output by 'Angle'. You can override using the `.groups` argument.\nggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_line() +\n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  facet_wrap(~Sex) +\n  guides(group = FALSE, color = FALSE, shape = FALSE) +\n  theme_bw()## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead."},{"path":"data-visualisation-through-ggplot2.html","id":"practice-your-skills-3","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.2 Practice Your Skills","text":"Check work solution tasks : Chapter 3 Practice Skills Solution.Return Task","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"additional-material","chapter":"3 Data Visualisation Through ggplot2","heading":"3.6 Additional Material","text":"additional material might help understand figures bit present reports. Thus, want clarification aes() call want know combine several plots one, read !","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"more-on-aes","chapter":"3 Data Visualisation Through ggplot2","heading":"3.6.1 More on aes()","text":"chapter, added short note using aes() call, see people issues thought quick demonstration might help. code previous activity:\nFigure 3.14: Changing Shape Size Data Points\nSpecifically, going focus geom_point() line. Inside aes() stated color = Sex, shape = Sex outside aes() stated, size = 3. Earlier said, outside aes() means observations take one value color type. Inside means observation within condition takes value color type, different conditions different values/color/type. based understanding, plot, shapes size (.e., 3), different shape color shape sex. Now demonstrate alternatives.points shape, color size - nothing aes().\nneed state color, shape size want observations . chosen red color (quotes) shape style 3 (+) size 6. using variable split observations groups.\nchanged size make visualisation clearer\nneed state color, shape size want observations . chosen red color (quotes) shape style 3 (+) size 6. using variable split observations groups.changed size make visualisation clearer\nFigure 3.15: points shape, color size\npoints shape size, color determined Sex (goes inside aes()).\nneed state shape size want dots . time giving different levels within Sex (.e., male female) different colors - putting inside aes()\nneed state shape size want dots . time giving different levels within Sex (.e., male female) different colors - putting inside aes()\nFigure 3.16: points shape size color determined Sex\nshowed code setting color shape Sex previously. Instead now set color, shape size variable Sex.\noptions aes() different colors, sizes, shapes males females, males color, size shape, females color size shape.\noptions aes() different colors, sizes, shapes males females, males color, size shape, females color size shape.\nFigure 3.17: points shape size color determined Sex\nactually get warning option. ? numerous options many different shapes created cause issues code may even crash . Pay attention warnings.reason decide best approach displaying data observations within condition , showing different colors shapes makes little sense. always need think trying convey. Look two figures think one easier understand observations condition. one left! one right suggests something different observations.\nFigure 3.18: plot left suggests observations condition. figure right suggests difference observations. Always think information convey figures!\nHopefully beginning become clearer. Insider aes() means observations within variable/condition shown , different observations different variable/condition. Outside aes() means observations shown regardless condition.","code":"\nmenrot_angle_sex <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))## `summarise()` has grouped output by 'Angle'. You can override using the `.groups` argument.\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_gray()\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(color = \"red\", shape = 3, size = 6) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_bw()\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex), shape = 3, size = 6) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_bw()\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex, size = Sex)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_bw()## Warning: Using size for a discrete variable is not advised."},{"path":"data-visualisation-through-ggplot2.html","id":"combining-plots-into-one-figure","chapter":"3 Data Visualisation Through ggplot2","heading":"3.6.2 Combining Plots into one Figure","text":"Space within report commodity. Figures can incredibly useful getting information across efficient manner, strict word count, multiple figures can really chew limit, given figure needs legend legend counts. One way get around merge figures together one big figure perhaps convey similar related information. going show using package called patchwork.\npatchwork unlikely lab machines please try machine. previously installed patchwork package machine , install first, e.g. install.packages(\"patchwork\").\nPlots like boxplots histograms, combined, can incredibly useful understanding overall shape data whether fits assumptions inferential tests, something come later. create two separate plots, might get something like :\nFigure 3.19: histogram distribution Mean Time counts Sex\n:\nFigure 3.20: boxplot spreads Mean Time Correct Responses Sex\nNow given divide data sex, can start see figure legend plot becomes bit repetitive, combining one figure potentially make things easier. number packages , patchwork straightforward flexible.now call patchworkThe first thing need using patchwork save plots object (just like output function). Using code , might look like boxplot:histogram:Note: reason inclusion title plot become clear second.Note: run codes plots generated saving objects - boxplot p_box histogram p_hist. important realise distinction someone asks make produce code figure generated code knits, saved plot object, figure might show. save plot object, can generate figure just calling name object. look ggplot cheatsheet see approach lot. call boxplot stored p_box.\nFigure 3.21: boxplot spreads Mean Time Correct Responses Sex\n\nFigure 3.22: histogram distribution Mean Time counts Sex\nNote: see warning histogram plot selecting binwidth. really looked yet due course. wanted \"fix\" warning changing histrogram code something like + geom_histogram(binwidth = 100) might work. value enter relative scale data. binwidth 1 create bin every increase 1 ms. binwidth 100 creates bin every 100 ms.far nothing exciting. Looks just like seen . say wanted plots single figure, right? Well patchwork, simply \"add\" plots together using plus sign (+), :\nFigure 3.23: boxplot (- left) spreads Mean Time Correct Responses, histogram (B - right) distribution Mean Time counts, separated Sex (female - red, male - cyan)\nNote: can use \"titles\"\" added plots original code tell readers plot, within combined figure, referring , B, left right, shown figure legend beneath figure. might seem bit pedantic, control somebody views published figure, clarity paramount!Awesome, ? ! can also change configuration plots combined figure. Say wanted plots top - portrait rather landscape - well instance divide plots using divide sign (/), :\nFigure 3.24: boxplot (- top) spreads Mean Time Correct Responses, histogram (B - bottom) distribution Mean Time counts, separated Sex (female - red, male - cyan)\nnow refer top bottom, rather left right. fact, patchwork really flexible can work multiple plots arrangements. Hypothetically, say three plots wanted two top one, use approach combining \"+\" \"/\" :Remember trick using patchwork save plots objects first (p1 <- ggplot(....)) rest easy. sure always know figure shown knitted ; often , seeing figure important seeing code.Happy Visualising!End Additional Material!","code":"\nmenrot_hist_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  filter(CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\")\n\nggplot(data = menrot_hist_correct, \n       aes(x = Mean_Time,\n           fill = Sex)) + \n  geom_histogram() +\n  theme_bw()\nmenrot_box_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  filter(CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\")## `summarise()` has grouped output by 'Participant'. You can override using the `.groups` argument.\nggplot(data = menrot_box_correct, \n       aes(x = CorrectResponse, \n           y = Mean_Time, \n           fill = Sex)) + \n  geom_boxplot() +\n  theme_bw()\nlibrary(patchwork)\np_box <- ggplot(data = menrot_box_correct, \n       aes(x = CorrectResponse, \n           y = Mean_Time, \n           fill = Sex)) + \n  geom_boxplot() +\n  labs(title = \"A\") +\n  theme_bw()\np_hist <- ggplot(data = menrot_hist_correct, \n       aes(x = Mean_Time,\n           fill = Sex)) + \n  geom_histogram() +\n  labs(title = \"B\") +\n  theme_bw()\np_box\np_hist## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\np_box + p_hist## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\np_box / p_hist## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n(plot1 + plot2)/plot3"},{"path":"revisiting-probability-distributions.html","id":"revisiting-probability-distributions","chapter":"4 Revisiting Probability Distributions","heading":"4 Revisiting Probability Distributions","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"overview-4","chapter":"4 Revisiting Probability Distributions","heading":"4.1 Overview","text":"Probability degree cornerstone Psychological theory based quantitative analysis. establish outcome (e.g., difference two events), establish probability outcome model standard. Probability important quantifying uncertainty conclusions. already heard probability lectures/journal articles/etc., try help gain deeper understanding probability course next chapters use make inference population.start looking general ideas behind probability. using lot Psychology data concepts can easier understand probability first everyday concrete examples. said, whilst reading examples, trying , think might relate Psychology examples sure ask questions.preclass bit read take time try understand fully. Much familiar though PsyTeachR Data Skills book recaps ideas. Also, cheatsheets chapter using specific package. However can make full use R help function (e.g. ?sample) clear function . Also, remember said previously; shy run Google Search finding stats concepts covered . loads videos help pages clear examples explain difficult concepts.chapter :Revise probability concepts discussed PsyTeachR Data Skills bookCalculate probabilitiesCreate probability distributionsMake estimations probability distributions.\npopulation whole group want know something - everyone everything group. sample part population testing. sample always smaller population unlikely ever able test everyone population, sample representative population based random sampling. means even though using whole population, sample using represents whole population randomly sampled people . true, sample representative population, testing sample allows make inference population; infer characteristic population testing sample.\nDiscrete versus Continuous DataLet's recap questions level measurement can alter way tackle probability - .e., whether data discrete continuous.Quickfire QuestionsDiscrete data can take specific/certain/exact values (e.g., groups, integers). example, number participants experiment discrete - half participant! Discrete variables can also broken nominal/categorical ordinal variables.\n\nFill blanks sentences using words: ordinal, nominal/categorical.NominalOrdinal data based set categories natural ordering (e.g., left right handed). example, separate participants according left right handedness course study (e.g., psychology, biology, history, etc.).NominalOrdinal data based set categories natural ordering (e.g., left right handed). example, separate participants according left right handedness course study (e.g., psychology, biology, history, etc.).NominalOrdinal data set categories natural ordering; know top/best worst/lowest, difference categories may constant. example, ask participants rate attractiveness different faces based 5-item Likert scale (unattractive, unattractive, neutral, attractive, attractive).NominalOrdinal data set categories natural ordering; know top/best worst/lowest, difference categories may constant. example, ask participants rate attractiveness different faces based 5-item Likert scale (unattractive, unattractive, neutral, attractive, attractive).Continuous data hand can take value scale measured. example, can measure age continuous scale (e.g., can age 26.55 years), also reaction time distance travel university.Fill blanks sentences using two remaining levels measurement offered :Continuous data can broken   data.read journal articles working data, really good practice take minute two figure type variables reading /working .\nfour level measurements nominal (also called categorical), ordinal, interval, ratio. Discrete data uses categories whole numbers therefore either nominal ordinal data. Continuous data can take value, e.g., 9.00 9.999999999, either interval ratio data.\n","code":""},{"path":"revisiting-probability-distributions.html","id":"discrete-data-and-binomial-distributions","chapter":"4 Revisiting Probability Distributions","heading":"4.2 Discrete Data and Binomial Distributions","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"general-probability-calculations","chapter":"4 Revisiting Probability Distributions","heading":"4.2.1 General Probability Calculations","text":"Today begin recapping concepts probability calculations lectures PsyTeachR Data Skills book, looking discrete distributions - values categories (e.g., house, face, car) whole numbers (e.g., 1,2, 3 1.1, 1.2 etc).talk probability mean interested likelihood event occurring. probability discrete event occurring can formulated :\\[p = \\frac{number \\   \\ ways \\ \\ event \\ \\  arise}{number \\ \\ possible \\ outcomes}\\]probability event represented number 0 1, letter p. example, probability flipping coin landing 'tails', people say, estimated p = .5, .e. likelihood getting tails \\(p = \\frac {1}{2}\\) one desired outcome (tails) two possibilities (heads tails).example:1. probability drawing ten clubs standard pack cards 1 52: \\(p = \\frac {1}{52} \\ = .019\\). One outcome (ten clubs) 52 possible outcomes (cards)2. Likewise, probability drawing either ten clubs seven diamonds first card draw full deck 2 52: \\(p = \\frac {2}{52} \\ = .038\\). case adding chance event occurring giving two possible outcomes becomes likely happen one outcome.3. Now say two standard packs cards mixed together. probability drawing 10 clubs mixed pack 2 104: \\(p = \\frac{2}{104}= .019\\). Two possible outcomes alternatives , 104 time, meaning less probable Example 2 probability Example 1. key thing remember probability ratio number ways specified outcome can happen number possible outcomes.4. instead say two separate packs cards. probability drawing 10 clubs packs : \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). probability gone created event even unlikely happen. called joint probability events.5. probability drawing 10 clubs pack 52, putting back (call replacement), subsequently drawing 7 diamonds? , represented multiplying together probability events happening: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\).6. Finally, say draw 10 clubs pack 52 time replace . probability draw 7 diamonds next draw (without replacing ) 3 hearts third draw? time number cards pack fewer second (51 cards) third draws (50 cards) take account multiplication: \\(p = \\frac{1}{52} \\times \\frac{1}{51}\\times \\frac{1}{50}= .000008\\).\n, probability event number possible ways event happen, divided number possible outcomes. combine probabilities two separate events multiple together obtain joint probability.\n\nmay noticed tend write p = .008, example, opposed p = 0.008 (0 decimal place). ? Convention really. probability can never go 1, 0 decimal place pointless. Meaning people write p = .008 instead p = 0.008, indicating max value 1.\n\nallow either version answers chapter still learning, try get habit writing probability without 0 decimal place.\nQuickfire QuestionsWhat probability randomly drawing name hat 12 names one name definitely name? Enter answer 3 decimal places: probability randomly drawing name hat 12 names one name definitely name? Enter answer 3 decimal places: probability randomly drawing name hat 12 names, putting back, drawing name ? Enter answer 3 decimal places: probability randomly drawing name hat 12 names, putting back, drawing name ? Enter answer 3 decimal places: Tricky: stimuli set 120 faces, 10 inverted 110 right way , probability randomly removing one inverted face first trial, replacing , removing another inverted face second trial? Enter answer three decimal places:Tricky: stimuli set 120 faces, 10 inverted 110 right way , probability randomly removing one inverted face first trial, replacing , removing another inverted face second trial? Enter answer three decimal places:\n\n12 possible outcomes looking one possible event.\n\n\n12 possible outcomes looking one possible event.\n\n\ntwo separate scenarios . scenarios 12 possible outcomes looking one possible event. Since two separate scenarios, make less likely draw name twice?\n\n\ntwo separate scenarios . scenarios 12 possible outcomes looking one possible event. Since two separate scenarios, make less likely draw name twice?\n\n\nfirst trial 120 possible outcomes (faces) looking 10 possible events (inverted faces). second trial removed first inverted face stimuli set now 119 trials total 9 inverted faces. Remember need multiply probabilities first trial second trial results together!\n\n\nfirst trial 120 possible outcomes (faces) looking 10 possible events (inverted faces). second trial removed first inverted face stimuli set now 119 trials total 9 inverted faces. Remember need multiply probabilities first trial second trial results together!\n\n\np = .083. One outcome (name) 12 possibilities, .e. \\(p = \\frac{1}{12}\\)\n\n\np = .083. One outcome (name) 12 possibilities, .e. \\(p = \\frac{1}{12}\\)\n\n\np = .007. replace name draws \\(p = \\frac{1}{12}\\) draw. \\(p = \\frac{1}{12} * \\frac{1}{12}\\) rounded three decimal places\n\n\np = .007. replace name draws \\(p = \\frac{1}{12}\\) draw. \\(p = \\frac{1}{12} * \\frac{1}{12}\\) rounded three decimal places\n\n\np = .006. first trial 10 120, remove one inverted face second trial 9 119. formula \\(p = \\frac{10}{120} * \\frac{9}{119}\\)\n\n\np = .006. first trial 10 120, remove one inverted face second trial 9 119. formula \\(p = \\frac{10}{120} * \\frac{9}{119}\\)\n","code":"* To find the joint probability of two separate events occuring you multiply together the probabilities of the two individual separate events (often stated as independent, mutually exclusive events). * The second event (drawing the 7 of diamonds) has the same probability as the first event (drawing the 10 of clubs) because we put the original card back in the pack, keeping the number of all possible outcomes at 52. This is **replacement**."},{"path":"revisiting-probability-distributions.html","id":"creating-a-simple-probability-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.2.2 Creating a Simple Probability Distribution","text":"now recap plotting probability distributions looking simulated coin toss. may remember PsyTeachR Data SKills book worry going work . Work read example apply logic quickfire questions end section.Scenario: Imagine want know probability X number heads 10 coin flips - example, probability flipping coin 10 times coming heads two times.simulate 10 coin flips use sample() function randomly sample (replacement) possible events: .e. either heads tails.begin:Open new script copy code lines .\nfirst line code loads library normal.\nsecond line code provides instruction sample options \"Heads\" \"Tails\", ten times, replacement set TRUE.\nfirst line code loads library normal.second line code provides instruction sample options \"Heads\" \"Tails\", ten times, replacement set TRUE.Note: event labels strings (text), enter function vector; .e. \"quotes\"Note: lines code, see output got ran code. worry sequence heads tails different output; expected generating random sample.Note: want get output , add line code script prior loading library. set.seed() function can put number time run randomisation get number.\nSampling simply choosing selecting something - randomly choosing one possible options; heads tails. examples 'sampling' include randomly selecting participants, randomly choosing stimuli present given trial, randomly assigning participants condition e.g.drug placebo...etc.\n\nReplacement putting sampled option back 'pot' possible options. example, first turn randomly sample HEADS options HEADS TAILS replacement, meaning next turn two options ; HEADS TAILS. Sampling without replacement means remove option subsequent turns. say first turn randomly sample HEADS options HEADS TAILS without replacement. Now second turn option TAILS 'randomly' sample . third turn without replacement options.\n\nreplacement means putting option back next turn turn possible outcome options.\n\nwant use sampling replacement coin toss scenario? sure set replacement FALSE (change last argument TRUE FALSE) run code . code stop working 2 coin flips. want sample replacement want options available sampling - run options quickly since 10 flips.far code returns outcomes 10 flips; either heads tails. want count many 'heads' can simply sum heads. However, heads number, make life easier can re-label events (.e. flips) 0 tails 1 heads. Now run code can pipe sample sum() function total 1s (heads) 10 flips.Run line code number times, notice output?Note: event labels now numeric, need vector.Note: 0:1 means numbers 0 1 increments 1. basically, 0 1.ouptut line changes every time run code randomly sampling 10 coin flips time. clear, get answer 6 example, means 6 heads, turn, 4 tails. running code basically demonstrating sampling distribution created.\nsampling distribution shows probability drawing sample certain characteristics population; e.g. probability 5 heads 10 flips, probability 4 heads 10 flips, probability X heads 10 flips coin.\n\nNow order create full accurate sampling distribution scenario need replicate 10 flips large number times - .e. replications. replications reliable estimates. 10000 replications 10 coin flips. means flip coin 10 times, count many heads, save number, repeat 10000 times. slow way demonstrated , just running line noting outcome time. use replicate() function.Copy line code script run .exactly said saving 10000 outputs (counts heads) dataframe called heads10k (k shorthand thousand).reiterate, sum heads (.e., number times got heads) 10000 replications now stored vector heads10k. look heads10k, shown box , series 10000 numbers 0 10 indicating number heads, specifically 1s, got set 10 flips.Now, order complete distribution need :Convert vector (list numbers heads counts) data frame (tibble) can work . numbers stored column called heads.group results number possible heads; .e. group times got 5 heads together, times got 4 heads together, etc.Finally, work probability heads result, (e.g., probability 5 heads), totaling number observations possible result (e.g., 5 heads) submitting probability formula (number outcomes event divided possible outcomes)\nnumber times got specific number heads (e.g., 5 heads) divided total number outcomes (.e., number replications - 10000).\nnumber times got specific number heads (e.g., 5 heads) divided total number outcomes (.e., number replications - 10000).can carry steps using following code:Copy code script run .now discrete probability distribution number heads 10 coin flips. Use View() function look data10k variable. now see heads outcome, total number occurrences 10000 replications (n) plus probability outcome (p).\nTable 4.1: sampling distribution number heads 10 flips coin. p = probability obtaining number heads 10000 replications 10 flips coin\nuseful visualize distribution:\nFigure 4.1: Probability Distribution Number Heads 10 Flips\nanalysis, probability getting 5 heads 10 flips 0.2401. remember, surprised get slightly different value. Ten thousand replications lot huge amount compared infinity. run analysis replications numbers become stable, e.g. 100K.Note possible number heads 10 flips related one another, summing probabilities different number heads give total 1. different looked earlier cards events unrelated . , can use information start asking questions probability obtaining 2 less Heads 10 flips? Well, probability getting heads (10 flips) distribution 0.0007, probability getting 1 head 0.0082, probability getting 2 heads 0.0465, probability 2 less Heads distribution simply sum values: 0.0554. Pretty unlikely !Quickfire QuestionsLook probability values corresponding number coin flips created data10k sample distribution (use View() see ):Choose following options, wanted calculate probability getting 4, 5 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, wanted calculate probability getting 4, 5 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, wanted calculate probability getting 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, wanted calculate probability getting 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, distribution created : continuousdiscreteChoose following options, distribution created : continuousdiscrete\nthink , get 5.5 heads 2.3 heads, can get whole numbers, 2 heads 5 heads. means data distribution discrete. (confused one functions saying continuous)\n\nfind probability getting say 4, 5, 6 heads 10 coin flips, combining related scenarios together, therefore need find individual probabilities getting 4, 5 6 heads 10 coin flips, sum probabilities together get appropriate probability obtaining 4, 5 6 heads. 6 heads, just sum probabilities 6, 7, 8, 9 10 heads get probability 6 heads.\n\nsure summing multiplying probabilities? good way remember, coin flip examples pack cards examples earlier, scenarios related summing probabilities, scenarios separate multiplying probabilities. Related scenarios usually asking probability 'either / ' scenarios occuring, whereas separate scenarios usually ask probability one scenario '' another scenario occuring.\n\nsample distribution data10k already completed first part calculation (finding individual probabilities n heads 10 coin flips), need sum required probabilities together!\n","code":"\nlibrary(\"tidyverse\")\nsample(c(\"HEADS\", \"TAILS\"), 10, TRUE) ##  [1] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n## [10] \"TAILS\"\nset.seed(1409)\nsample(0:1, 10, TRUE) %>% sum() ## [1] 5\nheads10k <- replicate(10000, sample(0:1, 10, TRUE) %>% sum())   ##  int [1:10000] 7 4 5 6 2 6 2 6 8 5 ...\ndata10k <- tibble(heads = heads10k) %>%       # creating a tibble/data frame\n                group_by(heads) %>%           # group by number of possibilities\n                summarise(n = n(), p=n/10000) # count occurences of possibility,\n                                              # & calculate probability (p) of\n                                              # each\nggplot(data10k, aes(heads,p)) + \n  geom_col(fill = \"skyblue\") + \n  labs(x = \"Number of Heads\", y = \"Probability of Heads in 10 flips (p)\") +\n  theme_bw() +\n  scale_x_discrete(limits=0:10)## Warning: Continuous limits supplied to discrete scale.\n## Did you mean `limits = factor(...)` or `scale_*_continuous()`?"},{"path":"revisiting-probability-distributions.html","id":"the-binomial-distribution---creating-a-discrete-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.2.3 The Binomial Distribution - Creating a Discrete Distribution","text":"Great, now learning probabilities distributions work. However, wanted calculate probability 8 heads 10 coin flips, go entire procedure time. Instead, dichotomous outcome, \"heads tails\", can establish probabilities using binomial distribution - effectively just created. can look R help page binomial distribution (type ?dbinom directly console) understand use walk essentials .use 3 functions work binomial distribution ask questions asked :dbinom() - density function. function gives probability x successes (e.g., heads) given size (e.g., number trials) probability success prob single trial (0.5, assume flipping fair coin - Heads Tails)dbinom() - density function. function gives probability x successes (e.g., heads) given size (e.g., number trials) probability success prob single trial (0.5, assume flipping fair coin - Heads Tails)pbinom() - cumulative probability function. function gives probability getting number successes certain cut-point given size prob. questions probability 5 heads less example. sums probability 0, 1, 2, 3, 4, 5 heads.pbinom() - cumulative probability function. function gives probability getting number successes certain cut-point given size prob. questions probability 5 heads less example. sums probability 0, 1, 2, 3, 4, 5 heads.qbinom() - quantile function. function inverse pbinom gives x-axis value (including value) summation probabilities greater equal given probability p, given size prob. words, many heads need probability p = 0.0554qbinom() - quantile function. function inverse pbinom gives x-axis value (including value) summation probabilities greater equal given probability p, given size prob. words, many heads need probability p = 0.0554Let's look functions turn little. thing keep mind probability every event likelihood occurring distribution. trying look numbers come mean us.","code":""},{"path":"revisiting-probability-distributions.html","id":"dbinom---the-density-function","chapter":"4 Revisiting Probability Distributions","heading":"4.2.4 dbinom() - The Density Function","text":"Using dbinom() function can create probabilities possible outcomes two possibilities outcome trial - e.g., heads tails, cats dogs, black red. going stick coin flip idea. showing code obtaining 3 heads 10 flips:possible outcomes heads (0:10) 10 flips:plot probability possible outcomes 10 flips look like :\nFigure 4.2: Probability Distribution Number Heads 10 Flips\ndbinom (density binom) function takes format dbinom(x, size, prob), arguments give :x number 'heads' want know probability . Either single one, 3 series 0:10.size number trials (flips) ; case, 10 flips.prob probability 'heads' one trial. chance 50-50 probability state 0.5 .5Now say wanted know probability 6 heads 10 flips. change first argument code used 3 heads, :probability 6 heads, using dbinom() p = 0.2050781. compare value data10k value 6 see similar quite . dbinom() uses lot replications 10000 used simulation.terms visualising just calculated, p = 0.2050781 height green bar plot .\nFigure 4.3: Probability Distribution Number Heads 10 Flips probability 6 10 Heads highlighted green\nQuickfire QuestionsTo three decimal places, probability 2 heads 10 flips? \nwant know probability 2 heads 10 flips.\n\nX therefore 2;\n\nSize therefore 10;\n\nprobability outcomes trial stays .5.\n\ndbinom(2, 10, 0.5) = .04394531 rounded = .044\n","code":"\ndbinom(3, 10, 0.5)\ndbinom(0:10, 10, 0.5)\ndbinom(6, 10, 0.5)## [1] 0.2050781"},{"path":"revisiting-probability-distributions.html","id":"pbinom---the-cumulative-probability-function","chapter":"4 Revisiting Probability Distributions","heading":"4.2.5 pbinom() - The Cumulative Probability Function","text":"wanted know probability including 3 heads 10 flips? asked similar questions . can either use dbinom outcome 3 heads sum results:can use pbinom() function; known cumulative probability distribution function cumulative density function. first argument give cut-value including value want know probability (3 heads). , , tell many flips want probability heads single trial.Copy line script run :probability including 3 heads 10 flips 0.172. visualization, done calculated cumulative probability lower tail distribution (lower.tail = TRUE; shown green ) cut-3:\nFigure 4.4: Probability Distribution Number Heads 10 Flips probability 0 3 Heads highlighted green - lower.tail = TRUE\npbinom function gives us cumulative probability outcomes including cut-. wanted know probability outcomes including certain value? Say want know probability 7 heads 10 coin flips. code :explain code little.First, switch lower.tail call TRUE FALSE tell pbinom() want lower tail distribution time (left including cut-), want upper tail, right cut-. results cumulative probability upper tail distribution cut-value (shown green ).Next specify cut-instead stating 7 might expect, even though want 7 , specify cut-6 heads. ? set cut-'6' working discrete distribution, lower.tail = TRUE includes cut-(6 ) whereas lower.tail = FALSE everything cut-including cut-(7 ).short, want upper tail using discrete distribution set cut-value (x) one lower number interested . wanted know 7 heads set cut-6.\nFigure 4.5: Probability Distribution Number Heads 10 Flips probability 7 Heads highlighted green - lower.tail = FALSE\n\nconfusing part people find concept lower.tail. look distribution, say binomial, find lot high bars middle distribution smaller bars far left right distribution. Well far left right distribution called tail distribution - tend extremity distribution taper like .....well like tail. lot time talk left right tails pbinom() function ever considers data relation left side distribution - calls lower.tail.\n\nconsider lower.tail = TRUE. default, state lower.tail = ... considered want. lower.tail = TRUE means values left value including value state. binomial distribution say give probability 5 heads less, set lower.tail = TRUE counting summing probability 0, 1, 2, 3, 4 5 heads. can check dbinom(0:5, 10, .5) %>% sum().\n\nHowever, say give probability 7 heads, need lower.tail = FALSE, consider right-hand side tail, also, need set code pbinom(6, 10, .5, lower.tail = FALSE). 6 7? pbinom() function, lower.tail = FALSE, starts value plus one value state; always considers value state part lower.tail say 6, includes 6 lower.tail gives 7, 8, 9 10 upper tail. said 7 lower.tail = FALSE, give 8, 9 10. tricky worth keeping mind using pbinom() function. remember, can always check using dbinom(7:10, 10, .5) %>% sum() seeing whether matches pbinom(6, 10, 0.5, lower.tail=FALSE) pbinom(7, 10, 0.5, lower.tail=FALSE)\nQuickfire QuestionsUsing format shown pbinom() function, enter code determine probability including 5 heads 20 flips, assuming probability 0.5: Using format shown pbinom() function, enter code determine probability including 5 heads 20 flips, assuming probability 0.5: two decimal places, probability obtaining including 50 heads 100 flips? two decimal places, probability obtaining including 50 heads 100 flips? \n\nlooking calcuate probability 5 less heads (x) 20 flips (size), probability 'heads' one trial (prob) remaining . need lower.tail call calculating cumulative probability lower tail distribution?\n\n\nlooking calcuate probability 5 less heads (x) 20 flips (size), probability 'heads' one trial (prob) remaining . need lower.tail call calculating cumulative probability lower tail distribution?\n\n\nlooking calculate probability 51 heads (x), 100 flips (size), probability 'heads' one trial (prob) remaining (0.5). need lower.tail call calculating cumulative probability upper tail distribution? Remember, looking lower.tail, value heads enter pbinom() included final calculation, e.g. entering pbinom(3, 100, lower.tail = FALSE) give probability 4 heads. instead looking lower.tail, entering pbinom(3, 100, lower.tail = TRUE) give probability 3 heads.\n\n\nlooking calculate probability 51 heads (x), 100 flips (size), probability 'heads' one trial (prob) remaining (0.5). need lower.tail call calculating cumulative probability upper tail distribution? Remember, looking lower.tail, value heads enter pbinom() included final calculation, e.g. entering pbinom(3, 100, lower.tail = FALSE) give probability 4 heads. instead looking lower.tail, entering pbinom(3, 100, lower.tail = TRUE) give probability 3 heads.\n\n\ncode first one : pbinom(5, 20, 0.5) pbinom(5, 20, 0.5, lower.tail = TRUE)\n\n\ncode first one : pbinom(5, 20, 0.5) pbinom(5, 20, 0.5, lower.tail = TRUE)\n\n\ncode second one : pbinom(50, 100, 0.5, lower.tail = FALSE), giving answer .46. Remember can confirm : dbinom(51:100, 100, 0.5) %>% sum()\n\n\ncode second one : pbinom(50, 100, 0.5, lower.tail = FALSE), giving answer .46. Remember can confirm : dbinom(51:100, 100, 0.5) %>% sum()\n","code":"\ndbinom(0:3, 10, 0.5) %>% sum()## [1] 0.171875\npbinom(3, 10, 0.5, lower.tail = TRUE)  ## [1] 0.171875\npbinom(6, 10, 0.5, lower.tail = FALSE) ## [1] 0.171875"},{"path":"revisiting-probability-distributions.html","id":"qbinom---the-quantile-function","chapter":"4 Revisiting Probability Distributions","heading":"4.2.6 qbinom() - The Quantile Function","text":"qbinom() function inverse pbinom() function. Whereas pbinom() supply outcome value x get tail probability, qbinom() supply tail probability get outcome value (approximately) cuts tail probability. Think rephrase questions pbinom() ask qbinom() question. Worth noting though qbinom() approximate discrete distribution \"jumps\" probability discrete outcomes (.e. can probability 2 heads 3 heads 2.5 heads).see two functions inverses one another. code probability 49 heads less 100 coin flipsThis tells us probability p = 0.4602054. now put probability (stored p1) qbinom get back started. E.g.can stated number heads required obtain probability 0.4602054 49. qbinom() function useful want know minimum number successes (‘heads’) needed achieve particular probability. time cut-specify number ‘heads’ want probability want.example say want know minimum number ‘heads’ 10 flips result 5% heads success rate (probability .05), use following code.dealing lower tail, telling us lower tail probability .05, expect 2 heads 10 flips. However, important note discrete distribution, value exact. can see :exactly p = .05 close. data working discrete distribution qbinom() gives us closest category boundary.\nfound lot students asking qbinom() works inputting two different probabilities arguments qbinom(). Let us try make things clearer.\n\nFirst, useful remember inverse pbinom(): pbinom() gives tail probability associated x, qbinom() gives closest x cuts specified tail probability. understand pbinom() try reversing question see helps understand qbinom().\n\nqbinom() function set qbinom(p, size, prob). First , used prob previous two functions, dbinom() pbinom(), represents probability success single trial (probability 'heads' one coin flip, prob = .5). Now, prob represents probability success one trial, whereas p represents tail probability want know . function gives value x yield probability asked . give qbinom() tail probability p = .05, 10 flips, probability success one flip prob = .5. tells answer 2, meaning getting 2 flips 10 trials probability roughly .05.\n\nqbinom() also uses lower.tail argument works similar fashion pbinom().\nQuickfire QuestionsType box, maximum number heads associated tail probability 10% (.1) 17 flips: \nanswer 6 code :\n\nqbinom(0.1, 17, 0.5, lower.tail = TRUE)\n\nRemember want overall probability 10% (p = .1), 17 flips go (size = 17), probability heads one flip .5 (prob = .5). want maximum number lower tail, lower.tail TRUE.\nKeep mind: trying get understanding every value distribution probability existing distribution. probability may large, meaning , bell-shaped distributions looked , value middle distribution, probability might rather low, meaning tail, ultimately every value distribution probability.","code":"\np1 <- pbinom(49, 100, .5, lower.tail = TRUE)\np1## [1] 0.4602054\nqbinom(p1, 100, .5, lower.tail = TRUE)## [1] 49\nqbinom(.05, 10, 0.5, lower.tail = TRUE) ## [1] 2\npbinom(2, 10, .5, lower.tail = TRUE) ## [1] 0.0546875"},{"path":"revisiting-probability-distributions.html","id":"continuous-data-and-normal-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.3 Continuous Data and Normal Distribution","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"continuous-data-properties","chapter":"4 Revisiting Probability Distributions","heading":"4.3.1 Continuous Data Properties","text":"previous section, seen can use distribution estimate probabilities determine cut-values (play important part hypothesis testing later chapters), looked discrete binomial distribution. Many variables encounter continuous tend show normal distribution (e.g., height, weight, IQ).say interested height population psychology students, estimate 146cm 194cm. plotted continuous, normal distribution, look like:\nFigure 4.6: Normal Distribution height Psychology students (black line). Green line represents mean. Blue line represent 1 Standard Deviation mean. Yellow line represents 2 Standard Deviation mean. Red line represents 3 Standard Deviation mean.\nfigure shows hypothetical probability density heights ranging 146cm 194cm population Psychology students (black curve). data normally distributed following properties:Properties Normal distribution1. distribution defined mean standard deviation: mean (\\(\\mu\\)) describes center, therefore peak density, distribution. largest number people population terms height. standard deviation (\\(\\sigma\\)) describes much variation mean distribution - figure, standard deviation distance mean inflection point curve (part curve changes upside-bowl shape right-side-bowl shape).2. Distribution symmetrical around mean: mean lies middle distribution divides area curve two equal sections - get typical bell-shaped curve.3. Total area curve equal 1: add probabilities (densities) every possible height, end value 1.4. mean, median mode equal: good way check given dataset normally distributed calculate measure central tendency see approximately (normal distribution) (skewed distribution).5. curve approaches, never touches, x axis: never probability 0 given x axis value.6. normal distribution follows Empirical Rule: Empirical Rule states 99.7% data within normal distribution falls within three standard deviations (\\(\\pm3\\sigma\\)) mean, 95% falls within two standard deviations (\\(\\pm2\\sigma\\)), 68% falls within one standard deviation (\\(\\pm\\sigma\\)).Continuous data can take precise specific value scale, e.g. 1.1, 1.2, 1.11, 1.111, 1.11111. Many variables encounter Psychology :continuous opposed discrete.tend show normal distribution.look similar - bell-shaped curve - plotted.","code":""},{"path":"revisiting-probability-distributions.html","id":"estimating-from-the-normal-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.3.2 Estimating from the Normal Distribution","text":"Unlike coin flips, outcome normal distribution just 50/50 ask create normal distribution complicated binomial distribution estimated previous section. Instead, just binomial distribution (distributions) functions allow us estimate normal distribution ask questions distribution. :dnorm() - Density function normal distributionpnorm() - Cumulative Probability function normal distributionqnorm() - Quantile function normal distributionYou might thinking look familiar. fact work similar way binomial counterparts. unsure function works remember can call help typing console, example, ?dnorm ?dnorm().\nQuickfire QuestionsType box binomial counterpart dnorm()? Type box binomial counterpart dnorm()? Type box binomial counterpart pnorm()? Type box binomial counterpart pnorm()? Type box binomial counterpart qnorm()? Type box binomial counterpart qnorm()? \ncounterpart functions start letter, d, p, q, just distribution name changes, binom, norm, t - though quite come across t-distribution yet.\n\n\ndbinom() binomial equivalent dnorm()\n\n\ndbinom() binomial equivalent dnorm()\n\n\npbinom() binomial equivalent pnorm()\n\n\npbinom() binomial equivalent pnorm()\n\n\nqbinom() binomial equivalent qnorm()\n\n\nqbinom() binomial equivalent qnorm()\n\nalso rnorm() rbinom() look another time.\n","code":""},{"path":"revisiting-probability-distributions.html","id":"dnorm---the-density-function","chapter":"4 Revisiting Probability Distributions","heading":"4.3.3 dnorm() - The Density Function","text":"Using dnorm(), like dbinom, can plot normal distribution. time however need:x, vector quantiles (words, series values x-axis - think max min distribution want plot)mean dataand standard deviation sd data.use IQ example. actually disagreement whether IQ continuous data degree depend measurement use. IQ however definitely normally distributed assume continuous purposes demonstration. Many Psychologists interested studying IQ, perhaps terms heritability, interested controlling IQ studies rule effect (e.g., clinical autism studies).","code":""},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT1","chapter":"4 Revisiting Probability Distributions","heading":"4.3.3.1 Task 1: Standard Deviations and IQ Score Distribution","text":"Copy code new script run . Remember need call tidyverse library first.code creates plot showing normal distribution IQ scores (M = 100, SD = 15) ranging 40 160. values considered typical general population.First set range IQ values 40 160Then plot distribution IQ_data, M = 100 SD = 15\nFigure 4.7: Distribution IQ scores mean = 100, sd = 15\npart code need change alter SD plot? mean = 100sd = 15(40, 160)Now copy edit code plot distribution mean = 100 sd = 10, visually compare two plots.\nThinking Cap PointWhat changing standard deviation (sd) shape distribution? Spend minutes changing code various values running , discussing group answer following questions:happens shape distribution change sd 10 20? distribution gets narrowerthe distribution gets widerWhat happens shape distribution change sd 10 20? distribution gets narrowerthe distribution gets widerWhat happens shape distribution change sd 10 5? distribution gets narrowerthe distribution gets widerWhat happens shape distribution change sd 10 5? distribution gets narrowerthe distribution gets widerWhat small large standard deviation sample tell data collected?small large standard deviation sample tell data collected?\nChanging SD 10 20 means larger standard deviation wider distribution.\n\nChanging SD 10 5 means smaller standard deviation narrower distribution.\n\nSmaller SD results narrower distribution meaning data less spread ; larger SD results wider distribution meaning data spread .\n\nnote Standard Deviation:\n\nknow lectures can estimate data two ways: point-estimates spread estimates. mean point-estimate condenses data one data point - tells average value data tells nothing spread data . standard deviation however spread estimate gives estimate spread data mean - measure standard deviation mean.\n\nimagine looking IQ scores test 100 people get mean 100 SD 5. means vast majority sample IQ around 100 - probably fall within 1 SD mean, meaning participants IQ 95 105.\n\nNow test find mean 100 SD 20, means data much spread . take 1 SD approach participants IQ 80 120.\n\none sample tight range IQs sample wide range IQs. , point-estimate spread estimate data can tell shape sample distribution.\n\nfar good! example told dnorm() values limit range rest; said give us range 40 160 IQ scores. However, plot another way telling dnorm() sequence, range, values want much precision want .","code":"\nIQ_data <- tibble(IQ_range = c(40, 160))\n\nggplot(IQ_data, aes(IQ_range)) + \n  stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n  labs(x = \"IQ Score\", y = \"probability\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT2","chapter":"4 Revisiting Probability Distributions","heading":"4.3.3.2 Task 2: Changing Range and Step Size of The Normal Distribution","text":"Copy code script run .\nplot standard Normal Distribution -4 4 steps 0.01. also stated mean 0 sd 1.\nFigure 4.8: Normal Distribution Mean = 0 SD = 1\nQuickfire QuestionsFill box show type create tibble containing column called ND_range values ranging -10 10 steps .05:ND_data <- Now know change, try plotting normal distribution following attributes:range -10 10 steps 0.05,range -10 10 steps 0.05,mean 0,mean 0,standard deviation 1.standard deviation 1.Compare new plot original one created. change distribution? Distribution widensNo change distributionDistribution narrowsCompare new plot original one created. change distribution? Distribution widensNo change distributionDistribution narrows\nchange distribution write: ND_data <- tibble(ND_range = seq(-10, 10, 0.05))\n\nHowever, comparing plots, whilst plot may look thinner, distribution changed. change appearance due range sd values extended -4 4 -10 10. density values within values changed however see, clearly second plot, values beyond -3 3 unlikely.\nRemember, every value probability distribution able use dnorm() function get visual representation probability values change normal distribution. values probable. values less probable. key concept comes thinking significant differences later.However, know, one important difference continuous discrete probability distributions - number possible outcomes. discrete probability distributions usually finite number outcomes take probability. instance, 5 coin flips, 5 possible outcomes number heads: 0, 1, 2, 3, 4, 5. binomial distribution exact finite outcomes, can use dbinom() get exact probability outcome.contrast, truly continuous variable, number possible outcomes infinite, 0.01 also .0000001 .00000000001 arbitrary levels precision. rather asking probability single value, ask probability range values, equal area curve (black line plots ) range values., leave dnorm() now move onto looking establishing probability range values using Cumulative Probability function","code":"\nND_data <- tibble(ND_range = seq(-4, 4, 0.01))\nggplot(ND_data, aes(ND_range)) + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  labs(x = \"SD units\", y = \"probability\", title = \"The Normal Distribution\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"pnorm---the-cumulative-probability-function","chapter":"4 Revisiting Probability Distributions","heading":"4.3.4 pnorm() - The Cumulative Probability Function","text":"Just dnorm() works like dbinom(), pnorm() works just like pbinom(). , pnorm(), given mean sd data, returns cumulative density function (cumulative probability) given probability (p) lies specified cut-point , unless course lower.tail = FALSE specified case cut-point .OK, English people can understand, means pnorm() function tells probability obtaining given value lower set lower.tail = TRUE. Contrastingly, pnorm() function tells probability obtaining given value higher set lower.tail = FALSE.use height give concrete example. Say test sample students (M = 170cm, SD = 7) want calculate probability given student 150cm shorter following:Remember, lower.tail = TRUE means lower including value XTRUE default actually need declare itThis tells us finding probability someone 150cm shorter class p = 0.0021374. Stated differently, expect proportion students 150cm shorter 0.21% (can convert probability proportion multiplying probability 100). small probability suggests pretty unlikely find someone shorter 150cm class. mainly small standard deviation distribution . Think back said earlier narrow standard deviations round mean!Another example might , probability given student 195 cm taller? , set following code:tells us finding probability someone 195cm taller class 0.02%. , really unlikely.notice something different cut-example using dbinom() function looking cut-? might ? discuss second first quick task.","code":"\npnorm(150, 170, 7, lower.tail = TRUE)\npnorm(195, 170, 7, lower.tail = FALSE)"},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT3","chapter":"4 Revisiting Probability Distributions","heading":"4.3.4.1 Task 3: Calculating Cumulative Probability of Height","text":"Edit pnorm() code calculate probability given student 190cm taller.three decimal places, Task 3, probability student 190cm taller class? \nanswer .002. See solution code end chapter.\n\nkey thing difference need specify cut-point pbinom() (discussed preclass activity) pnorm() functions values x, .e. lower.tail = FALSE.\n\ndiscrete data, say number coin flips result heads, wanted calculate probability x, apply pbinom() specify cut-point x-1 include x calculation. example, calculate probability 4 'heads' occuring 10 coin flips, specify pbinom(3, 10, 0.5, lower.tail = FALSE) lower.tail includes value state.\n\ncontinuous data, however, height, applying pnorm() therefore can specify cut-point simply x. example, cut-point 190, mean 170 standard deviation 7, can write pnorm(190, 170, 7, lower.tail = FALSE). way think setting x 189 continuous scale, want values greater 190, also include possible values 189 190. Setting x 190 starts 190.0000000...001.\n\ntricky difference pbinom() pnorm() recall easily, best include explanation point portfolio help carry correct analyses future!\n","code":""},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT4","chapter":"4 Revisiting Probability Distributions","heading":"4.3.4.2 Task 4: Using Figures to Calculate Cumulative Probability","text":"look distribution :\nFigure 4.9: Normal Distribution Height probability people 185cm highlighted purple, mean = 170cm SD = 7\nUsing information figure, mean SD , calculate probability associated shaded area.\nalready mean standard deviations input pnorm(), look shaded area obtain cut-point. lower.tail call set according shaded area?\nQuickfire QuestionTo three decimal places, cumulative probability shaded area Task 4? \nanswer p = .016. See solution code end chapter correct code.\n\nRemember, lower.tail set FALSE want area right.\npnorm() great telling us probability obtaining specific value greater distribution, given mean standard deviation distribution. significance come clearer coming chapters key point mind progress understanding analyses. leave now look last function normal distribution.","code":""},{"path":"revisiting-probability-distributions.html","id":"qnorm---the-quantile-function","chapter":"4 Revisiting Probability Distributions","heading":"4.3.5 qnorm() - The Quantile Function","text":"Using qnorm() can inverse pnorm(), instead finding cumulative probability given set values (cut-value), can find cut-value given desired probability. example, can use qnorm() function ask maximum IQ person bottom 10% IQ distribution (M = 100 & SD = 15)?Note: first need convert 10% probability dividing 10010% = 10 / 100 = 0.1.anyone IQ 80.8 lower bottom 10% distribution. rephrase , person bottom 10% distribution max IQ value 80.8.recap, calculated inverse cumulative density function (inverse cumulative probability) lower tail distribution, cut-probability 0.1 (10%), illustrated purple :\nFigure 4.10: Normal Distribution Height bottom 10% heights highlighted purple\n, English people can understand, means qnorm() function tells maximum value person can maintain given probability set lower.tail = TRUE. Contrastingly, pnorm() function tells minimum value person can maintain given probability set lower.tail = FALSE.","code":"\nqnorm(0.1, 100, 15) "},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT5","chapter":"4 Revisiting Probability Distributions","heading":"4.3.5.1 Task 5: Using pnorm() and qnorm() to find probability and cut-off values","text":"Calculate lowest IQ score student must top 5% distribution.Calculate lowest IQ score student must top 5% distribution.challenging: Using appropriate normal distribution function, calculate probability given student IQ 105 110, normal distribution mean = 100, sd = 15.challenging: Using appropriate normal distribution function, calculate probability given student IQ 105 110, normal distribution mean = 100, sd = 15.\nPart 1: Remember include lower.tail call required! unsure, visualise trying find (.e. lowest IQ score can top 5%) sketching normal distribution curve. may help reverse question sound like previous example.\n\nPart 2: second part, function, necessarily qnorm(), gives one value, looking separate calculation IQ. combine two values, summing subtracting ? less likely students IQ falls range cut-? try sketching trying achieve.\nQuickfire QuestionsTo one decimal place, enter answer Task 5 part 1: lowest IQ score student must top 5% distribution? one decimal place, enter answer Task 5 part 1: lowest IQ score student must top 5% distribution? two decimal places, enter answer Task 5 part 2: probability student IQ 105 110, normal distribution mean = 100, sd = 15? two decimal places, enter answer Task 5 part 2: probability student IQ 105 110, normal distribution mean = 100, sd = 15? \n\nquestion can rephrased value give 95% distribution - answer 124.7. See solution code Task 5 Question 1 end chapter.\n\n\nquestion can rephrased value give 95% distribution - answer 124.7. See solution code Task 5 Question 1 end chapter.\n\n\nuse pnorm() establish probability IQ 110. use pnorm() establish probability IQ 105. answer difference two probabilities p = .12. See solution code Task 5 Question 2 end chapter.\n\n\nuse pnorm() establish probability IQ 110. use pnorm() establish probability IQ 105. answer difference two probabilities p = .12. See solution code Task 5 Question 2 end chapter.\n","code":""},{"path":"revisiting-probability-distributions.html","id":"practice-your-skills-4","chapter":"4 Revisiting Probability Distributions","heading":"4.4 Practice Your Skills","text":"order complete exercise, first download assignment .Rmd file need edit - titled GUID_Ch4_PracticeSkills_Probabilities.Rmd. can downloaded within zip file link . downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top. Download Assignment .zip file .Now open assignment .Rmd file within RStudio. see code chunk 10 tasks. Follow instructions edit code chunk. often entering code single value based skills learnt current chapter well previous chapters.","code":""},{"path":"revisiting-probability-distributions.html","id":"topic-probabilities","chapter":"4 Revisiting Probability Distributions","heading":"4.4.1 Topic: Probabilities","text":"recapped expanded understanding probability, including number binom norm functions well basic ideas probability. need skills complete following exercises.starting, checkThe .Rmd file saved folder computer access manually set folder working directory. assessments ask save format GUID_Ch4_PracticeSkills_Probabilities.Rmd GUID replaced GUID. Though practice exercise may good practice .Note: complete code chunks replacing NULL (except library chunk appropriate code just entered). answers require coding. require code, can enter answer either code, mathematical notation, actual single value. Pay attention number decimal places required.","code":""},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueLib","chapter":"4 Revisiting Probability Distributions","heading":"4.4.2 Load in the Library","text":"good chance need tidyverse library point exercise load code chunk :Basic probability binomial distribution questionsBackground Information: conducting auditory discrimination experiment participants listen series sounds determine whether sound human . trial participants hear one brief sound (100 ms) must report whether sound human (coded 1) (coded 0). sounds either: person, animal, vehicle, tone, type sound equally likely appear.","code":"\n# hint: something to do with library() and tidyverse"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT1","chapter":"4 Revisiting Probability Distributions","heading":"4.4.3 Task 1","text":"single trial, probability sound person? Replace NULL t1 code chunk either mathematical notation single value. entering single value, give answer two decimal places.","code":"\nt1 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT2","chapter":"4 Revisiting Probability Distributions","heading":"4.4.4 Task 2","text":"sequence 4 trials, one trial sound, sampled replacement, probability following sequence sounds: animal, animal, vehicle, tone? Replace NULL t2 code chunk either mathematical notation single value. entering single value, give answer three decimal places.","code":"\nt2 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT3","chapter":"4 Revisiting Probability Distributions","heading":"4.4.5 Task 3","text":"sequence four trials, one trial sound, without replacement, probability following sequence sounds: person, tone, animal, person? Replace NULL t3 code chunk either mathematical notation single value.","code":"\nt3 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT4","chapter":"4 Revisiting Probability Distributions","heading":"4.4.6 Task 4","text":"Replace NULL code, using appropriate binomial distribution function, determine probability hearing exactly 17 'tone' trials sequence 100 trials. Assume probability tone single trial 1 4. Store output t4.","code":"\nt4 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT5","chapter":"4 Revisiting Probability Distributions","heading":"4.4.7 Task 5","text":"Replace NULL code using appropriate binomial distribution function determine probability hearing 30 'vehicle' trials sequence 100 trials. Assume probability vehicle trial one trial 1 4. Store output t5.\nHint: want upper lower tails distribution?","code":"\nt5 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT6","chapter":"4 Revisiting Probability Distributions","heading":"4.4.8 Task 6","text":"block experiment contained 100 trials, enter line code run 10000 replications one block, summing many living sounds heard replication. Code 1 living sounds (person/animal) 0 non living sounds (vehicle/tone) assume probability living sound given trial \\(p = .5\\).Normal Distribution QuestionsPreviously, Chapter 2, looked ageing research project investigating differences visual processing speed younger (M = 22 years) older adults (M = 71 years). One check experiment, prior analysis, make sure older participants show signs mild cognitive impairment (early symptoms Alzheimer's disease). , carry battery cognitive tests screen symptoms. One tests D2 test attention target cancellation task (.e., participants cross letter d's two dashes line letters). designed test peoples' selective sustained attention visual scanning speed. results test give single score Concentration Performance participant. key piece information analysis distributions D2 test scores typically normally distributed (M = 100, SD = 10).","code":"\nt6 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT7","chapter":"4 Revisiting Probability Distributions","heading":"4.4.9 Task 7","text":"Replace NULL code using appropriate function determine probability given participant D2 score 90 lower? Store output t7","code":"\nt7 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT8","chapter":"4 Revisiting Probability Distributions","heading":"4.4.10 Task 8","text":"Replace NULL code using appropriate function determine probability given participant D2 score 120 ? Store output t8","code":"\nt8 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT9","chapter":"4 Revisiting Probability Distributions","heading":"4.4.11 Task 9","text":"Replace NULL code using appropriate function(s) determine difference scores cut top 5% bottom 5% distribution? Store output t9.","code":"\nt9 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT10","chapter":"4 Revisiting Probability Distributions","heading":"4.4.12 Task 10","text":"Finally, participant says worried heard Concentration Performance bottom 2% scores distribution, maximum D2 score can ? Replace NULL single value two decimal places. enter code. Store t10Job Done - Activity Complete!Well done, finished! Now go check answers solutions end chapter. looking check questions looking single value answer , questions asking code code give answer alternative variations code allowed (e.g., including lower.tail = TRUE including default). Remember single value coded answer spelling matters, replicate() replicat(). alternative answers, means submitted one options return answer.Lastly, keep mind main point probability, interested determining probability given value distribution! .","code":"\nt10 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"solutions-to-questions-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"revisiting-probability-distributions.html","id":"continuous-data-and-normal-distribution-tasks","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1 Continuous Data and Normal Distribution Tasks","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"task-1-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.1 Task 1","text":"First set range IQ values 40 160Then plot distribution IQ_data, M = 100 SD = 10\nFigure 4.11: Distribution IQ scores mean = 100, sd = 10\nReturn Task","code":"\nlibrary(tidyverse)\n\nIQ_data <- tibble(IQ_range = c(40, 160))\n\nggplot(IQ_data, aes(IQ_range)) + \n  stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n  labs(x = \"IQ Score\", y = \"probability\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"task-2-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.2 Task 2","text":"\nFigure 4.12: Normal Distribution shown scale -10 10, mean = 0, sd = 1\nReturn Task","code":"\nND_data <- tibble(ND_range = seq(-10,10,0.05))\n\nggplot(ND_data, aes(ND_range)) + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  labs(x = \"SD units\", y = \"probability\", title = \"The Normal Distribution\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"task-3-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.3 Task 3","text":"Key thing set lower.tail FALSE calculate area. using pnorm() state actual cut-even using lower.tail = FALSE. using pbinom() state cut-minus one using lower.tail = FALSEReturn Task","code":"\npnorm(190, 170, 7, lower.tail = FALSE)## [1] 0.002137367"},{"path":"revisiting-probability-distributions.html","id":"task-4-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.4 Task 4","text":"highlighted area 185cm .Key thing set lower.tail FALSE calculate area cut-.Return Task","code":"\npnorm(185, 170, 7, lower.tail = FALSE)## [1] 0.01606229"},{"path":"revisiting-probability-distributions.html","id":"task-5-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.5 Task 5","text":"Question 1 - lowest IQ score student must top 5% distribution.Question 2 - calculate probability given student IQ 105 110, normal distribution mean = 100, sd = 15.Return Task","code":"\nqnorm(0.95, 100, 15, lower.tail = TRUE)## [1] 124.6728\npnorm(105, 100, 15, lower.tail = FALSE) - \n  pnorm(110, 100, 15, lower.tail = FALSE)## [1] 0.1169488"},{"path":"revisiting-probability-distributions.html","id":"practice-your-skills-5","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2 Practice Your Skills","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"load-in-the-library","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.1 Load in the Library","text":"Return Task","code":"\nlibrary(tidyverse)"},{"path":"revisiting-probability-distributions.html","id":"task-1-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.2 Task 1","text":"probability sound person 0.75Return Task","code":"\nt1 <- 3/4\nt1 <- .75"},{"path":"revisiting-probability-distributions.html","id":"task-2-4","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.3 Task 2","text":"probability sequence sounds 0.004Return Task","code":"\nt2 <- (1/4) * (1/4) * (1/4) * (1/4)\nt2 <- .004"},{"path":"revisiting-probability-distributions.html","id":"task-3-4","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.4 Task 3","text":"probability sequence sounds 0The reason replacement repeat person trial happen.Return Task","code":"\nt3 <- (1/4) * (1/3) * (1/2) * (0/1)\nt3 <- 0"},{"path":"revisiting-probability-distributions.html","id":"task-4-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.5 Task 4","text":"Assuming probability tone given trial 1 4, probability hearing 17 'tone' trials sequence 100 trials 0.0165156Return Task","code":"\nt4 <- dbinom(17, 100, 1/4)"},{"path":"revisiting-probability-distributions.html","id":"task-5-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.6 Task 5","text":"answered using either pbinom() dbinom(). trick remember set cut-depending function used.scenario, probability hearing 30 'vehicle' trials sequence 100 trials 0.149541Return Task","code":"\nt5 <- pbinom(29, 100, 1/4, lower.tail = FALSE)\nt5 <- dbinom(30:100, 100, 1/4) %>% sum()"},{"path":"revisiting-probability-distributions.html","id":"task-6-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.7 Task 6","text":"appropriate code :look output see something like following. Remember numbers vary due random sampling. showing first 10 values 10000Return Task","code":"\nt6 <- replicate(10000, sample(0:1, 100, TRUE, c(.5,.5)) %>% sum())##  int [1:10000] 44 54 47 52 54 45 54 48 48 55 ..."},{"path":"revisiting-probability-distributions.html","id":"task-7-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.8 Task 7","text":"probability given participant D2 score 90 lower 0.1586553Return Task","code":"\nt7 <- pnorm(90, 100, 10, lower.tail = TRUE)"},{"path":"revisiting-probability-distributions.html","id":"task-8-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.9 Task 8","text":"probability given participant D2 score 120 0.0227501Return Task","code":"\nt8 <- pnorm(120, 100, 10, lower.tail = FALSE)"},{"path":"revisiting-probability-distributions.html","id":"task-9","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.10 Task 9","text":"difference scores cut top bottom 5% distribution 32.8970725Return Task","code":"\nt9 <- qnorm(.95, 100, 10) - qnorm(.05, 100, 10)"},{"path":"revisiting-probability-distributions.html","id":"task-10","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.11 Task 10","text":"maximum D2 score can situation 79.46Return TaskChapter Complete!","code":"\nt10 <- 79.46"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"nhst-binomial-test-and-one-sample-t-test","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5 NHST: Binomial test and One-Sample t-test","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"overview-5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.1 Overview","text":"chapter introduce Null Hypothesis Significance Testing (NHST): quantitative research collect sample, calculate summary statistic sample, use probability establish likelihood statistic occurring given certain situations. aim draw inferences sample population.However concepts ideas hard grasp first take playing around data times help get better understanding . , demonstrate ideas , start introducing commonly used tests approaches reproducible science, look data related sleep. study look , explore NHST , one team members makes use well known task Psychology, Posner Paradigm: Woods et al. (2009). clock focus selective attention primary insomnia: experimental study using modified Posner paradigm .chapter, activities, :Introduce testing hypothesis null hypothesis significance testing (NHST).Learn Binomial tests.Learn One-sample t-tests.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"brief-introduction-to-nhst","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.2 Brief Introduction to NHST","text":"short introduction concept NHST dive practical aspects. main idea NHST testing significant difference two values - say two means moment. null hypothesis states difference two means (groups) interest. , test difference means two groups trying determine probability finding difference size found, larger, sample using experiment, actually real difference two groups population.say ran experiment collected sample : experiment, two groups, B, calculated difference means two groups D_diff = 7.39. Putting terms Null Hypothesis (H_0) now wanting know: probability finding difference means -7.39 (larger) real difference two groups population? order test question (Null Hypothesis) need compare observed difference distribution possible differences see likely observed difference distribution - extreme values, .e., large differences groups, located tails distribution.p-value probability finding difference equal greater one found sample difference population. Thus, say p-value p = .017. indicates small probability finding difference equal greater population difference. obtained p-value also smaller standard cut-use Psychology \\(p <= .05\\). reject null hypothesis suggest significant difference two groups.following resource Daniel Lakens helpful deepening understanding p-value recommend taking look : Understanding common misconceptions p-values","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"background-of-data-sleep","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.3 Background of data: Sleep","text":"Woods colleagues (2009) interested attention people poor sleep (Primary Insomnia - PI) tuned towards sleep-related stimuli attention people normal sleep (NS). Woods et al. hypothesised participants poor sleep attentive images related lack sleep (.e., alarm clock showing 2AM) participants normal sleep . test hypothesis, authors used modified Posner paradigm, shown Figure 1 paper, images alarm clock acted cue valid invalid trials, symbol ( .. ) target.can seen Figure 3 Woods et al. found , valid trials, whilst Primary Insomnia participants faster responding target, suggesting slight increase attention sleep related cue compared Normal Sleepers, difference groups. contrast, invalid trials, poor sleep participants expected distracted cue, authors indeed find significant difference groups consistent alternative hypothesis \\(H_{1}\\). Woods et al., concluded poor sleepers (Primary Insomnia participants) slower respond target invalid trials, compared Normal Sleepers, due attention Primary Insomnia participants drawn misleading cue (alarm clock) invalid trials. increased attention sleep-related cue led overall slower response target invalid trials.Now, imagine looking replicate finding Woods et al. (2009). pilot study, test recruitment procedures, gather data 22 Normal Sleepers. common use control participants pilot (case NS participants) plentiful population experimental group (case PI participants) saves using participants PI group may harder obtain long run.gathering data, want check recruitment process: Whether able draw sample normal sleepers similar sample drawn Woods et al. keep things straightforward, allowing us understand analyses better, look valid trials today, NS participants, effect perform test groups conditions.Participants Normal Sleepers (NS)?data 22 participants collected pilot study. mean reaction time valid trials (milliseconds) shown right hand column, valid_rt.\nTable 5.1: Pilot Data 22 Participants Sleep-Related Posner Paradigm. ID shown participant column mean reaction time (ms) valid trails shown valid_rt column.\nlook Woods et al (2009) Figure 3 see , valid trials, mean reaction time NS participants 590 ms SD = 94 ms. , part pilot study, want confirm 22 participants gathered indeed Normal Sleepers. use mean SD Woods et al., confirm . Essentially asking participants pilot responding similar fashion NS participants original study.using NHST working null hypothesis (\\(H_{0}\\)) alternative hypothesis (\\(H_{1}\\)). Thinking experiment makes logical sense think terms null hypothesis (\\(\\mu1 = \\mu2\\)). phrase hypothesis : \"hypothesise significant difference mean reaction times valid trials modified Posner task participants pilot study participants original study Woods et al.\"actually ways test null hypothesis. Today show two : tasks 1-3 use binomial test tasks 4-8 use one-sample t-test\nBinomial test simple test converts participants either cut-point, e.g. mean value, looking probability finding number participants cut-.\n\nOne-sample t-test similar compares participants cut-, compares mean standard deviation collected sample ideal mean standard deviation. comparing difference means, divided standard deviation difference (measure variance), can determine sample similar ideal mean.\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-binomial-test","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4 The Binomial Test","text":"Binomial test one \"basic tests\" null hypothesis testing uses little information. binomial test used study two possible outcomes (success failure) idea probability success . sound familiar work Chapter 4 Binomial distribution.Binomial test tests observed result different expected. example, number heads series coin flips different expected. case chapter, want test whether normal sleepers giving reaction times different measured Woods et al. following tasks take process.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4.1 Task 1: Creating a Dataframe","text":"First need create tibble data can work .Enter data 22 participants displayed tibble store ns_data. one column showing participant number (called participant) another column showing mean reaction time (called valid_rt).ns_data <- tibble(participant = c(NULL,NULL,...), valid_rt = c(NULL,NULL,...))type value copy paste hint .\ncan use code structure replace NULL values:\n\nns_data <- tibble(participant = c(NULL,NULL,...), valid_rt = c(NULL,NULL,...))\n\nvalues : 631.2, 800.8, 595.4, 502.6, 604.5, 516.9, 658.0, 502.0, 496.7, 600.3, 714.6, 623.7, 634.5, 724.9, 815.7, 456.9, 703.4, 647.5, 657.9, 613.2, 585.4, 674.1\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT2","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4.2 Task 2: Comparing Original and New Sample Reaction Times","text":"next step establish many participants pilot study mean original study Woods et al.original study mean reaction time valid trials 590 ms. Store value woods_mean.Now write code calculate number participants new sample (ns_data created Task 1) mean reaction time greater original paper's mean. Store single value n_participants.function nrow() may help .nrow() similar count() n(), nrow() returns number single value tibble.sure whatever method use end single value, tibble. may need use pull() pluck().\nPart 1\n\nwoods_mean <- value\n\nPart 2\n\nways achieve . couple try\n\nns_data %>% filter(x ? y) %>% count() %>% pull(?)\n\n\n\nns_data %>% filter(x ? y) %>% summarise(n = ?) %>% pull(?)\n\n\n\nns_data %>% filter(x ? y) %>% nrow()\n\n\n\ndim[] %>% pluck()\nQuickfire QuestionsThe number participants mean reaction time valid trials greater original paper : 6101616","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4.3 Task 3: Calculating Probability","text":"final step binomial test compare value Task 2, 16 participants, hypothetical cut-. work assumption mean reaction time original paper, .e. 590 ms, good estimate population good sleepers (NS). true new participant tested .5 chance mean reaction time (\\(p = .5\\) participant).phrase another way, expected number participants cut-\\(.5 \\times N\\), \\(N\\) number participants, \\(.5 \\times 22\\) = 11 participants.\n* Calculate probability observing least 16 participants 22 participants valid_rt greater Woods et al (2009) mean value.\n* hint: looked similar questions Chapter 4 using dbinom() pbinom()\n* hint: key thing asking obtaining X successes. need think back cut-offs lower.tails.\nThink back Chapter 4 used binomial distribution. question can phrased , probability obtaining X successes Y trials, given expected probability Z.\n\nmany Xs? (see question)\n\nmany Ys? (see question)\n\nprobability either mean/cut-? (see question)\n\ncan use dbinom() %>% sum() maybe pbinom()\nQuickfire QuestionsUsing Psychology standard \\(\\alpha = .05\\), think NS participants responding similar fashion participants original paper? Select appropriate answer: NoYesAccording Binomial test accept reject null hypothesis set start test? RejectAccept\nprobability obtaining 16 participants mean reaction time greater cut-590 ms p = .026. smaller field norm p = .05. can say , using binomial test, new sample appears significantly different old sample significantly larger number participants cut-(M = 590ms) expected new sample old sample responding similar fashion. therefore reject null hypothesis!\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-one-sample-t-test","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5 The One-Sample t-test","text":"binomial test null hypothesis testing suggested significant difference mean reaction times valid trials modified Posner task participants pilot study participants original study Woods et al. However, binomial test use available information data participant simply classified mean original paper, .e. yes . Information magnitude discrepancy mean discarded. information really interesting important however wanted maintain information need use One-sample \\(t\\)-test.One-sample \\(t\\)-test, test null hypothesis \\(H_0: \\mu = \\mu_0\\) :\\(H_0\\) symbol null hypothesis,\\(\\mu\\) (pronounced mu - like m) unobserved population mean. unobserved true mean possible participants. know value. best guess mean sample 22 participants use mean . substitute value formula, call \\(\\bar{X}\\) (pronounced X-bar), instead \\(\\mu\\).\\(\\mu_0\\) (mu-zero) mean compare (alternative population sample mean constant). us mean original paper observed 590 ms.calculating test statistic \\(t\\) comes \\(t\\)-distribution - distribution lectures. formula calculate observed test statistic \\(t\\) one-sample \\(t\\)-test :\\[t = \\frac{\\mu - \\mu_0}{s\\ / \\sqrt(n)}\\]\\(s\\) standard deviation sample collected,\\(n\\) number participants sample., testing null hypothesis \\(H_0: \\bar{X} =\\) 590. formula one-sample \\(t\\)-test becomes:\\[t = \\frac{\\bar{X} - \\mu_0}{s\\ / \\sqrt(n)}\\]Now just need fill numbers.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.1 Task 4: Calculating the Mean and Standard Deviation","text":"Calculate mean standard deviation valid_rt 22 participants (.e., participant data).Store mean ns_data_mean store standard deviation ns_data_sd. Make sure store single values!\ncode, replace NULL code find mean, m, ns_data.\n\nns_data_mean <- summarise(NULL) %>% pull(NULL)\n\nReplace NULL code find standard deviation, sd, ns_data.\n\nns_data_sd <- summarise(NULL) %>% pull(NULL)\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.2 Task 5: Calculating the Observed Test Statistic","text":"Task 4, found \\(\\bar{X}\\), sample mean, 625.464 ms, \\(s\\), sample standard deviation, 94.307 ms. Now, keeping mind \\(n\\) number observations/participants sample, \\(\\mu_0\\) mean Woods et al. (2009):Use One-sample t-test formula compute observed test statistic. Store answer t_obs .t_obs <- (x - y)/(s/sqrt(n))Quickfire QuestionsAnswering question help task also need numbers substitute formula:mean Woods et al. (2009) 595590580585, number participants sample : (type numbers) .Remember solutions end chapter stuck. check correct without looking solutions though - observed \\(t\\)-value t_obs, two decimal places, 1.661.761.861.96\nRemember BODMAS /PEDMAS given one operation calculate. (.e. Brackets/Parenthesis, Orders/Exponents, Division, Multiplication, Addition, Subtraction)\n\nt_obs <- (sample mean - woods mean) / (sample standard deviation / square root n)\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT6","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.3 Task 6: Comparing the Observed Test Statistic to the t-distribution using pt()","text":"Now need compare t_obs t-distribution determine likely observation (.e. test statistic) null hypothesis difference. need use pt() function.Use pt() function get \\(p\\)-value two-tailed test \\(\\alpha\\) level set .05. test \\(n - 1\\) degrees freedom, \\(n\\) number observations contributing sample mean \\(\\bar{X}\\). Store \\(p\\) value variable pval.reject null?Hint: pt() function works similar pbinom() pnorm().Hint: want p-value two-tailed test, multiply pt() two.\nRemember get help can enter ?pt console.\n\npt() function works similar pbinom() pnorm():\n\npval <- pt(test statistic, df, lower.tail = FALSE) * 2\n\nUse absolute value test statistic; .e. ignore minus signs.\n\nRemember, df equal n-1.\n\nUse lower.tail = FALSE wanting know probability obtaining value higher one got.\n\nReject null field standard p < .05\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT7","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.4 Task 7: Comparing the Observed Test Statistic to the t-distribution using t.test()","text":"Now done hand, try using t.test() function get result. Take moment read documentation function typing ?t.test console window. need store t-test output dataframe, check p-value matches pval Task 6.structure t.test() function t.test(column_of_data, mu = mean_to_compare_against)\nfunction requires vector, table, first argument. can use pull() function pull valid_rt column tibble ns_data pull(ns_data, valid_rt).\n\nalso need include mu t.test(), mu equal mean comparing .\nQuickfire QuestionsTo make sure understanding output t-test, try answer following questions.three decimal places, type p-value t-test Task 7 three decimal places, type p-value t-test Task 7 One-sample t-test significantnot significantAs One-sample t-test significantnot significantThe outcome binomial test one sample t-test produce samea different answerThe outcome binomial test one sample t-test produce samea different answer","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT8","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.5 Task 8: Drawing Conclusions about the new data","text":"Given results, conclude similar 22 participants original participants Woods et al (2009) whether managed recruit sleepers similar study?Think test used available information?Also, reliable finding two tests give different answers?given thoughts end chapter.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"practice-your-skills-6","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6 Practice Your Skills","text":"activity uses open data Experiment 1 Mehr, Song, Spelke (2016). exercise taken Open Stats Lab Trinity University, US.order complete tasks need download data .csv file .Rmd file, need edit, titled Ch5_PracticeSkills_Template.Rmd. can downloaded within zip file link . downloaded unzipped, create new folder use working directory; put data file .Rmd file folder set working directory folder drop-menus top. Download Exercises .zip file .Now open .Rmd file within RStudio. see code chunk task. Follow instructions edit code chunk. exercises section guide data wrangling, run One-Sample t-tests published data, visualise data. Thus, can check published paper see able replicate results.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"does-music-convey-social-information-to-infants","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.1 Does Music Convey Social Information to Infants?","text":"Parents often sing children , even infants, children listen look parents singing. Research Mehr, Song, Spelke (2016) sought explore psychological function music parents infants, examining hypothesis particular melodies convey important social information infants. Specifically, melodies convey information social affiliation.authors argue melodies shared within social groups. Whereas children growing one\nculture may exposed certain songs infants (e.g., “Rock--bye Baby”), children growing \ncultures (even groups within culture) may exposed different songs. Thus, novel person (someone infant never seen ) sings familiar song, may signal infant new person member social group.test hypothesis, researchers recruited 32 infants parents complete \nexperiment. first visit lab, parents taught new lullaby (one neither infants heard ). experimenters asked parents sing new lullaby child every day next 1-2 weeks.Following 1-2 week exposure period, parents infant returned lab complete\nexperimental portion study. Infants first shown screen side--side videos \ntwo unfamiliar people, silently smiling looking infant. researchers\nrecorded looking behavior (gaze) infants ‘baseline’ phase. Next, one one, two unfamiliar people screen sang either lullaby parents learned different lullaby (lyrics rhythm, different melody). Finally, infants saw silent video used baseline, researchers recorded looking behavior infants ‘test’ phase. details experiment’s methods, please refer Mehr et al. (2016) Experiment 1.exercise looking baseline data test trial data gazing proportion specifically.starting check:.csv file saved folder computer manually set folder working directory..csv file saved folder computer manually set folder working directory..Rmd file saved folder .csv files..Rmd file saved folder .csv files.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT0","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.2 Load in the data","text":"Call tidyverse library() load data (socialmelodies_exp1.csv) store object melodydata.View dataIt always good idea familiarise layout data just loaded . can using glimpse() View() Console window. Note, analyse variables. Try find variables relevant study description .Tasks:Now data loaded, tidyverse attached, viewed data, now try complete following tasks. Go tasks change NULL question asks make sure file knits end fully reproducible code.","code":"\nlibrary(\"tidyverse\")\n\nmelodydata <- NULL"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.3 Task 1 - Filter data","text":"data file includes variables 5 experiments reported paper. want\nanalyze data Experiment 1. Using filter() function, create new data frame melodydata_exp1 contains data Experiment 1 (variable exp1 value 1 indicates data fro Experiment 1). Hint: new data frame 32 observations.","code":"\nmelodydata_exp1 <- NULL"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT2","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.4 Task 2 - Select data","text":"noted , using variables melodydata_exp1 order get better overview variables particularly interested , want create object contains variables focusing .context:\n* First, want show infants' looking behavior differ chance \nbaseline trial (Baseline_Proportion_Gaze_to_Singer). words, infants show attentional bias prior hearing unfamiliar others sign song.\n* Second, want examine whether proportion infants' looking\nbehaviour toward singer familiar melody (Test_Proportion_Gaze_to_Singer) higher chance test phase.Thus, create new data frame called melodydata_exp1_reduced contains variables id, Baseline_Proportion_Gaze_to_Singer, Test_Proportion_Gaze_to_Singer.Hint: new data frame contains 32 observations 3 variables.","code":"\nmelodydata_exp1_reduced <- NULL"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.5 Task 3 - Testing baseline gazing proportion","text":"Perform One-sample t-test examine whether proportion time spent looking person singing familiar song baseline differ chance (0.5).","code":"\nt.test(pull(NULL, NULL), mu = NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.6 Task 4 - Testing test trial gazing proportion","text":"Now, perform One-sample t-test examine whether proportion infants' looking\nbehaviour toward singer familiar melody higher chance test phase\n(0.5).","code":"\nt.test(pull(NULL, NULL), mu = NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.7 Task 5 - Gathering data into a different format","text":"want create boxplot depict proportion time infants spent looking singer familiar song baseline test trials. However, can , need slightly change format melodydata_exp1_reduced data frame: need represent cases (individuals) twice figure, need reorganise data gaze proportions one variable (Proportion), separate variable indicating whether belongs baseline test trial (TrialType). Put differently, participant represented two rows; one row baseline proportion another row test trial proportion.Store new data frame object melodydata_exp1_wide.Hint: need pivot_longer function step.\nHint: new data frame 64 observations 3 variables (id, TrialType, Proportion).","code":"\nmelodydata_exp1_wide <- melodydata_exp1_reduced %>% \n  pivot_longer(cols = NULL,\n               names_to = NULL,\n               values_to = NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT6","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.8 Task 6 - Visualise baseline and test trial data","text":"Generate boxplot depict proportion time infants spent looking singer \nfamiliar song baseline test trials.Turn legend using guides() needed x-axis tells trial .","code":"\nggplot(NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT7","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.9 Task 7 - Compare your analyses with the analyses reported in the published paper","text":"Finally, check results section Experiment 1 published paper compare results analyses ones reported paper: Mehr, Song, Spelke (2016). find?Well done, finished! Make sure knit .Rmd file. successfully replicated analyses data visualisation published paper. Check answers solutions end chapter, .","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"solutions-to-questions-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-binomial-test-1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1 The Binomial Test","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-1-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1.1 Task 1","text":"Return Task","code":"\nns_data <- tibble(participant = 1:22,\n                  valid_rt = c(631.2,800.8,595.4,502.6,604.5,\n                               516.9,658.0,502.0,496.7,600.3,\n                               714.6,623.7,634.5,724.9,815.7,\n                               456.9,703.4,647.5,657.9,613.2,\n                               585.4,674.1))"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-2-5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1.2 Task 2","text":"Giving n_participants value 16Return Task","code":"\nwoods_mean <- 590\n\nn_participants <- ns_data %>%\n  filter(valid_rt > woods_mean) %>%\n  nrow()"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-3-5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1.3 Task 3","text":"can use density function:, cumulative probability function:, plug numbers directly code:, finally, remembering need specify value lower minimum participant number lower.tail = FALSE.better practice use first two solutions, pull values straight ns_data, run risk entering error code plug values manually.Return Task","code":"\nsum(dbinom(n_participants:nrow(ns_data), nrow(ns_data), .5))## [1] 0.0262394\npbinom(n_participants - 1L, nrow(ns_data), .5, lower.tail = FALSE)## [1] 0.0262394\nsum(dbinom(16:22,22, .5))## [1] 0.0262394\npbinom(15, 22, .5, lower.tail = FALSE)## [1] 0.0262394"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-one-sample-t-test-1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2 The One-Sample t-test","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-4-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.1 Task 4","text":"ns_data_mean use summarise() calculate mean pull() value.ns_data_sd use summarise() calculate sd pull() value.NOTE: print screen wanted \"\\n\" end line symbol print different linesReturn Task","code":"\n# the mean\nns_data_mean <- ns_data %>%\n  summarise(m = mean(valid_rt)) %>%\n  pull(m)  \n\n# the sd\nns_data_sd <- ns_data %>%\n  summarise(sd = sd(valid_rt)) %>%\n  pull(sd)\ncat(\"The mean number of hours was\", ns_data_mean, \"\\n\")\ncat(\"The standard deviation was\", ns_data_sd, \"\\n\")## The mean number of hours was 625.4636 \n## The standard deviation was 94.30693"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-5-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.2 Task 5","text":"Giving t_obs value 1.7638067Return Task","code":"\nt_obs <- (ns_data_mean - woods_mean) / (ns_data_sd / sqrt(nrow(ns_data)))"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-6-3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.3 Task 6","text":"using values straight ns_data, multiplying 2 two-tailed test, following:Giving pval 0.0923092But can also get answer plugging values - though method runs risk error better using first calculation values come straight ns_data. :Giving pval 0.0923092Return Task","code":"\npval <- pt(abs(t_obs), nrow(ns_data) - 1L, lower.tail = FALSE) * 2L\npval2 <- pt(t_obs, 21, lower.tail = FALSE) * 2"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-7-3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.4 Task 7","text":"t-test run follows, output shown :Return Task","code":"\nt.test(pull(ns_data, valid_rt), mu = woods_mean)## \n##  One Sample t-test\n## \n## data:  pull(ns_data, valid_rt)\n## t = 1.7638, df = 21, p-value = 0.09231\n## alternative hypothesis: true mean is not equal to 590\n## 95 percent confidence interval:\n##  583.6503 667.2770\n## sample estimates:\n## mean of x \n##  625.4636"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-8-3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.5 Task 8","text":"According one-sample t-test participants responding similar manner participants original study, , may inclined assume recruitment process pilot experiment working well.However, according binomial test participants responding differently original sample. test result take finding?Keep mind binomial test rough categorises participants yes . one-sample t-test uses much available data degree give accurate answer. However, fact two tests give really different answers may give reason question whether results stable potentially look gather larger sample get accurate representation population.Return Task","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"practice-your-skills-7","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3 Practice Your Skills","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"load-in-the-data-1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.1 Load in the data","text":"Return Task","code":"\nlibrary(\"tidyverse\")\n\nmelodydata <- read_csv(\"socialmelodies_exp1.csv\")"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-1---filter-data","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.2 Task 1 - Filter data","text":"Return Task","code":"\nmelodydata_exp1 <- melodydata %>%\n  filter(exp1 == 1)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-2---select-data","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.3 Task 2 - Select data","text":"Return Task","code":"\nmelodydata_exp1_reduced <- melodydata_exp1 %>%\n  select(id, Baseline_Proportion_Gaze_to_Singer, Test_Proportion_Gaze_to_Singer)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-3---testing-baseline-gazing-proportion","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.4 Task 3 - Testing baseline gazing proportion","text":"Return Task","code":"\nt.test(pull(melodydata_exp1_reduced, Baseline_Proportion_Gaze_to_Singer), mu = 0.5)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-4---testing-test-trial-gazing-proportion","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.5 Task 4 - Testing test trial gazing proportion","text":"Return Task","code":"\nt.test(pull(melodydata_exp1_reduced, Test_Proportion_Gaze_to_Singer), mu = 0.5)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-5---gathering-data-into-a-different-format","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.6 Task 5 - Gathering data into a different format","text":"Return Task","code":"\nmelodydata_exp1_wide <- melodydata_exp1_reduced %>% \n  pivot_longer(cols = Baseline_Proportion_Gaze_to_Singer:Test_Proportion_Gaze_to_Singer,\n               names_to = \"TrialType\",\n               values_to = \"Proportion\")"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-6---visualise-baseline-and-test-trial-data","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.7 Task 6 - Visualise baseline and test trial data","text":"Return Task","code":"\nggplot(data = melodydata_exp1_wide, \n       aes(x = TrialType, \n           y = Proportion, \n           fill = TrialType)) + \n  geom_boxplot() +\n  guides(fill = FALSE)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-7---compare-your-analyses-with-the-analyses-reported-in-the-published-paper","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.8 Task 7 - Compare your analyses with the analyses reported in the published paper","text":"looking results section see two reported one-sample t-tests published paper produced part exercise. addition, Fig 2a published paper resemble . Well done!Return Task","code":""},{"path":"nhst-two-sample-t-test.html","id":"nhst-two-sample-t-test","chapter":"6 NHST: Two-Sample t-test","heading":"6 NHST: Two-Sample t-test","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"overview-6","chapter":"6 NHST: Two-Sample t-test","heading":"6.1 Overview","text":"previous chapter looked situation gathered one sample data (one group participants) compared one group known value, e.g. standard value. extension design gather data two samples. Two-sample designs common Psychology often want know whether difference groups particular variable. Today look scenario closely \\(t\\)-test analysis.Comparing Means Two SamplesFirst thing note different types two-sample designs depending whether two groups independent (e.g., different participants different conditions) (e.g., participants different conditions). today's chapter focus independent samples, typically means observations two groups unrelated - usually meaning different people. next chapter, examine cases observations two groups pairs (paired samples) - often people, also matched-pairs design.","code":""},{"path":"nhst-two-sample-t-test.html","id":"background-of-data-speech-as-indicator-of-intellect","chapter":"6 NHST: Two-Sample t-test","heading":"6.2 Background of data: Speech as indicator of intellect","text":"lab revisiting data Schroeder Epley (2015), first encountered part homework Chapter 5. can take look Psychological Science article :Schroeder, J. Epley, N. (2015). sound intellect: Speech reveals thoughtful mind, increasing job candidate's appeal. Psychological Science, 26, 277--891.abstract article explains different experiments conducted (specifically looking dataset Experiment 4, courtesy Open Stats Lab):\"person's mental capacities, intellect, observed directly instead inferred indirect cues. predicted person's intellect conveyed strongly cue closely tied actual thinking: voice. Hypothetical employers (Experiments 1-3b) professional recruiters (Experiment 4) watched, listened , read job candidates' pitches hired. evaluators (employers) rated candidate competent, thoughtful, intelligent heard pitch rather read , result, favorable impression candidate interested hiring candidate. Adding voice written pitches, trained actors (Experiment 3a) untrained adults (Experiment 3b) read , produced results. Adding visual cues audio pitches alter evaluations candidates. conveying one's intellect, important one's voice, quite literally, heard.\"recap Experiment 4, 39 professional recruiters Fortune 500 companies evaluated job pitches M.B.. candidates (Masters Business Administration) University Chicago Booth School Business. methods results appear pages 887--889 article want look specifically details. original data, wide format, can found Open Stats Lab website later self-directed learning. Today however, working modified version \"tidy\" format can downloaded . unsure tidy format, refer back activities Chapter 2.","code":""},{"path":"nhst-two-sample-t-test.html","id":"the-two-sample-t-test","chapter":"6 NHST: Two-Sample t-test","heading":"6.3 The Two-Sample t-test","text":"overall goal today learn running \\(t\\)-test -subjects data, well learning analysing actual data along way. , task today reproduce figure results article (p. 887-888). two packages need tidyverse, used lot, broom, new , become friend. One main functions use broom broom::tidy() - incredibly useful function converts output inferential test R combination text lists, really hard work - technically called objects - tibble familiar can use much easily. show use function today ask use coming chapters.","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT1","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.1 Task 1: Evaluators","text":"Open new R Markdown file use code call broom tidyverse library.Note: Order important calling multiple libraries - two libraries function named thing, R use function library loaded last. recommend calling libraries order tidyverse called last functions library used often.file called evaluators.csv contains demographics 39 raters. downloading unzipping data, course setting working directory, load information file store tibble called evaluators.file called evaluators.csv contains demographics 39 raters. downloading unzipping data, course setting working directory, load information file store tibble called evaluators.Now, use code :\ncalculate overall mean standard deviation age evaluators.\ncount many male many female evaluators study.\nNow, use code :calculate overall mean standard deviation age evaluators.count many male many female evaluators study.Note: Probably easier task separate lines code. NAs data need include call na.rm = TRUE.\nRemember load libraries need!\n\nAlso make sure downloaded saved data folder working .\n\ncan use summarise() count() pipeline group_by() complete task.\n\nanalysing number male female evaluators, initially clear '1' represents males '2' represents females.\n\ncan use recode() convert numeric names indicate something meaningful. look ?recode see can work use . 'll help use mutate() create new variable recode() numeric names evaluators.\n\nwebsite also incredibly useful one save anytime need use recode(): https://debruine.github.io/posts/recode/\n\nanalysis future reproducible analyses, good idea make representations clearer others.\nQuickfire QuestionsFill answers check calculations correct:mean age evaluators study? Type answer one decimal place: standard deviation age evaluators study? Type answer two decimal places: many participants noted female: many participants noted male: Thinking Cap PointThe paper claims mean age evaluators 30.85 years (SD = 6.24) 9 male 30 female evaluators. agree? might differences?\npaper claimed 9 males. However, looking results can see 4 males, 5 NA entries making rest participant count. looks like NA male entries combined! information might clear person re-analysing data.\n\nimportant reproducible data analyses others examine. another pair eyes examining data can beneficial spotting discrepancies - allows critical evaluation analyses results improves quality research published. reason emphasize importance conducting replication studies!\n","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT2","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.2 Task 2: Ratings","text":"now going calculate overall intellect rating given evaluator. break bit, going calculate intellectual evaluators (raters) thought candidates overall, depending whether evaluators read listened candidates' resume pitches. calculated averaging ratings competent, thoughtful intelligent evaluator held within ratings.csv.Note: looking ratings individual candidates; looking overall ratings evaluator. bit confusing makes sense stop think little. can think terms \"raters rate differently depending whether read listen resume pitch\".combine overall intellect rating overall impression ratings overall hire ratings evaluator, ready found ratings.csv. end new tibble - call ratings2 - structure:eval_id shows evaluator ID. evaluator different ID. 1's evaluator.Category shows scale rating - intellect, hire, impressionRating shows overall rating given evaluator given scale.condition shows whether evaluator listened (e.g., evaluators 1, 2 3), read (e.g., evaluator 4) resume.following steps describe create tibble, might want bash without reading first. trick data analysis data wrangling first think want achieve - end goal - function need use. know want end - table - now get ?Steps 1-3 calculate new intellect rating. Steps 4 + 5 combine rating information.Load data found ratings.csv tibble called ratings.Load data found ratings.csv tibble called ratings.filter() relevant variables (thoughtful, competent, intelligent) new tibble (call like - solutions use iratings), calculate mean Rating evaluator.filter() relevant variables (thoughtful, competent, intelligent) new tibble (call like - solutions use iratings), calculate mean Rating evaluator.Add new column called Category every entry word intellect. tells us every number tibble intellect rating.Add new column called Category every entry word intellect. tells us every number tibble intellect rating.Now create new tibble called ratings2 filter just impression hire ratings original ratings tibble. Next, bind tibble tibble created step 3 bring together intellect, impression, hire ratings, ratings2.Now create new tibble called ratings2 filter just impression hire ratings original ratings tibble. Next, bind tibble tibble created step 3 bring together intellect, impression, hire ratings, ratings2.Join ratings2 evaluator tibble created Task 1. Keep necessary columns shown arrange Evaluator Category.Join ratings2 evaluator tibble created Task 1. Keep necessary columns shown arrange Evaluator Category.forget use hints solution end Chapter stuck. Take one step time.\n\nMake sure downloaded saved data folder working .\n\n\nMake sure downloaded saved data folder working .\n\n\nfilter(Category %% c()) might work use group_by() summarize() calculate mean Rating evaluator.\n\n\nfilter(Category %% c()) might work use group_by() summarize() calculate mean Rating evaluator.\n\n\nUse mutate() create new column.\n\n\nUse mutate() create new column.\n\n\nbind_rows() Chapter 2 help combine variables two separate tibbles.\n\n\nbind_rows() Chapter 2 help combine variables two separate tibbles.\n\n\nUse inner_join() common column tibbles. select() arrange() help , .\n\n\nUse inner_join() common column tibbles. select() arrange() help , .\n","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT3","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.3 Task 3: Creating a Figure","text":"recap, now ratings2 contains overall Rating score evaluator three Category (within: hire, impression, intellect) depending condition evaluator (: listened read). Great! Now information need replicate Figure 7 article (page 888), shown :\nFigure 6.1: Figure 7 Schroeder Epley (2015) try replicate.\nReplace NULLs create basic version figure.Thinking Cap PointImprove Figure: improve plot. geom_() options try? bar charts informative something else better? add change labels plot? change colours figure?Next, look possible solution see modern way presenting information. new functions solution play understand . Remember layering system, remove lines see happens. Note solution Figure shows raw data points well means condition; gives better impression true data just showing means can misleading. can continue exploration visualisations reading paper later chance: Weissberger et al., 2015, Beyond Bar Line Graphs: Time New Data Presentation ParadigmFilling code create basic figure shown:\nFigure 6.2: basic solution Figure 7\nalternatively, modern presentation data:\nFigure 6.3: possible alternative Figure 7\n","code":"\ngroup_means <- group_by(ratings2, NULL, NULL) %>%\n  summarise(Rating = mean(Rating))\n\nggplot(group_means, aes(NULL, NULL, fill = NULL)) +\n  geom_col(position = \"dodge\")\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(Rating = mean(Rating))## `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\nggplot(group_means, aes(Category, Rating, fill = condition)) + \n  geom_col(position = \"dodge\")\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(Rating = mean(Rating))## `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\nggplot(ratings2, aes(condition, Rating, color = condition)) +\n  geom_jitter(alpha = .4) +\n  geom_violin(aes(fill = condition), alpha = .1) +\n  facet_wrap(~Category) +\n  geom_point(data = group_means, size = 2) +\n  labs(x = \"Category\", y = \"Recruiters' Evaluation of Candidates\") +\n  coord_cartesian(ylim = c(0, 10), expand = FALSE) +\n  guides(color = \"none\", fill = \"none\") +\n  theme_bw()"},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT4","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.4 Task 4: t-tests","text":"Brilliant! far checked descriptives visualisations, last thing now check inferential tests; t-tests. still ratings2 stored Task 2. tibble, reproduce t-test results article time show run t-test. can refer back lectures understand maths -subjects t-test, essentially measure difference means variance means.paragraph paper describing results (p. 887):\"pattern evaluations professional recruiters replicated pattern observed Experiments 1 3b (see Fig. 7). particular, recruiters believed job candidates greater intellect---competent, thoughtful, intelligent---listened pitches (M = 5.63, SD = 1.61) read pitches (M = 3.65, SD = 1.91), t(37) = 3.53, p < .01, 95% CI difference = [0.85, 3.13], d = 1.16. recruiters also formed positive impressions candidates---rated likeable positive less negative impression ---listened pitches (M = 5.97, SD = 1.92) read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p < .01, 95% CI difference = [0.55, 3.24], d = 0.94. Finally, also reported likely hire candidates listened pitches (M = 4.71, SD = 2.26) read pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p < .01, 95% CI difference = [0.41, 3.24], d = 0.86.\"going run t-tests Intellect, Hire Impression; time comparing evaluators overall ratings listened group versus overall ratings read group see significant difference two conditions: .e., evaluators listened pitches give significant higher lower rating evaluators read pitches.terms hypotheses, phrase null hypothesis tests significant difference overall ratings {insert trait} scale evaluators listened resume pitches evaluators read resume pitches (\\(H_0: \\mu_1 = \\mu2\\)). Alternatively, state significant difference overall ratings {insert trait} scale evaluators listened resume pitches evaluators read resume pitches (\\(H_1: \\mu_1 \\ne \\mu2\\)).clarify, going run three -subjects t-tests total; one intellect ratings; one hire ratings; one impression ratings. show run t-test intellect ratings ask remaining two t-tests .run analysis intellect ratings need function t.test() use broom::tidy() pull results t-test tibble. , show create group means run t-test intellect. Run lines look .First calculate group means:can call look typing:Now just look intellect ratings need filter new tibble:run actual t-test tidy table.\nt.test() requires two vectors input\npull() pull single column tibble, e.g. Rating intellect\ntidy() takes information test turns tibble. Try running t.test without piping tidy() see differently.\nt.test() requires two vectors inputpull() pull single column tibble, e.g. Rating intellecttidy() takes information test turns tibble. Try running t.test without piping tidy() see differently.Now lets look intellect_ttibble created (assuming piped tidy()):\nTable 6.1: t-test output intellect condition.\ntibble, intellect_t, can see ran Two Sample t-test (meaning -subjects) two-tailed hypothesis test (\"two.sided\"). mean listened condition, estimate1, 5.635, whilst mean read condition, estimate2 3.648 - compare means group_means sanity check. overall difference two means 1.987. degrees freedom test, parameter, 37. observed t-value, statistic, 3.526, significant p-value, p.value, p = 0.0011, lower field standard Type 1 error rate \\(\\alpha = .05\\).know lectures, t-test presented t(df) = t-value, p = p-value. , t-test written : t(37) = 3.526, p = 0.001.Thinking interpretation finding, effect significant, can reject null hypothesis significant difference mean ratings listened resumes read resumes, intellect ratings. can go say overall intellect ratings listened resume significantly higher (mean diff = 1.987) read resumes, t(37) = 3.526, p = 0.001, accept alternative hypothesis. suggest hearing people speak leads evaluators rate candidates intellectual merely read words written.Now Try:Running remaining t-tests hire impression. Store tibbles called hire_t impress_t respectively.Running remaining t-tests hire impression. Store tibbles called hire_t impress_t respectively.Bind rows intellect_t, hire_t impress_t create table three t-tests called results. look like :Bind rows intellect_t, hire_t impress_t create table three t-tests called results. look like :\nTable 6.2: Output three t-tests\nQuickfire QuestionsCheck results hire. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):\nMean estimate1 (listened condition) = \nMean estimate2 (read condition) = \nt() = , p = \nLooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSE\nCheck results hire. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):Mean estimate1 (listened condition) = Mean estimate1 (listened condition) = Mean estimate2 (read condition) = Mean estimate2 (read condition) = t() = , p = t() = , p = Looking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSELooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSECheck results impression. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):\nMeanestimate1 (listened condition) = \nMean estimate2 (read condition) = \nt() = , p = \nLooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSE\nCheck results impression. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):Meanestimate1 (listened condition) = Meanestimate1 (listened condition) = Mean estimate2 (read condition) = Mean estimate2 (read condition) = t() = , p = t() = , p = Looking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSELooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSE\nt-test answers following structure:\n\nt(degrees freedom) = t-value, p = p-value,\n\n:\n\ndegrees freedom = parameter,\n\nt-value = statistic,\n\np-value = p.value.\n\nRemember result p-value lower (.e. smaller) equal alpha level said significant.\nrecap, looked data Schroeder Epley (2015), descriptives inferentials, plotted figure, confirmed , paper, significant differences three rating categories (hire, impression intellect), listened condition receiving higher rating read condition rating. , interpretation people rate higher hear speak resume opposed just reading resume!","code":"\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(m = mean(Rating), sd = sd(Rating))## `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\ngroup_means\nintellect <- filter(ratings2, Category == \"intellect\")\nintellect_t <- t.test(intellect %>% \n                        filter(condition == \"listened\") %>% \n                        pull(Rating),\n                      intellect %>% \n                        filter(condition == \"read\") %>% \n                        pull(Rating),\n                      var.equal = TRUE) %>%\n  tidy()"},{"path":"nhst-two-sample-t-test.html","id":"students-versus-welchs-t-tests","chapter":"6 NHST: Two-Sample t-test","heading":"6.4 Student's versus Welch's t-tests","text":"-subjects t-test comes two versions: one assuming equal variance two groups (Student's t-test) another making assumption (Welch's t-test). Student's t-test considered common standard t-test field, advisable use Welch's t-test instead.blog post \"Always use Welch's t-test instead Student's t-test\" Daniel Lakens shows groups equal variance tests return finding. However, assumption equal variance violated, .e. groups unequal variance, Welch's test produces accurate finding, based data.important often final decision whether assumptions \"held\" \"violated\" subjective; .e. researcher fully decide. Nearly data show level unequal variance (perfectly equal variance across multiple conditions actually revealing fraudulent data). Researchers using Student's t-test regularly make judgement whether variance across two groups \"equal enough\". , blog shows always better run Welch's t-test analyse -subjects design ) Welch's t-test assumption equal variance, b) Welch's t-test gives accurate results variance equal, c) Welch's t-test performs exactly Student t-test variance equal across groups.short, Welch's t-test takes level ambiguity (may called \"researcher degree freedom\") analysis makes analysis less open bias subjectivity. , now , unless stated otherwise, run Welch's t-test.practice easy run Welch's t-test, can switch tests shown:\n* run Student's t-test set var.equal = TRUE\n* run Welch's t-test set var.equal = FALSEThis run Student's t-test:run Welch's t-test:two ways know run Welch's t-test :output says ran Welch Two Sample t-testThe df likely decimal places Welch's t-test whereas whole number Student's t-test.Always run Welch's t-test -subjects design using R!","code":"\nt.test(my_dv ~ my_iv, data = my_data, var.equal = TRUE)\nt.test(my_dv ~ my_iv, data = my_data, var.equal = FALSE)"},{"path":"nhst-two-sample-t-test.html","id":"practice-your-skills-8","chapter":"6 NHST: Two-Sample t-test","heading":"6.5 Practice Your Skills","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"single-dose-testosterone-administration-impairs-cognitive-reflection-in-men.","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.1 Single-dose testosterone administration impairs cognitive reflection in men.","text":"order complete exercise, first download assignment .Rmd file need edit assignment: titled GUID_Ch6_PracticeYourSkills.Rmd. can downloaded within zip file link . downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top. Download Assignment .zip file .assignment using real data following paper:Nave, G., Nadler, ., Zava, D., Camerer, C. (2017). Single-dose testosterone administration impairs cognitive reflection men. Psychological Science, 28, 1398--1407.full data documentation can found Open Science Framework repository, assignment just use .csv file zipped folder: CRT_Data.csv. may also want read paper, least part, help fully understand analysis times unsure. article's abstract:nonhumans, sex steroid testosterone regulates reproductive behaviors fighting males mating. humans, correlational studies linked testosterone aggression disorders associated poor impulse control, neuropsychological processes work poorly understood. Building dual-process framework, propose mechanism underlying testosterone's behavioral effects humans: reduction cognitive reflection. largest study behavioral effects testosterone administration date, 243 men received either testosterone placebo took Cognitive Reflection Test (CRT), estimates capacity override incorrect intuitive judgments deliberate correct responses. Testosterone administration reduced CRT scores. effect remained controlled age, mood, math skills, whether participants believed received placebo testosterone, effects 14 additional hormones, held CRT questions isolation. findings suggest mechanism underlying testosterone's diverse effects humans' judgments decision making provide novel, clear, testable predictions.critical findings presented p. 1403 paper heading \"influence testosterone CRT performance\". task today attempt try reproduce main results paper.Note: unable get exact results authors necessarily mean wrong! authors might wrong, might left important details. Present find.starting lets check:.csv file saved folder computer manually set folder working directory..csv file saved folder computer manually set folder working directory..Rmd file saved folder .csv files. assessments ask save format GUID_Ch6_PracticeYourSkills.Rmd GUID replaced GUID. Though formative assessment, may good practice ..Rmd file saved folder .csv files. assessments ask save format GUID_Ch6_PracticeYourSkills.Rmd GUID replaced GUID. Though formative assessment, may good practice .","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT1A","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.2 Task 1A: Libraries","text":"today's exercise need tidyverse broom packages. Enter code t1A code chunk load libraries.","code":"\n## load in the tidyverse and broom packages"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT1B","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.3 Task 1B: Loading in the data","text":"Use read_csv() replace NULL t1B code chunk load data stored data file CRT_Data.csv. Store data variable crt. change file name data file.","code":"\ncrt <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT2","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.4 Task 2: Selecting only relevant columns","text":"look crt. three variables crt need find extract order perform t-test: subject ID number (hint: participant unique number); independent variable (hint: participant possibility one two treatments coded 1 0); dependent variable (hint: test specifically looks answers people get correct). Identify three variables. might help look first sentences heading \"influence testosterone CRT performance\" Figure 2a paper guidance correct variables.identified important three columns, replace NULL t2 code chunk select three columns crt store tibble crt2.Check work: correct, crt2 tibble 3 columns 243 rows.Note: remainder assignment use crt2 main source tibble crt.","code":"\ncrt2 <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT3","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.5 Task 3: Verify the number of subjects in each group","text":"Participants section article contains following statement:243 men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = 125) placebo (n = 118).t3 code block , replace NULLs lines code calculate:number men Treatment. tibble called cond_counts containing column called Treatment showing two groups column called n shows number men group.number men Treatment. tibble called cond_counts containing column called Treatment showing two groups column called n shows number men group.total number men sample. single value, tibble, stored n_men.total number men sample. single value, tibble, stored n_men.know answer tasks already. Make sure code gives correct answer!Now replace strings statements , using inline R code, reproduces sentence paper exactly shown . words, statement , anywhere says \"(code )\", replace string (including quotes), inline R code. clarify, looking .Rmd file see R code, looking knitted file, see values. Look back Chapter 1 unsure use inline code.Hint: One solution something cond_counts similar filter() pull() exercises Chapter.\"(code )\" men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = \"(code )\") placebo (n = \"(code )\").","code":"\ncond_counts <- NULL\nn_men <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT4","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.6 Task 4: Reproduce Figure 2a","text":"Figure 2A original paper:\nFigure 6.4: Figure 2A Nave, Nadler, Zava, Camerer (2017) replicate\nWrite code t4 code chunk reproduce version Figure 2a - shown . create plot, replace NULL make tibble called crt_means mean standard deviation number CorrectAnswers group.Use crt_means source data plot.Hint: need check recode() get labels treatments right. Look back Chapter 2 hint use recode(). check short resource examples use recode(): Recode short tutorial.worry including error bars (unless want ) line indicating significance plot. however make sure pay attention labels treatments y-axis scale label. Reposition x-axis label Figure. can use colour, like.","code":"\ncrt_means <- NULL\n\n## TODO: add lines of code using ggplot"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT5","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.7 Task 5: Interpreting your Figure","text":"Always good slight recap point make sure following analysis. Replace NULL t5 code chunk number statement best describes data calculated plotted thus far. Store single value answer_t5:Testosterone group (M = 2.10, SD = 1.02) appear fewer correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 2.10, SD = 1.02) appear fewer correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear fewer correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear fewer correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 2.10, SD = 1.02) appear correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 2.10, SD = 1.02) appear correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.","code":"\nanswer_t5 <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT6","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.8 Task 6: t-test","text":"Now calculated descriptives study need run inferentials. t6 code chunk , replace NULL line code run t-test taking care make sure output table Placebo mean Estimate1 (group 0) Testosterone mean Estimate2 (group 1). Assume variance equal use broom::tidy() sweep store results tibble called t_table.","code":"\nt_table <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT7","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.9 Task 7: Reporting results","text":"t7A code chunk , replace NULL line code pull df t_table. must single value stored t_df.t7B code chunk , replace NULL line code pull t-value t_table. Round three decimal places. must single value stored t_value.t7C code chunk , replace NULL line code pull p-value t_table. Round three decimal places. must single value stored p_value.t7D code chunk , replace NULL line code calculate absolute difference mean number correct answers Testosterone group Placebo group. Round three decimal places. must single value stored t_diff.completed t7A t7D accurately, knitted, one statements produce accurate coherent summary results. t7E code chunk , replace NULL number statement best summarises data study. Store single value answer_t7eThe testosterone group performed significantly better ( fewer correct answers) placebo group, t() = , p = .testosterone group performed significantly worse ( fewer correct answers) placebo group, t() = , p = .testosterone group performed significantly better ( correct answers) placebo group, t() = , p = .testosterone group performed significantly worse ( fewer correct answers) placebo group, t() = , p = .Well done, finished! Now go check answers solutions end chapter. looking check resulting output answers submitted exactly output solution - example, remember single value coded answer. alternative answers, means submitted one options return answer.","code":"\nt_df <- NULL\nt_value <- NULL\np_value <- NULL\nt_diff <- NULL\nanswer_t7e <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"solutions-to-questions-5","chapter":"6 NHST: Two-Sample t-test","heading":"6.6 Solutions to Questions","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"the-two-sample-t-test-1","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1 The Two-Sample t-test","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"task-1-5","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1.1 Task 1","text":"mean age evaluators 30.9The standard deviatoin age evaluators 6.24There 4 males e_count %>% filter(sex_names == \"female\") %>% pull(n) females, 5 people stating sex.Return Task","code":"\nlibrary(\"tidyverse\")\nlibrary(\"broom\") # you'll need broom::tidy() later\n\nevaluators <- read_csv(\"evaluators.csv\")\n\nevaluators %>%\n  summarize(mean_age = mean(age, na.rm = TRUE))\n\nevaluators %>%\n  count(sex)\n\n# If using `recode()`:\nevaluators %>%\n  count(sex) %>%\n  mutate(sex_names = recode(sex, \"1\" = \"male\", \"2\" = \"female\"))"},{"path":"nhst-two-sample-t-test.html","id":"task-2-6","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1.2 Task 2","text":"load dataFirst pull ratings associated intellectNext calculate means evaluatorMutate Category variable. way can combine 'impression' 'hire' single table useful!combine information one single tibble.Return Task","code":"\nratings <- read_csv(\"ratings.csv\")\niratings <- ratings %>%\n  filter(Category %in% c(\"competent\", \"thoughtful\", \"intelligent\"))\nimeans <- iratings %>%\n  group_by(eval_id) %>%\n  summarise(Rating = mean(Rating))\nimeans2 <- imeans %>%\n  mutate(Category = \"intellect\")\nratings2 <- ratings %>%\n  filter(Category %in% c(\"impression\", \"hire\")) %>%\n  bind_rows(imeans2) %>%\n  inner_join(evaluators, \"eval_id\") %>%\n  select(-age, -sex) %>%\n  arrange(eval_id, Category)"},{"path":"nhst-two-sample-t-test.html","id":"task-4-5","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1.3 Task 4","text":"First calculate group means:can call look typing:Now just look intellect ratings need filter new tibble:run actual t-test tidy table.\nt.test() requires two vectors input\npull() pull single column tibble, e.g. Rating intellect\ntidy() takes information test turns table. Try running t.test without piping tidy() see differently.\nt.test() requires two vectors inputpull() pull single column tibble, e.g. Rating intellecttidy() takes information test turns table. Try running t.test without piping tidy() see differently.Now repeat HIRE IMPRESSIONAnd ImpressionBefore combining one table showing three t-testsReturn Task","code":"\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(m = mean(Rating), sd = sd(Rating))## `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\ngroup_means\nintellect <- filter(ratings2, Category == \"intellect\")\nintellect_t <- t.test(intellect %>% filter(condition == \"listened\") %>% pull(Rating),\n                      intellect %>% filter(condition == \"read\") %>% pull(Rating),\n                      var.equal = TRUE) %>%\n  tidy()\nhire <- filter(ratings2, Category == \"hire\")\nhire_t <- t.test(hire %>% filter(condition == \"listened\") %>% pull(Rating),\n                      hire %>% filter(condition == \"read\") %>% pull(Rating),\n                 var.equal = TRUE) %>%\n  tidy()\nimpress <- filter(ratings2, Category == \"impression\")\nimpress_t <- t.test(impress %>% filter(condition == \"listened\") %>% pull(Rating),\n                    impress %>% filter(condition == \"read\") %>% pull(Rating),\n                    var.equal = TRUE) %>%\n  tidy()\nresults <- bind_rows(\"hire\" = hire_t, \n                     \"impression\" = impress_t,\n                     \"intellect\" = intellect_t, .id = \"id\")\n\nresults"},{"path":"nhst-two-sample-t-test.html","id":"practice-your-skills-9","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2 Practice Your Skills","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"task-1a-libraries","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.1 Task 1A: Libraries","text":"Return Task","code":"\nlibrary(broom)\nlibrary(tidyverse)"},{"path":"nhst-two-sample-t-test.html","id":"task-1b-loading-in-the-data","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.2 Task 1B: Loading in the data","text":"Use read_csv() read data!Return Task","code":"\ncrt <- read_csv(\"data/06-s01/homework/CRT_Data.csv\")\ncrt <- read_csv(\"CRT_Data.csv\")"},{"path":"nhst-two-sample-t-test.html","id":"task-2-selecting-only-relevant-columns","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.3 Task 2: Selecting only relevant columns","text":"key columns :IDTreatmentCorrectAnswersCreating crt2 tibble 3 columns 243 rows.Return Task","code":"\ncrt2 <- select(crt, ID, Treatment, CorrectAnswers)"},{"path":"nhst-two-sample-t-test.html","id":"task-3-verify-the-number-of-subjects-in-each-group","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.4 Task 3: Verify the number of subjects in each group","text":"Participants section article contains following statement:243 men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = 125) placebo (n = 118).t3 code block , replace NULLs lines code calculate:number men Treatment. tibble/table called cond_counts containing column called Treatment showing two groups column called n shows number men group.number men Treatment. tibble/table called cond_counts containing column called Treatment showing two groups column called n shows number men group.total number men sample. single value, tibble/table, stored n_men.total number men sample. single value, tibble/table, stored n_men.know answer tasks already. Make sure code gives correct answer!cond_counts, :alternativelyFor n_men, :alternativelySolution:formatted inline R code :`r n_men` men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = `r cond_counts %>% filter(Treatment == 1) %>% pull(n)`) placebo (n = `r cond_counts %>% filter(Treatment == 0) %>% pull(n)`).give:243 men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = 125) placebo (n = 118).Return Task","code":"\ncond_counts <- crt2 %>% group_by(Treatment) %>% summarise(n = n())\ncond_counts <- crt2 %>% count(Treatment)\nn_men <- crt2 %>% summarise(n = n()) %>% pull(n)\nn_men <- nrow(crt2)"},{"path":"nhst-two-sample-t-test.html","id":"task-4-reproduce-figure-2a","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.5 Task 4: Reproduce Figure 2A","text":"produce good representation Figure 2A following approach:\nFigure 6.5: representation Figure 2A\nReturn Task","code":"\ncrt_means <- crt2 %>% \n  group_by(Treatment) %>% \n  summarise(m = mean(CorrectAnswers), sd = sd(CorrectAnswers)) %>%\n  mutate(Treatment = recode(Treatment, \"0\" = \"Placebo\", \"1\" = \"Testosterone Group\"))\n\nggplot(crt_means, aes(Treatment, m, fill = Treatment)) + \n  geom_col() + \n  theme_classic() + \n  labs(x = \"CRT\", y = \"Number of Correct Answers\") +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = c(\"#EEEEEE\",\"#AAAAAA\")) +\n  coord_cartesian(ylim = c(1.4,2.4), expand = TRUE)"},{"path":"nhst-two-sample-t-test.html","id":"task-5-interpreting-your-figure","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.6 Task 5: Interpreting your Figure","text":"Option 3 correct answer given :Testosterone group (M = 1.66, SD = 1.18) appear fewer correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgements correct response.Return Task","code":"\nanswer_t5 <- 3"},{"path":"nhst-two-sample-t-test.html","id":"task-6-t-test","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.7 Task 6: t-test","text":"need pay attention order using first approach, making sure 0 group entered first. put Placebo groups Estimate1 output. reality change values, key thing pass code someone, expect Placebo Estimate1, need make sure coded way.Alternatively, use known formula approach shown . state DV ~ IV say name tibble data = .... just need make sure columns state DV IV actually tibble!Return Task","code":"\nt_table <- t.test(crt2 %>% filter(Treatment == 0) %>% pull(CorrectAnswers),\n                  crt2 %>% filter(Treatment == 1) %>% pull(CorrectAnswers),\n                  var.equal = TRUE) %>%\n  tidy()\nt_table <- t.test(CorrectAnswers ~ Treatment, data = crt2, var.equal = TRUE) %>% tidy()"},{"path":"nhst-two-sample-t-test.html","id":"task-7-reporting-results","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.8 Task 7: Reporting results","text":"degrees freedom (df) found parameterAn alternative option follows, using pull() method. work B D wellThe t-value found statisticThe p-value found p.valueThe absolute difference two means can calculated follows:completed t7A t7D accurately, knitted, Option 4 stated suchThe testosterone group performed significantly worse (0.438 fewer correct answers) placebo group, t(241) = 3.074, p = 0.002and therefore correct answer!Return TaskChapter Complete!","code":"\nt_df <- t_table$parameter\nt_df <- t_table %>% pull(parameter)\nt_value <- t_table$statistic %>% round(3)\np_value <- t_table$p.value %>% round(3)\nt_diff <- (t_table$estimate1 - t_table$estimate2) %>% round(3) %>% abs()\nanswer_t7e <- 4"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"nhst-paired-sample-t-test-and-nonparametric-tests","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7 NHST: Paired-Sample t-test and Nonparametric tests","text":"","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"overview-7","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.1 Overview","text":"previous chapters looked one-sample t-tests -samples (two-sample) t-tests. chapter continue look paired-sample t-test (sometimes called dependent sample within-subject t-test). paired-sample t-test statistical procedure used determine whether mean difference two sets observations matched participants zero.tests, paired-sample t-test two competing hypotheses: null hypothesis alternative hypothesis.null hypothesis assumes true mean difference paired samples zero: \\[H_0: \\mu1 = \\mu_2\\].alternative hypothesis assumes true mean difference paired samples equal zero: \\[H_1: \\mu1 \\ne \\mu_2\\].chapter going look running paired-sample t-test, begin little work checks data need perform prior analysis.Assumptions testsSo far focused skills data-wrangling, visualisations, probability, now moving towards actual analysis stage research. However, know lectures, tests, particularly parametric tests, make number assumptions data tested, , responsible researcher, need check assumptions \"held\" violation assumptions may make results invalid.t-tests assumptions change two-sample paired-sample designs (one-sample matched-pairs designs can thought within-subjects designs).assumptions two-sample t-test :data points independent.variance across groups/conditions equal.dependent variable must continuous (interval/ratio).dependent variable normally distributed.assumptions paired-sample t-test :participants appear conditions/groups.dependent variable must continuous (interval/ratio).dependent variable normally distributed.beginning analysis, using data-wrangling skills, must check see data deviates assumptions, whether contains outliers, order assess quality results. assumption violation present, may want use nonparametric tests instead.chapter :Run assumption checksAnalyse experiment paired-sample design.Understand run nonparametric tests.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"the-paired-sample-t-test","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2 The Paired-Sample t-test","text":"now covered one-sample t-test -subjects/two-sample t-test, talked little assumptions -subjects t-test. now going expand looking final t-test need cover, paired-sample/within-subject t-test; used participants conditions, two groups people closely matched various demographics age, IQ, verbal acuity, etc.exploring paired-sample t-test also look checking assumptions t-tests. refer back earlier activities know many assumptions different t-tests largely similar (apart equal variance, example), looking assumption check can apply tests.activity look replication Furnham (1986) School Psychology & Neuroscience, University Glasgow, carried 2016-2017. worth familiarising original study point information regarding concepts study, essential order complete exercises: Furnham, . (1986), Robustness Recency Effect: Studies Using Legal Evidence. explain little study carrying tasks check assumptions analyse data.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"data-set-juror-decision-making","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.1 Data Set: Juror Decision Making","text":"research question experiment : order information affect juror judgments guilt innocence? Thus, overall aim original experiment investigate whether decision jury member makes innocence guilt defendant influenced something simple crucial evidence presented trial. experiment participants (Level 2 Psychology students) listened series recordings recreated 1804 trial man known Joseph Parker accused assuming two identities marrying two women; .e. bigamy. participant listened recordings evidence, presented prosecution defense witnesses, asked judge guilty thought Mr. Parker 14 different time points experiment scale 1 9: 1 innocent 9 guilty. 14 time points came immediately certain pieces evidence.manipulation experiment order evidence altered half participants received one order evidence half received second order evidence. Key order change time critical piece evidence presented. critical evidence proved defendant innocent. Middle group heard evidence Timepoint 9 trial whereas Late group heard evidence Timepoint 13. opportunity look data due course , today's exercise, focus Late group.exercise, task analyse data examine whether participants' ratings guilt significantly changed presentation critical evidence Late condition. critical evidence, proved defendant's innocence, desired effect see significant drop ratings guilt hearing evidence (Timepoint 13) compared (Timepoint 12). words, hypothesised significant decrease ratings guilt, caused presentation critical evidence, Timepoint 12 Timepoint 13.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT1","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.2 Task 1: Load the data","text":"Download data experiment .Unzip data save folder access set folder working directory.Open new R Markdown document.Today need broom tidyverse libraries. Load order. Remember order load libraries matters.Using read_csv(), load data experiment contained GuiltJudgements.csv store tibble called ratings.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT2","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.3 Task 2: Wrangle the data","text":", interested 75 participants Late group Timepoints 12 (rating key evidence) 13 (rating key evidence). look ratings see Timepoints wide format (columns 1 14 - different timepoint) Evidence column contains Middle group well. wrangling make data look like shown Table 7.1 . steps follows:filter() participants Late condition.filter() participants Late condition.select() Timepoints 12 13.select() Timepoints 12 13.rename() Timepoints Twelve Thirteen numerical names hard deal .rename() Timepoints Twelve Thirteen numerical names hard deal .pivot_longer() gather data structure. (Note: first four rows shown example).pivot_longer() gather data structure. (Note: first four rows shown example).one pipe store tibble called lates. tibble, lates 150 rows 4 columns.one pipe store tibble called lates. tibble, lates 150 rows 4 columns.Check table looks like table .\nTable 7.1: table look Task 2\n\n\nneed specify column want filter , stating variable (.e. Late) column 'equal ' (.e. '==')\n\n\nneed specify column want filter , stating variable (.e. Late) column 'equal ' (.e. '==')\n\n\ntwo columns representing Timepoints 12 13, two columns need keep order identify participant group. Use table guide.\n\n\ntwo columns representing Timepoints 12 13, two columns need keep order identify participant group. Use table guide.\n\n\nrenaming, first state new variable name designate old variable name. .e. rename(data, new_column_name = old_column_name). old column number, put backticks, e.g. Five = backtick 5 backtick (sure use `s).\n\n\nrenaming, first state new variable name designate old variable name. .e. rename(data, new_column_name = old_column_name). old column number, put backticks, e.g. Five = backtick 5 backtick (sure use `s).\n\n\nstructure shown two new columns: Timepoint GuiltRating, created columns Twelve Thirteen. state new column names using pivot_longer(), well columns used create . Think completing : cols = X:Y, names_to = \"\", values_to = \"\"\n\n\nstructure shown two new columns: Timepoint GuiltRating, created columns Twelve Thirteen. state new column names using pivot_longer(), well columns used create . Think completing : cols = X:Y, names_to = \"\", values_to = \"\"\nQuickfire QuestionsTo check completed Task correctly, enter appropriate values boxes.\ndataset :  columns  rows  participants.\ndataset :  columns  rows  participants.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT3","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.4 Task 3: Look at the histogram for normality","text":"now going show start checking data terms assumptions normality. creating visualisations data thinking visualisations tell us data.\nFirst create histogram two timepoints see individual distributions appear normally distributed.Use data visualisation skills plot histogram TimePoint. two histograms side--side one figure set histogram binwidth something reasonable experiment.\n\nggplot() + geom_?\n\n\nggplot() + geom_?\n\n\nhistogram requires state aes(x) aes(x, y). examining differences guilt rating scores across participants. column lates 'x'? categorical Independent Variable.\n\n\nhistogram requires state aes(x) aes(x, y). examining differences guilt rating scores across participants. column lates 'x'? categorical Independent Variable.\n\n\nbinwidth argument can specify within geom_histogram() geom_historgram(binwidth = ...). Think appropriate binwidth. guilt rating scale runs 1 9 increments 1.\n\n\nbinwidth argument can specify within geom_histogram() geom_historgram(binwidth = ...). Think appropriate binwidth. guilt rating scale runs 1 9 increments 1.\n\n\nused something like facet_?() display categorical variables (.e. Timepoint) according different levels contains. need specify variable want use, using ~ variable name.\n\n\nused something like facet_?() display categorical variables (.e. Timepoint) according different levels contains. need specify variable want use, using ~ variable name.\n\n\nBeyond point, can think adding appropriate labels color like.\n\n\nBeyond point, can think adding appropriate labels color like.\n","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.5 Task 4: A boxplot of outliers","text":"can also check outliers different conditions. Outliers can obviously create skewed data make mean value misleading.Create boxplot Timepoint GuiltRating check outliers.\n\ntime using ggplot() create boxplot, need specify 'x', discrete/categorical variable, 'y', continuous variable.\n\n\ntime using ggplot() create boxplot, need specify 'x', discrete/categorical variable, 'y', continuous variable.\n\n\ngeom_boxplot() - see Chapter 3 example.\n\n\ngeom_boxplot() - see Chapter 3 example.\nQuickfire QuestionsHow many outliers see? 0123too many countRemember outliers normally represented dots stars beyond whiskers boxplot. see solution data contains outlier. deal outliers today, good able spot moment. worth thinking deal outliers. numerous methods replacing given value removing participants. Remember though decision, deal outliers, deviation normality, considered written advance part preregistration protocol.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT5","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.6 Task 5: The violin plot","text":"Boxplots histograms tell slightly different information data, discuss minute, developing skills plotting interpreting . already introduced new type figure can combine information boxplot histogram one figure. using violin plot can created using geom_violin() function.Take code written boxplot (Task 4) add geom_violin another layer. may need rearrange code (.e., boxplot violin plot functions) violin plots appear underneath boxplot. Think layers visualisation!\n\nggplot() works layers - first layer (.e., first plot call) underneath second layer. means get boxplot showing top violin plot, violin must come first (.e., need call geom_violin() call geom_boxplot())\n\n\nggplot() works layers - first layer (.e., first plot call) underneath second layer. means get boxplot showing top violin plot, violin must come first (.e., need call geom_violin() call geom_boxplot())\n\n\nembellished figure little solution can look basics sorted. Things like adding width call boxplot, alpha call violin.\n\n\nembellished figure little solution can look basics sorted. Things like adding width call boxplot, alpha call violin.\nsee violin plot relates histogram created earlier? head, rotate histograms bars pointing left imagine mirror-image (making two-sided histogram). look similar violin plot. sure able still locate outlier Thirteen condition.HOLD !!!know lectures assumptions paired-sample t-test, dealing within-subjects design, paired t-test, normality actually determined based difference two conditions. , fact check distribution scores difference two conditions normally distributed. looking terms separate conditions, degree show process visualisations, really follow paired-sample t-test need look normality difference. code create violin boxplot visualisation difference two conditions now able understand code.look output, code, think whether scores difference two conditions normally distributed.Note: code , outliers appear red circles individual data points appear blue Xs. Can see controlling ? important step? say second, worth thinking minute two.\nFigure 7.1: violin boxplot showing distribution scores difference Thirteen Twelve conditions. Individual participant data show blue stars. Positive values indicate rating Thirteen condition higher rating Twelve condition. Oultiers show red circles.\nThinking Cap PointWe now checked assumptions, sort still need make decision regarding normality. look figures created, spend minutes thinking whether data normally distributed .Also, think data shows outliers, individual conditions show outlier. Lastly, important code different presentations outliers individual data points.\ns\ndata normally distributed ?\n\nlooking nice symmetry around top bottom half violin boxplot. looking median (thick black line roughly middle boxplot), whiskers boxplot roughly equal length. Also, want violin bulge middle figure taper top bottom. also want just one bulge violin (symbolising scores ). Two bulges violin may indicative two different response patterns samples within one dataset.\n\nNow need make judgement normality. Remember real data never textbook curve normal distribution . always bit messier degree judgement call needed need use test compare sample distribution normal distribution. Tests Kolmogorov-Smirnov Shapiro-Wilks tests sometimes recommended determine violation normal distribution. However, last two tests reliable depending sample size often revert back judgement call. really important steps documented, future researcher can check confirm process.\n\nOverall, data looks normally distributed - least visually.\n\nOutliers vs Data Points?\n\ncareful using geom_jitter() show individual data points. can overcomplicate figure large number participants might make sense include individual data points can just create noisy figure. second point want make sure outliers confused individual data points; outliers data points, data points outliers. , order confuse data outliers, need make sure properly controlling colors shapes different information figure. sure explain figure legend element shows.\n\nLastly, noted plotted original boxplots individual conditions, Thirteen group outlier. However, now plot difference boxplot see outliers. important reinforces plot correct data order check assumptions. assumptions paired-sample t-test based difference scores individual participants. see calculate difference scores outlier, even though original data point outlier. fine within consideration assumption - assumption looks difference. said, might also want check original outlier acceptable value rating scale (e.g., 1 9) wild value come bad data entry (e.g., rating 13; say rating condition got mixed somehow).\nGreat. run data checks assumptions paired-sample t-test. checked data normal distribution, using violin plot boxplot, looking skewed data outliers. understanding data, arguments field, whilst rating scale used called ordinal, many, including Furnham treat interval. Finally, spotted issues code suggest participant give response conditions, can check descriptives - conditions n = 75.check also run descriptives start understanding relationship two levels interest: Timepoint 12 Timepoint 13.","code":"\nlates %>% \n  spread(Timepoint, GuiltRating) %>%\n  mutate(diff = Thirteen - Twelve) %>%\n  ggplot(aes(x = Evidence, y = diff)) +\n  geom_violin() +\n  geom_boxplot(fill = \"red\", \n               width = .5, \n               alpha = .1, \n               outlier.colour = \"red\") +\n  geom_jitter(color = \"blue\", \n              width = .1, \n              shape = 4) + \n  theme_classic()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.7 Task 6: Calculating Descriptives","text":"Calculate mean, standard deviation, Lower Upper values 95% Confidence Interval (CI) levels Independent Variable (two time points). need also calculate n() - see Chapter 2 - Standard Error complete task. Store data tibble called descriptives.solution shows values obtain, sure go first. sure confirm groups 75 people !Answering questions may help calculating CIs:Quickfire QuestionsFrom options, equation use calculate LowerCI? mean - 1.96 * sdmean * 1.96 - semean - 1.96 * semean * 1.96 - sdFrom options, equation use calculate UpperCI? mean + 1.96 * sdmean * 1.96 + semean + 1.96 * semean * 1.96 + sd\ngroup_by() categorical column Timepoint. column want compare groups .\n\nsummarise()\n\nDifferent calculations can used within summarise() function long calculated order require . example, first need calculate participant number, n = n(), standard deviation, sd = sd(variable), order calculate standard error, se = sd/sqrt(n), required calculate Confidence Intervals.\n\n95% Confidence Interval, need calculate LowerCI UpperCI using appropriate formula. Remember mean + 1.96se mean - 1.96se. include mean just calculating much higher lower mean CI . want actual interval.\n\nthink task. looking calculate 95% Confidence Interval normally distributed data. require z-score tells many standard deviations mean. 95% area normal distribution curve lies within 1.96 standard deviations mean; .e. 1.96 SD mean.\n\nlooking calculate 99% Confidence Interval instead use z-score 2.576. takes account greater area normal distribution curve away mean (.e., closer tail ends curve), resulting higher z-score.\n","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT7","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.8 Task 7: Visualising Means and Descriptives","text":"Using data descriptives, produce plot visualises mean 95% Confidence Intervals.\nOne way basic barplot, shown previous chapters, error bars indicating 95% CI.\nadd error bars add layer like .\nFeel free embellish figure see fit.\nshown couple options solution look try adjusting, tried plot.\nOne way basic barplot, shown previous chapters, error bars indicating 95% CI.add error bars add layer like .Feel free embellish figure see fit.shown couple options solution look try adjusting, tried plot.\nrecommend using geom_col()\n\nRemember add (+) geom_errorbar() line code! pipe .\n\ncode error bars, aesthetic, aes(), allows set min max values error bars - max min CIs.\n\nposition = \"dodge\" position = position_dodge() position = position_dodge(width = .9). number ways use position call thing.\nImportant remember: mentioned previous labs, barplots informative . Going ahead research, keep mind look use plots incorporate good indication distribution/spread individual data points well, needed. Barplots give good representation categorical counts like chi-square test, much ordinal interval data likely spread.Thinking Cap PointNow descriptives look need think tell us - really interpret . First thing think back hypothesis every interpretation phrased around hypothesis. hypothesised significant decrease ratings guilt, caused presentation critical evidence, Timepoint 12 Timepoint 13. think significant difference two time points? evidence ? Think overlap confidence intervals! Remember key thing stage subjective impression - \"appears might ....\" words effect.","code":"\ngeom_errorbar(aes(ymin = LowerCI, ymax = UpperCI),\n              position = \"dodge\", width = .15)"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT8","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.9 Task 8: The paired-sample t-test","text":"Now checked assumptions ran descriptives, last thing need perform paired-sample t-test test differences two time points.perform paired-sample t-test use t.test function Chapter 6. However, time add argument, paired = TRUE, tells code \"yes, paired-sample t-test\".Perform paired-sample t-test guilt ratings crucial time points (Twelve Thirteen) participants Late group. Store data (e.g., tidy) tibble called results.\nwork earlier chapters know two ways use t.test() function. two options :\n\nformula approach\n\nt.test(x ~ y, data, paired = TRUE/FALSE, alternative =\"two.sided\"/\"greater\"/\"less\")\n\nx columns containing DV y typically grouping variable (.e., independent variable).\n\nvector approach\n\n\nt.test(data %>% filter(condition1) %>% pull(data),data %>% filter(condition2) %>% pull(data), paired = TRUE)\n\n\nt.test(data %>% filter(condition1) %>% pull(data),data %>% filter(condition2) %>% pull(data), paired = TRUE)\n\n\npull Twelve Thirteen columns pass condition1 condition2, can use: lates %>% pull(Twelve) lates %>% pull(Thirteen).\n\n\npull Twelve Thirteen columns pass condition1 condition2, can use: lates %>% pull(Twelve) lates %>% pull(Thirteen).\n\nRegardless method\n\n\nforget state paired = TRUE run -subjects t-test\n\n\nforget state paired = TRUE run -subjects t-test\n\n\ncalculated results, forget tidy() - can add using pipe!\n\n\ncalculated results, forget tidy() - can add using pipe!\n\n\nquite understand use tidy() yet, run t.test() without tidy() see happens!\n\n\nquite understand use tidy() yet, run t.test() without tidy() see happens!\n\n\nNote: options running t-test give result. difference whether t-value positive negative. Remember vector approach allows state condition 1 condition 2. formula approach just runs conditions alphabetically.\n\n\nNote: options running t-test give result. difference whether t-value positive negative. Remember vector approach allows state condition 1 condition 2. formula approach just runs conditions alphabetically.\nThinking Cap PointNow look output test within tibble results. group, spend time breaking results can see. understand values mean come ? may match knowledge lectures. significant difference ? write best know sure.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT9","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.10 Task 9: The Write-up","text":"Fill blanks complete paragraph, summarising results study. need refer back information within results descriptives get correct answers make sure understand output t-test.Enter values two decimal places present absolute t-value.solutions contain completed version paragraph compare .\"paired-sample t-testone-sample t-testbetween-samples t-testmatched-pairs t-test ran compare change guilt ratings (M = , SD = ) (M = , SD = ) crucial evidence heard. significantnon-significant difference found (t() = , p = .05> .05= .001< .001) Timepoint 13 average rating  units lower Timepoint 12. tells us critical evidence influence rating guilt jury membersthat critical evidence influence rating guilt jury membersthat critical evidence rating guilt jury members unconnectedsomething quite sure right now, best check!`\n\nt-tests take following format: t(df) = t-value, p = p-value\n\n\nt-tests take following format: t(df) = t-value, p = p-value\n\n\nresults states degrees freedom parameter, t-value statistic.\n\n\nresults states degrees freedom parameter, t-value statistic.\n\n\nestimate mean difference ratings Timepoints Twelve Thirteen.\n\n\nestimate mean difference ratings Timepoints Twelve Thirteen.\n\n\nconf.low conf.high values 95% Confidence Intervals mean difference two conditions. included write-. written something like, \"difference two groups (M = -1.76, 95% CI = [-2.19, -1.33])\".\n\n\nconf.low conf.high values 95% Confidence Intervals mean difference two conditions. included write-. written something like, \"difference two groups (M = -1.76, 95% CI = [-2.19, -1.33])\".\nTwo tips:write future report, can make write-reproducible well using output tibbles calling specific columns. example, t(`r results$parameter`) = `r results$statistic %>% abs()`, p < .001, knitted become t(74) = 8.23, p < .001. , code can prevent mistakes write-ups! However, working rounding p-values can tricky offered code solutions show .write future report, can make write-reproducible well using output tibbles calling specific columns. example, t(`r results$parameter`) = `r results$statistic %>% abs()`, p < .001, knitted become t(74) = 8.23, p < .001. , code can prevent mistakes write-ups! However, working rounding p-values can tricky offered code solutions show .Another handy function writing round() function putting values given number decimal places. example wanted round absolute t-value two decimal places might results %>% pull(statistic) %>% abs() %>% round(2) give t = 8.23. maybe want three decimal places: results$statistic %>% abs() %>% round(3) give t = 8.232. really handy function follows format round(value_to_round, number_of_decimal_places).Another handy function writing round() function putting values given number decimal places. example wanted round absolute t-value two decimal places might results %>% pull(statistic) %>% abs() %>% round(2) give t = 8.23. maybe want three decimal places: results$statistic %>% abs() %>% round(3) give t = 8.232. really handy function follows format round(value_to_round, number_of_decimal_places).Excellent work! can see performing t-test small part entire process: wrangling data, calculating descriptives, plotting data check distributions assumptions major part analysis process. past chapters, building skills able see put good use now moved onto complex data analysis. Running inferential part usually just one line code.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"practice-your-skills-10","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.3 Practice Your Skills","text":"Using data , perform paired-sample t-test Middle group crucial evidence presented Timepoint 9. Save current .Rmd file, make copy , rename accordingly, go steps Middle group. Since exercise close example worked chapter, added solutions end. Feel free check peers discuss difference ratings guilt, caused presentation critical evidence, Timepoint 8 Timepoint 9?","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"non-parametric-tests","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4 Non-Parametric tests","text":"previous chapters really focusing -subjects within-subjects t-tests fall category parametric tests. One main things know tests fair number assumptions need check first make sure results valid. looked main body chapter, get practice progress, one question might , assumptions met (\"violated\"\" termed)?options ? tests known non-parametric tests fewer assumptions parametric tests can run quite quickly using approach seen t-tests. non-parametric \"t-tests\" generally require assumption normality tend work either medians data (opposed mean values) rank order data - .e., highest value, second highest, lowest - opposed actual value.just like slightly different versions t-tests different non-parametric tests -subjects designs within-subjects designs:Mann-Whitney U-test non-parametric equivalent -subjects t-testThe Wilcoxon Signed-Ranks Test non-parametric equivalent within-subjects t-test.Note: Mann-Whitney Wilcoxon Signed-Ranks tests now bit antiquated designed done hand computer processing power limited. However, still used Psychology still see older papers, worth seeing one action least.example, concerned data far normally distributed, might use Mann-Whitney Wilcoxon Signed-Ranks Test depending design. run Mann-Whitney U-test can try Wilcoxon Signed-Ranks Test time uses function - just matter saying paired = TRUE.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"our-scenario","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.1 Our Scenario","text":"Aim: examine influence perceived reward problem solving.Procedure: 14 Participants 2 groups (7 per group) asked solve difficult lateral thinking puzzle. One group offered monetary reward completing quick possible. One group offered nothing; just internal joy getting task completed correct.Task: participants asked solve following puzzle. \"Man walks bar asks glass water. barman shoots gun. man smiles, says thanks, leaves. ?\"IV: Reward group vs. Reward groupDV: Time taken solve puzzle measured minutes.Hypothesis: hypothesise participants given monetary incentive solving puzzle solve puzzle significantly faster, measured minutes solve puzzle, given incentive.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"assumption-check","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.2 Assumption check","text":"data boxplot data try visualise happening data.\n\nTable 7.2: Table showing time taken complete puzzle Reward Reward groups\n\nFigure 7.2: Boxplots showing time taken solve puzzle two conditions, Reward vs Reward. Outliers represented solid blue dots.\nLooking boxplots potentially issues skew data (see Reward group particular) conditions showing least one outlier. convinced assumption normality held run Mann-Whitney U-test - non-parametric equivalent two-sample t-test (.e., independent groups) - require assumption normal data.","code":"## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> =\n## \"none\")` instead."},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"the-descriptives","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.3 The descriptives","text":"Next, always, look descriptives well make subjective, descriptive inference pattern results. One thing note Mann-Whitney analysis based \"rank order\" data regardless group. code table put data order lowest highest added rank order column. used rank() function create ranks setting ties.method = \"average\". go case can read ?rank.\nTable 7.3: Table showing time taken complete puzzle Reward Reward groups rank order times.\ntable descriptives dataset code used create .\nTable 7.4: Descriptive (N, Medians Mean Ranks) two groups (Reward vs Reward) time taken solve puzzle.\nBased figure descriptive data, can suggest appears real difference two groups terms time taken solve puzzle. group offered reward slightly higher spread data reward group. However, medians mean ranks comparable.","code":"\nscores_rnk <- scores %>%\n  arrange(Time) %>%\n  mutate(ranks = rank(Time, ties.method = \"average\"))\nByGrp <- group_by(scores_rnk, Group) %>%\n  summarise(n_Pp = n(),\n            MedianTime = median(Time),\n            MeanRank = mean(ranks))"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"running-the-inferential-test","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.4 Running the inferential test","text":"now run Mann-Whitney U-test see difference two groups significant . , somewhat confusingly, us wilcox.test() function. code analysis current data (tibble scores, DV column Time, IV column Group) shown . works just like t-test() function can use either vectors formula approach.Note:couple additional calls function can read using ?wilcox.test() approach.just easily used scores_rnk tibble wilcox.test() opposed scores. using scores show need put ranks wilcox.test() function, create runs analysis. created run descriptives.output test tidied tibble using tidy()\nTable 7.5: Output Mann-Whitney U-test\nmain statistic (test-value) Mann-Whitney U-test called U-value shown table statistic; .e. U = 19 can see results difference non-significant p = 0.535.Note:eagle-eyed spot test actually says Wilcoxon rank sum test. fine. Mann-Whitney U-test calculated sum ranks (shown table ). Wilcoxon rank sum test just , sum ranks. U-value created summed ranks.mistake Wilcoxon rank sum test mentioned - -subjects - Wilcoxon Signed-Ranks test within-subjects mentioned . different tests.However, one thing note U unstandardised value - meaning dependent values sampled compared U values look magnitude one effect versus another. second thing note U-value wilcox.test() return different U-value depending condition stated Group 1 Group 2.Compare outputs two tests switched order conditions:Version 1:Version 2:U-value two tests , Version 1, U = 30 Version 2, U = 19. may seem odd, actually test correct. However, strictly speaking U-value smaller two-values given different outputs. U-value calculated. groups U-value one checked significance smaller two.U unstandardised value, present Mann-Whitney U-test usually also give Z-statistic, standardised version U-value. also present effect size, commonly r.Z r can calculated follows:Z = \\(\\frac{U - \\frac{N1 \\times N2}{2}}{\\sqrt\\frac{N1 \\times N2 \\times (N1 + N2 + 1)}{12}}\\)Z = \\(\\frac{U - \\frac{N1 \\times N2}{2}}{\\sqrt\\frac{N1 \\times N2 \\times (N1 + N2 + 1)}{12}}\\)r = \\(\\frac{Z}{\\sqrt(N1 + N2)}\\)r = \\(\\frac{Z}{\\sqrt(N1 + N2)}\\)Putting formulas coded format look like :write-written :time taken solve problem Reward group (n = 7, Mdn Time = 9.02, Mean Rank = 8.29) reward group (n = 7, Mdn Time = 8.05, Mean Rank = 6.71) compared using Mann-Whitney U-test. significance difference found, U = 19, Z = -0.703, p = 0.535, r = -0.188","code":"\nresult <- wilcox.test(Time ~ Group, \n                      data = scores, alternative = \"two.sided\", \n            exact = TRUE, correct = FALSE) %>%\n  tidy()\nresult_v1 <- wilcox.test(scores %>% filter(Group == \"Reward\") %>% pull(Time),\n                         scores %>% filter(Group == \"No Reward\") %>% pull(Time),\n                         data = scores, alternative = \"two.sided\",\n                         exact = TRUE, correct = FALSE) %>%\n  tidy()\nresult_v2 <- wilcox.test(scores %>% filter(Group == \"No Reward\") %>% pull(Time),\n                         scores %>% filter(Group == \"Reward\") %>% pull(Time),\n                         data = scores, alternative = \"two.sided\",\n                         exact = TRUE, correct = FALSE) %>%\n  tidy()\nU <- result$statistic\nN1 <- ByGrp %>% filter(Group == \"Reward\") %>% pull(n_Pp)\nN2 <- ByGrp %>% filter(Group == \"No Reward\") %>% pull(n_Pp)\nZ <- (U - ((N1*N2)/2))/ sqrt((N1*N2*(N1+N2+1))/12)\nr <- Z/sqrt(N1+N2)"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"last-point-on-calculating-u-and-reporting-the-test","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.5 Last point on calculating U and reporting the test","text":"final write-know, codes, U = 19 smallest U-value test. However put alternative U, U = 30, calculated Z got Z = 0.703, opposed Z = -0.703. standardised statistic value just opposite polarity (either positive negative). fine though can look medians mean ranks make sure interpreting data correctly.However, watch writing test presenting correct U-value - remembering technically present smallest two U-values (refer back Version 1 Version 2 using wilcox.test()) . Fortunately run analyses figure smaller U (though wanted). quicker way using formula:\\(U1 + U2 = N1 \\times N2\\)U1 U-value wilcox.test() functionN1 number people one group (technically matter group) N2 number people group.actually know U-values ran tests; U1 = 30 U2 = 19, know two groups N1 = 7 N2 = 7. put numbers formula get\\(U1 + U2 = N1 \\times N2\\)=> \\(30 + 19 = 7 \\times 7\\)=> \\(49 = 49\\)sides equal 49. say know one U-values; course know Ns. Well can quickly figure U-value based :\\(U2 = (N1 \\times N2) - U1\\)example, know U1 = 19, N1 = 7 N = 7 :\\(U2 = (7 \\times 7) - 19\\)=> \\(U2 = (49) - 19\\)=> \\(U2 = 30\\)just present smallest two U-values, case U = 19.Hopefully now decent understanding Mann-Whitney test. also try running Wilcoxon Signed-Ranks Test well though might read little present . similar Mann-Whitney though able get .Oh, last last point, remember test ? Mann-Whitney -subjects within-subjects, Wilcoxon Signed-Ranks test ? know different designs, ? Well, silly memory aid : name -subjects designs, know, independent designs. Add fact late great singer Whitney Houston starred \"Bodyguard\" maintaining right freedom independence. whenever get stuck knowing test , remember Whitney wanted independence \"Bodyguard\" ok. say good memory aid!","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"solutions-to-questions-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"the-paired-sample-t-test-1","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1 The Paired-Sample t-test","text":"","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-1-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.1 Task 1","text":"Return Task","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nratings <- read_csv(\"GuiltJudgements.csv\")"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-2-7","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.2 Task 2","text":"carried correctly, lates 150 rows 4 columns. comes 75 participants giving two responses - TimePoint 12 TimePoint 13.Return Task","code":"\nlates <- ratings %>%\n  filter(Evidence == \"Late\") %>% \n  select(Participant, Evidence, `12`, `13`) %>% \n  rename(Twelve = `12`, Thirteen = `13`) %>%\n  pivot_longer(cols = Twelve:Thirteen, \n               names_to = \"Timepoint\", \n               values_to = \"GuiltRating\")"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-3-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.3 Task 3","text":"\nFigure 7.3: Potential Solution Task 3\nReturn Task","code":"\nlates %>% \n  ggplot(aes(GuiltRating)) +\n  geom_histogram(binwidth = 1) +\n facet_wrap(~Timepoint) +\n  labs(x = \"GuiltRating\", y = NULL) +\ntheme_bw()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-4-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.4 Task 4","text":"Task asks boxplot. added additional functions tidy figure bit might want play .\nFigure 7.4: Potential Solution Task 4\ncan see one outlier Thirteen condition. represented single dot far whiskers boxplot.Return Task","code":"\nlates %>% \n  ggplot(aes(x = Timepoint,\n             y = GuiltRating)) + \n  geom_boxplot() +\n  scale_y_continuous(breaks = c(1:9)) + \n  coord_cartesian(xlim = c(.5, 2.5), ylim = c(1,9), expand = TRUE) +\n  theme_bw()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-5-5","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.5 Task 5","text":"added color necessary:\nFigure 7.5: Potential Solution Task 5\ncan still see outlier top figure solid black dot.even add geom_jitter data points:\nFigure 7.6: Alternative Potential Solution Task 5\nReturn Task","code":"\nlates %>% \n  ggplot(aes(x=Timepoint,y=GuiltRating))+\n  geom_violin(aes(fill = Timepoint), alpha = .2) + \n  geom_boxplot(width = 0.5) +\n  scale_y_continuous(breaks = c(1:9)) + \n  coord_cartesian(ylim = c(1,9), expand = TRUE) +\n  theme_bw()\nlates %>% \n  ggplot(aes(x=Timepoint,y=GuiltRating))+\n  geom_violin(aes(fill = Timepoint), alpha = .2) + \n  geom_boxplot(width = 0.5) +\n  geom_jitter(aes(fill = Timepoint), width = .1, alpha = .2) + \n  scale_y_continuous(breaks = c(1:9)) + \n  coord_cartesian(ylim = c(1,9), expand = TRUE) +\n  theme_classic()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-6-4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.6 Task 6","text":"show following data:\nTable 7.6: Descriptive data current study\nReturn Task","code":"\ndescriptives <- lates %>% \n  group_by(Timepoint) %>%\n  summarise(n = n(),\n            mean = mean(GuiltRating),\n            sd = sd(GuiltRating),\n            se = sd/sqrt(n),\n            LowerCI = mean - 1.96*se,\n            UpperCI = mean + 1.96*se)\nknitr::kable(descriptives, align = \"c\", caption = \"Descriptive data for the current study\")"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-7-4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.7 Task 7","text":"basic barplot 95% Confidence Intervals.embellished figure little can mess around code see bit .\nFigure 7.7: Possible Solution Task 7\nOne thing watch code scale_y_continuous() function helps us set length tick marks (-) y-axis. Rather oddly, set limits = ... values ylim = ... coord_cartesian() figure behave oddly may disappear. coord_cartesian() zoom function must set within limits scale, set scale_y_continuous().One thing watch code scale_y_continuous() function helps us set length tick marks (-) y-axis. Rather oddly, set limits = ... values ylim = ... coord_cartesian() figure behave oddly may disappear. coord_cartesian() zoom function must set within limits scale, set scale_y_continuous().alternative way display just means errorbars use pointrange approach. image shows 95% CIAn alternative way display just means errorbars use pointrange approach. image shows 95% CI\nFigure 7.8: Alternative Solution Task 7\nReturn Task","code":"\nggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + \n  geom_col(colour = \"black\") +\n  scale_fill_manual(values=c(\"#999000\", \"#000999\")) +\n  scale_x_discrete(limits = c(\"Twelve\",\"Thirteen\")) +\n  labs(x = \"Timepoint of Evidence\", y = \"GuiltRating\") +\n  guides(fill=\"none\") +\n  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI),\n                position = \"dodge\", width = .15) +\n  scale_y_continuous(breaks = c(1:9), limits = c(0,9)) +\n  coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) +\n  theme_classic()\nggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + \n  geom_pointrange(aes(ymin = LowerCI, ymax = UpperCI))+\n  scale_x_discrete(limits = c(\"Twelve\",\"Thirteen\")) +\n  labs(x = \"Timepoint of Evidence\", y = \"GuiltRating\") +\n  guides(fill=\"none\")+\n  scale_y_continuous(breaks = c(1:9), limits = c(0,9)) +\n  coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) +\n  theme_bw()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-8-4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.8 Task 8","text":"Remember set paired = TRUE run within-subjects t-testAlternatively, using filter() pull() functions make force given condition first condition. , , forcing condition Thirteen first condition values match approach. forced condition Twelve first condition difference t-value change polarity (positive negative vice versa).reason two outputs formula (top) method (x ~ y) actually process second approach, just sure first condition. second approach (bottom) just makes clearer.Note: conf.low conf.high values 95% Confidence Intervals mean difference two conditions. written something like, \"difference two groups (M = -1.76, 95% CI = [-2.19, -1.33])\".Return Task","code":"\nresults <- t.test(GuiltRating ~ Timepoint, \n                  data = lates, \n                  paired = TRUE, \n                  alternative = \"two.sided\") %>% tidy()\nresults <- t.test(lates %>% filter(Timepoint == \"Thirteen\") %>% pull(GuiltRating),\n                  lates %>% filter(Timepoint == \"Twelve\") %>% pull(GuiltRating),\n                  paired = TRUE, \n                  alternative = \"two.sided\") %>% tidy()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-9-1","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.9 Task 9","text":"potential write-study follows:\"paired-samples t-test ran compare change guilt ratings (M = 5.8, SD = 1.5) (M = 4.04, SD = 1.93) crucial evidence heard. significant difference found (t(74) = 8.23, p = 4.7113406^{-12}) Timepoint 13 average rating 1.76 units lower Timepoint 12. tells us critical evidence influence rating guilt jury members.\"Working rounding p-valuesWhen rounding p-values less .001, rounding give value 0 technically wrong - probability low 0. , according APA format, values less .001 normally written p < .001. create reader-friendly p-value, try something like following code:instead writing t(74) = 8.23, p = 4.7113406^{-12}, write t(74) = 8.23, p < .001The -line coding options look like:p = `r results %>% pull(p.value)` p = 4.7113406^{-12}&`r ifelse(results$p.value < .001, \"p < .001\", paste0(\"p = \", round(results$p.value,3)))` p < .001Return TaskChapter Complete!","code":"\nifelse(results$p.value < .001, \n       \"p < .001\", \n       paste0(\"p = \", round(results$p.value,3))) "},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"apes---alpha-power-effect-sizes-sample-size","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8 APES - Alpha, Power, Effect Sizes, Sample Size","text":"","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"overview-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.1 Overview","text":"now mainly spent time data-wrangling, understanding probability, visualising data, recently, running inferential tests, .e. t-tests. lectures, however, also started learn additional aspects inferential testing trying reduce certain types error analyses. balance minimising error inferential statisitcs focus chapter.First thing remember two types hypotheses Null Hypothesis Significance Testing (NHST) trying establish probability null hypothesis accepted. two hypotheses :null hypothesis states compared values equivalent , referring means, written : \\(H_0: \\mu_1 = \\mu_2\\)alternative hypothesis states compared values equivalent , referring means, written : \\(H_1: \\mu_1 \\ne \\mu_2\\).Now, decision hypothesis prone degree error , learn, two main types error worry Psychology :Type error - False Positives, error rejecting null hypothesis rejected (otherwise called alpha \\(\\alpha\\)). words, conclude real \"effect\" fact effect. field standard rate acceptable false positives \\(\\alpha = .05\\) meaning theory 1 20 studies may false positive.Type II error - False Negatives, error retaining null hypothesis false (otherwise called beta \\(\\beta\\)). words, conclude real \"effect\" fact one. field standard rate acceptable false negatives \\(\\beta = .2\\) meaning theory 1 5 studies may false negative.Adding ideas hypotheses errors, going look idea power learn long-run probability correctly rejecting null hypothesis fixed effect size fixed sample size; .e. correctly concluding effect real effect detect. Power calculated \\(power = 1-\\beta\\) directly related False Negative rate. field standard False Negatives \\(\\beta = .2\\) field standard power \\(power = 1 - .2 = .8\\), given effect size sample size (though papers, including Registered Reports often required power least \\(power >= .9\\)). , \\(power = .8\\) means majority studies find effect one detect, assuming study maintains rates error power.Unfortunately, however, psychological research criticised neglecting power \\(\\beta\\) planning studies resulting called \"underpowered\" \"low powered\" studies - meaning error rates higher think , power lower think , study unreliable. Note \\(\\beta\\) increases (false negative rate increases), power decreases; power false positive rates also related, though less directly. fact, low powered studies, combined undisclosed analytical flexibility publication bias, thought key issue replication crisis within field. may large number studies null hypothesis rejected , unpublished studies written find effect . turn, case, field becomes noisy unsure studies replicate. issues like led us redevelop courses really want understand power much possible.chapter power, error rates, effect sizes, sample sizes. learn:relationship power, alpha, effect sizes sample sizeshow calculate certain effect sizeshow determine appropriate sample sizes given scenariosand interpret power analyses.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"introduction-to-power","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2 Introduction to Power","text":"written selected material chapter give better understanding power interacts effect size, sample size, alpha. also suggest optional material can look play get rounder view.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"blog-post","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2.1 Blog post","text":"Read following blog Power: blog fictional conversation professor student importance power. Grab coffee read. worry reading additional papers unless want ; just blog fine get understanding. trying understand blog relationship sample size effect sizes, whether result study likely replicate based power original study.Power Dialogues PIGEE University Illinois.Using power design studyTo reiterate, power defined probability correctly rejecting null hypothesis fixed effect size fixed sample size. , power key decision design study, premise higher power planned study, better.Two relationships important understand:given sample size \\(\\alpha\\), power study higher effect looking assumed large effect opposed small effect; large effects easier detect., given effect size \\(\\alpha\\), power study higher increase sample size.relationships see , little control size effect trying detect (lives real world control), can instead increase power study increasing size sample (also reducing sources noise measurement error study). , planning study, good researcher consider following four key elements - APES:alpha (false positive rate - Type 1 error): commonly thought significance level; usually set \\(\\alpha = .05\\)power: probability rejecting null hypothesis given effect size sample size, \\(power = .8\\) usually cited minimum power aim based false negative rate set \\(\\beta = .2\\);effect size: size association difference trying detect;sample size: number observations (usually, participants, sometimes also stimuli) study.Note: power depends several variables, useful think power function varying value rather single fixed quantity.Now cool thing APES know three elements, can calculate fourth. reality, two common approaches designing study :determine appropriate sample size required reject null hypothesis, high probability, effect size interested . , decide \\(\\alpha\\), \\(power\\), effect size, calculate sample size required study. Generally, smaller assumed effect size, participants need, assuming power alpha held constant.determine smallest effect size can reliably detect given sample size. , know everything except effect size. example, say using open dataset know run 100 participants, add participants, want know minimum effect size detect dataset set \\(power\\) \\(\\alpha\\) field standards.Hopefully gives idea use power determine sample sizes studies - sample size just pulled thin air. approaches described called priori power analyses stating power level want (priori means ) study.However, may now thinking, everything connected, can use effect size study sample size determine power study run ? ! Well, can wrong . actually called Observed Post-Hoc power papers discourage calculating grounds effect size using true effect size population interested ; just effect size sample. indication power analysis misleading. Avoid . can read , , time like:  Lakens (2014) Observed Power, editor asks post-hoc power analyses. short, stick using priori power analyses approaches use determine required sample size achievable reliable effect size.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"video-about-power-and-sample-size","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2.2 Video about power and sample size","text":"now also watch short nonetheless highly informative video Daniel Lakens Power Sample Size. help consolidate points. shirt amazing!Power Analysis Sample Size Decisions Daniel Lakens","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"useful-links","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2.3 Useful links","text":"Finally, number great webpages blogs help understand concepts chapter. think might good look . look understand chapter, come back really help progress becoming responsible researcher. deliberately giving number options everyone one analogy work best one paper make everything click place. example different person person variety explanations help.YouTube video Dan Quintana (University Oslo) showing use pwr package calculate power t-tests, correlations, one-way ANOVAs https://www.youtube.com/watch?v=ZIjOG8LTTh8A YouTube video Dan Quintana (University Oslo) showing use pwr package calculate power t-tests, correlations, one-way ANOVAs https://www.youtube.com/watch?v=ZIjOG8LTTh8A shiny app created Lisa Debruine (University Glasgow) guessing effect size two conditions http://shiny.psy.gla.ac.uk/guess/shiny app created Lisa Debruine (University Glasgow) guessing effect size two conditions http://shiny.psy.gla.ac.uk/guess/blog Daniel Lakens (Eindhoven University Technology) determining smallest effect size interested . often referred Smallest Effect Size Interest (SESOI) http://daniellakens.blogspot.com/2017/05/-power-analysis-implicitly-reveals.htmlA blog Daniel Lakens (Eindhoven University Technology) determining smallest effect size interested . often referred Smallest Effect Size Interest (SESOI) http://daniellakens.blogspot.com/2017/05/-power-analysis-implicitly-reveals.htmlAn interactive webpage Kristoffer Magnusson (Karolina Instituet, Stockholm) interpreting Cohen's d effect size https://rpsychologist.com/d3/cohend/interactive webpage Kristoffer Magnusson (Karolina Instituet, Stockholm) interpreting Cohen's d effect size https://rpsychologist.com/d3/cohend/shiny app Hause Lin (University Toronto) showing conversion one effect size another http://escal.site/shiny app Hause Lin (University Toronto) showing conversion one effect size another http://escal.site/Frontiers Psychology paper Daniel Lakens calculating various effect sizes t-tests ANOVAs https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/fullA Frontiers Psychology paper Daniel Lakens calculating various effect sizes t-tests ANOVAs https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/fullA blog Daniel Lakens Type Type II errors acceptable. short, justify everything http://daniellakens.blogspot.com/2019/05/justifying--alpha--minimizing-.htmlA blog Daniel Lakens Type Type II errors acceptable. short, justify everything http://daniellakens.blogspot.com/2019/05/justifying--alpha--minimizing-.htmlChapter 9 Ian Walker's \"Research Methods statistics\" availble read online University Library. short chapter hypothesis testing power really brining everything last chapters together.Chapter 9 Ian Walker's \"Research Methods statistics\" availble read online University Library. short chapter hypothesis testing power really brining everything last chapters together.forget Chapter 3 \"7 Deadly Sins Psychology: Manifesto Reforming Culture Scientific Practice\" good read topic power unreliable research. book available University Library can bought reputable bookshops online repositories.forget Chapter 3 \"7 Deadly Sins Psychology: Manifesto Reforming Culture Scientific Practice\" good read topic power unreliable research. book available University Library can bought reputable bookshops online repositories.really nice paper Marjan Bakker colleagues whether people put power analyses ethics proposals. nice introduction power results show many different ways researchers actually calculate sample sizes https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079A really nice paper Marjan Bakker colleagues whether people put power analyses ethics proposals. nice introduction power results show many different ways researchers actually calculate sample sizes https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079A paper Marcus Munafo colleagues mentioned many times might help read now concepts ideas famility now. https://www.nature.com/articles/s41562-016-0021A paper Marcus Munafo colleagues mentioned many times might help read now concepts ideas famility now. https://www.nature.com/articles/s41562-016-0021A paper Schafer Schwarz (2019) aimed helping people make meaningful interpretation effect sizes Psychology. also explores differences commonly found effect sizes sub-disciplines Psychology. https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/fullA paper Schafer Schwarz (2019) aimed helping people make meaningful interpretation effect sizes Psychology. also explores differences commonly found effect sizes sub-disciplines Psychology. https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/fullFinally, paper Brysbaert (2019) shows just many participants need variety common designs Psychology studies. shocked difference number participants needed compared number participants used published studies https://www.journalofcognition.org/article/10.5334/joc.72/Finally, paper Brysbaert (2019) shows just many participants need variety common designs Psychology studies. shocked difference number participants needed compared number participants used published studies https://www.journalofcognition.org/article/10.5334/joc.72/Hopefully given good basis understanding power, sample sizes, alpha, effect sizes. difficult concepts grasp take lot time thinking interacting really start sink . Hopefully however, nothing else, least come away idea number participants run study arbitrary decision fact relationship effect size want test level error (Type Type II) willing accept.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practical-apes-calculations","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3 Practical APES Calculations","text":"Hopefully now decent understanding least four APES need considered designing study: \\(\\alpha\\), \\(power\\), effect size sample size. going look calculating understanding elements. fully understand everything power complete chapter - believe us say many seasoned researchers struggle parts - just need get general gist always level acceptable error hypothesis testing trying minimise given effect size (.e., magnitude difference, relationship, association).jump bit now start running analyses help understanding alpha, power, effect sizes sample size! help understanding focus t-tests chapter know well previous chapters.Effect Sizes - Cohen's \\(d\\)number different effect sizes can choose calculate, common one t-tests Cohen's d: standardised difference two means (units SD) written d = effect-size-value. key point Cohen's d standardised difference, meaning can used compare studies regardless measurement made. Take example height differences men women estimated 5 inches (12.7 cm). effect size, unstandardised effect size every sample test, difference dependent measurement tools, measurement scale, errors contained within . using standardised effect size allows make comparisons across studies regardless measurement error. standardised terms, height difference considered medium effect size (d = .5) Cohen (1988, cited Schafer & Schwarz (2019)) defined representing \"effect likely visible naked eye careful observer\". Cohen (1988) fact stated three sizes Cohen's d people use guide:may wish read paper later different effect sizes psychology - Schafer Schwarz (2019) Meaningfulness Effect Sizes Psychological Research: Differences Sub-Disciplines Impact Potential Biases.One thing note formula Cohen's d slightly different depending type t-test used. even within type t-test formula can sometimes change depending read. chapter, go following formulas:One-sample t-test & within-subjects (paired-sample) t-test:\\[d = \\frac{t}{sqrt(N)}\\]-subjects (two-sample) t-test:\\[d = \\frac{2t}{sqrt(df)}\\]now try using formulas order calculate effect sizes given scenarios; work calculating power later chapter.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.1 Task 1: Effect size from a one-sample t-test","text":"run one-sample t-test discover significant effect, t(25) = 3.24, p = .003. Calculate d determine whether effect size small, medium large.\nUse appropriate formula one-sample t-tests.\n\ngiven t-value df (degrees freedom), still need determine n calculate d.\n\nAccording Cohen (1988), effect size small (.2 .5), medium (.5 .8) large (> .8).\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter, digits, many people run study: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test considered: smallmediumlarge","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT2","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.2 Task 2: Effect size from a two-sample t-test","text":"run two-sample t-test discover significant effect, t(30) = 2.9, p = .007. Calculate d determine whether effect size small, medium large.\nUse appropriate formula two-sample t-tests.\n\nremember df = (N-1) + (N-1) two-sample t-test.\n\nAccording Cohen (1988), effect size small (.2 .5), medium (.5 .8) large (> .8).\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter, digits, many people run study: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test considered: smallmediumlarge","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT3","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.3 Task 3: Effect Size from a matched-sample t-test","text":"run paired-sample t-test ASD sample non-ASD sample discover significant effect t(39) = 2.1, p < .05. many people group? Calculate d determine whether effect size small, medium large.\nneed df value determine N.\n\nmatched pairs treated like paired-sample t-test two separate groups.\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter, digits, many people group study. Note, total number participants: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test considered: smallmediumlarge\ndf paired-samples matched-pairs t-test calculated df = N - 1.\nConversely, find total number participants: N = df + 1 N = 39 + 1 = 40.\n\nGiven matched-pairs t-test, design equal number participants group. Therefore 40 participants group.\n","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT4","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.4 Task 4: t-value and effect size for a between-subjects experiment","text":"run -subjects design study descriptives tell : Group 1, M = 10, SD = 1.3, n = 30; Group 2, M = 11, SD = 1.7, n = 30. Calculate t d -subjects experiment.\ncan calculate d (using appropriate formula -subjects experiment), need first calculate t using formula:\n\nt = (Mean1 - Mean2)/sqrt((var1/n1) + (var2/n2))\n\nvar stands variance formula. Variance standard deviation, right? Variance measured squared units. equation, require variance calculate t standard deviation, need remember \\(var = SD^2\\) (otherwise written \\(var = SD \\times SD\\).\n\nNow t-value, calculating d also need degrees freedom. Think calculate df -subjects experiment, taking n Group 1 Group 2 account.\n\nRemember convention people report t positive. , convention also dictates d reported positive value.\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter correct t-value test, rounded two decimal places: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Based t-value , enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test described : smallmediumlargeNow comfortable calculating effect sizes, look using establish sample size required power. One thing realise progress true effect size population something know, need justify one design. clever approach laid Daniel Lakens blog previous section Smallest Effect Size Interest (SESOI) - set smallest effect interested ! can determined theoretical analysis, previous studies, pilot studies, rules thumb like Cohen (1988). However, also keep mind lower effect size, larger sample size need. Everything trade-!Power CalculationsWe going use function pwr.t.test() run calculations pwr library. really useful library functions various tests, just use t-tests right now.Remember information function pwr.t.test(), simply ?pwr.t.test console. can look webpages get idea (bad ideas spot erroneously calculate post-hoc power!):quick-R summary pwr package - https://www.statmethods.net/stats/power.htmlthe pwr package vignette -  https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.htmlFrom see pwr.t.test() takes series inputs:n: observations/participants, per group independent samples version, number subjects matched pairs paired one-sample designs.d: effect size interestsig.level \\(\\alpha\\)power \\(1-\\beta\\)type: type t-test; e.g. \"two.sample\", \"one.sample\", \"paired\"alternative: type hypothesis; \"two.sided\", \"one.sided\"works leave one principle. give info returns element missing. , example, say needed know many people per group need detect effect size low d = .4 power = .8, alpha = .05 two.sample (-subjects) t-test two.sided hypothesis test. :show following output, , look , tells need 99.0803248 people per condition.get whole people like conservative estimates actually run 100 per condition. lot people!!!One problem though output pwr.t.test() object easy work terms getting values reproducible. However, function purr::pluck() allows us pluck values objects. code look like thisSo call n_test get answer , saved single value easier work :use ceiling() funtion round whole people:Note: ceiling() better use round() dealing people always rounds . example, ceiling(1.1) gives 2. round() hand useful rounding effect size, example, two decimal places - e.g. d = round(.4356, 2) give d = 0.44We use approach pwr.t.test() %>% pluck() pwr.t.test() %>% pluck() %>% ceiling() throughout rest chapter get used .\nstart next task, need make sure loaded tidyverse.","code":"\npwr.t.test(d = .4,\n           power = .8,\n           sig.level = .05,\n           alternative = \"two.sided\",\n           type = \"two.sample\")## \n##      Two-sample t test power calculation \n## \n##               n = 99.08032\n##               d = 0.4\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\nn_test <- pwr.t.test(d = .4, \n                     power = .8,\n                     sig.level = .05,\n                     alternative = \"two.sided\",\n                     type = \"two.sample\") %>%\n  pluck(\"n\")\nn_test## [1] 99.08032\nn_test %>% ceiling()## [1] 100"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.5 Task 5: Sample size for standard power one-sample t-test","text":"Assuming smallest effect size interest Cohen's d d = .23, minimum number participants need one-sample t-test, assuming \\(power = .8\\), \\(\\alpha = .05\\), two-sided hypothesis?Using pipeline, store answer single value called sample_size (e.g., think pluck()) round nearest whole participant.\nUse list inputs kind checklist clearly determine inputs known unknown. can help enter appropriate values code.\n\nstructure pwr.t.test() similar one shown except two.sample become one.sample\n\nalso need use pluck(\"n\") help obtain sample size %>% ceiling() round nearest whole participant.\nQuickfire QuestionsAnswer following question check answers. solutions end chapter check :Enter minimum number participants need one-sample t-test: ","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.6 Task 6: Effect size from a high power between-subjects t-test","text":"Assuming run -subjects t-test 50 participants per group want power .9, minimum effect size can reliably detect? Assume field standard \\(\\alpha = .05\\) alternative hypothesis settings (\"two-tailed\"). Using pipeline, store answer single value called cohens round two decimal places.\n, use list inputs kind checklist clearly determine inputs known unknown. can help enter values code.\n\nalso need use pluck() obtain Cohen's d, round() value rounded two decimal places.\n\nforget quotes using pluck(). .e. pluck(\"value\") pluck(value)\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Based information given, set type function? one.sampletwo.sampleBased output, enter minimum effect size can reliably detect test, rounded two decimal places: According Cohen (1988), effect size t-test smallmediumlargeSay run study find effect size determined d = .50. Given know power, select statement accurate: study sufficiently powered analysis indicates can reliably detect effect sizes smaller d = .65the study potentially underpowered analysis indicates can really reliably detect effect sizes larger d = .65","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.7 Task 7: Power of Published Research","text":"Thus far used hypothetical situations - now go look paper Open Stats lab website called Music Convey Social Information Infants?. can download pdf look , determine power significant t-tests reported Experiment 1 Results section page 489. one-sample t-test paired-samples t-test consider, summarised . Assume testing power = .8, alpha = .05. Based calculations either stated effects underpowered?one-sample: t(31) = 2.96, p = .006paired t-test: t(31) = 2.42, p = .022\none-sample t-test paired t-test use formula Cohen's d.\n\ncalculate n: n = df + 1.\n\nCalculate achievable Cohens d studies calculate established Cohen's d studies.\nThinking Cap PointBased found , think following questions discuss groups:t-tests believe potentially underpowered?think may ?Additional information discussion can found solutions end chapter.One caveat Tasks 6 7: keep mind looking single studies using one sample potentially huge number samples within population. degree variance true effect size within population regardless effect size one given sample. means little bit cautious making claims study. Ultimately higher power better can detect smaller effect sizes!Concluding remarksSo hopefully now starting see interaction alpha, power, effect sizes, sample size. always want high-powered studies depending size effect interested (small large), \\(\\alpha\\) level, determine number observations need make sure study well powered. Points note:Lowering \\(\\alpha\\) level (e.g., .05 .01) reduce power.Lowering effect size (e.g., .8 .2) reduce power.Increasing power (e.g., .8 .9) require participants.also possible increase power fixed sample size reducing sources noise study.high-powered study looking detect small effect size low alpha may require large number participants!","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practice-your-skills-11","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4 Practice Your Skills","text":"Lab 8: APES AssignmentIn order complete assignment first download assignment .Rmd file need edit assignment: titled GUID_PracticeSkills_Ch8.Rmd. can downloaded within zip file link. downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top Download assignment zip file .NOTE: nearly problems , need replace NULL value pipeline code computes value. Please pay special attention question asking output, e.g. value tibble; asked value output, make sure single value value stored tibble. Finally, altering code inside code blocks, please re-order rename code blocks (T1, T2, ... etc.).also recommended \"knit\" report able see accomplished spot potential errors. great thing close whole programme, restart , knit code. test whether remembered include essential elements, libraries, code.APES: Alpha, Power, Effect Size, Sample SizeIn chapter looking interplay four components Alpha, Power, Effect Size, Sample Size. important part experimental design understand help understand studies worth paying attention help design studies coming years know just many people run make effect find.starting check:.Rmd file saved working directory. assessments ask save format GUID_PracticeSkills_Ch8.Rmd GUID replaced GUID. Though formative assessment, may good practice .LibrariesYou need use tidyverse broom libraries assignment, load library code chunk .need use tidyverse broom libraries assignment, load library code chunk .Hint: library(package)Hint: library(package)Basic Calculations","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.1 Task 1","text":"set study power value \\(power = .87\\). two decimal places, Type II error rate study?Replace NULL T1 code chunk either single value, mathematical notation, error_rate returns actual value Type II error rate study. mathematical notation mean use appropriate formula insert actual values.","code":"\nerror_rate <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT2","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.2 Task 2","text":"run two-sample t-test discover significant effect, t(32) = 3.26, p < .05. Using appropriate formula, given chapter, calculate effect size t-test.Replace NULL T2 code chunk mathematical notation effect1 returns value effect size. round value.","code":"\neffect1 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT3","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.3 Task 3","text":"run paired-sample t-test discover significant effect, t(43) = 2.24, p < .05. Using appropriate formula, given chapter, calculate effect size t-test.Replace NULL T3 code chunk mathematical notation effect2 returns value effect size. round value.Using Power function","code":"\neffect2 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT4","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.4 Task 4","text":"Replace NULL T4 code chunk pipeline combining pwr.t.test(), pluck() ceiling(), determine many participants needed sufficiently power paired-samples t-test \\(power = .9\\) \\(d = .5\\)? Assume two-sided hypothesis \\(\\alpha = .05\\). Ceiling answer nearest whole participant store value participants.Replace NULL T4 code chunk pipeline combining pwr.t.test(), pluck() ceiling(), determine many participants needed sufficiently power paired-samples t-test \\(power = .9\\) \\(d = .5\\)? Assume two-sided hypothesis \\(\\alpha = .05\\). Ceiling answer nearest whole participant store value participants.Hint: Remember quotes pluckHint: Remember quotes pluck","code":"\nparticipants <- NULL "},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.5 Task 5","text":"Using pipeline similar Task 4, minimum effect size one-sample t-test study (two-tailed hypothesis) reliably detect given following details: \\(\\beta = .16, \\alpha = 0.01, n = 30\\). Round two decimal places replace NULL T5 code chunk store value effect3.Using pipeline similar Task 4, minimum effect size one-sample t-test study (two-tailed hypothesis) reliably detect given following details: \\(\\beta = .16, \\alpha = 0.01, n = 30\\). Round two decimal places replace NULL T5 code chunk store value effect3.Hint: Remember going round() ceiling()Hint: Remember going round() ceiling()","code":"\neffect3 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.6 Task 6","text":"Study 1You run -subjects study establish following descriptives: Group 1 (M = 5.1, SD = 1.34, N = 32); Group 2 (M = 4.4, SD = 1.27, N = 32). Replace NULL T6 code chunk following formula, substituting appropriate values, calculate t-value test. Calculate Group1 minus Group2. Store t-value tval. round tval include t = part formula.\\[ t = \\frac {{\\bar{x_{1}}} - \\bar{x_{2}}}{ \\sqrt {\\frac {{s_{1}}^2}{n_{1}} + \\frac {{s_{2}}^2}{n_{2}}}}\\]","code":"\ntval <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.7 Task 7","text":"Using tval calculated Task 6, calculate effect size study store d1 T7 code chunk , replacing NULL appropriate formula values. round d1.Using tval calculated Task 6, calculate effect size study store d1 T7 code chunk , replacing NULL appropriate formula values. round d1.Hint: Think -subjectsHint: Think -subjects","code":"\nd1 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.8 Task 8","text":"Assuming \\(power = .8\\), \\(\\alpha =.05\\) two-tailed hypothesis, based d1 value Task 7 smallest achievable effect size study, statements correct.smallest effect size study can determine d = .71. detected effect size, d1, larger study potentially suitably poweredThe smallest effect size study can determine d = .17. detected effect size, d1, larger study potentially suitably poweredThe smallest effect size study can determine d = .17. detected effect size, d1, smaller study potentially suitably poweredThe smallest effect size study can determine d = .71. detected effect size, d1, smaller study potentially suitably poweredReplace NULL T8 code chunk number statement true summary study. may help calculate store smallest achievable effect size study poss_d.Hint: use poss_d calculate smallest possible effect size study help answer question.","code":"\nposs_d <- NULL\n\nanswer_T8 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT9","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.9 Task 9","text":"Study 2Below paragraph results Experiment 4 Schroeder, J., & Epley, N. (2015). sound intellect: Speech reveals thoughtful mind, increasing job candidate's appeal. Psychological Science, 26, 877-891. saw paper previously can find details <href=\"https://sites.trinity.edu/osl/data-sets--activities/t-test-activities\", target = \"_blank\">Open Stats Lab.Recruiters believed job candidates greater intellect - competent, thoughtful, intelligent - listened pitches (M = 5.63, SD = 1.61, n = 21) read pitches (M = 3.65, SD = 1.91, n = 18), t(37) = 3.53, p < .01, 95% CI difference = [0.85, 3.13], d1 = 1.16. recruiters also formed positive impressions candidates - rated likeable positive less negative impression - listened pitches (M = 5.97, SD = 1.92) read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p < .01, 95% CI difference = [0.55, 3.24], d2 = 0.94. Finally, also reported likely hire candidates listened pitches (M = 4.71, SD = 2.26) read pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p < .01, 95% CI difference = [0.41, 3.24], d3 = 0.86.Using pwr.t.test() function, minimum effect size paper reliably detected? Test \\(power = .8\\) two-sided hypothesis. Use \\(\\alpha\\) stated paragraph smallest n stated; store value effect4 T9 code chunk . Replace NULL pipeline round effect size two decimal places.","code":"\neffect4 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT10","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.10 Task 10","text":"Given value effect4 calculated Task 9, stated alpha paragraph smallest n two groups, statements true.study enough power reliably detect effects size d3 larger.study enough power reliably detect effects size d1.study enough power reliably detect effects size d2 larger, d3.study enough power reliably detect effect sizes d1 lower.Replace NULL T10 code chunk number statement TRUE, storing single value answer_t10.","code":"\nanswer_t10 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT11","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.11 Task 11","text":"Last, least:Read following statements.general, increasing sample size increase power study.general, smaller effect sizes require fewer participants detect \\(power = .8\\).general, lowering alpha (.05 .01) decrease power study.Now look four summary statements validity statements , b c.Statements , b c TRUE.Statements c TRUE.Statements b c TRUE.None statements TRUE.Replace NULL T11 code chunk number statement correct, storing single value answer_t11.","code":"\nanswer_t11 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"the-pwr-package","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.12 The pwr package","text":"alternative solution Task 9 use pwr.t2n.test() function pwr package (Champely, 2020). allow enter n groups n1 n2 argument. use , entering n1 = 18, n2 = 21, alpha = .01, d drops just little, changing interpretation Task 10. Feel free try analysis see can figure alternative answer Task 10.Job Done - Activity Complete!Well done, finished! Now go check answers solution can found end chapter. looking check resulting output answers submitted exactly output solution - example, remember single value coded answer. alternative answers, means submitted one options return answer.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"solutions-to-questions-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5 Solutions to Questions","text":"find solutions questions activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practical-apes-calculations-1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1 Practical APES Calculations","text":"","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-1-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.1 Task 1","text":"Giving effect size d = 0.64 medium large effect size according Cohen (1988)Return Task","code":"\nd <- 3.24 / sqrt(25 +1)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-2-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.2 Task 2","text":"Giving effect size d = 1.06 large effect size according Cohen (1988)Return Task","code":"\nd <- (2*2.9) / sqrt(30)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-3-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.3 Task 3","text":"Giving N = 40 effect size d = 0.33. considered small effect size according Cohen (1988)Return Task","code":"\nN = 39 + 1\n\nd <- 2.1 / sqrt(N)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-4-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.4 Task 4","text":"Giving t-value t = 2.56 effect size d = 0.67.Remember convention people tend report t d positive values.Return Task","code":"\nt = (10 - 11)/sqrt((1.3^2/30) + (1.7^2/30))\n\nd = (2*t)/sqrt((30-1) + (30-1))"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-5-6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.5 Task 5","text":"Giving sample size n = 151Return Task","code":"\nlibrary(pwr)\n\nsample_size <- pwr.t.test(d = .23,\n                          power = .8, \n                          sig.level = .05, \n                          alternative = \"two.sided\", \n                          type = \"one.sample\") %>%\n  pluck(\"n\") %>% \n  ceiling()"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-6-5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.6 Task 6","text":"Giving Cohen's d effect size d = 0.65Return Task","code":"\ncohens <- pwr.t.test(n = 50,\n                    power = .9, \n                    sig.level = .05, \n                    alternative = \"two.sided\", \n                    type = \"two.sample\") %>% \n  pluck(\"d\") %>% \n  round(2)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-7-5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.7 Task 7","text":"Example 1Giving achievable effect size 0.51 found effect size 0.52.study seems ok authors achieve effect size low .51 found effect size .52Example 2Giving achievable effect size 0.51 found effect size 0.43.effect might reliable given effect size found much lower achievable effect size. issue researchers established sample size based previous effect size minimum effect size find important. effect size small .4 important powered studies level ran appropriate n ~52 babies (see ). Flipside course obtaining 52 babies easy; hence people consider Many Labs approach good way ahead.ONE CAVEAT making assumption study therefore flawed, keep mind one study using one sample potentially huge number samples within population. degree variance true effect size within population regardless effect size one given sample. means little bit cautious making claims study. Ultimately higher power better.calculate actual sample size required achieve power .8:Suggesting sample size n = 52 appropriate.Return Task","code":"\nach_d_exp1 <- pwr.t.test(power = .8, \n                         n = 32, \n                         type = \"one.sample\", \n                         alternative = \"two.sided\", \n                         sig.level = .05) %>% \n  pluck(\"d\") %>% \n  round(2) \n\nexp1_d <- 2.96/sqrt(31+1) \nach_d_exp2 <- pwr.t.test(power = .8, \n                         n = 32, \n                         type = \"paired\", \n                         alternative = \"two.sided\", \n                         sig.level = .05) %>% \n  pluck(\"d\") %>% \n  round(2) \n\nexp2_d <- 2.42/sqrt(31+1) \nsample_size <- pwr.t.test(power = .8,\n                          d = .4,\n                          type = \"paired\", \n                          alternative = \"two.sided\", \n                          sig.level = .05) %>%\n  pluck(\"n\") %>% \n  ceiling()"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practice-your-skills-12","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2 Practice Your Skills","text":"Libraries","code":"\nlibrary(pwr)\nlibrary(tidyverse)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-1-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.1 Task 1","text":"Type II error rate study \\(\\beta\\) = 0.13.Return Task","code":"\nerror_rate <- 1 - .87"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-2-9","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.2 Task 2","text":"effect size d = 1.1525841Return Task","code":"\neffect1 <- (2*3.26)/sqrt(32)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-3-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.3 Task 3","text":"effect size d = 0.3376927Return Task","code":"\neffect2 <- 2.24/sqrt(43+1)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-4-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.4 Task 4","text":"Given detailed scenario, appropriate number participants n = 44Return Task","code":"\nparticipants <- pwr.t.test(power = .9,\n                           d = .5,\n                           sig.level = 0.05,\n                           type = \"paired\",\n                           alternative = \"two.sided\") %>% \n  pluck(\"n\") %>% \n  ceiling()"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-5-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.5 Task 5","text":"Given detailed scenario, able detect effect size d = 0.69Return Task","code":"\neffect3 <- power.t.test(power = 1-.16,\n                      n = 30,\n                      sig.level = 0.01,\n                      type = \"one.sample\",\n                      alternative = \"two.sided\") %>% \n  broom::tidy() %>% \n  pull(delta) %>% \n  round(2)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-6-6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.6 Task 6","text":"Given stated means standard deviations, t-value study t = 2.1448226Return Task","code":"\ntval <- (5.1 - 4.4) / sqrt((1.34^2/32) + (1.27^2/32))"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-7-6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.7 Task 7","text":"Given t-value Task 6, effect size study d = 0.5447855.Return Task","code":"\nd1 <- (2*tval)/sqrt((32-1)+(32-1))"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-8-5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.8 Task 8","text":"smallest effect size study can determine d = 0.71. detected effect size, d1, smaller (d1 = 0.5447855) study suitably powered.Given outcome, 4th statement suitable answer - answer_T8 = 4.Return Task","code":"\nposs_d <- pwr.t.test(power = .8,\n                     n = 32,\n                     sig.level = 0.05,\n                     type = \"two.sample\",\n                     alternative = \"two.sided\") %>% \n  pluck(\"d\") %>% \n  round(2)\n\nanswer_T8 <- 4"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-9-2","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.9 Task 9","text":"smallest stated n n = 18 stated \\(\\alpha\\) \\(\\alpha\\) = .01Given details, minimum effect size paper reliably detected d = 1.198Return Task","code":"\neffect4 <- pwr.t.test(power = .8,\n                      n = 18,\n                      sig.level = .01,\n                      alternative = \"two.sided\",\n                      type = \"two.sample\") %>% \n  pluck(\"d\") %>% \n  round(3)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-10-1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.10 Task 10","text":"study enough power detect effect sizes d1 lower answer_t10 = 4However, worth keeping mind looking one study drew one sample population samples. means always uncertainty true effect size difference association - taking different sample may given different effect size. , comparison making entirely valid see reminder always think power planning studies rather search criticism.Return Task","code":"\nanswer_t10 <- 4"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-11","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.11 Task 11","text":"general, increasing sample size increase power study whereas lowering alpha (.05 .01) decrease power study. , statements c, answer_t11 = 2.Return TaskChapter Complete!","code":"\nanswer_t11 <- 2"},{"path":"correlations.html","id":"correlations","chapter":"9 Correlations","heading":"9 Correlations","text":"","code":""},{"path":"correlations.html","id":"overview-9","chapter":"9 Correlations","heading":"9.1 Overview","text":"read Miller Haden (2013) part Correlations Basics reading, correlations used detect quantify relationships among numerical variables. short, measure two variables correlation analysis tells whether related manner - positively (.e., one increases increases) negatively (.e., one decreases increases). Schematic examples three extreme bivariate relationships (.e., relationship two (bi) variables) shown :\nFigure 9.1: Schematic examples extreme bivariate relationships\nHowever, unfortunately, may heard people say lots critical things correlations:prove causalitythey suffer bidirectional problem (vs B B vs )relationship found may result unknown third variable (e.g. murders ice-creams)Whilst true must aware , correlations incredibly useful commonly used measure field, put using designing study uses correlations. fact, can used variety areas. examples include:looking reading ability IQ scores Miller Haden Chapter 11, also look today;exploring personality traits voices (see Phil McAleer's work Voices);traits faces (see Lisa DeBruine's work);brain-imaging analysis looking activation say amygdala relation emotion faces (see Alex Todorov's work Princeton);social attidues, implicit explicit, Helena Paterson discuss Social lectures semester;whole variety fields simply measure variables interest.actually carry correlation simple show little : can use cor.test() function. harder part correlations really wrangling data (learned first part book) interpreting data (focus chapter). , going run correlations, showing one, asking perform others give good practice running interpreting relationships two variables.Note: dealing correlations better refer relationships predictions. correlation, X predict Y, really regression look later book. correlation, can say X Y related way. Try get correct terminology please feel free pull us say wrong thing class. easy slip tongue make!chapter :Introduce Miller Haden book, using throughout rest bookShow thought process researcher running correlational analysis.Give practice running writing correlation.","code":""},{"path":"correlations.html","id":"correlations-basics","chapter":"9 Correlations","heading":"9.2 Correlations Basics","text":"majority basics activities first part chapters now involve reading chapter two Miller Haden (2013) trying couple tasks. excellent free textbook use introduce General Linear Model, model underlies majority analyses seen see year - e.g., t-tests, correlations, ANOVAs (Analysis Variance), Regression related part General Linear Model (GLMs).start chapter introduction correlations.","code":""},{"path":"correlations.html","id":"read","chapter":"9 Correlations","heading":"9.2.1 Read","text":"ChapterRead Chapters 10 11 Miller Haden (2013).chapters really short, give good basis understanding correlational analysis. Please note, Chapter 10 might know terminology yet, e.g. ANOVA means Analysis Variance GLM means General Linear Model (Reading Chapter 1 Miller Haden might help). go depth terms coming chapters.","code":""},{"path":"correlations.html","id":"watch","chapter":"9 Correlations","heading":"9.2.2 Watch","text":"VisualisationHave look visualisation correlations Kristoffer Magnusson: https://rpsychologist.com/d3/correlation/.read Miller Haden Chapter 11, use visualisation page visually replicate scatterplots Figures 11.3 11.4 - use sample 100. , visually replicate scatterplots Figure 11.5. time change correlation, pay attention shared variance (overlap two variables) see changes changing level relationship two variables. greater shared variance, stronger relationship.Also, try setting correlation r = .5 moving single dot see one data point, potential outlier, can change stated correlation value two variables","code":""},{"path":"correlations.html","id":"play","chapter":"9 Correlations","heading":"9.2.3 Play","text":"Guess correlationNow well versed interpreting scatterplots (scattergrams) go online app guessing correlation: https://www.rossmanchance.com/applets/GuessCorrelation.html.basic application allows see good recognising different correlation strengths scatterplots. recommend click \"Track Performance\" tab can keep overview overall bias underestimate overestimate correlation.just bit fun? Well, yes stats actually fun, , serves purpose helping determine correlations see data real, help see correlations published research match told. know examples one data point can lead misleading relationship even might considered medium strong relationship may actually limited relevance real world. One needs mention Anscombe's Quartet reminded importance visualising data.Anscombe (1973) showed four sets bivariate data (X,Y) exact means, medians, relationships:\nTable 9.1: Four bivariate datasets means, medians, standard deviations, relationships\nCan look different plotted:\nFigure 9.2: Though datasets correlation r = .82, plotted four datasets look different. Grp standard linear relationship pearson correlation suitable. Grp II appear non-linear relationship non-parametric analysis appropriate. Grp III shows linear relationship (approaching r = 1) outlier lowered correlation coefficient. Grp IV shows relationship two variables (X, Y) oultier inflated correlation higher.\nclear example visualise data rely just numbers. can read Anscombe's Quartet time, wikipedia (https://en.wikipedia.org/wiki/Anscombe%27s_quartet) offering good primer data used example.","code":"## `geom_smooth()` using formula 'y ~ x'"},{"path":"correlations.html","id":"correlations-application","chapter":"9 Correlations","heading":"9.3 Correlations Application","text":"going jump straight one! get used running correlation use examples read Miller Haden (2013), Chapter 11, looking relationship four variables: reading ability, intelligence, number minutes per week spent reading home, number minutes per week spent watching TV home. great example correlation works best unethical manipulate variables measuring exist environment appropriate.Click download data today.","code":""},{"path":"correlations.html","id":"Ch10InClassQueT1","chapter":"9 Correlations","heading":"9.3.1 Task 1 - The Data","text":"downloading data folder, unzip folder, set working directory appropriately, open new R Markdown, load Miller Haden data (MillerHadenData.csv), storing tibble called mh. always, solutions end chapter.Note 1: Remember reality store data name like, make easier demonstrator debug handy use names.Note 2: find instructions tasks sparser now , compared earlier book. want really push develop skills learnt previously. worry though, always help, get stuck, ask!Note 3: see additional data set vaping. use later chapter, can ignore now.\nHint 1: going need following libraries: tidyverse, broom\n\nHint 2: mh <- read_csv()\n\nRemember , book, ALWAYS ask use read_csv() load data. Avoid using functions read.csv(). Whilst functions similar, .\n\nlook data mh - showed number ways . Miller Haden, five columns:Participant (Participant),Reading Ability (Abil),Intelligence (IQ),Number minutes spent reading home per week (Home),Number minutes spent watching TV per week (TV).exercises focus relationship Reading Ability IQ, practice can look relationships .probable hypothesis today Reading Ability increases Intelligence (see issue causality direction?). phrasing alternative hypothesis (\\(H_1\\)) formally, hypothesise reading ability school children, measured standardized test, intelligence, measured standardized test, show linear positive relationship. hypothesis test today, remember always state null hypothesis (\\(H_0\\)) relationship reading ability IQ.First, however, must check assumptions correlation tests. stated hypothesise linear relationship, means going looking Pearson's product-moment correlation often just shortened Pearson's correlation symbolised letter r.main assumptions need check Pearson's correlation, look turn, :data interval, ratio, ordinal?data point participant variables?data normally distributed variables?relationship variables appear linear?spread homoscedasticity?","code":""},{"path":"correlations.html","id":"Ch10InClassQueT2","chapter":"9 Correlations","heading":"9.3.2 Task 2 - Interval or Ordinal","text":"Assumption 1: Level MeasurementThinking Cap PointIf going run Pearson correlation need interval ratio data. alternative correlation, non-parametric equivalent Pearson correlation Spearman correlation can run ordinal, interval ratio data. type data ?Check thinking: type data analysis probably ratiointervalordinalnominal data continuousdiscrete unlikely true zero\nvariables continuous?\n\ndifference 1 2 scale equal difference 2 3?\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT3","chapter":"9 Correlations","heading":"9.3.3 Task 3 - Missing Data","text":"Assumption 2: Pairs DataAll correlations must data point participant two variables correlated. make sense - correlate empty cell! Check data point columns participant. , try answering question Task 3. discussion solutions end chapter.Note: can check missing data visual inspection - literally using eyes. missing data point show NA, short applicable, available, answer. alternative use .na() function. can really handy lots data visual inspection just take long. example ran following code:look output function, FALSE tells data-point cell. .na() asks cell NA; empty. cell empty come back TRUE. cells data , showing FALSE. wanted ask opposite question, data cell, write !.na() read \"NA\". exclamation mark ! turns question opposite.looks like everyone data columns test skills little whilst . Answer following questions:missing data represented cell tibble? empty cellNAa large numberdon't knowWhich code leave just participants missing Reading Ability data mh:\nfilter(mh, .na(Ability)filter(mh, .na(Abil)filter(mh, !.na(Ability)filter(mh, !.na(Abil)code leave just participants missing Reading Ability data mh: filter(mh, .na(Ability)filter(mh, .na(Abil)filter(mh, !.na(Ability)filter(mh, !.na(Abil)\nfilter(dat, .na(variable)) versus filter(dat, !.na(variable))\n","code":"\nis.na(mh)"},{"path":"correlations.html","id":"Ch10InClassQueT4","chapter":"9 Correlations","heading":"9.3.4 Task 4 - Normality","text":"Assumption 3: shape dataThe remaining assumptions best checked visualisations. use histograms check data (Abil IQ) normally distributed, use scatterplot (scattergrams) IQ function Abil check whether relationship linear, homoscedasticity, without outliers!alternative use z-scores check outliers cut-usually set around \\(\\pm2.5SD\\). using mutate function (e.g. mutate(z = (X - mean(X))/SD(X))), today just use visual checks.Create following figures discuss outputs group. discussion solutions end chapter.histogram Ability histogram IQ.\nnormally distributed?\nnormally distributed?scatterplot IQ (IQ) function Ability (Abil).\nsee outliers?\nrelationship appear linear?\nspread appear ok terms homoscedasticity?\nsee outliers?relationship appear linear?spread appear ok terms homoscedasticity?\nggplot(mh, aes(x = )) + geom_histogram()\n\nggplot(mh, aes(x = , y = )) + geom_point()\n\nNormality: something keep mind 25 participants, 'normal' expect relationships \n\nhomoscedasticity spread data points around (imaginary) line best fit equal sides along line; opposed narrow one point wide others.\n\nRemember judgement calls!\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT5","chapter":"9 Correlations","heading":"9.3.5 Task 5 - Descriptives","text":"Descriptives correlationA key thing keep mind scatterplot actually descriptive correlation. Meaning article, report, use scatterplot determine type correlation use, also describe potential relationship regards hypothesis. always expect see scatterplot write-type analysisThinking Cap PointLooking scatterplot created Task 4, spend couple minutes describing relationship Ability IQ terms hypothesis. Remember descriptive analysis stage, nothing confirmed. relationship appear predicted hypothesis? discussion solutions end chapter.\nHint 1: hypothesised reading ability intelligence positively correlated. see scatterplot?\n\nHint 2: Keep mind subjective stage.\n\nHint 3: Remember talk relationship prediction. correlational work, regression.\n\nHint 4: Can say something strength (weak, medium, strong) direction (positive, negative)?\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT6","chapter":"9 Correlations","heading":"9.3.6 Task 6 - Pearson or Spearman?","text":"correlationFinally run correlation using cor.test() function. Remember help function can type ?cor.test console window. cor.test() function requires:name tibble column name Variable 1the name tibble column name Variable 2the type correlation want run: e.g. \"pearson\", \"spearman\"type NHST tail want run: e.g. \"one.sided\", \"two.sided\"example, data stored dat wanting two-sided pearson correlation variables (columns) X Y, :dat$X means column X tibble dat. dollar sign ($) way indexing, calling , specific column.Based answers Task 5, spend couple minutes deciding correlation method use (e.g., Pearson Spearman) type NHST tail set (e.g., two.sided one.sided). Now, run correlation IQ Ability save tibble called results (hint: broom::tidy()). solution end chapter.\nHint 1: data looked reasonably normal linear method ?\n\nHint 2: results <- cor.test(mh$Abil......, method = ....., alternative....) %>% tidy()\n","code":"cor.test(dat$X, dat$Y, method = \"pearson\", alternative = \"two.sided\")"},{"path":"correlations.html","id":"Ch10InClassQueT7","chapter":"9 Correlations","heading":"9.3.7 Task 7 - Interpretation","text":"Interpreting CorrelationYou now tibble called results gives output correlation Reading Ability IQ school children measured Miller Haden (2013) Chapter 11. left now interpret output correlation.Look results. Locate correlation value, e.g. results %>% pull(estimate), answer following questions. discussion can found end chapter.direction relationship Ability IQ : positivenegativeno relationshipThe strength relationship Ability IQ : strongmediumweakBased \\(\\alpha = .05\\) relationship Ability IQ : significantnot significantBased output, given hypothesis reading ability school children, measured standardized test, intelligence, standardized test, positively correlated, can say hypothesis: supportedis supportedis provenis proven\nHint1: Y increases X increases relationship positive. Y increases X decreases relationship negative. change Y X changes relationship\n\nHint2: Depending field correlation values greater .5 strong; .3 .5 medium, .1 .3 small.\n\nHint3: field standard says less .05 significant.\n\nHint4: Hypotheses can supported supported, never proven.\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT8","chapter":"9 Correlations","heading":"9.3.8 Task 8 - The Matrix","text":"Great, far set hypothesis correlation, checked assumptions, run correlation interpreted appropriately. can see running correlation easy bit. lot analyses getting data order, checking assumptions, interpreting output hard part.now walked one analysis can always go run Miller Haden dataset. six total run watch Multiple Comparisons - False Positive Error rate (Type 1 error rate) inflated chance finding significant effect inflated simply running numerous tests. Alternatively, another data set want run correlation first want show something can handy want view lots correlations .Advanced 1: Matrix ScatterplotsAbove ran one correlation wanted different correlation edit cor.test() line run . However, lots variables dataset, get quick overview patterns, one thing might want run correlations time create matrix scatterplots one time. can functions Hmisc library - already installed Boyd Orr Labs. use Miller Haden data still tibble called mh.First, need get rid Participant column want correlate anything. tell us anything. Copy run line codeNow run following line. pairs() function Hmisc library creates matrix scatterplots can use view relationships one time.\nFigure 9.3: Matrix Correlation plots Miller Haden (2013) data\nrcorr() function creates matrix correlations p-values. watch , accepts data matrix format. Run following two lines code.running lines, spend minutes answering following questions. first table outputted correlation values second table p-values.tables look symmetrical around blank diagonal?strongest positive correlation?strongest negative correlation?\nHint1: hint, just cheeky test make sure read correlation chapter Miller Haden, like asked ! :-) unsure answers, solutions end chapter.\n","code":"\nlibrary(\"Hmisc\")\nlibrary(\"tidyverse\")\nmh <- read_csv(\"MillerHadenData.csv\") %>% select(-Participant)\npairs(mh)\nmh_mx <- as.matrix(mh)\nrcorr(mh_mx, type = \"pearson\")##       Abil   IQ  Home    TV\n## Abil  1.00 0.45  0.74 -0.29\n## IQ    0.45 1.00  0.20  0.25\n## Home  0.74 0.20  1.00 -0.65\n## TV   -0.29 0.25 -0.65  1.00\n## \n## n= 25 \n## \n## \n## P\n##      Abil   IQ     Home   TV    \n## Abil        0.0236 0.0000 0.1624\n## IQ   0.0236        0.3337 0.2368\n## Home 0.0000 0.3337        0.0005\n## TV   0.1624 0.2368 0.0005"},{"path":"correlations.html","id":"Ch10InClassQueT9","chapter":"9 Correlations","heading":"9.3.9 Task 9 - Attitudes to Vaping","text":"Advanced 2: Attitudes towards VapingGreat work far! Now really want see can . mentioned earlier, data folder another file called VapingData.csv. data comes data set looking implicit explicit attitudes towards vaping.Explicit attitudes measured via questionnaire higher scores indicated positive attitude towards vaping.Implicit attitudes measured Implicit Association Test (IAT) using images Vaping Kitchen utensils associating positive negative words.IAT works principle associations go together (congruent, e.g. warm sun) quicker respond associations go together (incongruent, e.g. warm ice). can read procedure later date Noba Project https://nobaproject.com/modules/research-methods--social-psychology good description procedure section \"Subtle/Nonsconscious Research Methods\".exercise, need know \"Block 3\" experiment tested reaction times accuracy towards congruent associations, pairing positive words Kitchen utensils negative words Vaping. \"Block 5\" experiment tested reaction times accuracy towards incongruent associations, pairing positive words Vaping negative words Kitchen Utensils. , reaction times longer \"Block 5\" \"Block 3\" people considered hold view Vaping negative (.e., congruent associations quicker incongruent associations). However, reaction times quicker \"Block 5\" \"Block 3\" people considered hold view Vaping positive (.e., incongruent associations quicker congruent associations). difference reaction times \"Block 5\" \"Block 3\" called participants IAT score.Load data VapingData.csv analyse test hypothesis Implicit Explicit attitudes towards Vaping positively related. pointers, hints tips. , solutions walk step, help try steps first. Think step. Sometimes need think backwards, want tibble look like. can steps. nothing new.loading data, start looking data. 8 columns. Reaction times Accuracy scores Blocks 3 5 well Explicit Vaping Questionnaire scores, Gender Age, participant.Accuracy calculated proportion go 1. Participants entered data might made mistake. Remove participants accuracy greater 1 either Block 3 Block 5 unclear accuracy values.also want participants paying attention best remove anybody whose average accuracy score across Blocks 3 5 less 80%. Note - value arbitrary wanted, experiment, use relaxed strict cut-based studies guidance. Note decisions set start research part pre-registration part Registered Report. Finally, instance, remember, values proportions percentages (80% .8).Now create IAT score participants subtracting Block 3 reaction times (RT) away Block 5 reaction times e.g \\(Block5 - Block3\\). Use information understand scores relate attitudes.Create descriptives summary number people, mean RT Vaping Questionnaire Score. might averages useful? averages always useful correlations?Check assumptions correlations descriptives, thinking compares hypothesis.Run appropriate correlation based assumptions interpret output.\n\nlibraries might include tidyverse broom\n\n\nlibraries might include tidyverse broom\n\n\nHint Step 1: read_csv()\n\n\nHint Step 1: read_csv()\n\n\nHint Step 2: filter(Accuracy < 1 Accuracy <= 1 Accuracy > 1 Accuracy >= 1)\n\n\nHint Step 2: filter(Accuracy < 1 Accuracy <= 1 Accuracy > 1 Accuracy >= 1)\n\n\nHint Step 3: average accuracy: mutate(data, name = (1 + 2)/2) %>% filter(name > ...)\n\n\nHint Step 3: average accuracy: mutate(data, name = (1 + 2)/2) %>% filter(name > ...)\n\n\nHint Step 4: RT: mutate(data, nom = 1 - 2)\n\n\nHint Step 4: RT: mutate(data, nom = 1 - 2)\n\n\nHint Step 5: descriptives <- summarise()\n\n\nHint Step 5: descriptives <- summarise()\n\n\nHint Step 6: assumptions type data, normality, linear relationship, homoscedasicity, data points everyone!\n\n\nHint Step 6: assumptions type data, normality, linear relationship, homoscedasicity, data points everyone!\n\n\nHint Step 7: results <- cor.test(method = \"pearson\")?\n\n\nHint Step 7: results <- cor.test(method = \"pearson\")?\nExcellent work, thought Explicit Implicit attitudes towards Vaping?","code":""},{"path":"correlations.html","id":"Ch9PracticeSkills","chapter":"9 Correlations","heading":"9.4 Practice Your Skills","text":"purpose exercise test ability run interpret correlation , shown chapter, time get think skills learnt previously. hard work data wrangling; running actual analysis, much like t-tests, straightforward. Remember refer back previous chapters stuck.order complete tasks need download data .csv file .Rmd file, need edit, titled GUID_L2_Ch9_PracticeSkills.Rmd. can downloaded within zip file link. Download Exercises .zip file .zip file, also find activity html file can check table figure reproduced tasks 8 9.downloaded unzipped create new folder use working directory; put data file .Rmd file folder set working directory folder drop-menus top. .Rmd file currently knit errors code good test can perform time time make sure still errors code. Obviously mean answers correct; just means code error free.previous exercises number code chunks already set . code chunks require entering number entering adjusting code practiced chapters. Follow instructions edit code chunk. often entering code based covered point.Background\nbackdrop exercise following study:Dawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). wealthier people think people wealthier, matters: social sampling attitudes redistribution. Psychological Science, 26, 1389-1400. Available link VPN switched onThe abstract Dawtry, Sutton Sibley (2015) reads:present studies provide evidence social-sampling processes lead wealthier people oppose redistribution policies. samples American Internet users, wealthier participants reported higher levels wealth social circles (Studies 1a 1b). associated, turn, estimates higher mean wealth wider U.S. population, greater perceived fairness economic status quo, opposition redistribution policies. Furthermore, mods large-scale, nationally representative New Zealand survey revealed low levels neighborhood-level socioeconomic deprivation - objective index wealth within participants' social circles - mediated relation income satisfaction economic status quo (Study 2). findings held controlling relevant variables, including political orientation perceived self-interest. Social-structural inequalities appear combine social-sampling processes shape different political attitudes wealthier poorer people.summarised Open Stats Lab asIn research, Dawtry, Sutton, Sibley (2015) wanted examine people differ assessments increasing wealth inequality within developed nations. Previous research reveals people desire society overall level wealth high wealth spread somewhat equally across society. However, support approach income distribution changes across social strata. particular, wealthy people tend view society already wealthy thus satisfied status quo, less likely support redistribution. paper Dawtry et al., (2015) sought examine case. authors propose one reason wealthy people tend view current system fair social-circle comprised wealthy people, biases perceptions wealth, leads overestimate mean level wealth across society.test hypothesis, authors conducted study 305 participants, recruited online participant pool. Participants reported annual household income, income level within social circle, income entire population. Participants also rated perception level equality/inequality across social circle across society, level satisfaction perceived fairness current system (measured using two scales), attitudes toward redistribution wealth (measured using four-item scale), political preference.Open Stats Lab activity paper already set can work later skills development. use amended version pay close attention instructions assignment specifically task asks. Today running correlation measure Fairness Satisfaction versus measure Support Redistribution.","code":""},{"path":"correlations.html","id":"solutions-to-questions-8","chapter":"9 Correlations","heading":"9.5 Solutions to Questions","text":"find solutions questions activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"correlations.html","id":"correlations-application-1","chapter":"9 Correlations","heading":"9.5.1 Correlations Application","text":"","code":""},{"path":"correlations.html","id":"task-1-9","chapter":"9 Correlations","heading":"9.5.1.1 Task 1","text":"Loading data two libraries neededGood point remind :\nuse read_csv() load data\norder libraries read important. conflicts terms libraries last library loaded functions using.\nuse read_csv() load datathe order libraries read important. conflicts terms libraries last library loaded functions using.Return Task","code":"\nlibrary(\"broom\")\nlibrary(\"tidyverse\")\nmh <- read_csv(\"MillerHadenData.csv\")"},{"path":"correlations.html","id":"task-2-10","chapter":"9 Correlations","heading":"9.5.1.2 Task 2","text":"Actually information within textbook unclear whether data interval ordinal accepted make case arguments. quick Google search show just many people think IQ interval think ordinal. terms Reading Ability, probably know enough information scale make clear judgement least ordinal well interval.Return Task","code":""},{"path":"correlations.html","id":"task-3-9","chapter":"9 Correlations","heading":"9.5.1.3 Task 3","text":"Missing data represented NA. stands Available good way improving Scottish accent. example, \"number\" can replied \"NA!\".Missing data represented NA. stands Available good way improving Scottish accent. example, \"number\" can replied \"NA!\".want keep everybody whole dataset score Ability use:want keep everybody whole dataset score Ability use:Alternatively, want keep everybody whole dataset score Ability use:Remember need store output step, really something like mh <- filter(mh, !.na(Abil))Return Task","code":"\nfilter(mh, !is.na(Abil))\nfilter(mh, is.na(Abil))"},{"path":"correlations.html","id":"task-4-9","chapter":"9 Correlations","heading":"9.5.1.4 Task 4","text":"Reading ability data appears normal expected 25 participants. Hard say close normality something look participants.\nFigure 9.4: Histogram showing distribution Reading Ability Scores Miller Haden (2013)\nIQ data appears normal expected 25 participants\nFigure 9.5: Histogram showing distribution IQ Scores Miller Haden (2013)\nrelationship reading ability IQ scores appears appears linear clear outliers. Data also appears homeoscedastic.added geom_smooth() function help clarify line best fit (also know regression line slope).\nFigure 9.6: Scatterplot IQ scores function Reading Ability Miller Haden (2013) data\nReturn Task","code":"\nggplot(mh, aes(x = Abil)) + \n  geom_histogram(binwidth = 5) +\n  theme_bw()\nggplot(mh, aes(x = IQ)) + \n  geom_histogram(binwidth = 5) +\n  theme_bw()\nggplot(mh, aes(x = Abil, y = IQ)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()## `geom_smooth()` using formula 'y ~ x'"},{"path":"correlations.html","id":"task-5-8","chapter":"9 Correlations","heading":"9.5.1.5 Task 5","text":"Based scatterplot might suggest reading ability scores increase, IQ scores also increase appear data inline hypothesis two variables positively correlated. appears medium strength relationship.Return Task","code":""},{"path":"correlations.html","id":"task-6-7","chapter":"9 Correlations","heading":"9.5.1.6 Task 6","text":"going run pearson correlation argue data interval relationship linear.correlation run follows - tidying nice usable table.output table look follows:\nTable 9.2: correlation output Reading Ability IQ relationship.\nReturn Task","code":"\nresults <- cor.test(mh$Abil, \n                    mh$IQ, \n                    method = \"pearson\", \n                    alternative = \"two.sided\") %>% \n  broom::tidy()"},{"path":"correlations.html","id":"task-7-7","chapter":"9 Correlations","heading":"9.5.1.7 Task 7","text":"Task 6 output:correlation value (r) stored estimateThe degrees freedom (N-2) stored parameterThe p-value stored p.valueAnd statistic t-value associated analysis correlations use t-distribution (Chapters 6, 7 8) determine probability outcome.Remember can use pull() function get individual values shown .can use information write-following using inline coding accuracy:pearson correlation found reading ability intelligence positively correlated medium strong relationship, (r(`r df`) = `r correlation`, p = `r pvalue`). can say hypothesis supported appears relationship reading ability IQ reading ability increases intelligence.knitted read :pearson correlation found reading ability intelligence positively correlated medium strong relationship, (r(23) = 0.45, p = 0.024). can say hypothesis supported appears relationship reading ability IQ reading ability increases intelligence.Return Task","code":"\npvalue <- results %>% \n  pull(p.value) %>% \n  round(3)\n\ndf <- results %>% \n  pull(parameter)\n\ncorrelation <- results %>% \n  pull(estimate) %>% \n  round(2)"},{"path":"correlations.html","id":"task-8-6","chapter":"9 Correlations","heading":"9.5.1.8 Task 8","text":"table looks across diagonal correlation e.g. Abil vs Abil shown, correlation Abil vs Home correlation Home vs Abil.strongest positive correlation number minutes spend reading home (Home) Reading Ability (abil), r(23) = .74, p < .001.strongest negative correlation number minutes spend reading home (Home) minutes spent watching TV per week (TV), r(23) = -.65, p < .001.Return Task","code":""},{"path":"correlations.html","id":"task-9-3","chapter":"9 Correlations","heading":"9.5.1.9 Task 9","text":"Step 1Reading Vaping Data using read_csv()Steps 2 4The main wrangle parts 2 4Step 5It always worth thinking averages informative .\nKnowing average explicit attitude towards vaping well informative.\ncontrast, using ordinal scale people use whole scale average may just tell middle scale using - already know really informative. always worth thinking descriptives calculating.\nKnowing average explicit attitude towards vaping well informative.contrast, using ordinal scale people use whole scale average may just tell middle scale using - already know really informative. always worth thinking descriptives calculating.Step 6A couple visual checks normality histograms\nFigure 9.7: Histogram showing distribution Scores Vaping Questionnaire (Explicit)\n\nFigure 9.8: Histogram showing distribution IAT Reaction Times (Implicit)\ncheck relationship reaction times IAT scores Vaping Questionnaire.Remember , often, scatterplot considered descriptive correlation, hence see including journal articles support stated relationship.scatterplot can used make descriptive claims direction relationship, strength relationship, whether linear , check outliers homeoscedasticity.\nFigure 9.9: scatterplot showing relationship implicit IAT reaction times (x) explicit Vaping Questionnaire Scores (y)\nquick look data reveals people Vaping Questionnaire score IAT score. correlation works people score factors remove score one factors.Step 7The analysis:extracting values:inline coding:Testing hypothesis relationship beween implicit explicit attitudes towards vaping, pearson correlation found significant relationship IAT reaction times (implicit attitude) answers Vaping Questionnaire (explicit attitude), r(`r df`) = `r correlation`, p = `r pvalue`. Overall suggests direct relationship implicit explicit attitudes relating Vaping hypothesis supported; reject null hypothesis.appears knitted:Testing hypothesis relationship beween implicit explicit attitudes towards vaping, pearson correlation found significant relationship IAT reaction times (implicit attitude) answers Vaping Questionnaire (explicit attitude), r(143) = -0.1043797, p = 0.2115059. Overall suggests direct relationship implicit explicit attitudes relating Vaping hypothesis supported; reject null hypothesis.Remember though r-values p-values often rounded three decimal places, appropriate write :Testing hypothesis relationship beween implicit explicit attitudes towards vaping, pearson correlation found significant relationship IAT reaction times (implicit attitude) answers Vaping Questionnaire (explicit attitude), r(143) = -.104, p = .212. Overall suggests direct relationship implicit explicit attitudes relating Vaping hypothesis supported; reject null hypothesis.","code":"\ndat <- read_csv(\"VapingData.csv\")\ndat <- dat %>% \n  filter(IAT_BLOCK3_Acc <= 1) %>%\n  filter(IAT_BLOCK5_Acc <= 1) %>%\n  mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %>%\n  filter(IAT_ACC > .8) %>%\n  mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT)\ndescriptives <- dat %>% summarise(n = n(),\n                          mean_IAT_ACC = mean(IAT_ACC),\n                          mean_IAT_RT = mean(IAT_RT),\n                          mean_VPQ = mean(VapingQuestionnaireScore, \n                                          na.rm = TRUE))\nggplot(dat, aes(x = VapingQuestionnaireScore)) + \n  geom_histogram(binwidth = 10) +\n  theme_bw()## Warning: Removed 11 rows containing non-finite values (stat_bin).\nggplot(dat, aes(x = IAT_RT)) + \n  geom_histogram(binwidth = 10) +\n  theme_bw()\nggplot(dat, aes(x = IAT_RT, y = VapingQuestionnaireScore)) + \n  geom_point() + \n  theme_bw()## Warning: Removed 11 rows containing missing values (geom_point).\ndat <- dat %>% \n  filter(!is.na(VapingQuestionnaireScore)) %>% \n  filter(!is.na(IAT_RT))\nresults <- cor.test(dat$VapingQuestionnaireScore, \n                    dat$IAT_RT, \n                    method = \"pearson\") %>% \n  broom::tidy()\ncorrelation <- results %>% \n  pull(estimate)\n\ndf <- results %>% \n  pull(parameter)\n\npvalue <- results %>% \n  pull(p.value)"},{"path":"correlations.html","id":"practice-your-skills-13","chapter":"9 Correlations","heading":"9.5.2 Practice Your Skills","text":"Check work solution tasks : Chapter 9 Practice Skills Solution.Return Task","code":""},{"path":"correlations.html","id":"additional-material-1","chapter":"9 Correlations","heading":"9.6 Additional Material","text":"additional material might help understand correlations bit additional ideas.","code":""},{"path":"correlations.html","id":"checking-for-outliers-with-z-scores","chapter":"9 Correlations","heading":"Checking for outliers with z-scores","text":"briefly mentioned activities use z-scores check outliers, instead visual inspection. covered lectures works interval ratio dataset, just ones correlations, demonstrate using IQ data Miller Haden. First, lets get just IQ data.z-score just standardised value based mean (\\(\\mu\\)) standard deviation (SD \\(\\sigma\\), proonounced \"sigma\") sample comes . formula z-score :\n\\[z = \\frac{x - \\mu}{\\sigma}\\]Using z-scores away effectively converting data onto Normal Distribution. , known cut-offs Normal Distribution (e.g., 68% data \\(\\pm1SD\\), etc - See Chapter 4), can use information determine value outlier. , convert data within variable respective z-score values fall cut-considered outlier.give us following data (showing first 6 rows):\nTable 9.3: Raw IQ data z-scored IQ data Miller Haden\nrun data see whole range z-scores ranging -2.1053138 1.9858948. class said use cut-\\(\\pm2.5SD\\) can see range IQ z-scores value , outliers. confirm following code:run code, can see 0 outliers. also demonstrates however must stipulate cut-offs advance - otherwise might fall foul adjusting data fit predictions hiding decision making numerous researcher degrees freedom exist. example, say run analysis see outliers find result want/expect/believe . questionable research practice now start adjusting z-score cut-different values see difference makes. instance, said cut-stringent \\(\\pm2SD\\) found 1 outlier.two take-aways :spot outliers using z-scores.must set exclusion criteria advance running analysis.","code":"\nmh_IQ <- mh %>% \n  select(IQ)\nmh_IQ_z <- mh_IQ %>%\n  mutate(z_scores = (IQ - mean(IQ))/sd(IQ))\nmh_IQ_z %>% filter(abs(z_scores) > 2.5) %>% count() %>% pull(n)"},{"path":"correlations.html","id":"a-different-approach-to-making-a-correlation-table","chapter":"9 Correlations","heading":"A different approach to making a correlation table","text":"activities, Task 8, looked making table correlation functions. bit messy actually alternative approach can use, useful additional functions. makes use corrr package may need install onto laptop want follow along.code can try explore . main functions correlate(), shave(), fashion():correlate() runs correlations can changed pearson, spearman, etc. quiet argument just removes information reminders dont really need. Switch FALSE see happens. nice aspect function default use complete rows can alter .shave() can used convert one set correlation values showing test NAs. example, bottom half table just reflects top half table can convert one half. default convert upper half table.fashion() can used tidy table terms removing NAs, leading zeros, setting number decimal places.code look like :look ouput get nice clean correlation matrix table shown .bit nicer approach one shown Task 8, gives control output correlation matrix table. However, downside approach, reason use approach show p-values easily. fact, show need bit work. can read using corrr package webpage author package: https://drsimonj.svbtle.com/exploring-correlations--r--corrr","code":"\nlibrary(corrr)\n\nmh <- read_csv(\"MillerHadenData.csv\")\n\nmh %>% \n  select(-Participant) %>%\n  correlate(method = \"pearson\", quiet = TRUE) %>%\n  shave() %>%\n  fashion(decimals = 3, leading_zeros = FALSE)"},{"path":"correlations.html","id":"comparing-correlations","chapter":"9 Correlations","heading":"Comparing Correlations","text":"One step often overlooked working correlations data set numerous variables, compare whether correlations significantly different . example, say looking whether relationship height attractiveness males females. run two correlations find one stronger relationship . Many try conclude means significantly stronger relationship one gender . However tested. can tested though. quickly show , paper also help discussion: Diedenhofen Musch (2015) cocor: Comprehensive Solution Statistical Comparison Correlations. PLOS ONE 10(6): e0131499. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121945As always need libraries first. need install cocor package laptop.Now going walk three different examples just show work different experimental designs. need match values code values text make full sense examples. use examples work faces, voices personality traits encounter later book.Comparing correlation -subjects designs (.e., independent-samples)Given scenario males voice pitch height (r(28) = .89) female voice pitch height (r(28) = .75) can say difference correlations significant?correlations come two different groups - male voices female voices - used cocor.indep.groups() function.Note: df groups 28, means N 30 groups (based N = df-2)Gives output :actually get two test outputs function: Fisher's Z Zou's Confidence Interval. one commonly used Fisher's Z look one. p-value comparison two correlations, p = .1, greater p = .05, tells us reject null hypothesis significant difference two correlations similar magnitude - .e. correlation similar groups.report outcome along lines , \"despite correlation male voice pitch height found stronger relationship female voices, Fisher's Z suggested significant difference (Z = 1.65, p = .1)\"Within-Subjects (.e., dependent samples) common variableWhat scenario 30 participants rating faces scales trust, warmth likability, want know relationship trust warmth (r = .89) significantly different relationship trust likability (r = .8).data comes participants, crossover/overlap traits - .e. one trait appears correlations interest - use cocor.dep.groups.overlap(). comparison also need know relationship remaining correlation warmth likability (r = .91).Note: order input correlations matters. See ?cocor.dep.groups.overlap() help.Gives output :test actually produces output lot tests; many probably know whether use . However, can run analysis choose just specific test adding argument function: test = steiger1980 test = pearson1898 example. probably two common.run analysis using just Pearson's Z (1898)Gives output :can see output, test significant (p = .047) suggesting significant difference correlation trust warmth (r = .89) correlation trust likeability (r = .8).Within-Subjects (.e., dependent samples) common variableOk last scenario. 30 participants rating faces scales trust, warmth, likability, attractiveness, want know relationship trust warmth (r = .89) significantly different relationship likability attractiveness (r = .93).correlations interest crossover variables come participants, example use cocor.dep.groups.nonoverlap().Note: order need correlations comparisons:trust likability (.88)trust attractiveness (.91)warmth likability (.87)warmth attractiveness (.92)Note: order input correlations matters. See ?cocor.dep.groups.nonoverlap() help.Gives output :can see output, test non-significant (p = .253) suggesting significant difference correlation trust warmth (r = .89) correlation likability attractiveness (r = .8).End Chapter!","code":"\nlibrary(cocor)\nlibrary(tidyverse)\ncompare1 <- cocor.indep.groups(r1.jk = .89, \n                               r2.hm = .75, \n                               n1 = 30, \n                               n2 = 30)## \n##   Results of a comparison of two correlations based on independent groups\n## \n## Comparison between r1.jk = 0.89 and r2.hm = 0.75\n## Difference: r1.jk - r2.hm = 0.14\n## Group sizes: n1 = 30, n2 = 30\n## Null hypothesis: r1.jk is equal to r2.hm\n## Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided)\n## Alpha: 0.05\n## \n## fisher1925: Fisher's z (1925)\n##   z = 1.6496, p-value = 0.0990\n##   Null hypothesis retained\n## \n## zou2007: Zou's (2007) confidence interval\n##   95% confidence interval for r1.jk - r2.hm: -0.0260 0.3633\n##   Null hypothesis retained (Interval includes 0)\ncompare2 <- cocor.dep.groups.overlap(r.jk = .89, \n                                     r.jh = .8, \n                                     r.kh = .91, \n                                     n = 30)## \n##   Results of a comparison of two overlapping correlations based on dependent groups\n## \n## Comparison between r.jk = 0.89 and r.jh = 0.8\n## Difference: r.jk - r.jh = 0.09\n## Related correlation: r.kh = 0.91\n## Group size: n = 30\n## Null hypothesis: r.jk is equal to r.jh\n## Alternative hypothesis: r.jk is not equal to r.jh (two-sided)\n## Alpha: 0.05\n## \n## pearson1898: Pearson and Filon's z (1898)\n##   z = 1.9800, p-value = 0.0477\n##   Null hypothesis rejected\n## \n## hotelling1940: Hotelling's t (1940)\n##   t = 2.4208, df = 27, p-value = 0.0225\n##   Null hypothesis rejected\n## \n## williams1959: Williams' t (1959)\n##   t = 2.4126, df = 27, p-value = 0.0229\n##   Null hypothesis rejected\n## \n## olkin1967: Olkin's z (1967)\n##   z = 1.9800, p-value = 0.0477\n##   Null hypothesis rejected\n## \n## dunn1969: Dunn and Clark's z (1969)\n##   z = 2.3319, p-value = 0.0197\n##   Null hypothesis rejected\n## \n## hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)\n##   t = 2.4208, df = 27, p-value = 0.0225\n##   Null hypothesis rejected\n## \n## steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations\n##   z = 2.2476, p-value = 0.0246\n##   Null hypothesis rejected\n## \n## meng1992: Meng, Rosenthal, and Rubin's z (1992)\n##   z = 2.2410, p-value = 0.0250\n##   Null hypothesis rejected\n##   95% confidence interval for r.jk - r.jh: 0.0405 0.6061\n##   Null hypothesis rejected (Interval does not include 0)\n## \n## hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure\n##   z = 2.2137, p-value = 0.0268\n##   Null hypothesis rejected\n## \n## zou2007: Zou's (2007) confidence interval\n##   95% confidence interval for r.jk - r.jh: 0.0135 0.2353\n##   Null hypothesis rejected (Interval does not include 0)\ncompare3 <- cocor.dep.groups.overlap(r.jk = .89, \n                                     r.jh = .8, \n                                     r.kh = .91, \n                                     n = 30,\n                                     test = \"pearson1898\")## \n##   Results of a comparison of two overlapping correlations based on dependent groups\n## \n## Comparison between r.jk = 0.89 and r.jh = 0.8\n## Difference: r.jk - r.jh = 0.09\n## Related correlation: r.kh = 0.91\n## Group size: n = 30\n## Null hypothesis: r.jk is equal to r.jh\n## Alternative hypothesis: r.jk is not equal to r.jh (two-sided)\n## Alpha: 0.05\n## \n## pearson1898: Pearson and Filon's z (1898)\n##   z = 1.9800, p-value = 0.0477\n##   Null hypothesis rejected\ncompare5 <- cocor.dep.groups.nonoverlap(r.jk = .89, \n                                        r.hm = .93, \n                                        r.jh = .88, \n                                        r.jm = .91, \n                                        r.kh = .87, \n                                        r.km = .92, \n                                        n = 30,\n                                        test = \"pearson1898\")## \n##   Results of a comparison of two nonoverlapping correlations based on dependent groups\n## \n## Comparison between r.jk = 0.89 and r.hm = 0.93\n## Difference: r.jk - r.hm = -0.04\n## Related correlations: r.jh = 0.88, r.jm = 0.91, r.kh = 0.87, r.km = 0.92\n## Group size: n = 30\n## Null hypothesis: r.jk is equal to r.hm\n## Alternative hypothesis: r.jk is not equal to r.hm (two-sided)\n## Alpha: 0.05\n## \n## pearson1898: Pearson and Filon's z (1898)\n##   z = -1.1424, p-value = 0.2533\n##   Null hypothesis retained"},{"path":"simple-regression.html","id":"simple-regression","chapter":"10 Simple Regression","heading":"10 Simple Regression","text":"chapter, working real data using regression explore question whether relationship statistics anxiety engagement course activities. hypothesis students anxious statistics less likely engage course-related activities. avoidance behaviour ultimately responsible lower performance students (although examining assessment scores activity).going analyse data STARS Statistics Anxiety Survey, administered students third-year statistics course Psychology University Glasgow. responses anonymised associating responses student arbitrary ID number (integer).STARS survey (Cruise, Cash, & Bolton, 1985) 51-item questionnaire, response 1 5 scale, higher numbers indicating greater anxiety.Cruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development validation instrument measure statistical anxiety. Proceedings American Statistical Association, Section Statistical Education, Las Vegas, NV.Example items STARS survey (Cruise, Cash, & Bolton, 1985)measure engagement course, use data Moodle usage analytics. course term, eight optional weekly -line sessions students attend extra support. variable n_weeks psess.csv file tells many (eight) given student attended.hypothesis greater anxiety reflected lower engagement. Answer following question.hypothesis correct positivenoa negative correlation students' mean anxiety levels n_weeks.","code":""},{"path":"simple-regression.html","id":"regression-a1","chapter":"10 Simple Regression","heading":"10.1 Activity 1: Setup","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Regression\".Download L3_stars.csv psess.csv save folder. Make sure change file name .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads pwr, broom, see, performance, report tidyverse using library() function.Load two CSV datasets variables called stars engage using read_csv().","code":""},{"path":"simple-regression.html","id":"regression-a2","chapter":"10 Simple Regression","heading":"10.2 Activity 2: Tidy the data","text":"Take look datasets loaded .next thing need calculate mean anxiety score student (recall individual students identified ID variable).Recall difference wide tidy data. wide data, row represents individual case, observations case separate columns; tidy data, row represents single observation, observations grouped together cases based value variable (data, ID variable).STARS data currently widetidy format.calculate means, need use pivot_longer() restructure STARS data appropriate \"tidy\" format; .e., looks like table .Write run code tidy STARS data, store resulting table stars2.","code":""},{"path":"simple-regression.html","id":"regression-a3","chapter":"10 Simple Regression","heading":"10.3 Activity 3: Calculate mean anxiety for each student","text":"Now got data tidy format, use summarise() group_by() calculate mean anxiety scores (mean_anxiety) student (ID). Store resulting table variable named stars_means.","code":""},{"path":"simple-regression.html","id":"regression-a4","chapter":"10 Simple Regression","heading":"10.4 Activity 4: Join the datasets together","text":"order perform regression analysis, combine data stars_means engage using inner_join(). Call resulting table joined. look like :","code":""},{"path":"simple-regression.html","id":"regression-a5","chapter":"10 Simple Regression","heading":"10.5 Activity 5: Calculate descriptives for the variables overall","text":"also useful calculate descriptives statistics sample overall can check sample scores expecting (e.g., comparable previous studies samples?). also useful write-.Run code. Read line ensure understand calculated.","code":"\ndescriptives <- joined %>%\n  summarise(mean_anx = mean(mean_anxiety, na.rm = TRUE),\n            sd_anx = sd(mean_anxiety, na.rm = TRUE),\n            mean_weeks = mean(n_weeks, na.rm = TRUE),\n            sd_weeks = sd(n_weeks, na.rm = TRUE))"},{"path":"simple-regression.html","id":"regression-a6","chapter":"10 Simple Regression","heading":"10.6 Activity 6: Visualisations","text":"Now variables one place, write code reproduce exact scatterplot (using ggplot2).\nFigure 10.1: Scatteplot mean anxiety attendance\nAccording scatterplot, apparent relationshipas anxiety increases, engagement decreasesas anxiety increases, engagement increases","code":""},{"path":"simple-regression.html","id":"regression-a7","chapter":"10 Simple Regression","heading":"10.7 Activity 7: Run the regression","text":"lm() function Base R main function estimate Linear Model (hence function name lm). lm() uses formula syntax seen , .e., DV ~ predictor.Use lm() function predict n_weeks (DV) mean_anxiety (predictor). Store result call lm() variable mod. see results, use summary(mod).Answer following questions model. may wish refer lecture notes help answer questions.estimate y-intercept model, rounded three decimal places, three decimal places, GLM model \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\), \\(\\beta_1\\) three decimal places, unit increase anxiety, n_weeks decreases two decimal places, overall F-ratio model? overall model significant? YesNoWhat proportion variance model explain? summary table, estimate intercept.summary table, estimate mean_anxiety, .e., slope.summary table, also estimate mean_anxiety, slope much decreases just remove - sign.summary table, F-ratio noted F-statistic.overall model p.value .001428 less .05, therefore significant.variance explained determined R-squared, simply multiple 100 get percent. always use adjusted R-squared value.","code":"\nmod <- lm(n_weeks ~ mean_anxiety, joined)\nmod_summary <- summary(mod)"},{"path":"simple-regression.html","id":"regression-a8","chapter":"10 Simple Regression","heading":"10.8 Activity 8: Assumption checking","text":"check assumptions run regression now check whether anything concerned . covered lecture, assumptions regression :outcome/DV interval/ratio level dataThe predictor variable interval/ratio categorical (two levels)values outcome variable independent (.e., score come different participant)predictors non-zero varianceThe relationship outcome predictor linearThe residuals normally distributedThere homoscedasticity (homogeneity variance, residuals)Assumptions 1-3 nice easy. know data design study. Assumption 4 simply means spread data - example, point running regression age variable participants 20 years old. can check using scatterplot created Activity 4 can see assumption met, indeed spread scores.rest assumptions, going use functions packages see performance make life whole lot easier.packages installed machine yet, can go ahead install typing install.packages(\"NAME PACKAGE\") console . working server, packages already installed.First, can use check_model() produce range assumption test visualisations. Helpfully, function also provides brief explanation looking plot - functions R helpful!get error message Failed error:  ‘package called ‘qqplotr’’, install package qqplotr, need load using library(), check_model() uses background.\n(#fig:check_model)Visual assumption checks\nAssumption 5, linearity, plot suggests perfect looks pretty good.already noted, good visualise assumption checks just relying statistics can problematic, can sensitive small large sample sizes. However, can also reassuring statistical test back intuitions plot.Assumption 6, normality residuals, plot suggest residuals might normal, can check check_normality() runs Shapiro-Wilk test.result confirms residuals normally distributed, something likely exacerbated relatively small sample size. feeling confident, can see might resolve , core aims chapter conclude sample continue.multiple ways can transform data deal non-normality, can find information data transformation Appendix .First, need get sense issue dependent variable, case n_weeks. simple histogram shows DV normal distribution, instead, looks like uniform distribution.important remember assumptions regression residuals normally distributed, raw data, however, transforming DV can help.transform uniform distribution normal distribution, going use unif2norm function faux package (may need install).code uses mutate() create new variable n_weeks_transformed result transformation.notice histogram transformed variable still look amazing, remember residuals, raw data matters. re-run regression transformed data check model , things looking much better.worth saying point transformation use, whether works, can bit trial--error.homoscedasticity, plot looks mostly fine, can double check check_heteroscedasticity() result confirms data met assumption.","code":"\ncheck_model(mod)\ncheck_normality(mod)## Warning: Non-normality of residuals detected (p = 0.008).\nggplot(joined, aes(x = n_weeks)) +\n  geom_histogram(binwidth = 1)\nlibrary(faux)\njoined <- mutate(joined, \n                 n_weeks_transformed = unif2norm(n_weeks))\nggplot(joined, aes(x = n_weeks_transformed)) +\n  geom_histogram()\nmod_transformed <- lm(n_weeks_transformed ~ mean_anxiety, joined)\ncheck_normality(mod_transformed)\ncheck_model(mod_transformed)## OK: residuals appear as normally distributed (p = 0.107).\ncheck_heteroscedasticity(mod)## OK: Error variance appears to be homoscedastic (p = 0.542)."},{"path":"simple-regression.html","id":"regression-a9","chapter":"10 Simple Regression","heading":"10.9 Activity 9: Power and effect size","text":"First can calculate minimum effect size able detect given sample size design study using pwr.f2.test(). usual, fill information set effect size argument, case f2, NULL.u - Numerator degrees freedom. number coefficients model (minus intercept)\nv - Denominator degrees freedom. calculated v=n-u-1, n number participants\nf2 - effect size - solving effect size, parameter left NULL\nsig.level - significance level study\npower - power level studyBased power analysis, minimum effect size able detect rounded 2 decimal places? According Cohen's guidelines, SmallMediumLarge effect.formula calculate observed f2, must manually using formula lecture.observed effect size larger minimum effect size detect? Yes, study sufficiently poweredNo, study underpowered","code":"\npwr.f2.test(u = 1, v = 35, f2 = NULL, sig.level = .05, power = .8)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)"},{"path":"simple-regression.html","id":"regression-a10","chapter":"10 Simple Regression","heading":"10.10 Activity 10: Write-up","text":"two ways can use R help write-. first inline coding like done previously, second use report package. one use entirely nice options.need manually calculate p-value inline coding extract lm() model. Run code .Now, copy paste code white-space knit document.simple linear regression performed engagement (M = 4.54, SD = 0.56) outcome variable statistics anxiety (M = 2.08, SD = 0.56) predictor variable. results regression indicated model significantly predicted course engagement (F(1, 35) = 11.99, p < .001, Adjusted R2 = 0.23, f2 = .63), accounting 23% variance. Anxiety significant predictor (β = -2.17, p < 0.001.\n)second option uses report. output functions tend usable without editing particularly first learning write-stats can useful kind template (also see different ways reporting stats).Running report() output summary results copy past Word document.","code":"\nf <-mod_summary$fstatistic\nmod_p <- pf(f[1], f[2], f[3], lower=FALSE) A simple linear regression was performed with engagement (M = `r descriptives$mean_weeks %>% round(2)`, SD = `r descriptives$sd_anx %>% round(2)`) as the outcome variable and statistics anxiety (M = `r descriptives$mean_anx %>% round(2)`, SD = `r descriptives$sd_anx %>% round(2)`) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3]`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f2 = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Anxiety was a significant predictor (β = `r mod$coefficients[2] %>% round(2)`, p < `r mod_p %>% round(3)`.\n)\nreport(mod)## Warning: 'effectsize::interpret_d' is deprecated.\n## Use 'interpret_cohens_d' instead.\n## See help(\"Deprecated\")\n\n## Warning: 'effectsize::interpret_d' is deprecated.\n## Use 'interpret_cohens_d' instead.\n## See help(\"Deprecated\")## We fitted a linear model (estimated using OLS) to predict n_weeks with mean_anxiety (formula: n_weeks ~ mean_anxiety). The model explains a statistically significant and moderate proportion of variance (R2 = 0.26, F(1, 35) = 11.99, p = 0.001, adj. R2 = 0.23). The model's intercept, corresponding to mean_anxiety = 0, is at 9.06 (95% CI [6.32, 11.80], t(35) = 6.71, p < .001). Within this model:\n## \n##   - The effect of mean anxiety is statistically significant and negative (beta = -2.17, 95% CI [-3.45, -0.90], t(35) = -3.46, p = 0.001; Std. beta = -0.51, 95% CI [-0.80, -0.21])\n## \n## Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."},{"path":"simple-regression.html","id":"regression-sols","chapter":"10 Simple Regression","heading":"10.11 Solutions to Activities","text":"","code":""},{"path":"simple-regression.html","id":"regression-a1sol","chapter":"10 Simple Regression","heading":"10.11.1 Activity 1","text":"** Click tab see solution **","code":"\nlibrary(\"pwr\")\nlibrary(\"broom\")\nlibrary(\"see\")\nlibrary(\"performance\")\nlibrary(\"report\")\nlibrary(\"tidyverse\")\nstars <- read_csv(\"L3_stars.csv\")\nengage <- read_csv(\"psess.csv\")"},{"path":"simple-regression.html","id":"regression-a2sol","chapter":"10 Simple Regression","heading":"10.11.2 Activity 2","text":"** Click tab see solution **","code":"\nstars2 <- pivot_longer(data = stars, names_to = \"Question\", values_to = \"Score\",cols = Q01:Q51) %>%\n  arrange(ID)"},{"path":"simple-regression.html","id":"regression-a3sol","chapter":"10 Simple Regression","heading":"10.11.3 Activity 3","text":"** Click tab see solution **","code":"\nstars_means <- stars2 %>%\n  group_by(ID) %>%\n  summarise(mean_anxiety = mean(Score, na.rm = TRUE),\n            min = min(Score), \n            max = min(Score),\n            sd = sd(Score))"},{"path":"simple-regression.html","id":"regression-a4sol","chapter":"10 Simple Regression","heading":"10.11.4 Activity 4","text":"** Click tab see solution **","code":"\njoined <- inner_join(stars_means, engage, \"ID\")"},{"path":"simple-regression.html","id":"regression-a6sol","chapter":"10 Simple Regression","heading":"10.11.5 Activity 6","text":"** Click tab see solution **","code":"\nggplot(joined, aes(mean_anxiety, n_weeks)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  theme_minimal()"},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"11 Multiple regression","heading":"11 Multiple regression","text":"previous chapter, looked simple regressions - predicting outcome variable using one predictor variable. chapter, expand look scenarios predict outcome using one predictor model - hence, multiple regression.DataThere currently much debate (hype) surrounding smartphones effects well-, especially regard children teenagers. looking data recent study English adolescents:Przybylski, . & Weinstein, N. (2017). Large-Scale Test Goldilocks Hypothesis. Psychological Science, 28, 204--215.\nlarge-scale study found support \"Goldilocks\" hypothesis among adolescents: \"just right\" amount screen time, amount less amount associated lower well-. huge survey study: data contain responses 120,000 participants!Fortunately, authors made data study openly available, allows us dig deeper results. exercise, look whether relationship screen time well-modulated participants' (self-reported) gender.dependent measure used study Warwick-Edinburgh Mental Well-Scale (WEMWBS). 14-item scale 5 response categories, summed together form single score ranging 14-70.Przybylski & Weinstein's page study Open Science Framework, can find participant survey asks large number additional questions (see page 14 WEMWBS questions pages 4-5 questions screen time). Within page can also find raw data; however, purpose exercise, using local pre-processed copies data provide.Przybylski Weinstein looked multiple measures screen time, focusing smartphone use. found decrements well-started appear respondents reported one hour weekly smartphone use. question: negative association hours use well-(beyond one-hour point) differ boys girls?Note analysis, :continuous\\(^*\\) DV, well-;continuous\\(^*\\) DV, well-;continuous\\(^*\\) predictor, screen time;continuous\\(^*\\) predictor, screen time;categorical predictor, gender.categorical predictor, gender.\\(^*\\)variables quasi-continuous, inasmuch discrete values possible. However, sufficient number discrete categories can treat effectively continuous.want estimate two slopes relating screen time well-, one girls one boys, statistically compare slopes. problem seems simultaneously like situation run regression (estimate slopes) also one need t-test (compare two groups). expressive power regression allows us within single model.","code":""},{"path":"multiple-regression.html","id":"mulregression-a1","chapter":"11 Multiple regression","heading":"11.1 Activity 1: Set-up","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Multiple Regression\".Download wellbeing.csv, participant_info.csv screen_time.csv save Chapter folder. Make sure change file names .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads pwr, see, performance, report, tidyverse using library() function.Load CSV datasets variables called pinfo, wellbeing screen using read_csv().","code":""},{"path":"multiple-regression.html","id":"mulregression-a2","chapter":"11 Multiple regression","heading":"11.2 Activity 2: Look at the data","text":"Take look resulting tibbles pinfo, wellbeing, screen. wellbeing tibble information WEMWBS questionnaire; screen information screen time use weekends (variables ending ) weekdays (variables ending wk) four types activities: using computer (variables starting Comph; Q10 survey), playing video games (variables starting Comp; Q9 survey), using smartphone (variables starting Smart; Q11 survey) watching TV (variables starting Watch; Q8 survey). want information variables, look items 8-11 pages 4-5 PDF version survey OSF website.variable corresponding gender located table named pinfowellbeingscreen variable called .variable corresponding gender located table named pinfowellbeingscreen variable called .WEMWBS data longwide format, contains observations  participants  items.WEMWBS data longwide format, contains observations  participants  items.Individual participants dataset identified variable named  [sure type name exactly, including capitalization]. variable allow us link information across three tables.Individual participants dataset identified variable named  [sure type name exactly, including capitalization]. variable allow us link information across three tables.Run summary() three data-sets. missing data points? YesNoRun summary() three data-sets. missing data points? YesNo","code":""},{"path":"multiple-regression.html","id":"mulregression-a3","chapter":"11 Multiple regression","heading":"11.3 Activity 3: Compute the well-being score for each respondent","text":"WEMWBS well-score simply sum items.Write code create new table called wemwbs, two variables: Serial (participant ID), tot_wellbeing, total WEMWBS score.\"pivot\" table wide longgroup_by(); summarise(tot_wellbeing = ...)Sanity check: Verify scores fall 14-70 range. Przybylski Weinstein reported mean 47.52 standard deviation 9.55. Can reproduce values?summarise(), min(), max()Now visualise distribution tot_wellbeing histogram using ggplot2.geom_histogram()distribution well-scores symmetricnegatively skewedpositively skewed.","code":"\nggplot(wemwbs, aes(tot_wellbeing)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"multiple-regression.html","id":"mulregression-a4","chapter":"11 Multiple regression","heading":"11.4 Activity 4: Visualise the relationship","text":"take quick look relationship screen time (four different technologies) measures well-. code .Run code try explain words line code (remember, pronounce %>% \"\"). may find easier look tables produced.\nFigure 11.1: Relationship wellbeing screentime usage technology weekday\ngraph makes evident smartphone use 1 hour per day associated increasingly negative well-. Note combined tables using inner_join(), include data observations across wemwbs screen2 tables.next step, going focus smartphone/well-relationship.","code":"\nscreen_long <- screen %>%\n  pivot_longer(names_to = \"var\", values_to = \"hours\", -Serial) %>%\n  separate(var, c(\"variable\", \"day\"), \"_\")\nscreen2 <- screen_long %>%\n  mutate(variable = dplyr::recode(variable,\n               \"Watch\" = \"Watching TV\",\n               \"Comp\" = \"Playing Video Games\",\n               \"Comph\" = \"Using Computers\",\n               \"Smart\" = \"Using Smartphone\"),\n     day = dplyr::recode(day,\n              \"wk\" = \"Weekday\",\n              \"we\" = \"Weekend\"))\ndat_means <- inner_join(wemwbs, screen2, \"Serial\") %>%\n  group_by(variable, day, hours) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\nggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~variable, nrow = 2)"},{"path":"multiple-regression.html","id":"mulregression-a5","chapter":"11 Multiple regression","heading":"11.5 Activity 5: Smartphone and well-being for boys and girls","text":"analysis, going collapse weekday weekend use smartphones.Create new table, smarttot, mean number hours per day smartphone use participant, averaged weekends/weekdays.need filter dataset include smartphone use technologies.also need group results participant ID (.e., Serial).final data-set two variables: Serial (participant) tothours.need use data-set screen2 .filter() group_by() summarise()Next, create new tibble called smart_wb includes (filters) participants smarttot used smartphone one hour per day week, combine (join) table information wemwbs pinfo.**filter() inner_join() another inner_join()","code":""},{"path":"multiple-regression.html","id":"mulregression-a6","chapter":"11 Multiple regression","heading":"11.6 Activity 6: Mean-centering variables","text":"continuous variables regression, often sensible transform mean centering. mean center predictor X simply subtracting mean (X_centered = X - mean(X)). two useful consequences:model intercept reflects prediction \\(Y\\) mean value predictor variable, rather zero value unscaled variable;model intercept reflects prediction \\(Y\\) mean value predictor variable, rather zero value unscaled variable;interactions model, lower-order effects can given interpretation receive ANOVA (main effects, rather simple effects).interactions model, lower-order effects can given interpretation receive ANOVA (main effects, rather simple effects).categorical predictors two levels, become coded -.5 .5 (mean two values 0).Use mutate add two new variables smart_wb: tothours_c, calculated mean-centered version tothours predictor; male_c, recoded -.5 female .5 male.create male_c need use if_else(male == 1, .5, -.5) can read code \"variable male equals 1, recode .5, , recode -.5\".Finally, recode male male_c factors, R knows treat real numbers.","code":""},{"path":"multiple-regression.html","id":"mulregression-a7","chapter":"11 Multiple regression","heading":"11.7 Activity 7: Visualise the relationship","text":"Reverse-engineer plot. Calculate mean well-scores combination male tothours, create scatterplot plot includes separate regression lines gender.may find useful refer Data Visualisation chapter.group_by() variables summarise()colour = variable_you_want_different_colours_for\nFigure 11.2: Relationship mean wellbeing smartphone use gender\nWrite interpretation plot plain English.Girls show lower overall well-compared boys. addition, slope girls appears negative boys; one boys appears relatively flat.suggests negative association well-smartphone use stronger girls.","code":""},{"path":"multiple-regression.html","id":"mulregression-a8","chapter":"11 Multiple regression","heading":"11.8 Activity 8: Running the regression","text":"Now going see statistical support interpretation graph.data smart_wb, use lm() function calculate multiple regression model:\\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\)\\(Y_i\\) well-score participant \\(\\);\\(X_{1i}\\) mean-centered smartphone use variable participant \\(\\);\\(X_{2i}\\) gender (-.5 = female, .5 = male);\\(X_{3i}\\) interaction smartphone use gender (\\(= X_{1i} \\times X_{2i}\\))lm() function R main function estimate Linear Model (hence function name lm). function takes format :lm(dv ~ iv, data = my_data) simple linear regression (used previous chapter)lm(dv ~ iv1 + iv2, data = my_data) multiple linear regression; see just adds predictors model.Note: code lm(dv ~ iv1 + iv2, data = my_data) add predictors iv1 iv2. provide contributions predictor explain outcome variable. However, also interested interaction two predictors, use * instead +, .e., lm(dv ~ iv1 * iv2, data = my_data). Try see output changes.\nquestion trying answer , want look interaction gender (male_c) smartphone use time (thours_c) predict outcome variable well-(tot_wellbeing). Use lm() function run multiple regression. use summary() view results store object called mod_summary().R formulas look like : y ~ + b + :b :b means interactionThe interaction smartphone use gender shown variable thours_cmale_cthours_c:male_c, interaction significantnonsignificant \\(\\alpha = .05\\) level.interaction smartphone use gender shown variable thours_cmale_cthours_c:male_c, interaction significantnonsignificant \\(\\alpha = .05\\) level.2 decimal places, proportion variance well-scores overall model explain? 2 decimal places, proportion variance well-scores overall model explain? p-value overall model fit < 2.2e-16. significant? YesNoThe p-value overall model fit < 2.2e-16. significant? YesNoWhat reasonable interpretation results? smartphone use harms girls boyssmartphone use harms boys girlsthere evidence gender differences relationship smartphone use well-beingsmartphone use negatively associated wellbeing girls boysWhat reasonable interpretation results? smartphone use harms girls boyssmartphone use harms boys girlsthere evidence gender differences relationship smartphone use well-beingsmartphone use negatively associated wellbeing girls boys","code":""},{"path":"multiple-regression.html","id":"mulregression-a9","chapter":"11 Multiple regression","heading":"11.9 Activity 9: Assumption checking","text":"Now time test pesky assumptions. assumptions multiple regression simple regression one additional assumption, multicollinearity, idea predictor variables highly correlated.outcome/DV interval/ratio level dataThe predictor variable interval/ratio categorical (two levels)values outcome variable independent (.e., score come different participant)predictors non-zero varianceThe relationship outcome predictor linearThe residuals normally distributedThere homoscedasticity (homogeneity variance, residuals)Multicollinearity: predictor variables highly correlatedFrom work done far know assumptions 1 - 4 met can use functions performance package check rest, like simple linear regression chapter.One difference used check_model() previously rather just letting run tests wants, going specify tests, stop throwing error. word warning - assumptions tests take longer usual run, big dataset. first line code run assumption tests save object, calling object name display plots.\nFigure 4.11: Assumption plots\nassumption 5, linearity, already know looking scatterplot relationship linear, residual plot also confirms .assumption 6, normality residuals, residuals look good plots provides excellent example often better visualise rely statistics use check_normality() calls Shapiro-Wilk test:tells us residuals normal, despite fact plots look almost perfect. large sample sizes, deviation perfect normality can flagged non-normal.assumption 7, homoscedasticity, plot missing reference line - fun fact, took us several days lives asking help Twitter figure . reason line dataset large creates memory issue need create plot using code developers package see provided us Twitter. default code try draw confidence intervals around line causes memory issue, code removes se = FALSE.Please note datasets extra step, good example comes programming, matter long , always problem come across asking help part process.\nFigure 11.3: Adjusted homogeneity plot produce reference line\nlike normality, plot perfect pretty good another example visualisation better running statistical tests see significant result run:assumption 8, linearity, plot looks fine, also used grouped scatterplots look .Finally, assumption 9, multicollinearity, plot also indicates issues can also test statistically using check_collinearity().Essentially, function estimates much variance coefficient “inflated” linear dependence predictors, .e., predictor actually adding unique variance model, just really strongly related predictors. can read . Thankfully, VIF affected large samples like tests.various rules thumb, converge VIF 2 - 2.5 one predictor problematic.","code":"\nassumptions <- check_model(mod, check = c(\"vif\", \"qq\", \"normality\", \"linearity\", \"homogeneity\"))\nassumptions\ncheck_normality(mod)## Warning: Non-normality of residuals detected (p < .001).\nggplot(assumptions$HOMOGENEITY, aes(x, y)) +\n    geom_point2() +\n    stat_smooth(\n      method = \"loess\",\n      se = FALSE,\n      formula = y ~ x,\n    ) +\n    labs(\n      title = \"Homogeneity of Variance\",\n      subtitle = \"Reference line should be flat and horizontal\",\n      y = expression(sqrt(\"|Std. residuals|\")),\n      x = \"Fitted values\"\n    ) \ncheck_homogeneity(mod)## Warning: Variances differ between groups (Bartlett Test, p = 0.000).\ncheck_collinearity(mod)"},{"path":"multiple-regression.html","id":"mulregression-a10","chapter":"11 Multiple regression","heading":"11.10 Activity 10: Power and effect size","text":"Finally, calculate power effect size using pwr package. run power analysis previous chapter simple regressions,Using code power analysis calculate minimum effect size reliably observe given sample size design 99% power. Report 2 decimal places Using code power analysis calculate minimum effect size reliably observe given sample size design 99% power. Report 2 decimal places observed effect size study 2 decimal places? observed effect size study 2 decimal places? study sufficiently powered? YesNoIs study sufficiently powered? YesNo","code":""},{"path":"multiple-regression.html","id":"mulregression-a11","chapter":"11 Multiple regression","heading":"11.11 Activity 11: Making predictions","text":"successfully constructed linear model relating wel-gender smartphone use time. However, one last thing might want quickly show make prediction using predict() function. One way use , though see solution chapter alternatives, :newdata tibble new observations/values X (e.g. male_c /thours_c) want predict corresponding Y values (tot_wellbeing). try now.Make tibble two columns, one called male_c one called thours_c - exactly spelt model. Give male_c value -0.5 (girls) give thours_c value 4. code making tibble using tibble() function tibble(male_c = value, thours_c = value). Store tibble object newdata.Now put tibble, newdata, predict() function run regression model mod, predict(NAME MODEL, NAME TIBBLE).Now, based output try answer question:one decimal place, predicted well-rating girl smartphone use 4 hours - ","code":"\npredict(mod, newdata)"},{"path":"multiple-regression.html","id":"mulregression-a12","chapter":"11 Multiple regression","heading":"11.12 Activity 12: Write-up","text":"simple regression, can use inline coding report() function help write-. First, copy paste code white-space knit document. Note p-values entered manually APA p < .001 formatting.continuous predictors mean-centered deviation coding used categorical predictors. results regression indicated model significantly predicted course engagement (F(3, 7.1029^{4}) = 2450.89, p < .001, Adjusted R2 = 0.09, f2 = .63), accounting 9% variance. Total screen time significant negative predictor well-scores (β = -0.77, p < .001, gender (β = 5.14, p < .001, girls lower well-scores boys. Importantly, significant interaction screen time gender (β = 0.45, p < .001), smartphone use negatively associated well-girls boys.\nNow, can use report() produce automated summary. , need editing may useful aid interpretation reporting.","code":"All continuous predictors were mean-centered and deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3] %>% round(2)`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f^2^ = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Total screen time was a significant negative predictor of wellbeing scores (β = `r mod$coefficients[2] %>% round(2)`, p < .001, as was gender (β = `r mod$coefficients[3] %>% round(2)`, p < .001, with girls having lower wellbeing scores than boys. Importantly, there was a significant interaction between screentime and gender (β = `r mod$coefficients[4] %>% round(2)`, p < .001), smartphone use was more negatively associated with wellbeing for girls than for boys. \nreport(mod)## Warning: 'effectsize::interpret_d' is deprecated.\n## Use 'interpret_cohens_d' instead.\n## See help(\"Deprecated\")\n\n## Warning: 'effectsize::interpret_d' is deprecated.\n## Use 'interpret_cohens_d' instead.\n## See help(\"Deprecated\")## We fitted a linear model (estimated using OLS) to predict tot_wellbeing with thours_c and male_c (formula: tot_wellbeing ~ thours_c * male_c). The model explains a statistically significant and weak proportion of variance (R2 = 0.09, F(3, 71029) = 2450.89, p < .001, adj. R2 = 0.09). The model's intercept, corresponding to thours_c = 0 and male_c = -0.5, is at 44.87 (95% CI [44.78, 44.96], t(71029) = 1001.87, p < .001). Within this model:\n## \n##   - The effect of thours c is statistically significant and negative (beta = -0.77, 95% CI [-0.82, -0.73], t(71029) = -32.96, p < .001; Std. beta = -0.15, 95% CI [-0.16, -0.15])\n##   - The effect of male c [0 5] is statistically significant and positive (beta = 5.14, 95% CI [5.00, 5.28], t(71029) = 72.25, p < .001; Std. beta = 0.54, 95% CI [0.52, 0.55])\n##   - The interaction effect of male c [0 5] on thours c is statistically significant and positive (beta = 0.45, 95% CI [0.38, 0.52], t(71029) = 12.24, p < .001; Std. beta = 0.09, 95% CI [0.08, 0.11])\n## \n## Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation."},{"path":"multiple-regression.html","id":"mulregression-sols","chapter":"11 Multiple regression","heading":"11.13 Solutions to Activities","text":"","code":""},{"path":"multiple-regression.html","id":"mulregression-a3sol","chapter":"11 Multiple regression","heading":"11.13.1 Activity 3","text":"","code":"\nwemwbs <- wellbeing %>%\n  pivot_longer(names_to = \"var\", values_to = \"score\", -Serial) %>%\n  group_by(Serial) %>%\n  summarise(tot_wellbeing = sum(score))\n# sanity check values\nwemwbs %>% summarise(mean = mean(tot_wellbeing),\n                     sd = sd(tot_wellbeing),\n                     min = min(tot_wellbeing), \n                     max = max(tot_wellbeing))"},{"path":"multiple-regression.html","id":"mulregression-a5sol","chapter":"11 Multiple regression","heading":"11.13.2 Activity 5","text":"","code":"\nsmarttot <- screen2 %>%\n  filter(variable == \"Using Smartphone\") %>%\n  group_by(Serial) %>%\n  summarise(tothours = mean(hours))\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") "},{"path":"multiple-regression.html","id":"mulregression-a6sol","chapter":"11 Multiple regression","heading":"11.13.3 Activity 6","text":"","code":"\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") %>%\n  mutate(thours_c = tothours - mean(tothours),\n         male_c = ifelse(male == 1, .5, -.5),\n         male_c = as.factor(male_c),\n         male = as.factor(male))"},{"path":"multiple-regression.html","id":"mulregression-a7sol","chapter":"11 Multiple regression","heading":"11.13.4 Activity 7","text":"","code":"\nsmart_wb_gen <- smart_wb %>%\n  group_by(tothours, male) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\nggplot(smart_wb_gen, aes(tothours, mean_wellbeing, color = male)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\"))+\n  scale_x_continuous(name = \"Total hours smartphone use\") +\n  scale_y_continuous(name = \"Mean well-being score\")"},{"path":"multiple-regression.html","id":"mulregression-a8sol","chapter":"11 Multiple regression","heading":"11.13.5 Activity 8","text":"","code":"\nmod <- lm(tot_wellbeing ~ thours_c * male_c, smart_wb)\n# alternatively: \n# mod <- lm(tot_wellbeing ~ thours_c + male_c + thours_c:male_c, smart_wb)\nmod_summary <- summary(mod)"},{"path":"multiple-regression.html","id":"mulregression-a9sol","chapter":"11 Multiple regression","heading":"11.13.6 Activity 9","text":"","code":"\nqqPlot(mod$residuals)"},{"path":"multiple-regression.html","id":"mulregression-a10sol","chapter":"11 Multiple regression","heading":"11.13.7 Activity 10","text":"","code":"\npwr.f2.test(u = 3, v = 71029, f2 = NULL, sig.level = .05, power = .99)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)"},{"path":"multiple-regression.html","id":"mulregression-a11sol","chapter":"11 Multiple regression","heading":"11.13.8 Activity 11","text":"Note: value gender -0.5 0.5 recoded categorical variable. reason, value gender entered within quotes.","code":"\nnewdata <- tibble(male_c = \"-0.5\", thours_c = 4)\npredict(mod, newdata)"},{"path":"acknowledgements.html","id":"acknowledgements","chapter":"Acknowledgements","heading":"Acknowledgements","text":"book work many people, staff students within School Psychology & Neuroscience, University Glasgow. special mention, however, go following people: Stephanie Boyle, Molly Burr, Morgan Daniel, Amalia Gomoiu, Kate Haining, Jesse Klein, Rebecca Lai, Steven McNair, Shannon McNee, Jennifer Murch, Jack Taylor, Jaimie Torrance, Ana Skolaris & Hollie Sneddon.hugely appreciate comments help creating material contained within book.special shout-students worked book pointed errors made suggestions: helped us consistently increase quality book. Thanks.","code":""},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video .","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). install R, also install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page. install R, also install RTools; use \"recommended\" version highlighted near top list.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.3 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. Foe example, Lisa prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.4 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. course require make PDFs. generate PDF reports, additionally need install tinytex (Xie, 2021) run following code:","code":"\ntinytex::install_tinytex()"},{"path":"updating-r-rstudio-and-packages.html","id":"updating-r-rstudio-and-packages","chapter":"B Updating R, RStudio, and packages","heading":"B Updating R, RStudio, and packages","text":"time--time, updated version R, RStudio, packages use (e.g., ggplot) become available. Remember separate, different process come different considerations. recommend updating latest version three start academic year.","code":""},{"path":"updating-r-rstudio-and-packages.html","id":"updating-rstudio","chapter":"B Updating R, RStudio, and packages","heading":"B.1 Updating RStudio","text":"RStudio easiest component update. Typically, updates RStudio affect code, instead add new features, like spell-check upgrades RStudio can . usually little downside updating RStudio easy .Click Help - Check updates\nFigure B.1: Updating RStudio\nupdate available, prompt download can install usual.","code":""},{"path":"updating-r-rstudio-and-packages.html","id":"updating-packages","chapter":"B Updating R, RStudio, and packages","heading":"B.2 Updating packages","text":"Package developers occasionally release updates packages. typically add new functions package, fix amend existing functions. aware package updates may cause previous code stop working. tend happen minor updates packages, occasionally major updates, can serious issues developer made fundamental changes code works. reason, recommend updating packages beginning academic year (semester) - assessment deadline just case!update individual package, easiest way use install.packages() function, always installs recent version package.update multiple packages, indeed packages, RStudio provides helpful tools. Click Tools - Check Package Updates. dialogue box appear can select packages wish update. aware select packages, may take time unable use R whilst process completes.\nFigure B.2: Updating packages RStudio\nOccasionally, might problem packages seemingly refuse update, , rlang vctrs cause end trouble. packages likely every explicitly load, required beneath surface R things like knit Markdown files etc.try update package get error message says something like Warning install.packages : installation package ‘vctrs’ non-zero exit status perhaps Error loadNamespace(, c(lib.loc, .libPaths()), versionCheck = vI[[]]) :  namespace 'rlang' 0.4.9 loaded, >= 0.4.10 required one solution found manually uninstall package, restart R, install package new, rather trying update existing version. installr package also useful function uninstalling packages.","code":"\ninstall.packages(\"tidyverse\")\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")"},{"path":"updating-r-rstudio-and-packages.html","id":"updating-r","chapter":"B Updating R, RStudio, and packages","heading":"B.3 Updating R","text":"Finally, may also wish update R . key thing aware update R, just download latest version website, lose packages. easiest way update R cause huge headache use installr package. use updateR() function, series dialogue boxes appear. fairly self-explanatory full step--step guide available use installr, important bit select \"Yes\" asked like copy packages older version R.always, issues, please ask Teams book GTA session.","code":"\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Load installr\nlibrary(installr)\n\n# Run the update function\nupdateR()"},{"path":"exporting-files-from-the-server.html","id":"exporting-files-from-the-server","chapter":"C Exporting files from the server","heading":"C Exporting files from the server","text":"using R server, may need export files share people submit assignments.First, make sure saved changes made file. clicking \"File - Save\", Ctrl + S, clicking save icon. changes saved, save icon greyed . new unsaved changes, able click icon.Select file download files pane (bottom right) ticking box next , click \"- Export\" save file computer.R installed, try open computer. , open Word, Endnote similar, may corrupt code. open file R R Studio installed.want double check file definitely right one submit assignment, can re-upload server open make sure answers .","code":""},{"path":"symbols.html","id":"symbols","chapter":"D Symbols","heading":"D Symbols","text":"\nFigure D.1: Image James Chapman/Soundimals\n","code":""},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit, provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
