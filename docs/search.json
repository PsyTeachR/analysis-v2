[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"Authors: Phil McAleer, Carolina E. Kuepper-Tetzel, & Helena M. PatersonAim: course covers data skills R Markdown, data wrangling tidyverse, data visualisation ggplot2. also introduces statistical concepts probabilities, Null Hypothesis Significance Testing (NHST), alpha, power, effect size, sample size. common statistical analyses covered book t-test, correlations, ANOVAs, Regressions.Note: book currently updated means chapters published rolling basis.Contact: book living document regularly checked updated improvements. issues using book queries, please contact Carolina E. Kuepper-Tetzel.R Version: book written R version 4.1.3 (2022-03-10)Randomising Seed: chapters use level randomisation, remembered, seed set 1409.\nbook help learn whole host skills methods based around psychologist. completed Data Skills book PsyTeachR series (https://psyteachr.github.io/) first chapters familiar , additions. deliberate order refresh knowledge skills moving advanced topics. First, remind work R Markdown, recapping main functions use visualisation data wrangling. build understanding probability going using refreshed skills analyse variety different experiments. main idea book reproducible data analysis approach.book requires higher level self-directed learning first book; part learning trying things recognising need help. get stuck, google problem like see helps.working book remember learning software. teach R independent statistical knowledge content. Rather, teach data analytical skills knowledge within R. goal continuously improve data analysis skills!can !","code":""},{"path":"starting-with-r-markdown.html","id":"starting-with-r-markdown","chapter":"1 Starting with R Markdown","heading":"1 Starting with R Markdown","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"overview-1","chapter":"1 Starting with R Markdown","heading":"1.1 Overview","text":"key goal researcher carry experiment tell others . One main ways Psychologists publication journal articles. numerous ways people combine different software create journal article, recent innovation field want know creating reports articles R Markdown. like, can see example research team school recent PLOS article. link within article methods section (one - https://osf.io/eb9dq/) allows see one file creates whole manuscript. Obviously writing full journal articles just yet, use R Markdown throughout lab series assignments. also use subjects write reports, make portfolio hints, tips, study aids suggest throughout labs.Today, start showing skills using R Markdown efficiently.chapter learn:R Markdown?create R Markdown file knit .add code edit rules R Markdown file.format text.","code":""},{"path":"starting-with-r-markdown.html","id":"what-is-r-markdown","chapter":"1 Starting with R Markdown","heading":"1.1.1 What is R Markdown?","text":"R Markdown (abbreviated Rmd) great way create dynamic documents embedded chunks code. documents self-contained fully reproducible makes easy share. information R Markdown, feel free look main webpage sometime: R Markdown Webpage. key advantage R Markdown allows write code document, along regular text, knit using package knitr() create document either webpage (HTML), PDF, Word document (.docx).\nThroughout labs see little tabs give \ninformation, answers quick questions, helpful hints, solutions \ntasks, suggestions information want note somewhere.\nread find get less \ncourse progresses, might help stuck \nsomething.\n\nKnit say want turn R Markdown file \neither webpage, PDF, Word document. Often labs \nhear someone say, “tried knitting ?” “happens \nknit ?”. simply means happens try turning \nfile pdf webpage.\n\npractical data assignments, one check run \nsubmitting knit code html (webpage) file see\ncan open file browser. doesn’t check \ncode correct. however confirm code runs \ncritical issues stop code running. \nvaluable check.\n","code":""},{"path":"starting-with-r-markdown.html","id":"advantages-of-using-r-markdown","chapter":"1 Starting with R Markdown","heading":"1.1.2 Advantages of using R Markdown","text":"output one file includes figures, text, citations. additional files needed easy keep work one place.output one file includes figures, text, citations. additional files needed easy keep work one place.R code can put directly R Markdown report, necessary keep writing (e.g., Word document) analysis (e.g., R script) separate.R code can put directly R Markdown report, necessary keep writing (e.g., Word document) analysis (e.g., R script) separate.Including R code directly lets others see analysis - good thing science! reproducible transparent, key components Open Science!Including R code directly lets others see analysis - good thing science! reproducible transparent, key components Open Science!write report plain text, non software-specific format easy share, necessary learn new coding language HTML, can create various outputs depending need.write report plain text, non software-specific format easy share, necessary learn new coding language HTML, can create various outputs depending need.","code":""},{"path":"starting-with-r-markdown.html","id":"creating-an-r-markdown-.rmd-file","chapter":"1 Starting with R Markdown","heading":"1.1.3 Creating an R Markdown (.Rmd) File","text":"chapter going create R Markdown document. Knowing :help navigate R Markdown.show create homework assignment documents.help create reports using .point unsure something remember think can get help, , google (R markdown cheat sheets internet). example, forget put words bold, simply go Google type \"rmarkdown bold\" doubt get lot useful hints. nothing wrong . Nobody expecting keep every function head; need reminders. find elements stick head better others. remember, Google friend!Quickfire QuestionsWe put questions throughout help test knowledge. type choose correct answer, dashed box change color become solid.following options, creating R Markdown document instead simply using R script? R Markdown can combine report writing analysisR Scripts run codeReproducible Science!\n’s one answer question! R Markdown can\ncombine report writing analysis, providing open access others \nexamine data, create Reproducible Science. \nincorrect answer? R Scripts fact run R code may remember\nLevel 1 labs. key difference R Scripts really \nused documentation creating reports easily - R\nMarkdown used ensure code can added \ninformation research can reproduced others.\n","code":""},{"path":"starting-with-r-markdown.html","id":"one-last-thing-before-beginning","chapter":"1 Starting with R Markdown","heading":"1.1.4 One last thing before beginning! :)","text":"Remember: can always go back Data Skills Book Level 1 remind skills want already learned using R RStudio. first chapters book partly overlap learned previously, extend skills knowledge.","code":""},{"path":"starting-with-r-markdown.html","id":"r-markdown-basics","chapter":"1 Starting with R Markdown","heading":"1.2 R Markdown Basics","text":"read Overview chapter, reason behind using R, now going work making reproducible code. laptop, best install R Rstudio use. Appendix find reminder install R Rstudio.","code":""},{"path":"starting-with-r-markdown.html","id":"create-a-new-r-markdown-document","chapter":"1 Starting with R Markdown","heading":"1.2.1 Create a new R Markdown document","text":"Create new R Markdown file (.Rmd) opening Rstudio, top menu, selecting File >> New File >> R Markdown.... now see following dialog box:\nFigure 1.1: Starting R Markdown file\nClick Document left-hand panel give document Title.file call want make sure informative reader.Put name student ID Author field author. now focus making HTML output, make sure selected shown Figure 1.1 hit OK done . now .Rmd file open Rstudio.first thing see R Markdown file header section enclosed top bottom ---. Technically called yaml header, section lists title, author, date output format. layout header precise look like shown Figure 1.2, currently set output HTML.\nFigure 1.2: Rmd yaml header\ndefault file header includes info shown Figure 1.2 many options available. can learn spare time like links \n.html options{_target=\"_blank\"} \n.pdf options.\nWAIT!! spelt name wrong? \nchange ?\n\nlong way close file start . shorter\nway just correct info header - just remember \nkeep quotes. E.g. “Si Cologe” instead “Untitled”\n","code":""},{"path":"starting-with-r-markdown.html","id":"code-chunks","chapter":"1 Starting with R Markdown","heading":"1.2.2 Code Chunks","text":"Immediately header information see default setup code chunk shown Figure 1.3. time, lab series, edit information chunk. Instead, add information, text, code, chunks, chunk.\nFigure 1.3: defualt setup code chunk\nRMarkdown can type text want directly document just word document. However, want include code need include one code chunks similar Figure 1.3. Code chunks start line contains three backwards apostrophes ` (called grave accents - often top-left QWERTY keyboards), set curly brackets letter r inside:always need parts create code chunk:three back ticks ` part Rmd file says code inserted document.{r} part says specifically including R code.default setup code chunk provides basic options R Markdown file knits work. , now, best leave particular code chunk alone. Instead show use R Markdown editing code chunks come default chunk.next code chunk file look bit like :Within curly brackets, first line chunk, word cars included letter r. simply name label code chunk really called anything. example, called code chunk cars1 later chunk cars2 show first second chunk relating cars. Whilst always advisable name code chunks, need name . However, put names chunks use name twice cause script crash knit , e.g. use data data; instead maybe use personality-data participant-info whatever makes sense chunk. OK? Different names different chunks! individual.\nRemember knitting just means converting rendering file \npdf, webpage, etc. Crashing means error code\nstopped knitting working finishing. can usually\nfind problem line code error message ’ll see.\n\nsecond line code chunk R code written: summary(cars). case, just asking summary() inbuilt dataset cars. R lot inbuilt datasets practice ; cars one .third line closes code chunk, three backwards apostrophes. means whatever contained first third lines code run.\npeople first starting using R Markdown, common issue\ncode working started code chunk correctly,\nforgotten close bottom three backticks.\nRemember, three backticks open, three backticks close, \nchunk bind .\nQuickfire QuestionsFrom following options name, label, default setup code chunk (.e. first code chunk R Markdown file)? includersetupFALSE\nlook default setup code chunk can see code\nchunk name setup. include=FALSE rule explain\nlittle bit.\n","code":"```{r}``````{r cars}\nsummary(cars)```"},{"path":"starting-with-r-markdown.html","id":"knitting-code","chapter":"1 Starting with R Markdown","heading":"1.2.3 Knitting Code","text":"Now good time try knitting file see code chunks . can using Knit button top RStudio screen:\nFigure 1.4: knit button. Clicking knit file.\nclick Knit ask save file .Rmd file. Call file L2Psych_Ch1_RMarkdownBasics.Rmd save folder keep information lab. working Psychology labs University Library need save location drive space full access can save files . best one campus M: drive. using device anywhere can save file work. However, good folder structure help navigate labs better.\nbeneficial create folder \nM: drive contain data skills work \nrest Level 2. Maybe something like\nPsychology_Level2_DataSkills_Work folders\nwithin lab, e.g Chapter1. clearer \nstructure folders easier find use \nfiles ! important one thing keep telling \nLOOK BACK (politely) previously\n.\n\nCouple tips:\n\nAvoid spaces file names folder names. can make life really\ncomplicated bad habit start . Use underscores \nwords filenames folder names.\n\nNever call folder “R”. crash R potentially\nlead reinstall R Rstudio. Rstudio opens\nlooks folder called R expects contain software\nlibraries. aren’t now looking \ndifferent folder name, things go wrong.\n\nsaving file, webpage appear. first thing notice lines code chunks disappeared: ```{r} closing ``` code chunk gone. Whenever knit R Markdown file lines disappear leaving code within. also notice output code also now showing webpage. next section show control showing output code, , adding rules.\nFigure 1.5: knitted summary output\n","code":""},{"path":"starting-with-r-markdown.html","id":"adding-code-chunk-rules-and-options","chapter":"1 Starting with R Markdown","heading":"1.2.4 Adding Code Chunk Rules and Options","text":"can often good idea even necessary show data outcome test report, example writing report wanted include table results. code displayed table 10,000 lines long? case might want show output show code. can including rule within first line code chunk - ```{r name, rule = option} line. already seen rule standard default chunk, include rule, number others. look now:First, look hide output show code. , use results = \"hide\" rule:\nFigure 1.6: results Rule\nAdd rule example code chunk, shown , knit file . happens? Note comma separating name chunk rule. now see code data. key thing note code still \"running\", just showing output. example, say code said x <- 2 + 2. results = \"hide\" rule, still running line code, x assigned 4, just see output.\n\nAlternatively, can hide code, show ouput using echo = FALSE rule:\nFigure 1.7: echo Rule\n\ntemplate Rmd file, rule echo set FALSE meaning show figure code. Change rule code echo set TRUE, knit file . happens?\nRemember Level 1 called libraries \nenvironment. “echo = FALSE” option useful commands like\nlibrary() just calling package \nlibrary don’t necessarily want display final report\nfinal HTML file. Another example might wanted \nmake plot didn’t want include code, just want show\nplot report.\n\nNext, say want hide code output still run code. can using include rule:\nFigure 1.8: include Rule\nChange rule example code chunk, shown , include = FALSE knit file . happens? Note code still runs. just show anything.Finally, can use eval rule specifies whether want code chunk written evaluated knit RMarkdown file. Evaluated means run carry code. , eval = FALSE rule stop code evaluated. code shown rule stopping output get evaluated eval rule FALSE.\nFigure 1.9: eval Rule\n\nmight useful cases want show code relating programmed stimuli experiment, necessarily want run part R Markdown file.\n\nprobably wee summary :\n\nTable 1.1: Rules! Rules! Rules!\ncan also mix match rules get code/output display want. takes little getting used first doubt, just ask.\ncan use RStudio’s autocomplete (tab button) see \ndifferent options different rules. example, type\ninclude = hit tab button keyboard. \nsee options TRUE FALSE.\n\nAutocomplete also works lot functions can’t quite\nremember spell well. gg-? gg-{tab button}… Ah yes,\nggplot().\nQuickfire QuestionsYou've got large dataset thousands participants' personality happiness scores want analyse present RMarkdown.want show code running analysis show output much display. Note want code run. Type box (e.g. rule = set) set results rule ? want show code running analysis show output much display. Note want code run. Type box (e.g. rule = set) set results rule ? create plot happiness versus neuroticism scores want hide code show output. can ? echo = TRUEinclude = FALSEcode = HIDEecho = FALSEYou create plot happiness versus neuroticism scores want hide code show output. can ? echo = TRUEinclude = FALSEcode = HIDEecho = FALSE\nfirst answer results = \"hide\" want\nshow code run code necessarily show output \ncode.\n\nsecond question, include = FALSE technically\nhide code, also hides output!\necho = FALSE allows still see plot hiding\ncode want hidden. code = HIDE - \nsimple!\n\nRemember, aim questions aren’t help memorise\ncodes (one can !); ’re help gain better\nunderstanding apply codes come across \nfuture.\nTrue False, writing echo = TRUE effect output code chunk echo rule : TRUEFALSE\ncode chunk rules default option. example,\necho, include, eval \nusually default set TRUE. result, don’t\nset echo rule, .e. don’t specifically set\necho = FALSE code chunk, \nsetting echo = TRUE. specifying option give\ndefault setting option.\nTrue False, difference setting results = \"hide\" eval = FALSE hide output: TRUEFALSE\nsetting results = \"hide\", code evaluated \nresults produced output hidden. setting\neval = FALSE, code evaluated therefore \nresults output produced. need output \nlater part code might use\nresults = \"hide\". don’t need output just\nwant show code example might use\neval = FALSE.\n","code":""},{"path":"starting-with-r-markdown.html","id":"adding-inline-code","chapter":"1 Starting with R Markdown","heading":"1.2.5 Adding Inline Code","text":"alternative way add code report called using inline code. inline code use code chunk. Instead code appears inline text. Inline code can inserted using back-tick, letter r, followed space, code want include, finally another back-tick. example, writing `r 2 + 2` return answer 4 knit file instead showing code. Remember, inside code chunk, line text, e.g.:\n\"ran `r 2+2` people\".\n\nknitted becomes:\n\"ran 4 people\".inline coding really useful want calculations within text insert values text, say dataframe, make informative sentence. look complex examples later labs really useful tool writing manuscripts R Markdown comfortable get .Quickfire QuestionsYou need TwoOneThree back tick(s) insert code chunksYou need TwoOneThree back tick(s) insert code chunksWhy inline code, `{r} 6 * 8` , going show calculated answer knit file? Try editing code line Rmarkdown knitting get work. need space back tick codeInline code complete calcuationsCurly brackets around r needed code chunksWhy inline code, `{r} 6 * 8` , going show calculated answer knit file? Try editing code line Rmarkdown knitting get work. need space back tick codeInline code complete calcuationsCurly brackets around r needed code chunks\n\ncode chunks start end three back-ticks.\n\n\ncode chunks start end three back-ticks.\n\n\nInline coding use curly brackets around \nr.\n\n\nInline coding use curly brackets around \nr.\n\n\nneed inline coding back-tick, r, space, code,\nfinal back-tick.\n\n\nneed inline coding back-tick, r, space, code,\nfinal back-tick.\n","code":""},{"path":"starting-with-r-markdown.html","id":"formatting-the-r-markdown-file","chapter":"1 Starting with R Markdown","heading":"1.2.6 Formatting the R Markdown File","text":"last thing want show preclass activity format text.writing code chunks can format document lots different ways just like Word document (expensive license-based software). R Markdown cheatsheet provides lots information show couple things might want try .can make text bold including two ** (two asterisks) start end text want present bold font. example:\n\n\"ran **4 people**.\n\nknitted becomes:\n\n\"ran 4 people\".\nNow write text Rmd file put bold. Knit file check worked.also try using italics putting single * (asterisk) start end word sentence. Try now. example help.\n\n\"ran *4 people*.\n\nknitted becomes:\n\n\"ran 4 people\".\nNote: italics can difficult read many people tried avoid using book. find italics, necessary, please let us know claim reward packet minstrels. Yes, whole packet!Finally, might want add headings sub-headings file. example, maybe writing Psychology journal article want put header Introduction, Methods, Results, Discussion sections. using # (hashtag) symbol shown Figure 1.10.\nFigure 1.10: Inputting different Header levels using #s\nNow, type four main sections found Psychology journal article R Markdown file, typing one separate line. mentioned . Knit file. look like?Now add different number #'s heading, space heading hashtag (e.g. # Introduction) knit file . notice different number hashtags?Quickfire QuestionsIf * puts words italics, ** puts words bold, type box might put (technically ) word put italics bold? * puts words italics, ** puts words bold, type box might put (technically ) word put italics bold? True False: '#'s include, smaller header : TRUEFALSETrue False: '#'s include, smaller header : TRUEFALSEFrom options, common order headings found Psychology Journal : Discussion, Introduction, Methods, ResultsDiscussion, Results, Methods, IntroductionIntroduction, Methods, Results, DiscussionIntroduction, Results, Methods, DiscussionFrom options, common order headings found Psychology Journal : Discussion, Introduction, Methods, ResultsDiscussion, Results, Methods, IntroductionIntroduction, Methods, Results, DiscussionIntroduction, Results, Methods, Discussion\n* start end word puts italics\n(e.g. italics) ** puts bold\n(e.g. bold), putting three *** start \nend put italics bold\n(e.g. italics-bold).\n\ntrue #’s use, smaller heading .\nWord document writers use different headings well. , #\ngives biggest heading, gets smaller smaller every\nextra #.\n\nFinally, Psychology, vast majority journal articles \nwritten format : Introduction, Methods, Results, Discussion.\nformat always hold journals ask authors use \ndifferent format, depending much emphasis journal (erroneously)\nlikes put results hypothesis methods. however teach\norder stated . question approach always \nimportant, , results! course know\nlearning Registered Reports labs lectures.\n","code":""},{"path":"starting-with-r-markdown.html","id":"r-markdown-application","chapter":"1 Starting with R Markdown","heading":"1.3 R Markdown Application","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"r-markdown-and-the-experimental-design-portfolio","chapter":"1 Starting with R Markdown","heading":"1.3.1 R Markdown and The Experimental Design Portfolio","text":"going create R Markdown scratch. also start create Experimental Design Analysis Portfolio R Markdown. aim portfolio consolidate learning experimental design analysis, allowing reflect back learning progressed. add whenever think \"Oh good tip!\" \"something want remember!\". chapter way consolidate knowledge. portfolio assessed marked anyway. learning aid help develop understanding research methods analysis Psychology.Across following nine tasks, help structure format R Markdown files; can apply learn portfolio time. begin!\nThroughout book see Portfolio Points. \njust points suggest add portfolio. Ultimately,\nkeep portfolio, \nexamples kind things recommend include:\n\nKey points classic experiments\n\nmain goal, outcome, authors, year\n\n\ntop tip write short summary every paper read,\nincluding authors’ names help consolidate \ninformation\n\n\nmain goal, outcome, authors, year\n\ntop tip write short summary every paper read,\nincluding authors’ names help consolidate \ninformation\n\nAspects Reports’ designs analyses\n\ndecisions made ; compare \nstudies.\n\n\ndecisions made ; compare \nstudies.\n\nGlossary points R code functions\n\ncodes find challenging understand function\n\n\n\ncodes might use frequently future activities\n\n\ndeveloping glossary can send us items include\nget involved . still development can see \nhttps://psyteachr.github.io/glossary/.\n\n\ncodes find challenging understand function\n\n\ncodes might use frequently future activities\n\ndeveloping glossary can send us items include\nget involved . still development can see \nhttps://psyteachr.github.io/glossary/.\n\nReflection Points learned week.\n","code":""},{"path":"starting-with-r-markdown.html","id":"the-ponzo-illusion-and-age","chapter":"1 Starting with R Markdown","heading":"1.3.2 The Ponzo Illusion and Age","text":"activities chapter make use open dataset.\nopen dataset made available everyone see stored\ninternet researchers use. previous section,\nsaw example start PLOS One article.\nMany journals now ask researchers make data available \npost somewhere accessible like \nOpen Science\nFramework.\n\nInterestingly, art making data available standard \nclassic older articles. data using today comes 1967.\nSometime recent times, data started made\nunavailable - closed. believe data made available \nencourage coming years. Transparent science\nOpen Science!\n\ndata use today paper looking Ponzo illusion Age:Leibowitz, H. W. & Judisch, J. M. (1967). Relation Age Magnitude Ponzo Illusion. American Journal Psychology, 80(1), 105-109. can accessed campus (University Glasgow) link. campus can sign read University Glasgow library student Glasgow.basics Ponzo illusion (Wikipedia page) two lines size viewed different length based surrounding information - like sleepers traintrack. See Figure 1 Leibowitz Judisch (1967) example (P106). authors showed people two vertical lines surrounded differing horizontal lines running angles behind main vertical lines. authors varied size one vertical lines (left line) asked participants judge two vertical lines bigger longer; left line (variable) right one (standard). paper also tested illusion influenced age. info, see paper. Operationalising dependent variable, Leibowitz & Judisch measured size left line considered size standard line right. data using can seen page 107, includes:Group participants assigned according age, group made 10 participants sexThe Sex GroupThe Mean Age GroupThe Mean Length left vertical line","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT1","chapter":"1 Starting with R Markdown","heading":"1.3.3 Task 1: Setting up Your R Markdown Portfolio","text":"overall goal make reproducible \"report\" summarising data Leibowitz Judisch (1967) paper. begin!Create new R Markdown document.Give title, e.g. Psychology Research Methods PortfolioEnter GUID name authorSet output HTML.\nThroughout labs see Helpful Hints. Usually \nsolutions nearby end chapter prevent\ntemptation.\n\nsetting Rmd file, followed steps\ncorrectly, probably see new R Markdown file header\ncontaining title, author, date output information shown \nprevious section.\n\ndon’t see document header, ’ve probably created \nR Script instead. Refer back R Markdown Basics activity \ntry . Look list File options top\nmenu.\n\ncan now remove parts generic R Markdown code need; anything setup code chunk can removed (see Figure 1.3). anything line 11 can removed. Leave first code chunk however - lines 8 10 - lines make R Markdown show code chunks unless otherwise specified - note echo =  TRUE.\nWrite reminder somewhere portfolio code chunk\n. Writing notes somewhere accessible mean\ncan find easily.\n","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT2","chapter":"1 Starting with R Markdown","heading":"1.3.4 Task 2: Give your Report a Heading","text":"going start portfolio creating brief report Leibowitz Judisch (1967), give heading.setup code chunk, give report heading, e.g. Lab 1 - Magnitude Ponzo Illusion varies function Age.Using hashtags, give heading Header 1 size.\nRemember fewer number hashtags larger heading\nsize.\n","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT3","chapter":"1 Starting with R Markdown","heading":"1.3.5 Task 3: Creating a Code Chunk","text":"going need data soon best bring start code.Set working directory: Session >> Set Working Directory >> Choose Directory\nOne common issues see people using Rstudio \nforget set working directory folder containing\ndata file working . means try knit\nrun code line won’t work Rstudio doesn’t know \ndata . Remember set working directory start \nsession, using\nSession >> Set Working Directory >> Choose Directory\n\nAvoid using code set working directory often \nwork machine others therefore fully\nreproducible without editing script.\nDownload data lab zip file clicking link. Unzip save folder working .Create new code chunk R Markdown script, give code chunk name load_data.Copy paste code code chunk. Spend couple minutes partner reminding code . answer hint .Now, add change echo rule code chunk knit file, code included final document.Knit document now see output looks like. ask save file somewhere. Remember Boyd Orr Lab PCs best done M: drive, given available space.Important: good chance , webpage knitted, see either warnings messages. can suppress using message warning rules within code chunks well. Try now - PreClass Activities R-Markdown cheatsheet help.\nHints:\n\nStep 4 - echo can equal TRUE FALSE.\n\nRemember separate rules code chunk commas. E.g. {r,\nrule1 = FALSE, rule2 = TRUE}\n\ncode ?\n\nLine 1 loads tidyverse packages associated\npackages e.g. dplyr, readr \nggplot2. used Level 1 Grassroots book -\nrecap lot coming labs.\nLine 2 loads data using read_csv() function\nstores ponzo_data.\n\nImportant points note:\n\nponzo_data called anything best \ncall something makes clear . rule spaces\nname. ponzo_data ponzo.data \nacceptable, different . ponzo data \nacceptable crash code.\n\nread_csv() actually readr\npackage available loaded \ntidyverse library(tidyverse). \nalways tell use read_csv() read\ndata csv file. codes load data - one\nsimilar one read.csv(). work differently. \never use read_csv() Psychology labs unless\notherwise instructed.\n\nremember <- essentially means\nassign . Assigning ponzo data table\nponzo_data can actually can written way around\n- read_csv(\"PonzoAgeData.csv\") -> ponzo_data - \nconvention usually puts way code.\n","code":"\nlibrary(\"tidyverse\")\nponzo_data <- read_csv(\"PonzoAgeData.csv\")"},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT4","chapter":"1 Starting with R Markdown","heading":"1.3.6 Task 4: Writing your Report","text":"start giving brief report information structure full report.Underneath code chunk entered, put new heading called Introduction give Header 2 size.Next, little research Ponzo Illusion write sentence two describing works tells us; include citation support research. link wikipedia page illusion top section might help.Finally, copy text box report finish text putting names two hypotheses behind illusion sentence ordered list style; .e. 1... 2..., etc. two hypotheses Framing hypothesis Perspective hypothesis.\nLists can tricky begin straightforward \nknow key points.\n\nlist begins blank line text. start \nlist without leaving blank line top won’t work.\n\npoint starts asterisk (*) integer \nfull-stop (e.g. 1.)\n\nmust space * 1. writing \npoint.\n\npoint new line.\n\nstagger points list (.e. indent), leave 4 blank spaces (two\ntabs) put * etc.\nQuickfire QuestionHere couple questions try group remind using citations:writing report, cite:Papers five authors first mention? Author 1, Author 2, Author 3, Author 4, & Author 5Author 1, Author 2, Author 3, Author 4, & Author 5, YearAuthor 1 et al., YearPapers five authors second mention? Author 1, Author 2, Author 3, Author 4, & Author 5Author 1, Author 2, Author 3, Author 4, & Author 5, YearAuthor 1 et al., YearPapers seven authors first mention? Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, & Author 7Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, & Author 7, YearAuthor 1 et al., YearPapers two authors citation? (Author 1 & Author 2)(Author 1 et al., Year)Two papers one paretheses? Order chronologically according year, separated semi-colonOrder alphabetically according first author surname, separated semi-colonTwo papers author? Order chronologically according year, separated semi-colonOrder chronologically according year, separated commaOrder alphabetically adding letter year","code":"\"There are two underlying hypotheses that may explain the Ponzo Illusion. These are: ...\""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT5","chapter":"1 Starting with R Markdown","heading":"1.3.7 Task 5: Making Text Bold or Italicized","text":"Sometimes want add emphasis text.report, format line two underlying hypotheses... bold. Answering question might help remember .Quickfire QuestionBold text italicized text created similarly, create italicized text? * (text)** (text)* (text)** (text)good idea knit file point make sure codes working correctly.","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT6","chapter":"1 Starting with R Markdown","heading":"1.3.8 Task 6: Adding Links to the Data in your Methods","text":"Good practice Report include information got data .Create new heading list two hypotheses call Methods. Set Header 2 size.Methods write new heading called Data set Header 3 size.Underneath Methods heading, copy paste sentence turn citation internet link paper.\"data report obtained within original paper (Lebowitz & Judisch, 1967). \"Now knit document make sure formatting working. Titles bigger normal text list indented numbers start line.\ncan get web address following link paper shown\ntowards beginning lab activity. Include https part.\n\nUse R Markdown cheatsheet see insert links. \nsomething square brackets [] circular brackets () next \n.\n","code":""},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT7","chapter":"1 Starting with R Markdown","heading":"1.3.9 Task 7: Adding an Image to your Methods","text":"certain studies, may want add image Methods section, either stimuli, materials, procedure. look R Markdown cheatsheet see adding image similar adding link, difference exclamation mark, !, beforehand. Surprising, know!now just add image illusion taken internet illustrate add images documents.sentence added Task 6, add new heading called Stimuli set Header 3 size.Stimuli heading, insert image following web address:\nRemember good methods section contain necessary\ninformation required another researcher replicate\nexperiment exactly! normally split three sections\nincluding Participants, Materials, Procedure.\n\nmay sound obvious surprised many\nMethods sections don’t give enough information replicating \nstudy. Articles tend word counts - just like assignments.\nAuthors tended cut words can fit \ndiscussion results. Methods sections suffered result.\n!\n","code":"https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg"},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT8","chapter":"1 Starting with R Markdown","heading":"1.3.10 Task 8: Adding a Table to your Results","text":"Another benefit R Markdown can insert tables results directly report without format - though aesthetics want learn format tables eventually. now...Create new heading methods sentence, called Results format Header 2 size.Add new code chunk give name table, include code shown . first part code my_table <- group_by %>% summarise creates table stores my_table. second part code my_table calls table. Calls means display show sense.Add echo rule code included final document ouput table included.Now, knit document see produced. see code, just output table.","code":"\nmy_table <- group_by(ponzo_data, Sex) %>% \n  summarise(NofGroups=n(), mean_length = mean(ComparisonLength))\n\nmy_table"},{"path":"starting-with-r-markdown.html","id":"Ch1InClassQueT9","chapter":"1 Starting with R Markdown","heading":"1.3.11 Task 9: Adding a Figure to your Results","text":"Nearly research reports figure want add one well.Underneath table code chunk, add new code chunk give name plot.Add code chunk set include rule code plot included final report.\nmay notice assigned data table \nmy_table called my_table show .\nHowever, didn’t figure. just put code \nfigure assign . ?\n\ngreat answer assign assign\neither, chop change throughout labs show \ndifference tendency assign tables assign figures.\nSimply often creating figures show \ntherefore assigning calling requires code.\nTables hand often stored work later, makes\nsense assign .\n\nhard fast rule often assign\nfigures just makes quicker . ever assign \nfigure remember call , figure won’t displayed!\n, knit document make sure working correctly. table now ggplot code followed nice scatterplot. Thinking Cap Point think figure answer following question.\"Based distribution data, shown Figure, ...\" age increases, people perceive shorter vertical line length standard vertical lineas age increases, people perceive longer vertical line length standard vertical lingeThere relationship Age Ponzo illusionThis figure tells nothing relationship Age Ponzo illusion\ndot represent Figure, pattern\ndots?\nlearn improve visualisations progress, now completed bones first report! Compare report one created see match, can found end chapter click download .Rmd file zip folder. Fix anything formatted template.\nreal-world scenario plotting R Markdown can save\nlot effort. Say carried experiment, made figure \nresults using R Script, wrote report using Microsoft Word.\nrealised forgot include two participants. fix ,\nre-run R script, make new plot, save plot,\ntransfer Word document. However,\nused R Markdown begin analysis\nreport place, can simply update code\nwithin document new figure created exact \nplace old one. Magic!\n\ncode uses ggplot2 package used .\nmain package use plots, figures, visualisations, \nhowever like call . can called library \n, automatically called call \ntidyverse package. Later, revist\nggplot2 detail. now, using make \nscatterplot (geom_point) Age (Mean_Age) \nComparison Length (ComparisonLength), splitting \ndata males females.\nJob Done - Activity Complete!Great work! now created rough layout report. section missing Discussion relate information previous research study showed. Feel free add one time; read short summary end actual paper help get thoughts together. Well done successfully creating R Markdown file!practice newly acquired skills really strengthen , complete exercise .","code":"\nggplot(ponzo_data, \n       aes(x = Mean_Age, y = ComparisonLength, color = Sex)) +\n  geom_point()"},{"path":"starting-with-r-markdown.html","id":"practice-your-skills","chapter":"1 Starting with R Markdown","heading":"1.4 Practice Your Skills","text":"brief exercise practice skills taught chapter. future assignments ask coding interpretation, exercise just want familiarise working .Rmd files.set task can practice 1) downloading assignment files, 2) renaming files, 3) editing .Rmd file, 4) saving edited .Rmd file.Download filesYou first need download file zip folder Moodle open R RStudio. exercise, can also download ZIP file .Simply follow instructions .Rmd document find ZIP file. Enjoy!","code":""},{"path":"starting-with-r-markdown.html","id":"solutions-to-questions","chapter":"1 Starting with R Markdown","heading":"1.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"starting-with-r-markdown.html","id":"task-2-give-your-report-a-heading","chapter":"1 Starting with R Markdown","heading":"1.5.1 Task 2: Give your Report a Heading","text":"used one hashtag give biggest heading size.# Lab 1 - magnitude Ponzo Illusion varies function AgeReturn Task","code":""},{"path":"starting-with-r-markdown.html","id":"task-3-creating-a-code-chunk","chapter":"1 Starting with R Markdown","heading":"1.5.2 Task 3: Creating a Code Chunk","text":"echo rule, warning rule message rule set FALSE. , start code chunk look like:Return Task","code":"```{r load_data, echo = FALSE, warning = FALSE, message = FALSE}```"},{"path":"starting-with-r-markdown.html","id":"task-4-writing-your-report","chapter":"1 Starting with R Markdown","heading":"1.5.3 Task 4: Writing your Report","text":"Task 4 setting title Header 2 style. done via two ## start line - word Introduction case forget space.## IntroductionWorth noting: basic R Scripts, # start line result turning line comment. , R Markdown, # sets header size much like Word document headerFor second part, create ordered list putting 1 followed . space first piece information. 2 . second, . Note lists work empty line list well:Return Task","code":"1. The Perspective Hypothesis\n2. The Framing Hypothesis"},{"path":"starting-with-r-markdown.html","id":"task-5-making-text-bold-or-italicized","chapter":"1 Starting with R Markdown","heading":"1.5.4 Task 5: Making Text Bold or Italicized","text":"turn text bold need put two ** start end word sentence want bold, e.g.Return Task","code":"**make me bold**"},{"path":"starting-with-r-markdown.html","id":"task-6-adding-links-to-the-data-in-your-methods","chapter":"1 Starting with R Markdown","heading":"1.5.5 Task 6: Adding Links to the Data in your Methods","text":"set header Header 2 style use ## start line.set header Header 3 style use ### start line.link created putting words want act link [] link immediately (). example:","code":"[Lebowitz and Judisch (2016)](https://www.jstor.org/stable/1420548?seq=1#page_scan_tab_contents)"},{"path":"starting-with-r-markdown.html","id":"task-7-adding-an-image-to-your-methods","chapter":"1 Starting with R Markdown","heading":"1.5.6 Task 7: Adding an Image to your Methods","text":"set header Header 3 style use ### start line.image created putting words want act name image [] link image immediately (). key thing start exclamation mark !. example:thereforeReturn Task","code":"![name](link)![The Ponzo Illusion](https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg)"},{"path":"starting-with-r-markdown.html","id":"task-8-adding-a-table-to-your-results","chapter":"1 Starting with R Markdown","heading":"1.5.7 Task 8: Adding a Table to your Results","text":"set header Header 2 style use ## start line.set header Header 2 style use ## start line.code chunk heading read follows:code chunk heading read follows:Return Task","code":"```{r table, echo = FALSE}```"},{"path":"starting-with-r-markdown.html","id":"task-9-adding-a-figure-to-your-results","chapter":"1 Starting with R Markdown","heading":"1.5.8 Task 9: Adding a Figure to your Results","text":"code chunk heading read follows:Return Task","code":"```{r plot, include = TRUE}```"},{"path":"starting-with-r-markdown.html","id":"example-of-output-after-completing-all-activities","chapter":"1 Starting with R Markdown","heading":"1.5.9 Example of output after completing all activities","text":"section shows output expected follow inclass activities correctly.Note: Headings comparison appear one size smaller knit Rmd due rendering. worry look bit bigger, headers key part. output match output knitting .Rmd document found .","code":""},{"path":"starting-with-r-markdown.html","id":"the-magnitude-of-the-ponzo-illusion-varies-as-a-function-of-age","chapter":"1 Starting with R Markdown","heading":"The Magnitude of the Ponzo Illusion Varies as a Function of Age","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"introduction","chapter":"1 Starting with R Markdown","heading":"Introduction","text":"Ponzo Illusion ...two underlying hypotheses may explain Ponzo Illusion. : Framing hypothesisThe Perspective hypothesis","code":""},{"path":"starting-with-r-markdown.html","id":"methods","chapter":"1 Starting with R Markdown","heading":"Methods","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"data","chapter":"1 Starting with R Markdown","heading":"Data","text":"data report obtained within original paper, Lebowitz Judisch (2016)","code":""},{"path":"starting-with-r-markdown.html","id":"stimuli","chapter":"1 Starting with R Markdown","heading":"Stimuli","text":"","code":""},{"path":"starting-with-r-markdown.html","id":"results","chapter":"1 Starting with R Markdown","heading":"Results","text":"\nFigure 1.11: caption. cover later!\nChapter Complete!","code":"\nggplot(ponzo_data, \n       aes(x = Mean_Age, y = ComparisonLength, color = Sex)) +\n  geom_point()"},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-a-key-skill","chapter":"2 Data-Wrangling: A Key Skill","heading":"2 Data-Wrangling: A Key Skill","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"overview-2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.1 Overview","text":"One key skills researcher's toolbox ability work data. run experiment get lots data various files. instance, uncommon experimental software create new file every participant run participant's file contain numerous columns rows data, important. able wrangle data, manipulate different layouts, extract parts need, summarise , one important skills.next chapters aimed refreshing consolidating skills working data. chapter focuses organizing data using tidyverse package. course activities, recap main functions use , use number real datasets give wide range exposure Psychology , reiterate skills apply across different datasets. skills change, just data!\nstyle programming teach, efficient\nformat/layout data known Tidy Data, \ndata format easily processed \ntidyverse package. can read type \ndata layout paper:\nTidy\nData (Wickham, 2014). surprisingly good read.\n\nHowever, data work always formatted \nefficient way possible. happens first step \nput Tidy Data format. two fundamental\nprinciples defining Tidy Data:\n\nvariable must column.\n\nobservation must row.\n\nTidy\nData (Wickham, 2014) adds following principle:\n\ntype observation unit forms table.\n\n\nGrolemund\nWickham (2017) restate third principle :\n\nvalue must cell (.e. grouping two variables\ntogether, e.g. time/date one cell).\n\ncell specific row column meet; single data\npoint tibble cell example. \nGrolemund\nWickham (2017) book useful read free, \nbrowsing chapter Tidy Data help visualise want\narrange data. Try keep principles mind whilst .\n\nquestions answer go along help build skills: use example code guide check answer solutions end chapter. Finally, remember pro-active learning, work together community, get stuck: google trying , use cheatsheets Data Skills R Book. key cheatsheet activity Data Transformation Cheatsheet dplyr.chapter recap :Data-Wrangling Wickham Six one-table verbsAdditional useful functions count, pivot_longer, joinsPiping making efficient codes\nRemember open portfolio created Chapter 1 \ncan add useful information work tasks. Also\nsummarising information give chapter, words,\ngreat way learn! don’t read \nmight help time time explain parts .\n\ninstance, remember get help R function \nRStudio? Console window, can call help function\n(e.g. ?mutate) view reference page \nfunction. example shows get help \nmutate() function within dplyr, \nuse later labs.\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-basics","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2 Data Wrangling Basics","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"revisiting-the-wickham-six","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.1 Revisiting the Wickham Six","text":"main way teach data-wrangling skills using Wickham Six one-table verbs. part tidyverse package introduced first PsyTeachR book, specifically dplyr package contained within tidyverse. six verbs often referred Wickham Six \"one-table\" dplyr verbs perform actions single table data.look basics , try look back exercises Data Skills book see used verbs (functions) previously.Wickham Six :\nuse Wickham Six frequently wrangling data\ndefinitely something making notes -\njust names, work particular nuances \nspot. Perhaps recreate table add \nexamples.\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"learning-to-wrangle","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.2 Learning to Wrangle","text":"Today going using data paper: Witt et al. (2017). chastity belt perception. main research question asks: ability perform action influence perception? instance, ability hit tennis ball influence fast perceive ball moving? phrase another way, expert tennis players perceive tennis ball moving slower novice tennis players?experiment use tennis players however, used Pong task: \"computerised game participants aim block moving balls various sizes paddles\". bit like classic retro arcade game. Participants tend estimate balls moving faster block smaller paddle opposed bigger paddle. can read paper get details wish hopefully gives enough idea help understand wrangling data. cleaned data little start . begin!Download data zip file link save somewhere access. lab, use M: drive.Download data zip file link save somewhere access. lab, use M: drive.Set working directory folder data. Session >> Set Working Directory >> Choose DirectorySet working directory folder data. Session >> Set Working Directory >> Choose DirectoryOpen new script copy paste two lines load tidyverse library session load data read_csv() function storing tibble called pong_data.Open new script copy paste two lines load tidyverse library session load data read_csv() function storing tibble called pong_data.\ninstall packages Boyd Orr labs; already \njust need called library().\n\nHowever, using computer haven’t\npreviously installed tidyverse package , \ninstall first\n(install.packages(\"tidyverse\")).\n\nalready installed tidyverse, \nlong time ago, might worth running updates packages \nmay old version works differently. easiest way \nRStudio using menu top -\nTools >> Check Package Updates. can update\npackages individually just run updates. Tends better \njust update packages many packages linked.\n\nthree common mistakes see :\n\nMake sure spelt data file name exactly\nshown. Spaces everything. change name \n.csv file, fix code instead. reason \ndifferent name file someone else code \nreproducible. say avoid using spaces filenames create,\none created another researcher already ,\nleave work .\n\nRemember uploading data use read_csv \nunderscore, whereas data file dot name,\nfilename.csv.\n\nCheck datafile actually folder set \nworking directory.\nlook pong_data see organized. Type View(pong_data) glimpse(pong_data) Console window (Capital V little g).dataset, see row (observation) represents one trial per participant 288 trials 16 participants. columns (variables) dataset follows:use data master skills Wickham Six verbs, taking verb turn looking briefly. develop skills setting new challenges based ones set. 6 verbs work briefly recap two functions finishing quick look pipes. Try everything let us know anything quite get.\nData research methods stored two-dimensional tables; called\ndata-frames, tables, tibbles. ways storing data\ndiscover time, mainly using tibbles (\nlike info, type vignette(\"tibble\") Console\nwindow). tibble table data columns rows \ninformation, within tibble can get different\nr glossary(\"data type\", display = \"types data\"),\n.e. r glossary(\"double\"),\nr glossary(\"integer\"), \nr glossary(\"character\").\n\nNote: Double Integer can referred Numeric data,\nsee word time time. clarity, use\nDouble term number decimal (e.g., 3.14) Integer\nterm whole number (e.g., 3).\n","code":"\nlibrary(\"tidyverse\")\npong_data <- read_csv(\"PongBlueRedBack 1-16 Codebook.csv\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.3 select() Function - keep only specific columns","text":"select() function lets us pick variables within dataset want work . example, say pong_data wanted keep columns Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss, need BackgroundColor BlockNumber.can two ways:Tell function variables includeTell function variables exclude -ColumnName approach (.e., minus ColumnName)second example, -BackgroundColor means 'BackgroundColor', saying columns except BackgroundColor BlockNumber. minus sign crucial part!\nTask 1: Using select() functionEither inclusion exclusion, select columns Participant, PaddleLength, TrialNumber, BackgroundColor HitOrMiss pong_data.know select() can also used reorder columns?Use select() keep columns Participant, JudgedSpeed, BallSpeed, TrialNumber, HitOrMiss, display alphabetical order, left right.\n\nremembered include dataset\npong_data? Pay attention upper/lower case letters \nspelling!\n\n\nremembered include dataset\npong_data? Pay attention upper/lower case letters \nspelling!\n\n\nThink first entered column names \nappeared. happens change order enter \ncolumn names?\n\n\nThink first entered column names \nappeared. happens change order enter \ncolumn names?\n","code":"\nselect(pong_data, Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss)\nselect(pong_data, -BackgroundColor, -BlockNumber)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.4 arrange() Function - sort and arrange columns","text":"arrange() function sorts rows tibble according column tell sort .Arrange data one column, e.g., BallSpeed:Arrange data multiple columns, e.g., BallSpeed (fastest first) BackgroundColor:\ndesc() sort largest smallest, .e.,\ndescending order.\n\nCompare output two lines \nBallSpeed column.\n\ndesc() also work \nBackgroundColor?\nTask 2: Arranging Data arrange() functionArrange data pong_data two variables: HitOrMiss (putting hits - 1 - first), JudgedSpeed (fast judgement - 1 - first).\n","code":"\narrange(pong_data, BallSpeed)\narrange(pong_data, desc(BallSpeed), BackgroundColor)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.5 filter() Function - keep only parts of the data","text":"filter() function lets us parse subset data, meaning keep parts data.example, might want keep red BackgroundColoror keep BallSpeed 4 pixelsor keep trials match red BackgroundColor BallSpeed 4 pixels. trial red background color slower 5 pixels removed.last example can also written follows. Two arguments requirements separated comma equivalent & (ampersand - meaning \"\"):say want keep specific Participant IDs. Say want just data Participants 1, 3, 10, 14 16. write follows.%% called group membership means keep ParticipantsThe c() creates little container items called vector.\n== operator compares first value one vector first\nvalue another vector, whereas %% operator compares first\nvalue first vector values second vector.\n\nexample, let’s create following vectors:\n\nx <- c(4, 5, 6) y <- c(6, 5, 4)\n\nhappens use %% operator compare \nvalues vector x y:\n\nx %% y [1] TRUE TRUE TRUE\n\nHooray! values vector x found exist vector y -\nshown word ‘TRUE’ correspondng number vector x.\nhappens use == operator instead?\n\nx == y [1] FALSE TRUE FALSE\n\nOh ! first last values vector x found exist\nvector y == operator directly compared first number\nvector x (4) first number vector y (6) . \nmiddle number (5) found exist vectors number\npositioned location vector x y.\n\n, want keep rows value equals ones\nspecify, use %% operator!\nfinally, say wanted keep Participants except Participant 7:can read != (exclamation mark followed equals) 'equal'. Participant != \"7\" means keep Participants values Participant column 7.exclamation mark can sometimes used negate function follows .Task 3: Using filter() FunctionUse filter() extract Participants fast speed judgement, speeds 2, 4, 5, 7, missed ball. Store remaining data variable called pong_fast_miss\nthree parts filter best think \nindividually combine .\n\n\nFilter fast speed judgements\n(JudgedSpeed)\n\n\nFilter fast speed judgements\n(JudgedSpeed)\n\n\nFilter speeds 2, 4, 5 7\n(BallSpeed)\n\n\nFilter speeds 2, 4, 5 7\n(BallSpeed)\n\n\nFilter Misses (HitOrMiss)\n\n\nFilter Misses (HitOrMiss)\n\nthree filters one uses output \npreceeding one, remember filter functions can take \none argument - see example . Also, \nJudgedSpeed HitOrMiss Integer \nneed == instead just =.\nCommon mistakes filter()filter function useful, used wrongly can give misleading findings. important always check data perform action. say working comparative psychology run study looking cats, dogs, horses perceive emotion. say data stored tibble animal_data column called animals tells type animal participant . Something like :Ok, imagine wanted data just cats:filter(animal_data, animals == \"cat\")Exactly! wanted cats dogs?filter(animal_data, animals == \"cat\", animals == \"dog\")Right? Wrong! actually says \"give everything cat dog\". nothing cat dog, weird - like dat cog! actually want everything either cat dog, stated :filter(animal_data, animals == \"cat\" | animals == \"dog\")vertical line | symbol , just & symbol ., always pay attention want importantly code produces.","code":"\nfilter(pong_data, BackgroundColor == \"red\")\nfilter(pong_data, BallSpeed > 4)\nfilter(pong_data, BackgroundColor == \"red\", BallSpeed > 4)\nfilter(pong_data, BackgroundColor == \"red\" & BallSpeed > 4)\nfilter(pong_data, Participant %in% c(\"1\", \"3\", \"10\", \"14\", \"16\")) \nfilter(pong_data, Participant != \"7\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.6 mutate() Function - add new columns","text":"mutate() function lets us create new variable dataset. example, add new column pong_data background color represented numbers, red represented 1, blue represented 2.look code detail:BackgroundColorNumeric name new column adding tibble.BackgroundColor name original column tibble one take information .1 2 new codings red blue, respectively.mutate() function also handy making calculations across columns data. example, say realise made mistake experiment participant numbers 1 higher every participant, .e. Participant 1 actually numbered Participant 2, etc. something like:Note: \"new column\" name old column, .e. Participant. resulting table, Participant column new values differ values original pong_data table. may seem like overwritten values, reality created copy table altered values, lost anything: original values still pong_data store (assign) action pong_data. save change basically.general, good practice overwrite pong_data new version pong_data, store altered table new tibble, e.g., pong_data_mutated, like :Task 4: Mutating variables mutate()realise another mistake trial numbers wrong. first trial (trial number 1) practice excluded experiment actually started trial 2. Tidy :Creating new tibble called pong_data_filt store data pong_data filtering trials number 1 (TrialNumber column).Now use mutate() function renumber remaining trial numbers, pong_data_filt, starting 1 instead 2. Store output new tibble called pong_data2.\nStep 1:\n\nfilter(TrialNumber equal 1).\n\nremember store output tibble called\npong_data_filt\n\nStep 2:\n\nmutate(TrialNumber = TrialNumber minus 1)\n\nexclamation mark, equals\n","code":"\npong_data <- mutate(pong_data, \n                    BackgroundColorNumeric = recode(BackgroundColor, \n                                                    \"red\" = 1, \n                                                    \"blue\" = 2))\nmutate(pong_data, Participant = Participant + 1)\npong_data_mutated <- mutate(pong_data, Participant = Participant + 1)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.7 group_by() Function - group parts of data together","text":"group_by() function groups rows dataset according category specify, e.g., animals example , grouping cat data together, dog data together, horse data together.Looking data within pong_data2, say wanted eventually create means, etc., different background color conditions, start grouping trials BackgroundColor, grouping data red background data blue background data:can add numerous grouping variables depending want split data. group Hit Miss (HitOrMiss column) background color (Red Blue). gives four groups (.e, Hit Red, Miss Red, Hit Blue, Miss Blue):Note: Nothing actually appears change data, unlike functions, big operation taken place. Look output console run group_by(pong_data2, BackgroundColor). top output notice 2nd line output tells us grouping criteria many groups now exist: see line Groups: BackgroundColor [2]: grouped BackgroundColor [2] groups - one red one blue.Task 5: Grouping Data group_by()Group data BlockNumber BackgroundColor, order, enter number groups (.e., number) get result : \nprocedure different column names:\n\ngroup_by(pong_data2, HitOrMiss, BackgroundColor)\n\nnumber groups product (.e., multiplication)\nsum number background colors (red blue) \nnumber blocks (12).\ngroup_by() incredibly useful , data organised groups, can apply functions (filter, arrange, mutate,...) groups within data interested , instead entire dataset. instance, common second step group_by might summarise data.Good know: ungroup() functionThe ungroup() function undoes action group_by() function.grouping data together using group_by() function performing task , e.g., filter(), summarise(), can good practice ungroup data performing another function. Forgetting ungroup dataset always affect processing, sometimes can really mess things.just good reminder always check data getting function ) makes sense b) expect.","code":"\ngroup_by(pong_data2, BackgroundColor)\ngroup_by(pong_data2, HitOrMiss, BackgroundColor)"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2PreClassQueT6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.8 summarise() Function - do some calculations on the data","text":"summarise() function lets calculate descriptive statistics data. example, say want count number hits different paddle lengths number hits background color red blue.First group data accordingly, storing pong_data2_groupThen summarise , storing answer total_hitsFinally, fun can filter just red, small paddle hits summarised data.leave us :\nTable 2.1: Summarising group_by() summarise()\nTip: name column within pong_data2_hits_red_small summarised data total_hits; called creating pong_data2_hits. called anything wanted, always try use something sensible. Make sure call variables something (anyone looking code) understand recognize later (.e., variable1, variable2, variable3. etc.), avoid spaces (use_underscores_never_spaces).summarise() range internal functions make life really easy, e.g., mean(), median(), n(), sum(), max, min, etc. common ones shown , see dplyr cheatsheets examples.Task 6: Summarising Data summarise()Use lines code calculate mean number hits made small paddle (50) red color background. Enter value box two decimal places (e.g., 0.12): Quickfire QuestionsWhich Wickham Six use sort columns smallest largest: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use sort columns smallest largest: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use calculate mean column: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use calculate mean column: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use remove certain observations - e.g. remove males: selectfiltermutatearrangegroup_bysummariseWhich Wickham Six use remove certain observations - e.g. remove males: selectfiltermutatearrangegroup_bysummarise","code":"\npong_data2_group <- group_by(pong_data2, BackgroundColor, PaddleLength)\npong_data2_hits <- summarise(pong_data2_group, total_hits = sum(HitOrMiss))\npong_data2_hits_red_small <- filter(pong_data2_hits, BackgroundColor == \"red\", PaddleLength == 50)## `summarise()` has grouped output by 'BackgroundColor'. You can override using\n## the `.groups` argument.\n## `summarise()` has grouped output by 'BackgroundColor'. You can override using\n## the `.groups` argument."},{"path":"data-wrangling-a-key-skill.html","id":"other-useful-functions-bind_rows-and-count","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.9 Other Useful Functions: bind_rows() and count()","text":"Wickham Six verbs let lot things data however thousands functions disposal. want something data sure using functions, Google search alternative function - chances someone else problem help guide.Two useful functions bind_rows() function count() function. briefly show .Binding columns bind_rows()bind_rows() function useful want combine two tibbles together one larger tibble column structure, .e. exactly columns want combine attaching one bottom :Say tibble data ball speeds 1 2:another tibble data ball speeds 6 7:Now combine tibbles together one big tibble containing extreme ball speeds:Count count() functionThe count() function shortcut can sometimes used count number rows groups data, without use group_by() summarise() functions. tally basically. sum values. just counts many observations .example, Task 6 combined group_by() summarise() calculate many hits based background color paddle length. Alternatively, done:results , just count() version get information, including misses, just counting rows. summarise() method got hits effect summed. two different methods give similar answers.","code":"\nslow_ball<- filter(pong_data2, BallSpeed < 3) \nfast_ball <- filter(pong_data2, BallSpeed >= 6) \nextreme_balls <- bind_rows(slow_ball, fast_ball) \ncount(pong_data2, BackgroundColor, PaddleLength, HitOrMiss)"},{"path":"data-wrangling-a-key-skill.html","id":"pipes---make-your-code-efficient","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.2.10 Pipes (%>%) - make your code efficient","text":"now noticed thattidyverse functions generally take following grammatical structure (called syntax): function_name(dataset, arg1, arg2,..., argN) dataset entire tibble data using argument (arg) operation particular column variable, column name want work . example:examples follow structure function_name(dataset, arg1, arg2, ....)first example, filtering (function) whole pong_data2 dataset particular paddle length, particular speeds (arguments). second, grouping BallSpeed Participant. Note order arguments specific performs argument1 argument2, etc. Changing order arguments may give different output. order work important, called pipeline. example, pipeline used find many hits small paddle length red background.First group data accordingly, storing pong_data2_groupThen summarise , storing answer total_hitsAnd finally filter just red, small paddle hitsPipelines allow us quickly reproducibly perform action take much longer manually. However, can make code even efficient, using less code, stringing sequence functions together using 'pipes', written %>%. Changing code one using pipes give us:chunks show exactly procedure, adding pipes can make code easier read understand piping.Compare code without pipe:function_name(dataset, arg1, arg2,...,argN)o code pipe:dataset %>% function_name(arg1, arg2,...,argN)premise can pipe (%>%) functions input function output previous function. Alternatively, can use pipe put data first function, shown directly .can think pipe (%>%) saying '' 'goes ', e.g., data goes function function function.One last point pipes can written single line code much easier see pipe function takes line. Every time add function pipeline, remember add %>% first note using separate lines function, %>% must appear end line start next line. Compare two examples . first work, second second puts pipes end line need !Example 1: work pipes (%>%) wrong place.Example 2: work pipes (%>%) correct place.\npiping becomes useful string series \nfunctions together, rather using separate steps \nsave data time new tibble name getting \nconfused. non-piped version create new tibble\ntime, example, data, data_filtered,\ndata_arranged, data_grouped,\ndata_summarised just get final one actually\nwant, data_summarised. creates lot \ntibbles environment can make everything unclear \neventually slow computer. piped version however uses one\ntibble name, saving space environment, clear easy \nread. pipes, skip unnecessary steps avoid cluttering \nenvironment.\nQuickfire QuestionsWhat line code say? data %>% filter() %>% group_by() %>% summarise(): take data group filter summarise ittake data filter group summarise ittake data summarise filter group ittake data group summarise filter ","code":"\nfilter(pong_data2, PaddleLength == \"50\", BallSpeed > 4)\ngroup_by(pong_data2, BallSpeed, Participant)\npong_data2_group <- group_by(pong_data, BackgroundColor, PaddleLength)\npong_data2_hits <- summarise(pong_data2_group, total_hits = sum(HitOrMiss))\npong_data2_hits_red_small <- filter(pong_data2_hits, BackgroundColor == \"red\", PaddleLength == 50)\npong_data_hits_red_small <- pong_data2 %>% \n  group_by(BackgroundColor, PaddleLength) %>% \n  summarise(total_hits = sum(HitOrMiss)) %>%\n  filter(BackgroundColor == \"red\", PaddleLength == 50)data_arrange <- pong_data2 \n                %>% filter(PaddleLength == \"50\")\n                %>% arrange(BallSpeed) \ndata_arrange <- pong_data2 %>%\n                filter(PaddleLength == \"50\") %>%\n                arrange(BallSpeed) "},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-application","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3 Data Wrangling Application","text":"looked series functions known Wickham six one-table filter, arrange, group_by, select, mutate summarise. Now focus working data across two tables using functions come across Data Skills book. two main functions add Wickham six pivot_longer() inner_join().pivot_longer() allows us transform table wide format long format.\nuse Tidy Data really efficient \nworks well tidyverse. However, people used use\ndata structured long format wide\nformat.\n\nLong format row single\nobservation, typically single trial experiment response \nsingle item questionnaire. multiple trials per\nparticipant, multiple rows participant. \nidentify participants, need variable kind \nparticipant id, can simple distinct integer value \nparticipant. addition participant identifier, \nmeasurements taken observation (e.g., response\ntime) experimental condition observation taken\n.\n\nwide format data, row corresponds \nsingle participant, multiple observations participant\nspread across columns. instance, survey data, \nseparate column survey question.\n\nTidy mix approaches \nfunctions tidyverse assume tidy format, typically \nfirst thing need get data, particularly wide-format\ndata, reshape wrangling. teach \nreally important skills.\ninner_join() allows us combine two tables together based common columns.Analysing Autism Spectrum Quotient (AQ)continue building data wrangling skills recap skills Data Skills book tidying data Autism Spectrum Quotient (AQ) questionnaire. completed Data Skills book may familiar AQ10; non-diagnostic short form AQ 10 questions per participant. discrete scale higher participant scores AQ10 autistic-like traits said display. Anyone scoring 7 recommended diagnosis. can see example AQ10 link: AQ10 Example.four data files work :responses.csv containing AQ survey responses 10 questions 66 participantsqformats.csv containing information question coded, .e. forward reverse codedscoring.csv containing information many points specific response get; depending whether forward reverse codedpinfo.csv containing participant information Age, Sex importantly ID number.Click download files zip file. Now unzip files folder access .\ncsv stands ‘comma separated values’ \nbasic format storing data plain text file. really just\nstores numbers text separated commas nothing else. great\nthing basic can read many different\nsystems non-proprietary, .e., don’t need purchase\ncommercial software open .\nNow set working directory folder saved .csv files. dropdown menus top toolbar: Session >> Set Working Directory >> Choose Directory find folder .csv files.Today work RScript instead .Rmd, want turn R Markdown report add elements Portfolio please feel free.Thinking Cap PointNow good time make sure using RStudio effectively know window .TRUE FALSE, Console best practice Script window saving: TRUEFALSETRUE FALSE, Environment holds data objects loaded created: TRUEFALSETRUE FALSE, clicking name table Environment window open Script window: TRUEFALSE\nanswer True.\n\n\nScript window write code comments\ngoing save send people. Console \npractice stuff - nothing saved ; like sandbox \njust gets wiped away.\n\n\nScript window write code comments\ngoing save send people. Console \npractice stuff - nothing saved ; like sandbox \njust gets wiped away.\n\n\ndata load create held Environment (\nGlobal Environment) window variable name gave\n.\n\n\ndata load create held Environment (\nGlobal Environment) window variable name gave\n.\n\n\nclicking name table Environment window \nopen Script window can look make sure \nexpect. works tables types\ndata. learn difference go along!\n\n\nclicking name table Environment window \nopen Script window can look make sure \nexpect. works tables types\ndata. learn difference go along!\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-1-open-a-script","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.1 Task 1: Open a Script","text":"Start new RScript save folder .csv files, calling RScript something informative like AQ_DataWrangling.R.Make sure environment completely empty mix one analysis . can run following code line console clear environment clicking little brush environment window.\nRemember using script can write notes \nremind line code . Just put hashtag start \nline R ignore line. clear\nusing Script versus R Markdown file. Script, # means \nline ignored, Markdown # sets line header!.\n\nrun line script, simplest way click anywhere \nline either press Run top script window press\nCTRL+Enter keyboard (mac equivalent).\n","code":"\nrm(list = ls()) "},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.2 Task 2: Bring in Your Library","text":"Add line code brings tidyverse package working environment run .\nCombine function library() package\ntidyverse remember solutions end \nchapter.\n\nlab machines Psychology necessary packages \nalready machines, just need called library. \nhowever using machine install \npackages first ().\n\ninstall packages Psychology machines! ?\n\nalready installed can cause package stop working\nstudent tries install package machines.\n\nalready installed bit like using apps \nphone. Install putting app onto phone, library just\nopening app. ’ve already downloaded app (package) \njust need open (library()) use !\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.3 Task 3: Load in the Data","text":"Now load .csv datafiles using read_csv() function save tibbles environment. example, load data responses.csv save tibble responses type:Add following lines code script complete load four .csv datafiles. Use code example name tibble original filename (minus .csv part), , e.g. responses.csv gets saved responses. Remember run lines data loaded stored environment.\nwork data functions find functions\nsimilar names, give different results. One \nread function csv. Make sure \nalways use read_csv() function \nload csv files. Nothing else. part \nreadr package automatically brought \ntidyverse.\n\nsimilarly named function called\nread.csv(). use function. always expect\nuse read_csv(). Although similar name \nwork way create differences data.\n","code":"\nresponses <- read_csv(\"responses.csv\") responses <-  read_csv()    # survey responses\nqformats <-                 # question formats\nscoring <-                  # scoring info\npinfo <-                    # participant information"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.4 Task 4: Review Your Data.","text":"Now data loaded always best look data get idea layout. showed one way , clicking name environment, can also use glimpse() View() functions Console window. Put name data brackets see arranged. add script though - just one-offs testing.look data responses see think Tidy answer following question: data responses TidyLongWide format\nreponses tibble far tidy; row\nrepresents multiple observations participant, .e., \nrow shows responses multiple questions -\nwide format.\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.5 Task 5: Gathering Data with pivot_longer().","text":"order make easier us get AQ score participant, need change layout responses tibble wide format long format using pivot_longer() function.Copy code line script run .look code detail:first argument given pivot_longer() function tibble holds data want wrangle, responses.\nRemember written pipe well, e.g. rlong <- responses %>% pivot_longer(...)\nRemember written pipe well, e.g. rlong <- responses %>% pivot_longer(...)second argument names specific columns original tibble want gather together, Q1:Q10 meaning columns Q1 Q10.\nactually need write cols = makes things clearer.\n\"Gathering\" columns based position tibble. order columns tibble Q1 Q10, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q10, therefore code gathers columns Q1 Q10.\nColumn names put quotes exist already tibble responses.\nactually need write cols = makes things clearer.\"Gathering\" columns based position tibble. order columns tibble Q1 Q10, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q10, therefore code gathers columns Q1 Q10.Column names put quotes exist already tibble responses.third fourth arguments names new columns creating;\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".\nsecond store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".\nnew column names put quotes already exist tibble. always case, case function.\nNote names anything using names code makes sense.\nLastly, need write names_to = ... values_to = ... otherwise columns created correctly.\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".second store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".new column names put quotes already exist tibble. always case, case function.Note names anything using names code makes sense.Lastly, need write names_to = ... values_to = ... otherwise columns created correctly.case wondering, wanted go back way ungather data just gathered, use pivot_wider() function: e.g. rwide <- rlong %>% pivot_wider(names_from = Question, values_from = Response). want add code.Quickfire QuestionsLet's see understand pivot_longer(). Say wanted gather first three columns responses (Q1, Q2, Q3), put question numbers column called Jam, responses column called Strawberry, store everything tibble called sandwich. Fill box write: \nsandwich <- pivot_longer(responses, cols = Q1:Q3, names_to = \"Jam\", values_to = \"Strawberry\")\n\npivot_longer() wants data first, columns \ngather, name new column store gathered column\nnames , finally name new column store values\n.\n","code":"\nrlong <- pivot_longer(responses,\n                      cols = Q1:Q10,\n                      names_to = \"Question\",\n                      values_to = \"Response\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.6 Task 6: Combining Data.","text":"now responses data tidy format, closer getting AQ score person. However, still need add information :show question reverse forward scored - found qformatsshow number points give specific response - found scoring.typical analysis situation different information different tables need join together. pieces information contained qformats scoring, respectively, want join data rlong create one informative tidy table info.can sort join function inner_join(); function combine information two tibbles using column (columns) common tibbles.Copy line code run . piece code combines rows tibble rlong rows tibble qformats, based common column \"Question\".Now look rlong2. matched question scoring format, forward reverse.\nlot questionnaires questions Forward scored\nquestions Reverse scored. mean? Imagine\nsituation options replying question : 1 -\nextremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely\ndisagree. forward-scoring question get 1 point \nextremely agree, 2 agree, 3 neutral, etc. reverse scoring\nquestion, get 5 extremely agree, 4 agree, 3 \nneutral, etc.\n\nreasoning behind shift sometimes agreeing \ndisagreeing might favourable depending question \nworded. Secondly, sometimes questions used just catch\npeople - imagine two similar questions one \nreverse meaning . scenario, people respond\nopposites. respond might paying\nattention.\nNow need combine information table, rlong2, scoring table know many points attribute question based answer participant gave, whether question forward reverse coded. , use inner_join() function, time common columns found rlong2 scoring QFormat Response. combine two columns just write sequence shown . Note: one common column two tibbles joining, combine columns avoid repeat columns new tibble. forget , new tibble names column_name.x column_name.y. cause confusion avoid combining common columns.Copy line code run . code combine rows rlong2 scoring based columns, QFormat Response.","code":"\nrlong2 <- inner_join(rlong, qformats, \"Question\")\nrscores <- inner_join(rlong2, scoring, c(\"QFormat\", \"Response\"))"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT7","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.7 Task 7: Calculating the AQ Scores.","text":"now created rscores information participant responded question question coded scored, within one tibble. need now sum scores participant get AQ score.Based learning previous section, copy line code complete obtain individual aq_scores participant.Save script run start make sure works!\nparticipant grouped Id.\n\nsummed value Score might get full AQ\nScore particpipant.\n\nYep, well spotted. Pipes friend. Think saying\n‘’ ‘goes ’. example take\nrscores group something summarise\nAQ scores based …\n\ncases, pipe serves purpose putting input \nfunction taking output one function treating \ninput another function.\n\nexample first pipe takes rscores \ninput group_by, second pipe takes output \ngroup_by puts input \nsummarise. See can almost read chain \nactions steps.\nQuickfire QuestionsThe whole purpose chapter calculate AQ scores individual participants. Try answer following questions. Try using code possible help based knowledge chapter. Remember cheatsheets well. Look dplyr one!options, choose correct citation AQ 10 question questionnaire: Allison, Auyeung, Baron-Cohen, (2011)Allison, Auyeung, Baron-Cohen, (2012)Allison Baron-Cohen, (2012)Auyeung, Allison, Baron-Cohen, (2012)options, choose correct citation AQ 10 question questionnaire: Allison, Auyeung, Baron-Cohen, (2011)Allison, Auyeung, Baron-Cohen, (2012)Allison Baron-Cohen, (2012)Auyeung, Allison, Baron-Cohen, (2012)Complete sentence, higher AQ score...less autistic-like traits displayedhas relation autistic-like traitsthe autistic-like traits displayedComplete sentence, higher AQ score...less autistic-like traits displayedhas relation autistic-like traitsthe autistic-like traits displayedType AQ score (just number) Participant ID . 87: Type AQ score (just number) Participant ID . 87: Type many participants AQ score 3 (just number): Type many participants AQ score 3 (just number): cut-AQ10 usually said around 6 meaning anyone score 6 referred diagnostic assessment. Type many participants refer sample: cut-AQ10 usually said around 6 meaning anyone score 6 referred diagnostic assessment. Type many participants refer sample: \n\nlink can see appropriate citation \nAQ10 (Allison, Auyeung, Baron-Cohen, (2012))\n\n\nlink can see appropriate citation \nAQ10 (Allison, Auyeung, Baron-Cohen, (2012))\n\n\nmentioned, higher score AQ10 \nautistic-like traits participant said show.\n\n\nmentioned, higher score AQ10 \nautistic-like traits participant said show.\n\n\ncode \nfilter(aq_scores, Id == 87), give tibble\n1x2 showing ID number score. just wanted score \nuse pull() haven’t shown yet works\nfollows: filter(aq_scores, Id == 87) %>% pull(AQ).\nanswer AQ score 2.\n\n\ncode \nfilter(aq_scores, Id == 87), give tibble\n1x2 showing ID number score. just wanted score \nuse pull() haven’t shown yet works\nfollows: filter(aq_scores, Id == 87) %>% pull(AQ).\nanswer AQ score 2.\n\n\nchanging argument filter.\nfilter(aq_scores, AQ == 3) %>% count(). answer \n13. Remember can counting code makes \nreproducible every time.\n\n\nchanging argument filter.\nfilter(aq_scores, AQ == 3) %>% count(). answer \n13. Remember can counting code makes \nreproducible every time.\n\n\nfilter(aq_scores, AQ > 6) %>% count() \nfilter(aq_scores, AQ >= 7) %>% count(). answer \n6.\n\n\nfilter(aq_scores, AQ > 6) %>% count() \nfilter(aq_scores, AQ >= 7) %>% count(). answer \n6.\n","code":"\naq_scores <- rscores %>% \n             group_by() %>% # how will you group individual participants?\n             summarise(AQ = sum()) # which column will you sum to obtain AQ scores?"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2InClassQueT8","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.3.8 Task 8: Practice using pipes","text":"now complete code load data, convert Tidy, combine tables calculate AQ score participant. , look , code efficient using pipes.Go back code rewrite using pipes %>% efficient possible.\npoint first argument function name \nvariable created line, good chance \nused pipe! bits code piped\ntogether one chain:\n\nrlong <- pivot_longer(responses, cols = Q1:Q10, names_to = \"Question\", values_to = \"Response\")\n\nrlong2 <- inner_join(rlong, qformats, \"Question\")\n\nrscores <- inner_join(rlong2, scoring, c(\"QFormat\", \"Response\"))\n\naq_scores <- rscores %>% group_by(Id) %>% summarise(AQ = sum(Score))\n","code":""},{"path":"data-wrangling-a-key-skill.html","id":"practice-your-skills-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4 Practice Your Skills","text":"order complete tasks need download data .csv files .Rmd file, need edit, titled Ch2_PracticeSkills_Template.Rmd. can downloaded within zip file link. downloaded unzipped, create new folder use working directory; put data files .Rmd file folder set working directory folder drop-menus top. Download Exercises .zip file .Now open .Rmd file within RStudio. see code chunk task. Follow instructions edit code chunk. often entering code based covered point.chapter recapped data-wrangling using Wickham 6 verbs, looked additional functions pivot_longer() inner_join(), piping chains code efficiency using %>%. need skills complete following exercises, make sure worked chapter attempting exercise. Two useful online resources :Hadley Wickham's R Data Science book @ http://r4ds..co.nz RStudio's dplyr cheatsheet @ Rstudio.comPsyTeachR Data Skills book","code":""},{"path":"data-wrangling-a-key-skill.html","id":"the-ageing-brain","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.1 The Ageing Brain","text":"key topic current psychological research, one forms main focus research School, human ageing. research use brain imaging techniques understand changes brain function structure relate changes perception behaviour. typical 'ageing' experiment compare measure (number measures) performance cognitive perceptual task younger older adults (.e., -subjects design experiment).However, order make sure studying 'healthy' ageing, first 'screen' older participants symptoms age-related dementia (Alzheimer's Disease), cognitive function can significantly impaired. using range cognitive tests. studies also test participants' sensory acuity (ability perceive something), function age (particularly eyesight hearing).data downloaded exercise example screening data taken research investigating ageing brain processes different types sounds. tests used study detailed . Please note links provide information examples tests completed assignment wish; read complete exercises.Montreal Cognitive Assessment (MoCA) : test specifically devised stand-alone screening tool mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, abstraction skills. Example hereMontreal Cognitive Assessment (MoCA) : test specifically devised stand-alone screening tool mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, abstraction skills. Example hereWorking Memory Digit Span Test (D-SPAN): measures capacity participants' short-term (working) memory.Working Memory Digit Span Test (D-SPAN): measures capacity participants' short-term (working) memory.D2 Test Attention: measures participants' selective sustained concentration visual scanning speed.D2 Test Attention: measures participants' selective sustained concentration visual scanning speed.Better Hearing Institute Quick Hearing Check: self-report questionnaire measures participants' subjective experience hearing abilities.Better Hearing Institute Quick Hearing Check: self-report questionnaire measures participants' subjective experience hearing abilities.Data FilesYou just downloaded three .csv files containing data need. list .csv file names description variables contains:p_screen.csv contains particpants demographic information including:\nID Participant Id number - confidentiality (names identifying info)\nAGE years\nSEX M male, F female\nHANDEDNESS L left-handed, R right-handed\nEDUCATION years\nMUSICAL whether musical abilties/experience (YES )\nFLANG speak foreign languages (YES )\nMOCA Montreal Cognitive Assessment score\nD-SPAN Working Memory Digit Span test score\nD2 D2 Test Attention score\nID Participant Id number - confidentiality (names identifying info)AGE yearsSEX M male, F femaleHANDEDNESS L left-handed, R right-handedEDUCATION yearsMUSICAL whether musical abilties/experience (YES )FLANG speak foreign languages (YES )MOCA Montreal Cognitive Assessment scoreD-SPAN Working Memory Digit Span test scoreD2 D2 Test Attention scoreQHC_responses.csv contains participants' responses question \"Better Hearing Institute Quick Hearing Check (QHC)\" questionnaire.\nColumn 1 represents participants' ID (matching p_screen.csv).\ncolumn thereafter represents 15 questions questionnaire.\nrow represents participant response question.\nColumn 1 represents participants' ID (matching p_screen.csv).column thereafter represents 15 questions questionnaire.row represents participant response question.QHC_scoring.csv contains scoring key question QHC, columns:\nRESPONSE types responses participants give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE)\nSCORE points awarded response type (0 4). score participant can calculated converting categorical responses values summing values.\nRESPONSE types responses participants give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE)SCORE points awarded response type (0 4). score participant can calculated converting categorical responses values summing values.starting lets check:.csv files saved folder computer manually set folder working directory..csv files saved folder computer manually set folder working directory..Rmd file saved folder .csv files..Rmd file saved folder .csv files.","code":""},{"path":"data-wrangling-a-key-skill.html","id":"load-in-the-data","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.2 Load in the data","text":"see code chunk called libraries, similar one , top .Rmd assignment file. set-load data call tidyverse library(). Run code chunk now bring data tidyverse. can console, script, even code chunk clicking small green play symbol top right code chunk.View dataIt always good idea familiarise layout data just loaded . can using glimpse() View() Console window, must never put functions assignment file.Tasks:Now data loaded, tidyverse attached, viewed data, now try complete following 9 tasks. may want practice first get correct code format, make sure work. can console script, remember, correct code, edit necessary parts assignment .Rmd file produce reproducible .Rmd file. now assessment files practicing now really help. short, go tasks change NULL question asks make sure file knits end fully reproducible code.","code":"\nlibrary(\"tidyverse\")\n\nscreening <- read_csv(\"p_screen.csv\")\nresponses <- read_csv(\"QHC_responses.csv\")\nscoring <- read_csv(\"QHC_scoring.csv\")"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.3 Task 1 - Oldest Participant","text":"Replace NULL T1 code chunk Participant ID oldest participant. Store single value oldest_participant (e.g. oldest_participant <- 999).hint: look data, oldest?","code":"\noldest_participant <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.4 Task 2 - Arranging D-SPAN","text":"Replace NULL T2 code chunk code arranges participants' D-SPAN performance highest lowest using appropriate one-table dplyr (.e., Wickham) verb. Store output cogtest_sort. (e.g. cogtest_sort <- verb(data, argument))hint: arrange screening data","code":"\ncogtest_sort <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.5 Task 3 - Foreign Language Speakers","text":"Replace NULL two lines code chunk T3, descriptives column called n shows number participants speak foreign language number participants speak foreign language, another column called median_age shows median age two groups. done correctly, descriptives 3 columns 2 rows data, including header row.hint: First need group_by() foreign languagehint: Second need summarise(). need n() function. Pay attention specific column names given.","code":"\nscreen_groups <- NULL\ndescriptives <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.6 Task 4 - Creating Percentage MOCA scores","text":"Replace NULL T4 code chunk code using one dplyr verbs add new column called MOCA_Perc dataframe screening new column MOCA scores converted percentages. maximum achievable score MOCA 30 percentages calculated (participant score / max score) * 100. Store output screening.hint: mutate() something using MOCA percentage formula","code":"\nscreening <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.7 Task 5 - Remove the MOCA column","text":"Now MoCA score expressed percentage MOCA_Perc longer need raw scores held MOCA. Replace NULL T5 code chunk using one-table dplyr verb keep columns screening, order, without MOCA column. Store output screening.hint: select columnsThe remaining tasks focus merging two tables.suspect older adults musical experience might report finely-tuned hearing abilities without musical experience. therefore decide check whether trend exists data. measured participants' self reported hearing abilities using Better Hearing Institute Quick Hearing Check Questionnaire. questionnaire, participants rated extent agree disagree list statements (e.g., 'problem hearing telephone') using 5 point Likert scale (Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree).participant's response question contained responses dataframe environment. response type worth certain number points (e.g., Strongly Disagree = 0, Strongly Agree = 5) scoring key contained scoring dataframe. score participant calculated totaling number points across questions derive overall score. lower overall score, better participants' self-reported hearing ability.order score questionnaire first need perform couple steps.","code":"\nscreening <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.8 Task 6 - Gather the Responses together","text":"Replace NULL T6 code chunk using code gather responses questions QHC wide format tidy/long format. Put names Question values RESPONSE. Store output responses_long.hint: pivot_longer()hint: names \"Question\"hint: values \"RESPONSE\"","code":"\nresponses_long <- NULL "},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT7","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.9 Task 7 - Joining the data","text":"Now need join number points response scoring participants' responses responses_long.Replace NULL T7 code chunk using inner_join() combine responses_long scoring new variable called responses_points.hint: join column common scoring responses_long","code":"\nresponses_points <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT8","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.10 Task 8 - Working the Pipes","text":"given five lines code takes data current long format creates QHC score participant (group_by()...summarise()). joins screening information (inner_join()) calculating mean QHC score two groups participants - play musical instruments . final step stored tibble called musical_means.Use five lines code replace NULL T8 code chunk functioning code pipeline using pipes. Put function new line one . pipeline result mean QHC values musical non-musical people stored tibble musical_means. final tibble consist two rows two columns (.e. four cells total).hint: pipes, output previous function input subsequent function.hint: function1(...) %>% function2(...)","code":"participant_groups <- group_by(responses_points, ID)\nparticipant_scores <- summarise(participant_groups, Total_QHC = sum(SCORE))\nparticipant_screening <- inner_join(participant_scores, screening, \"ID\")\nscreening_groups_new <- group_by(participant_screening, MUSICAL)\nmusical_means <- summarise(screening_groups_new, mean_score = mean(Total_QHC))\nmusical_means <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"Ch2AssignQueT9","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.4.11 Task 9 - Difference in Musical Means","text":"Finally, replace NULL T9 code chunk value much higher QHC score people play music compared people play music. single numeric value, two decimal places, e.g. 2.93hint: look musical means enter difference two means.Well done, finished! Now go check answers solutions end chapter. looking check answers submitted exactly ones solution - example, remember Mycolumn different mycolumn one correct.","code":"\nQHC_diff <- NULL"},{"path":"data-wrangling-a-key-skill.html","id":"solutions-to-questions-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-basics-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1 Data Wrangling Basics","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.1 Task 1","text":"Using select() include stated columns:Using select() exclude certain columns:Using select() change order columns:Return Task","code":"\nselect(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss)\nselect(pong_data, -JudgedSpeed, -BallSpeed, -BlockNumber)\nselect(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber)"},{"path":"data-wrangling-a-key-skill.html","id":"task-2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.2 Task 2","text":"Return Task","code":"\narrange(pong_data, desc(HitOrMiss), desc(JudgedSpeed))"},{"path":"data-wrangling-a-key-skill.html","id":"task-3","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.3 Task 3","text":"Return Task","code":"\nfilter(pong_data, \n       JudgedSpeed == 1, \n       BallSpeed %in% c(\"2\", \"4\", \"5\", \"7\"), \n       HitOrMiss == 0)"},{"path":"data-wrangling-a-key-skill.html","id":"task-4","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.4 Task 4","text":"first step created filter()second step created mutate()Return Task","code":"\npong_data_filt <- filter(pong_data, TrialNumber >= 2) \npong_data2 <- mutate(pong_data_filt, TrialNumber = TrialNumber - 1)"},{"path":"data-wrangling-a-key-skill.html","id":"task-5","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.5 Task 5","text":"Return Task","code":"\ngroup_by(pong_data2, BlockNumber, BackgroundColor)"},{"path":"data-wrangling-a-key-skill.html","id":"task-6","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.1.6 Task 6","text":"find number hits made small paddle (50) red color background 0.450655Return Task","code":"\npong_data2_group <- group_by(pong_data2, BackgroundColor, PaddleLength)\npong_data2_hits <- summarise(pong_data2_group, mean_hits = mean(HitOrMiss))## `summarise()` has grouped output by 'BackgroundColor'. You can override using\n## the `.groups` argument."},{"path":"data-wrangling-a-key-skill.html","id":"data-wrangling-application-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2 Data Wrangling Application","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-2-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.1 Task 2","text":"orNote, difference library(tidyverse) library(\"tidyverse\") work.Return Task","code":"\nlibrary(tidyverse)\nlibrary(\"tidyverse\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-3-1","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.2 Task 3","text":"Note difference responses <- read_csv(\"responses.csv\") responses <- read_csv(responses.csv). need quotes around .csv filename shown code chunk (e.g. responses <- read_csv(\"responses.csv\")), code work.Return Task","code":"\nresponses <- read_csv(\"responses.csv\")                  \nqformats <- read_csv(\"qformats.csv\")                 \nscoring <- read_csv(\"scoring.csv\")                  \npinfo <- read_csv(\"pinfo.csv\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-7","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.3 Task 7","text":"Return Task","code":"\naq_scores <- rscores %>% \n             group_by(Id) %>% # group by the ID number in column Id\n             summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores."},{"path":"data-wrangling-a-key-skill.html","id":"task-8","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.2.4 Task 8","text":"Return Task","code":"\naq_scores2 <- responses %>% \n  pivot_longer(cols = Q1:Q10,\n               names_to = \"Question\",\n               values_to = \"Response\") %>%\n  inner_join(qformats, \"Question\") %>%\n  inner_join(scoring, c(\"QFormat\", \"Response\")) %>%\n             group_by(Id) %>% \n             summarise(AQ = sum(Score))"},{"path":"data-wrangling-a-key-skill.html","id":"practice-your-skills-2","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3 Practice Your Skills","text":"","code":""},{"path":"data-wrangling-a-key-skill.html","id":"task-1---oldest-participant","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.1 Task 1 - Oldest Participant","text":"Whether coded answer just read data, Participant ID Number 3 oldest.also answered code. quite shown yet look like :Return Task","code":"\noldest_participant <- 3\noldest_participant_code <- arrange(screening, desc(AGE)) %>% \n  slice(1) %>% \n  pull(ID)"},{"path":"data-wrangling-a-key-skill.html","id":"task-2---arranging-d-span","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.2 Task 2 - Arranging D-SPAN","text":"arrange() main function hereYou also needed use desc() sort high lowReturn Task","code":"\ncogtest_sort <- arrange(screening, desc(DSPAN))"},{"path":"data-wrangling-a-key-skill.html","id":"task-3---foreign-language-speakers","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.3 Task 3 - Foreign Language Speakers","text":"First group screening data FLANG using group_by()Next, summarise, paying attention use variable names instructedn() function use within summarise() count many observations . works like count() use count() within summarise()median() function use within summarise() calculate median. Much like sum() mean() sd(), etc.Return Task","code":"\nscreen_groups <- group_by(screening, FLANG) \ndescriptives <- summarise(screen_groups, \n                          n = n(), \n                          median_age = median(AGE))"},{"path":"data-wrangling-a-key-skill.html","id":"task-4---creating-percentage-moca-scores","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.4 Task 4 - Creating Percentage MOCA scores","text":"mutate() function add new column dataHere mutating/adding column called MOCA_Perc shows participant's MOCA score divided 30 multiplied 100.Return Task","code":"\nscreening <- mutate(screening, MOCA_Perc = (MOCA / 30) * 100)"},{"path":"data-wrangling-a-key-skill.html","id":"task-5---remove-the-moca-column","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.5 Task 5 - Remove the MOCA column","text":"select() key function keep remove certain columns.Two options ; give dataframe.first option shows deselect column keep everything else.second option shows select columns want.Remember order important select columns order want.Option 1:Option 2:Return Task","code":"\nscreening <- select(screening, -MOCA)\nscreening <- select(screening, ID, AGE, SEX, HANDEDNESS, EDUCATION, MUSICAL, FLANG, DSPAN, D2, MOCA_Perc)"},{"path":"data-wrangling-a-key-skill.html","id":"task-6---gather-the-responses-together","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.6 Task 6 - Gather the Responses together","text":"pivot_longer() function use .People take understand function spend time looking example start make sense.first argument data. case responses.second argument name columns want gather. gathering columns Q1 column Q15 column. Remember colon (:) says \"... ...\"\nactually need write cols = makes things clearer.\n\"Gathering\" columns based position tibble. order columns tibble Q1 Q15, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q15, therefore code gathers columns Q1 Q15.\nColum names put quotes exist already tibble responses.\nactually need write cols = makes things clearer.\"Gathering\" columns based position tibble. order columns tibble Q1 Q15, code gather two columns. , tibble, order, Q1, Q2, Q3, ... Q15, therefore code gathers columns Q1 Q15.Colum names put quotes exist already tibble responses.third fourth arguments names new columns creating;\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".\nsecond store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".\nnew column names put quotes already exist tibble. always case case function.\nNote names anything using names code makes sense.\nLastly, need write names_to = ... values_to = ... otherwise columns created correctly.\nfirst store question numbers, Question. .e. put question names (names_to = ...) column called \"Question\".second store values/responses, Response. .e. put values/responses questions (values_to = ...) column called \"Response\".new column names put quotes already exist tibble. always case case function.Note names anything using names code makes sense.Lastly, need write names_to = ... values_to = ... otherwise columns created correctly.Return Task","code":"\nresponses_long <- pivot_longer(responses, \n                         cols = Q1:Q15, \n                         names_to = \"Question\", \n                         values_to = \"RESPONSE\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-7---joining-the-data","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.7 Task 7 - Joining the data","text":"inner_join() combine common information two sets data common column columns.joining data responses_long data scoring common column RESPONSE.Keep mind inner_join() keeps rows data datasets. remove rows data one dataset.joining two datasets, join common columns one column common.Return Task","code":"\nresponses_points <- inner_join(responses_long, scoring, \"RESPONSE\")"},{"path":"data-wrangling-a-key-skill.html","id":"task-8---working-the-pipes","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.8 Task 8 - Working the Pipes","text":"code started .transcribe series functions pipeline.Remember, using pipes, output previous function input subsequent functionReturn Task","code":"\nparticipant_groups <- group_by(responses_points, ID)\nparticipant_scores <- summarise(participant_groups, Total_QHC = sum(SCORE))\nparticipant_screening <- inner_join(participant_scores, screening, \"ID\")\nscreening_groups_new <- group_by(participant_screening, MUSICAL)\nmusical_means <- summarise(screening_groups_new, mean_score = mean(Total_QHC))\nmusical_means <- group_by(responses_points, ID) %>%\n                  summarise(Total_QHC = sum(SCORE)) %>%\n                  inner_join(screening, \"ID\") %>%\n                  group_by(MUSICAL) %>%\n                  summarise(mean_score = mean(Total_QHC))"},{"path":"data-wrangling-a-key-skill.html","id":"task-9---difference-in-musical-means","chapter":"2 Data-Wrangling: A Key Skill","heading":"2.5.3.9 Task 9 - Difference in Musical Means","text":"People play music QHC score 1.53 units higher people play music.can looking musical_means, reading values, quick maths.second option code. Code always better can reduce error reproducible!Return Task","code":"\n# Option 1\nQHC_diff <- 1.53\n\n# Option 2\n# You will soon learn the functions to do this by code but here is how you could do it.\nQHC_diff_code <- pivot_wider(musical_means, \n                             names_from = \"MUSICAL\", \n                             values_from = \"mean_score\") %>% \n  mutate(diff = YES - NO) %>% \n  pull(diff) %>% \n  round(2)"},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-through-ggplot2","chapter":"3 Data Visualisation Through ggplot2","heading":"3 Data Visualisation Through ggplot2","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"overview-3","chapter":"3 Data Visualisation Through ggplot2","heading":"3.1 Overview","text":"Data visualisation important understanding data. key part data analysis exploring data, checking assumptions, displaying results. Figures plots allow get insight patterns data. example, regards seeing differences groups, also seeing things quite match think happening. great example Anscombe's Quartet, can read later date like - see - four datasets given exact means, different underlying structures visualised. key point always good visualise data visualisation common step data skills set.PsyTeachR Data Skills book introduced data visualisation using ggplot2, main visualisation package tidyverse. look back working chapter, can find additional info main page package: ggplot2.visualisaion use ggplot2 listed great online resources might want consult want fuller understanding:R Graphics Cookbookggplot2 bookggplot2 cheatsheetggplot2 Reference GuideIn chapter, revisit plotting data expand skills order make effective informative figures. become really beneficial progress data visualisation skill applies multiple careers, just Psychology.chapter :Recap data visualisationExpand skills produce new figuresLearn Mental Rotation research","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-basics","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2 Data Visualisation Basics","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"introducing-the-data-set-mental-rotation-ability","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.1 Introducing the Data Set: Mental Rotation Ability","text":"data use today comes replication classic experiment merging fields Perception Cognition. Shepard Metzler (1971) demonstrated participants shown two similar three-dimensional shapes, one just rotated version (see figure - top panel), asked whether shape , reaction time error rates responses function rotation; .e. larger difference rotation two shapes, longer took participants say \"\" \"different\", errors made.\nFigure 3.1: Mental Rotation Task shown Ganis Kievit (2016) Figure 1\nimage shown Figure 3.1 actually comes replication Ganis Kievit (2016). top panel two shapes shape right rotated vertically 150 degrees original (left shape) participants respond \"\". bottom panel, however, two shapes different; one right rotated 150 degrees, trial takes longer participants realise different shapes.can read Ganis Kievit (2016) time, basic methods ran 54 participants series images using four angles rotation (0, 50, 100, 150 degrees) asked people respond '' 'different' trial. data can downloaded . use data follow along try answer questions.Look dataDownload data folder, unzip , save folder access .Set working directory folder Session >> Set Working Directory >> Choose DirectoryOpen new Rscript .RMarkdown file save within folder contains data, giving script sensible name, e.g. Chapter3_visualisations.R. prefer work RMarkdown, just remember need embed code code chunks, shown Chapter 1.Copy three code lines script run bring tidyverse library read two data files.Note, difference library(tidyverse) library(\"tidyverse\") work.However, difference demog <- read_csv(\"demographics.csv\") demog <- read_csv(demographics.csv). need quotes around .csv file name shown code chunk (e.g. demog <- read_csv(\"demographics.csv\")), code work.\nreally great question always seem saying use\ndplyr readr ggplot, \nnever actually call . Remember, however, \ntidyverse actually collection \npackages, common packages fact, use \nbring common packages (including ggplot2) \nprobably need packages along codes \nrun smoothly. try tell need call \npackages alongside tidyverse, keep mind \ncodes least start tidyverse\npackage.\n\nSmall point, looking help ggplot, package\nactually called ggplot2. newer version \npackage, search ggplot2 need help.\n\nstart look data brought . can whichever way choose; mentioned three ways previous chapter.First, demog - short demographics. three columns:Participant - ID participantAge - age participantSex - sex participantSecondly, menrot - short mental rotation. 8 columns:Participant - ID participant; matches ID demogTrial - trial number experiment participantCondition - name image shown; R indicates rotated image differentTime - reaction time respond trial millisecondsDesiredResponse - participants responded trial; Different SameActualResponse - participants respond trial; Different SameAngle - angle shape right rotated compared shape left (0, 50, 100, 150)CorrectResponse - whether participant correct incorrect given trial\nGanis Kievit (2016) short paper really \nintroduce stimuli set rather give extensive background \ntopic mental rotation - call ‘methods paper’. \nsaid, writing paper clear procedure well\ndetailed ran actual experiment.\n\nwriting procedure, remember give much information \nneeded allow someone exactly replicate study. read \nprocedure time think information \n, also information , help develop \nwriting reports. example, fingers \nparticipants use respond important?\nnow data want create plots visualise . show code create four types plots get practice , remember PsyTeachR Data Skills book. go plots, edit/change code give see differences can control changes can create plots. Editing altering code works see happens change something great way working.","code":"\nlibrary(\"tidyverse\")\n\nmenrot <- read_csv(\"MentalRotationBehavioralData.csv\")\ndemog <- read_csv(\"demographics.csv\")"},{"path":"data-visualisation-through-ggplot2.html","id":"basic-structure-of-ggplot","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.2 Basic Structure of ggplot()","text":"two main things know working ggplot :usual ggplot format :ggplot(data, aes(x = x_axis, y = y_axis)) + geom_type_of_plot()first thing enter dataframe/tibble; data. , within aes() say x_axis y_axis, using column names within tibble. aes stands aesthetics maps data visual features. Finally, tell code type plot want.ggplot works concept layersLayers common way graphics work. Think ggplot() function creating first layer every function adding layers top create figure want. first layer always data axis/axes, .e. `ggplot(....). second layer, added using plus symbol '+', type plot. look adding layers progress.ggplot() powerful package used whole range industries, including newspapers mainstream media outlets, can make quite sophisticated images. One beauties data skills just transferable across many fields.","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"scatterplots---geom_point","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.3 Scatterplots - geom_point()","text":"Scatterplots great way visualising continuous data - data can take value scale measured. example, current dataset, can use scatterplots explore potential relationship two continuous variables Age Reaction Time: variables increase/decrease rate (.e., positive relationship)? one variable increase decrease (.e., negative relationship)? maybe overall relationship?data, say want test overall average time respond mental rotation task related age participant highlighting sex participants. show relationship scatterplot using code , :Wrangles data create average response time participant, Mean_Time, joins information demographic data, Participant. stored tibble menrot_time_age.plots scatterplot (geom_point()) age plotted x-axis, Mean_Time y-axisFinally, uses additional aes call color Sex color point based whether male female participant responding. default coloring call two options. Later look controlling using colors .\nFigure 3.2: scatterplot Mean Time function Age\nQuickfire QuestionsLooking scatterplot Figure 3.2, can say relationship age overall response time? age increases, overall response time increasesas age increases, overall response time decreasesthere overall relationshipLooking scatterplot Figure 3.2, can say relationship age overall response time? age increases, overall response time increasesas age increases, overall response time decreasesthere overall relationshipLooking scatterplot, can say difference male female participants? males show increase overall response time age femalesfemales show increase overall response time age malesthere real difference males females terms overall response time ageLooking scatterplot, can say difference male female participants? males show increase overall response time age femalesfemales show increase overall response time age malesthere real difference males females terms overall response time age\nlook figure, appear age increases\n(x-axis) overall resposne time (y-axis)? age decreases \noverall response time? maybe even age increases, overall\nresponse time decreases? Well, actually, looking figure \nappears relationship two variables \ncase one either increases decreases \n. relationship appears flat.\n\ncomparing sex, based color dots, \nappears major differences relationship looks flat\nsex.\n\nLater book look correlational analysis - method\nquantifying relationship two variables.\nNote: often case visualise data first wrangle format. using functions saw Chapter 2, make sure tasks understood wrangle verbs pipes work. Keep mind functions use format, function(data, argument)","code":"\nmenrot_time_age <- group_by(menrot, Participant) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  inner_join(demog, \"Participant\")\n\nggplot(data = menrot_time_age, \n       aes(x = Age, \n           y = Mean_Time, \n           color = Sex)) +\n  geom_point()"},{"path":"data-visualisation-through-ggplot2.html","id":"histograms---geom_histogram","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.4 Histograms - geom_histogram()","text":"Histograms great way showing overall distribution data. data look normally distributed? data skewed - positive skew negative skew? peaky? flat? terms become familiar learn statistics, try think terms concepts visualising looking data.Looking data, say wanted test overall distribution mean response times correct trials normally distributed. visualise question following code, :Wrangles data create average response time participant, Mean_Time, filters information correct trials . stored tibble menrot_hist_correct.Plots histogram (geom_histogram()) Mean_Time plotted x-axis, count value Mean_Time plotted y-axis. code creates y-axis automatically state :\nFigure 3.3: histogram distribution Mean Time counts\nQuickfire QuestionsLooking histogram Figure 3.3, can say overall shape distribution? data looks reasonably normally distributedthe data looks positively skewedthe data looks negatively skewedLooking histogram Figure 3.3, can say overall shape distribution? data looks reasonably normally distributedthe data looks positively skewedthe data looks negatively skewedLooking histogram, common average overall response time correct trials? approximately 2000 millisecondsapproximately 2500 millisecondsapproximately 3000 millisecondsLooking histogram, common average overall response time correct trials? approximately 2000 millisecondsapproximately 2500 millisecondsapproximately 3000 milliseconds\nKeep mind real data never give beautiful textbook\nshape see classic diagrams looking normally\ndistributed data skewed data. decisions regarding \ndistributions often require degree judgement.\n\nPositive skewed data means data shifted \nleft (low numbers) tail stretching right (high numbers).\nNegative skew data shifted right (high\nnumbers) tail stretching left (low numbers). Normally\ndistributed data data middle even tails \neither side. Although perfect, data shown histogram \nreasonable representation normally distributed data real\nworld; particularly small sample participants.\n\ny-axis count values x-axis, \ncommon overall response time can found reading highest column\ndata. distribution, looks around 2500\nmilliseconds 2.5 seconds.\n","code":"\nmenrot_hist_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  filter(CorrectResponse == \"Correct\")\n\nggplot(data = menrot_hist_correct, \n       aes(x = Mean_Time)) + \n  geom_histogram()"},{"path":"data-visualisation-through-ggplot2.html","id":"boxplots---geom_boxplot","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.5 Boxplots - geom_boxplot()","text":"Boxplots great means visualising spread data highlighting outliers data. looking boxplots, consider:whether median (thick horizontal black line) middle box higher lower middle box?whether box evenly distributed around median ?box whiskers (vertical tails top bottom box) similar length sides box?outliers - usually highlighted star dot beyond whiskers?look compare distributions mean reaction times correct incorrect responses. can done using code, :Repeats first two wrangle steps created scatterplot, additionally groups CorrectResponse, stores data tibble menrot_box_correctPlots boxplot (geom_boxplot()) overall average response times y-axis, Mean_Time, based condition, CorrectResponse, x-axisUses additional aes call fill colour boxplots, two categories, based whether CorrectResponse correct incorrect. default look editing later.Turns legend using guides() call needed x-axis tells group . later though.Run code first. , run code fill = TRUE instead. difference? Notice fill name call ggplot(...) function. linked.\nFigure 3.4: boxplot spreads Mean Time Correct Incorrect Responses\nQuickfire QuestionsLooking boxplots, many outliers ? 1230Looking boxplots, many outliers ? 1230Looking boxplots Figure 3.4, condition longer median overall average response time mental rotation task? Median response time longer Correct responsesMedian response time longer Incorrect responsesBoth medians approximatelyLooking boxplots Figure 3.4, condition longer median overall average response time mental rotation task? Median response time longer Correct responsesMedian response time longer Incorrect responsesBoth medians approximately\nnumber ways determining outliers. Two methods \nstandard deviations (usually 2.5 3 SD used cut-offs)\nboxplots, outlier determined \\(1.5*IQR\\) (inter-quartile range) \ntop bottom box. Outliers shown dots \nwhiskers boxplot. can see figure \noutliers see data.\n\nmedian one five values required make boxplot \nshown horizontal thick black line within box .\nLooking two conditions comparing position median\ny-axis (response time) can see median response time\nincorrect trials higher correct trials. suggest\npeople take longer make mind give decision \ntrials get wrong. Makes sense think ;\nuncertainty takes longer leads errors.\n","code":"\nmenrot_box_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  inner_join(demog, \"Participant\")\n\nggplot(data = menrot_box_correct, \n       aes(x = CorrectResponse, \n           y = Mean_Time, \n           fill = CorrectResponse)) + \n  geom_boxplot() +\n  guides(fill = FALSE)## `summarise()` has grouped output by 'Participant'. You can override using the\n## `.groups` argument.## Warning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated."},{"path":"data-visualisation-through-ggplot2.html","id":"barplots---geom_bar-or-geom_col","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.6 Barplots - geom_bar() or geom_col()","text":"Barplots typically show specific values condition. Sometimes really simple like count variable mean, e.g. many people replied yes. Others show something bit complex average spread values via error bars, e.g. standard error. looking barplots, main considerations whether appears difference conditions interested whether conditions .worth knowing barplots now used less frequently actually show lot information, discussed blog, One simple step improving statistical inference. However, still see used great able create interpret .Using geom_bar()Using data, say interested whether difference average percentage correct incorrect responses across male female participants. visualise using following code, :Wrangles data series steps establish overall percent average correct incorrect responses sex, stored menrot_resp_sex.Plots barplot (geom_bar()) condition Sex x-axis, Avg_Percent y-axis, created wrangle, fill bars based CorrectResponse.Finally, within geom_bar says treat data final values average , stat = \"identity\", makes columns visible moving apart position = position_dodge(.9)) - without last step bars overlap see everything. Try changing .9.Note: participant 96 trials study.\nFigure 3.5: barplot average percent Correct Incorrect responses Female Male participants - using geom_bar()\nUsing geom_col()geom_col() - short column - alternative geom_bar() require part code say anything data, .e. stat=\"identity\". shown . Notice difference codes, produce figure.\nFigure 3.6: barplot average percent Correct Incorrect responses Female Male participants - using geom_col()\nQuickfire QuestionsLooking barplot data, average, sex correct responses? femalemaleboth samecan't tellLooking barplot data, average, sex correct responses? femalemaleboth samecan't tellLooking barplot data, average, sex incorrect responses? femalemaleboth samecan't tellLooking barplot data, average, sex incorrect responses? femalemaleboth samecan't tellLooking code, happens decrease position.dodge() value? bars get apartthe bars start overlapnothing changes figureLooking code, happens decrease position.dodge() value? bars get apartthe bars start overlapnothing changes figureLooking code, happens change aes call fill color? bars stay different colorthe bars become grey outlines become different colorsnothing changes figureLooking code, happens change aes call fill color? bars stay different colorthe bars become grey outlines become different colorsnothing changes figure\nRemember barplots plotting mean, top column \naverage value condition. actually people \nlike barplots; though commonly used, really show one value\ndata, average, disregard information\nunless indication spread given.\n\nmind, comparing two Correct columns can see \nfemales average correct responses males. \nIncorrect columns can see males incorrect\nresponses females. actually makes sense response option\nexperiment either correct incorrect, add \ncorrect incorrect percentage responses one sex together \nget 100%. females gave correct reponses must\ngiven less incorrect responses.\n\nlast two questions playing code. Remember \nsaid plots work concept layers. set\nposition.dodge() 0, find one \ncolumns disappears completely overlap now. need \nset position.dodge() reasonable value \ncolumns separate. set 1? barplots, often find \ndifferent levels (categories) variable \ntouching. Note, however, value dodge, case 1, \nrelative size x-axis - scale x-axis ran\n0 100 dodge 1 little effect. Sometimes\nneed little trial error. Always look output \ncode.\n\nfinal point shows can add lot calls just x\ny axis change presentation figures. fill\nchanges color columns, color changes outline\ncolor columns. see progress \nlook difference putting inside \naes() outside . play \nfigures see happens. worth pointing \nthough, turned legend using\nguides(fill = FALSE), works used \nfill = ... call change colours. used \ncolor = ... call change colours need use\nguides(colors = FALSE) turn legend. See \nlinked? guide matches called.\n","code":"\ntotal_n_trials <- 96\n\nmenrot_resp_sex <- count(menrot, Participant, CorrectResponse) %>% \n  inner_join(demog, \"Participant\") %>%\n  mutate(PercentPerParticipant = (n/total_n_trials)*100) %>%\n  group_by(Sex, CorrectResponse) %>%\n  summarise(Avg_Percent = mean(PercentPerParticipant))\n\nggplot(data = menrot_resp_sex, \n       aes(x = Sex, \n           y = Avg_Percent, \n           fill = CorrectResponse)) + \n  geom_bar(stat = \"identity\", \n           position = position_dodge(.9))## `summarise()` has grouped output by 'Sex'. You can override using the `.groups`\n## argument.\nggplot(data = menrot_resp_sex, \n       aes(x = Sex, \n           y = Avg_Percent, \n           fill = CorrectResponse)) +\n  geom_col(position = position_dodge(.9))"},{"path":"data-visualisation-through-ggplot2.html","id":"themes-labels-guides-and-facet_wraps","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.7 Themes, Labels, Guides, and facet_wraps()","text":"couple layers can add ggplot calls make figures look professional. show code , want run teach work changing code, removing parts within ggplot, adding figures shown .themes - changing overall presentation figure. Try running code comparing figure barplot . Remember, ?theme_bw() give information look cheatsheets different themes theme_light(), theme_classic(), theme_gray() theme_dark().\ntheme_gray() actually default equivalent stating theme function.\n_ theme_classic() close basic APA figure presentation.\nthemes - changing overall presentation figure. Try running code comparing figure barplot . Remember, ?theme_bw() give information look cheatsheets different themes theme_light(), theme_classic(), theme_gray() theme_dark().theme_gray() actually default equivalent stating theme function.\n_ theme_classic() close basic APA figure presentation.labs - putting appropriate labels figures readers understand displayed. Try changing text within quotes.labs - putting appropriate labels figures readers understand displayed. Try changing text within quotes.facet_wraps - splitting data separate figures clarity. work one conditions categorical, can really effective means displaying information.facet_wraps - splitting data separate figures clarity. work one conditions categorical, can really effective means displaying information.guides - remove see happens. understand use fill situation, perhaps others?guides - remove see happens. understand use fill situation, perhaps others?Try running editing code:","code":"\ntotal_n_trials <- 96\n\nmenrot_better_plot <- count(menrot, Participant, CorrectResponse) %>% \n  inner_join(demog, \"Participant\") %>%\n  mutate(PercentPerParticipant = n/total_n_trials) %>%\n  group_by(Sex, CorrectResponse) %>%\n  summarise(Avg_Percent = mean(PercentPerParticipant))\n\nggplot(data = menrot_better_plot, \n       aes(x = Sex,\n           y = Avg_Percent, \n           fill = CorrectResponse)) + \n  geom_col(position = position_dodge(.9)) +\n  labs(x = \"Sex of Participant\", \n       y = \"Percent Average (%)\") +\n  guides(fill = FALSE) +\n  facet_wrap(~CorrectResponse) +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"choosing-appropriate-figures","chapter":"3 Data Visualisation Through ggplot2","heading":"3.2.8 Choosing Appropriate Figures","text":"progress Psychology, come across variety different figures plots, looking slightly different giving different information. looking figures, indeed choosing one analyses, think figure appropriate data. example, scatterplots great variables continuous; boxplots histograms great viewing spreads data; barplots commonly used one variable categorical - note barplots can misleading lots new approaches display categorical information created. Violin-boxplots taught Chapter 7 Data Skills book provide complete way data visualisation. Always keep asking , plot display data correctly. Also, right number dots/conditions/groups figure? many suggest something quite right. Look data!","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-application","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3 Data Visualisation Application","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"mental-rotation-angle-and-reaction-time","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.1 Mental Rotation: Angle and Reaction Time","text":"continue using data set Ganis Kievit (2016). understanding experiment well develop skills visualisation interpretation, look mean reaction time correct trials, function angle rotation sex. idea rotated test image original image, longer take participants determine image completely different images. Fifty-four participants responded '' 'different' trial series rotated images using four angles rotation (0, 50, 100, 150 degrees rotated compared original). Link data.\ncome across phrase ‘function ’ quite bit \ndealing visualisations. means something like ‘compared ’ \n‘across’. say going look Mean Reaction Time\nacross four different Angles Rotation written \nMean Reaction Time function Angle Rotation. Usually, \nplotted y-axis function x-axis. similar idea \nfunctions use codes want see happens\nput y function x. good become familiar \nterms language used reports ) understand\nreading b) can use language \nwriting.\n","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.2 Task 1: Loading and Viewing the Data","text":"Download data, unzip , save folder access .Set working directory folder data .Start new script .Rmd file (RMarkdown), save folder data .script, load tidyverse library.Load datasets exactly , storing experimental data menrot demographic data demog.\nlibrary()\n\nmenrot <- read_csv()\n\ndemog <- read_csv()\n\nRemember always looking data good practice make sure expect . Use either View() glimpse(), console window RStudio, never script .Rmd file. useful functions can use check data structure :str() - shows type data . Look words like table, dataframe, character, integer, double.head(), tail(), names() - show top six bottom six rows column names, just column names names().dim() - shows dimensions data.Refer previous section list columns refer sure. keep mind reproducible means careful spelling punctuation names functions, tibbles, columns, conditions, etc, times - e.g. Juggler juggler.Quickfire QuestionsTake couple minutes try functions answer following questions.options, type data variable Angle, found dataframe menrot? characterintegerdouble/numericalFrom options, type data variable Angle, found dataframe menrot? characterintegerdouble/numericalType box name dataframe contains information regarding sex participants: Type box name dataframe contains information regarding sex participants: options, column within menrot? CorrectresponseCorretcResponseCorrectResponsecorrectReponseFrom options, column within menrot? CorrectresponseCorretcResponseCorrectResponsecorrectReponseFrom options, according dim() call, many rows demog? 38545184From options, according dim() call, many rows demog? 38545184\nmaking sure loading data correctly,\nusing instructed names - reproducible - \nunderstand data looking . completed Task 1\nsuccessfully, loading data correct dataframes, \nfollowing answers work questions.\n\n\ncalling str(menrot) looking \ninformation comes see data column\nAngle loaded double/numerical. Technically \nintegers (whole numbers decimal places), default\nload make numerical.\n\n\ncalling str(menrot) looking \ninformation comes see data column\nAngle loaded double/numerical. Technically \nintegers (whole numbers decimal places), default\nload make numerical.\n\n\ndemog loaded information\nregarding demographics including sex participant.\n\n\ndemog loaded information\nregarding demographics including sex participant.\n\n\nnames(menrot) give column names. \nquestion making sure correct spelling: CorrectResponse.\nspellings work data spelling \ncolumn names specific!\n\n\nnames(menrot) give column names. \nquestion making sure correct spelling: CorrectResponse.\nspellings work data spelling \ncolumn names specific!\n\n\ndim(demog) shows number rows (54) \nnumber columns (3).\n\n\ndim(demog) shows number rows (54) \nnumber columns (3).\n","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.3 Task 2: Recreating the Figure","text":"start making representation top part Figure 2 Ganis Kievit (2016) - Mean Reaction Time function Angle Rotation.Copy lines code script.Copy lines code script.Replace NULLs order recreate figure similar Ganis Kievit (2016) Figure 2 (top).\nNote figure shows information correct responses just Ganis Kievit (2016).\nRemember ggplot case layers. first layer says data want axis. Every subsequent layer says want data displayed - points (geom_point()) connecting line (geom_line()).\nrunning tasks, come back one see can figure coord_cartesian() changing numbers ylim = () call. comment solutions.\nReplace NULLs order recreate figure similar Ganis Kievit (2016) Figure 2 (top).Note figure shows information correct responses just Ganis Kievit (2016).Remember ggplot case layers. first layer says data want axis. Every subsequent layer says want data displayed - points (geom_point()) connecting line (geom_line()).running tasks, come back one see can figure coord_cartesian() changing numbers ylim = () call. comment solutions.\nfirst four lines, create menrot_angle, \nfunctions Wickham Six verbs refer back see \nwork.\n\n\nanswer CorrectResponse allow \nkeep just correct answers?\n\n\nanswer CorrectResponse allow \nkeep just correct answers?\n\n\ncommon variable allow join information \ndemog?\n\n\ncommon variable allow join information \ndemog?\n\n\nreally care four levels rotation, \nvariable/column group_by?\n\n\nreally care four levels rotation, \nvariable/column group_by?\n\n\nvariable/column want mean mean response\ntime?\n\n\nvariable/column want mean mean response\ntime?\n\nggplot line, think format, data, \nx-axis name, y-axis name.\n\nFigure 3.7: Basic Scatterplot Response Time Angle Rotation\nThinking Cap PointGreat, replicated figure! However, know means? figure tell mean reaction time angle rotation, fit overall theory introduced ? Answering question may help:options, figure suggest angle rotation increases: mean reaction time decreasesmean reaction time stays samemean reaction time increases\ncan see figure, consistent Shepard Metzler\n(1971), participants Ganis Kievit (2016) showed increase\nreaction time angle rotation increased. Therefore, Ganis\nKievit (2016) replicated findings Shepard Metzler\n(1971).\n\nquick note though , yes, mean reaction time increase\nangle rotation, consistent increase. \nsee difference mean reaction times 150 100\ndegrees smaller 0 50. Reaction times start \nplateau certain angle rotation.\n","code":"\nmenrot_angle <- filter(menrot, CorrectResponse == NULL) %>%\n  inner_join(demog, NULL) %>%\n  group_by(NULL) %>% \n  summarise(mean_Resp = mean(NULL))\n\nggplot(data = NULL, aes(x = NULL, y = NULL)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT3","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.4 Task 3: Examining Additional Variable Effects","text":"previous section, looked sex participant quite interesting. covered Ganis Kievit (2016), let us take look .pipeline Task 2, add variable Sex group_by() function group data Angle Sex.Running code creates figure.\nRemember, add grouping variables just separate \ncomma. Everything else code stays .\n\nFigure 3.8: Separating points Sex\nHmmm, figure look informative. looks similar one created dots doubled - now 8 instead 4 - know male female, connecting line confusing. need add little code tell separate data based Sex.","code":"## `summarise()` has grouped output by 'Angle'. You can override using the\n## `.groups` argument."},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT4","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.5 Task 4: Grouping the Figure Data","text":"previous section used fill color inside aes() change basic information figure. also one called group. Add group call inside aes(...) separate data Sex.Run code see figure looks like\nggplot(....., aes(x = , y = , group = ???))\nnow least see different lines two sex, still tell sex line, can ? just looks like two black parallel lines, one slightly higher . ideal changing color points based whether male female participants! Fortunately, geoms can also take information well.","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT5","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.6 Task 5: Identifying Groups Using aes()","text":"Add aes() call inside geom_point() function color dots Sex.\ngeom_point(aes(??? = ???)\n\nnow something like :\n\nFigure 3.9: Separate lines Sex\nGreat! can now see female line top male line bottom. start interpreting figure finish tidying tasks. example dots data point perhaps little small see, increase size. Also, color great can print color, also change shape dots help people distinguish Sex displaying color option. use additional calls shape size within geom_point().","code":"## `summarise()` has grouped output by 'Angle'. You can override using the\n## `.groups` argument."},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT6","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.7 Task 6: Changing the Shape and Size of Data Points","text":"want Sex different shaped points, add call shape within aes() call geom_point() function, just like color.want Sex different shaped points, add call shape within aes() call geom_point() function, just like color.However, want Sex size point, add call size within geom_point() function, inside aes() call. Set appropriate number size instead naming variable. Maybe size = 3?However, want Sex size point, add call size within geom_point() function, inside aes() call. Set appropriate number size instead naming variable. Maybe size = 3?\ngeom_point(aes(color = Sex, Shape = ???), size = ???)\ngive figure like :\nFigure 3.10: Changing Shape Size Data Points\nquick point - , , aes ?Hopefully beginning spot difference setting call within aes() (stands aesthetics) setting outside aes(). Outside aes() means observations take one value color type. Inside means observation within condition takes value color type, different conditions different values/color/type.look done help us compare:size called outside aes() assign specific value. can see Task 6 figure, condition now size points. set size want, keep smallish: 3 5 ok; 50 artistic, informative. set size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) male female different shapes different sizes. can play around code see things work.size called outside aes() assign specific value. can see Task 6 figure, condition now size points. set size want, keep smallish: 3 5 ok; 50 artistic, informative. set size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) male female different shapes different sizes. can play around code see things work.contrast, called shape inside aes() set based variable, Sex. way ensures level Sex variable, male female, get different shape. instead set shape outside aes(), something like geom_point(shape = 3, size = 3) conditions shape size. Different numbers relate different shapes different sizes. example compare shape = 3 shape = 13In contrast, called shape inside aes() set based variable, Sex. way ensures level Sex variable, male female, get different shape. instead set shape outside aes(), something like geom_point(shape = 3, size = 3) conditions shape size. Different numbers relate different shapes different sizes. example compare shape = 3 shape = 13There arguments use: example, wanted points color, say red example, geom_point(color = \"red\"). Remember put quotes around color.hopefully starting make sense can think implementing figures. Note arguments separated comma. e.g. geom_point(color = \"red\", size = 3, shape = 2).","code":"## `summarise()` has grouped output by 'Angle'. You can override using the\n## `.groups` argument."},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT7","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.8 Task 7: Adding Labels and Changing the Background","text":"figure looking really nice now. finish adding appropriate labels editing background. introduced previous section hopefully play see work.change label, use labs() function works like labs(x = \"Name\", y = \"Name\", title = \"Name\").Add function code y-axis indicates Mean Reaction Time (ms) x-axis indicates Angle Rotation (degrees). set title , can want practice. Titles Psychology figures common.Add function code y-axis indicates Mean Reaction Time (ms) x-axis indicates Angle Rotation (degrees). set title , can want practice. Titles Psychology figures common.Set figure theme_bw() - looks nice, options might want try can explore ?theme cheatsheet.Set figure theme_bw() - looks nice, options might want try can explore ?theme cheatsheet.\nlabs(x = \"...\", y = \"...\") + theme_bw()\n\nkey thing remember + layer \nggplot chain. don’t get confused pipes (%>%).\n\nNote: add (+) layers figures, pipe\n(%>%) functions.\n","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3InClassQueT8","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.9 Task 8: Separating a Variable and Removing Legends","text":"Finally, showed two functions use tidy figures: facet_wrap() guides().facet_wrap() really effective splitting figures panels based variable; works like facet_wrap(~variable) ~ can read . \"split figure variable\", example Sex. two variables split figure , : facet_wrap(~variable1 + variable2).facet_wrap() really effective splitting figures panels based variable; works like facet_wrap(~variable) ~ can read . \"split figure variable\", example Sex. two variables split figure , : facet_wrap(~variable1 + variable2).guides() handy turning legends might taking space. instance, use facet_wrap() split panels Female Male, really need legend right saying Female Male? normally guide color, shape, etc, calls pipeline within aes() calls. works like guide(call = FALSE).guides() handy turning legends might taking space. instance, use facet_wrap() split panels Female Male, really need legend right saying Female Male? normally guide color, shape, etc, calls pipeline within aes() calls. works like guide(call = FALSE).Add facet_wrap() separate panels figure based Sex.Add facet_wrap() separate panels figure based Sex.Turn guides legend figure.Turn guides legend figure.\nfacet_wrap(~variable)\n\nguides(group = FALSE, ???? = False, ....)\nThinking Cap PointIf followed tasks correctly, following figure:\nFigure 3.11: finished figure!\nTake minutes look figure try interpret terms reaction time function rotation sex. Try answering following questions:sexes, mean reaction time decreases withincreases withis unaffected angle rotation.sexes, mean reaction time decreases withincreases withis unaffected angle rotation.Angle Rotation influences female participantsmale participants female participantsmale participantsAngle Rotation influences female participantsmale participants female participantsmale participants\nlooking figure, angle rotation increases (moving \nright x-axis), mean reaction time increases (getting\nhigher y-axis), indicating participants take longer \nrespond target image rotated original. Also,\nmale mean reaction times quicker overall female mean\nreaction times, differences reaction times 0 degrees\n150 degrees smaller males, perhaps say \nmales affected less females, males perform task\nquicker.\n\nKeep mind looking correct responses . \n, figure suggest difference just male\nparticipants just responding quicker overall; instead may suggest\nmales responding correctly quicker overall.\n\nDifferences mental rotation tasks received much attention\nyears refer reference sections two\nmain papers activity wish follow topic\n.\n","code":"## `summarise()` has grouped output by 'Sex'. You can override using the `.groups`\n## argument.## Warning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated."},{"path":"data-visualisation-through-ggplot2.html","id":"final-considerations","chapter":"3 Data Visualisation Through ggplot2","heading":"3.3.10 Final Considerations","text":"Many options seen terms geom_point() applied geom_line() make alterations line. Try playing options. example, code line result sexes red line equal size, style line different. Give shot!geom_line(aes(linetype = Sex), size = .5, color = \"red\")Finally, look closely figure, see line points actually goes front points. looks bit messy. make tidier line run behind data points? sure? Remember figures constructed based series layers. draw one layer next, try changing Task 8 draw lines first points top. Give go!chapter looked working layers variety calls shape, color, fills, etc, create professional looking figures. Understanding figures ggplot can seem like trial error lot experience. beauty figure really like, run code get exactly figure .","code":"\nggplot(data = menrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_line() +\n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  facet_wrap(~Sex) +\n  guides(group = FALSE, color = FALSE, shape = FALSE) +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"Ch3PracticeSkills","chapter":"3 Data Visualisation Through ggplot2","heading":"3.4 Practice Your Skills","text":"order complete tasks need download data .csv files .Rmd file, need edit, titled Ch3_PracticeSkills_Visualisations.Rmd. can downloaded within zip file link. downloaded unzipped, create new folder use working directory; put data files .Rmd file folder set working directory folder drop-menus top. Download Exercises .zip file .Now open .Rmd file within RStudio. see code chunk task. Follow instructions edit code chunk. often entering code based covered point.Happy Data Visualising!","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"solutions-to-questions-2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"data-visualisation-application-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1 Data Visualisation Application","text":"","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"task-1-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.1 Task 1","text":"Return Task","code":"\nlibrary(\"tidyverse\")\n\nmenrot <- read_csv(\"MentalRotationBehavioralData.csv\")\ndemog <- read_csv(\"demographics.csv\")"},{"path":"data-visualisation-through-ggplot2.html","id":"task-2-2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.2 Task 2","text":"coord_cartesian() function can used show certain parts figure, controlling visible X Y axes. expand = TRUE adds smaller buffer numbers set. want remove buffer set expand = FALSE.Return Task","code":"\nmenrot_angle <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_angle, aes(x = Angle, y = mean_Resp)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-3-2","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.3 Task 3","text":"Return Task","code":"\nmenrot_angle_sex <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-4-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.4 Task 4","text":"Return Task","code":"\nmenrot_grouped <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_grouped, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point() +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-5-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.5 Task 5","text":"Return Task","code":"\nmenrot_grouped_color <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_grouped_color, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-6-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.6 Task 6","text":"Return Task","code":"\nmenrot_shape_size <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_shape_size, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE)"},{"path":"data-visualisation-through-ggplot2.html","id":"task-7-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.7 Task 7","text":"Return Task","code":"\nmenrot_lab_theme <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))\n\nggplot(data = menrot_lab_theme, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"task-8-1","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.1.8 Task 8","text":"\nFigure 3.12: Task 8\nRemembering layer system, use code lines behind points.\nFigure 3.13: Task 8 Alternative\nReturn Task","code":"\nmanrot_facet_guide <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))## `summarise()` has grouped output by 'Angle'. You can override using the\n## `.groups` argument.\nggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  facet_wrap(~Sex) +\n  guides(group = FALSE, color = FALSE, shape = FALSE) +\n  theme_bw()## Warning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated.\nmanrot_facet_guide <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))## `summarise()` has grouped output by 'Angle'. You can override using the\n## `.groups` argument.\nggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_line() +\n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  labs(x = \"Angel of Rotation (degrees)\", y = \"Mean Reaction Time (ms)\") +\n  facet_wrap(~Sex) +\n  guides(group = FALSE, color = FALSE, shape = FALSE) +\n  theme_bw()"},{"path":"data-visualisation-through-ggplot2.html","id":"practice-your-skills-3","chapter":"3 Data Visualisation Through ggplot2","heading":"3.5.2 Practice Your Skills","text":"Check work solution tasks : Chapter 3 Practice Skills Solution.Return Task","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"additional-material","chapter":"3 Data Visualisation Through ggplot2","heading":"3.6 Additional Material","text":"additional material might help understand figures bit present reports. Thus, want clarification aes() call want know combine several plots one, read !","code":""},{"path":"data-visualisation-through-ggplot2.html","id":"more-on-aes","chapter":"3 Data Visualisation Through ggplot2","heading":"3.6.1 More on aes()","text":"chapter, added short note using aes() call, see people issues thought quick demonstration might help. code previous activity:\nFigure 3.14: Changing Shape Size Data Points\nSpecifically, going focus geom_point() line. Inside aes() stated color = Sex, shape = Sex outside aes() stated, size = 3. Earlier said, outside aes() means observations take one value color type. Inside means observation within condition takes value color type, different conditions different values/color/type. based understanding, plot, shapes size (.e., 3), different shape color shape sex. Now demonstrate alternatives.points shape, color size - nothing aes().\nneed state color, shape size want observations . chosen red color (quotes) shape style 3 (+) size 6. using variable split observations groups.\nchanged size make visualisation clearer\nneed state color, shape size want observations . chosen red color (quotes) shape style 3 (+) size 6. using variable split observations groups.changed size make visualisation clearer\nFigure 3.15: points shape, color size\npoints shape size, color determined Sex (goes inside aes()).\nneed state shape size want dots . time giving different levels within Sex (.e., male female) different colors - putting inside aes()\nneed state shape size want dots . time giving different levels within Sex (.e., male female) different colors - putting inside aes()\nFigure 3.16: points shape size color determined Sex\nshowed code setting color shape Sex previously. Instead now set color, shape size variable Sex.\noptions aes() different colors, sizes, shapes males females, males color, size shape, females color size shape.\noptions aes() different colors, sizes, shapes males females, males color, size shape, females color size shape.\nFigure 3.17: points shape size color determined Sex\nactually get warning option. ? numerous options many different shapes created cause issues code may even crash . Pay attention warnings.reason decide best approach displaying data observations within condition , showing different colors shapes makes little sense. always need think trying convey. Look two figures think one easier understand observations condition. one left! one right suggests something different observations.\nFigure 3.18: plot left suggests observations condition. figure right suggests difference observations. Always think information convey figures!\nHopefully beginning become clearer. Insider aes() means observations within variable/condition shown , different observations different variable/condition. Outside aes() means observations shown regardless condition.","code":"\nmenrot_angle_sex <- filter(menrot, CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\") %>%\n  group_by(Angle, Sex) %>% \n  summarise(mean_Resp = mean(Time))## `summarise()` has grouped output by 'Angle'. You can override using the\n## `.groups` argument.\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex), size = 3) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_gray()\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(color = \"red\", shape = 3, size = 6) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_bw()\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex), shape = 3, size = 6) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_bw()\nggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + \n  geom_point(aes(color = Sex, shape = Sex, size = Sex)) +\n  geom_line() +\n  coord_cartesian(ylim = c(0, 3500), expand = TRUE) +\n  theme_bw()## Warning: Using size for a discrete variable is not advised."},{"path":"data-visualisation-through-ggplot2.html","id":"combining-plots-into-one-figure","chapter":"3 Data Visualisation Through ggplot2","heading":"3.6.2 Combining Plots into one Figure","text":"Space within report commodity. Figures can incredibly useful getting information across efficient manner, strict word count, multiple figures can really chew limit, given figure needs legend legend counts. One way get around merge figures together one big figure perhaps convey similar related information. going show using package called patchwork.\npatchwork unlikely lab machines \nplease try machine. haven’t previously\ninstalled patchwork package machine ,\ninstall first,\ne.g. install.packages(\"patchwork\").\nPlots like boxplots histograms, combined, can incredibly useful understanding overall shape data whether fits assumptions inferential tests, something come later. create two separate plots, might get something like :\nFigure 3.19: histogram distribution Mean Time counts Sex\n:\nFigure 3.20: boxplot spreads Mean Time Correct Responses Sex\nNow given divide data sex, can start see figure legend plot becomes bit repetitive, combining one figure potentially make things easier. number packages , patchwork straightforward flexible.now call patchworkThe first thing need using patchwork save plots object (just like output function). Using code , might look like boxplot:histogram:Note: reason inclusion title plot become clear second.Note: run codes plots generated saving objects - boxplot p_box histogram p_hist. important realise distinction someone asks make produce code figure generated code knits, saved plot object, figure might show. save plot object, can generate figure just calling name object. look ggplot cheatsheet see approach lot. call boxplot stored p_box.\nFigure 3.21: boxplot spreads Mean Time Correct Responses Sex\n\nFigure 3.22: histogram distribution Mean Time counts Sex\nNote: see warning histogram plot selecting binwidth. really looked yet due course. wanted \"fix\" warning changing histrogram code something like + geom_histogram(binwidth = 100) might work. value enter relative scale data. binwidth 1 create bin every increase 1 ms. binwidth 100 creates bin every 100 ms.far nothing exciting. Looks just like seen . say wanted plots single figure, right? Well patchwork, simply \"add\" plots together using plus sign (+), :\nFigure 3.23: boxplot (- left) spreads Mean Time Correct Responses, histogram (B - right) distribution Mean Time counts, separated Sex (female - red, male - cyan)\nNote: can use \"titles\"\" added plots original code tell readers plot, within combined figure, referring , B, left right, shown figure legend beneath figure. might seem bit pedantic, control somebody views published figure, clarity paramount!Awesome, ? ! can also change configuration plots combined figure. Say wanted plots top - portrait rather landscape - well instance divide plots using divide sign (/), :\nFigure 3.24: boxplot (- top) spreads Mean Time Correct Responses, histogram (B - bottom) distribution Mean Time counts, separated Sex (female - red, male - cyan)\nnow refer top bottom, rather left right. fact, patchwork really flexible can work multiple plots arrangements. Hypothetically, say three plots wanted two top one, use approach combining \"+\" \"/\" :Remember trick using patchwork save plots objects first (p1 <- ggplot(....)) rest easy. sure always know figure shown knitted ; often , seeing figure important seeing code.Happy Visualising!End Additional Material!","code":"\nmenrot_hist_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  filter(CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\")\n\nggplot(data = menrot_hist_correct, \n       aes(x = Mean_Time,\n           fill = Sex)) + \n  geom_histogram() +\n  theme_bw()\nmenrot_box_correct <- group_by(menrot, Participant, CorrectResponse) %>% \n  summarise(Mean_Time = mean(Time, na.rm = TRUE)) %>%\n  filter(CorrectResponse == \"Correct\") %>%\n  inner_join(demog, \"Participant\")## `summarise()` has grouped output by 'Participant'. You can override using the\n## `.groups` argument.\nggplot(data = menrot_box_correct, \n       aes(x = CorrectResponse, \n           y = Mean_Time, \n           fill = Sex)) + \n  geom_boxplot() +\n  theme_bw()\nlibrary(patchwork)\np_box <- ggplot(data = menrot_box_correct, \n       aes(x = CorrectResponse, \n           y = Mean_Time, \n           fill = Sex)) + \n  geom_boxplot() +\n  labs(title = \"A\") +\n  theme_bw()\np_hist <- ggplot(data = menrot_hist_correct, \n       aes(x = Mean_Time,\n           fill = Sex)) + \n  geom_histogram() +\n  labs(title = \"B\") +\n  theme_bw()\np_box\np_hist## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\np_box + p_hist## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\np_box / p_hist## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n(plot1 + plot2)/plot3"},{"path":"revisiting-probability-distributions.html","id":"revisiting-probability-distributions","chapter":"4 Revisiting Probability Distributions","heading":"4 Revisiting Probability Distributions","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"overview-4","chapter":"4 Revisiting Probability Distributions","heading":"4.1 Overview","text":"Probability degree cornerstone Psychological theory based quantitative analysis. establish outcome (e.g., difference two events), establish probability outcome model standard. Probability important quantifying uncertainty conclusions. already heard probability lectures/journal articles/etc., try help gain deeper understanding probability course next chapters use make inference population.start looking general ideas behind probability. using lot Psychology data concepts can easier understand probability first everyday concrete examples. said, whilst reading examples, trying , think might relate Psychology examples sure ask questions.preclass bit read take time try understand fully. Much familiar though PsyTeachR Data Skills book recaps ideas. Also, cheatsheets chapter using specific package. However can make full use R help function (e.g. ?sample) clear function . Also, remember said previously; shy run Google Search finding stats concepts covered . loads videos help pages clear examples explain difficult concepts.chapter :Revise probability concepts discussed PsyTeachR Data Skills bookCalculate probabilitiesCreate probability distributionsMake estimations probability distributions.\npopulation whole group want know something\n- everyone everything group. sample part \npopulation testing. sample always smaller \npopulation unlikely ever able test\neveryone population, sample representative \npopulation based random sampling. means even though \nusing whole population, sample using represents \nwhole population randomly sampled people . \ntrue, sample representative population, testing\nsample allows make inference population;\ninfer characteristic population testing \nsample.\nDiscrete versus Continuous DataLet's recap questions level measurement can alter way tackle probability - .e., whether data discrete continuous.Quickfire QuestionsDiscrete data can take specific/certain/exact values (e.g., groups, integers). example, number participants experiment discrete - half participant! Discrete variables can also broken nominal/categorical ordinal variables.\n\nFill blanks sentences using words: ordinal, nominal/categorical.NominalOrdinal data based set categories natural ordering (e.g., left right handed). example, separate participants according left right handedness course study (e.g., psychology, biology, history, etc.).NominalOrdinal data based set categories natural ordering (e.g., left right handed). example, separate participants according left right handedness course study (e.g., psychology, biology, history, etc.).NominalOrdinal data set categories natural ordering; know top/best worst/lowest, difference categories may constant. example, ask participants rate attractiveness different faces based 5-item Likert scale (unattractive, unattractive, neutral, attractive, attractive).NominalOrdinal data set categories natural ordering; know top/best worst/lowest, difference categories may constant. example, ask participants rate attractiveness different faces based 5-item Likert scale (unattractive, unattractive, neutral, attractive, attractive).Continuous data hand can take value scale measured. example, can measure age continuous scale (e.g., can age 26.55 years), also reaction time distance travel university.Fill blanks sentences using two remaining levels measurement offered :Continuous data can broken   data.read journal articles working data, really good practice take minute two figure type variables reading /working .\nfour level measurements nominal (also called categorical),\nordinal, interval, ratio. Discrete data uses categories \nwhole numbers therefore either nominal ordinal data.\nContinuous data can take value, e.g., 9.00 9.999999999, \neither interval ratio data.\n","code":""},{"path":"revisiting-probability-distributions.html","id":"discrete-data-and-binomial-distributions","chapter":"4 Revisiting Probability Distributions","heading":"4.2 Discrete Data and Binomial Distributions","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"general-probability-calculations","chapter":"4 Revisiting Probability Distributions","heading":"4.2.1 General Probability Calculations","text":"Today begin recapping concepts probability calculations lectures PsyTeachR Data Skills book, looking discrete distributions - values categories (e.g., house, face, car) whole numbers (e.g., 1,2, 3 1.1, 1.2 etc).talk probability mean interested likelihood event occurring. probability discrete event occurring can formulated :\\[p = \\frac{number \\   \\ ways \\ \\ event \\ \\  arise}{number \\ \\ possible \\ outcomes}\\]probability event represented number 0 1, letter p. example, probability flipping coin landing 'tails', people say, estimated p = .5, .e. likelihood getting tails \\(p = \\frac {1}{2}\\) one desired outcome (tails) two possibilities (heads tails).example:1. probability drawing ten clubs standard pack cards 1 52: \\(p = \\frac {1}{52} \\ = .019\\). One outcome (ten clubs) 52 possible outcomes (cards)2. Likewise, probability drawing either ten clubs seven diamonds first card draw full deck 2 52: \\(p = \\frac {2}{52} \\ = .038\\). case adding chance event occurring giving two possible outcomes becomes likely happen one outcome.3. Now say two standard packs cards mixed together. probability drawing 10 clubs mixed pack 2 104: \\(p = \\frac{2}{104}= .019\\). Two possible outcomes alternatives , 104 time, meaning less probable Example 2 probability Example 1. key thing remember probability ratio number ways specified outcome can happen number possible outcomes.4. instead say two separate packs cards. probability drawing 10 clubs packs : \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). probability gone created event even unlikely happen. called joint probability events.5. probability drawing 10 clubs pack 52, putting back (call replacement), subsequently drawing 7 diamonds? , represented multiplying together probability events happening: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\).6. Finally, say draw 10 clubs pack 52 time replace . probability draw 7 diamonds next draw (without replacing ) 3 hearts third draw? time number cards pack fewer second (51 cards) third draws (50 cards) take account multiplication: \\(p = \\frac{1}{52} \\times \\frac{1}{51}\\times \\frac{1}{50}= .000008\\).\n, probability event number possible\nways event happen, divided number possible\noutcomes. combine probabilities two separate events \nmultiple together obtain joint probability.\n\nmay noticed tend write p = .008, example, \nopposed p = 0.008 (0 decimal place). ?\nConvention really. probability can never go 1, 0\ndecimal place pointless. Meaning people \nwrite p = .008 instead p = 0.008, indicating max value \n1.\n\nallow either version answers chapter \nstill learning, try get habit writing probability\nwithout 0 decimal place.\nQuickfire QuestionsWhat probability randomly drawing name hat 12 names one name definitely name? Enter answer 3 decimal places: probability randomly drawing name hat 12 names one name definitely name? Enter answer 3 decimal places: probability randomly drawing name hat 12 names, putting back, drawing name ? Enter answer 3 decimal places: probability randomly drawing name hat 12 names, putting back, drawing name ? Enter answer 3 decimal places: Tricky: stimuli set 120 faces, 10 inverted 110 right way , probability randomly removing one inverted face first trial, replacing , removing another inverted face second trial? Enter answer three decimal places:Tricky: stimuli set 120 faces, 10 inverted 110 right way , probability randomly removing one inverted face first trial, replacing , removing another inverted face second trial? Enter answer three decimal places:\n\n12 possible outcomes looking one possible\nevent.\n\n\n12 possible outcomes looking one possible\nevent.\n\n\ntwo separate scenarios . scenarios \n12 possible outcomes looking one possible\nevent. Since two separate scenarios, make \nless likely draw name twice?\n\n\ntwo separate scenarios . scenarios \n12 possible outcomes looking one possible\nevent. Since two separate scenarios, make \nless likely draw name twice?\n\n\nfirst trial 120 possible outcomes (faces) \nlooking 10 possible events (inverted faces). \nsecond trial removed first inverted face stimuli\nset now 119 trials total 9 inverted faces.\nRemember need multiply probabilities first trial \nsecond trial results together!\n\n\nfirst trial 120 possible outcomes (faces) \nlooking 10 possible events (inverted faces). \nsecond trial removed first inverted face stimuli\nset now 119 trials total 9 inverted faces.\nRemember need multiply probabilities first trial \nsecond trial results together!\n\n\np = .083. One outcome (name) 12 possibilities,\n.e. \\(p = \\frac{1}{12}\\)\n\n\np = .083. One outcome (name) 12 possibilities,\n.e. \\(p = \\frac{1}{12}\\)\n\n\np = .007. replace name draws \\(p = \\frac{1}{12}\\) draw. \\(p = \\frac{1}{12} * \\frac{1}{12}\\) \nrounded three decimal places\n\n\np = .007. replace name draws \\(p = \\frac{1}{12}\\) draw. \\(p = \\frac{1}{12} * \\frac{1}{12}\\) \nrounded three decimal places\n\n\np = .006. first trial 10 120, \nremove one inverted face second trial 9 119. \nformula \\(p = \\frac{10}{120} * \\frac{9}{119}\\)\n\n\np = .006. first trial 10 120, \nremove one inverted face second trial 9 119. \nformula \\(p = \\frac{10}{120} * \\frac{9}{119}\\)\n","code":"* To find the joint probability of two separate events occuring you multiply together the probabilities of the two individual separate events (often stated as independent, mutually exclusive events). * The second event (drawing the 7 of diamonds) has the same probability as the first event (drawing the 10 of clubs) because we put the original card back in the pack, keeping the number of all possible outcomes at 52. This is **replacement**."},{"path":"revisiting-probability-distributions.html","id":"creating-a-simple-probability-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.2.2 Creating a Simple Probability Distribution","text":"now recap plotting probability distributions looking simulated coin toss. may remember PsyTeachR Data SKills book worry going work . Work read example apply logic quickfire questions end section.Scenario: Imagine want know probability X number heads 10 coin flips - example, probability flipping coin 10 times coming heads two times.simulate 10 coin flips use sample() function randomly sample (replacement) possible events: .e. either heads tails.begin:Open new script copy code lines .\nfirst line code loads library normal.\nsecond line code provides instruction sample options \"Heads\" \"Tails\", ten times, replacement set TRUE.\nfirst line code loads library normal.second line code provides instruction sample options \"Heads\" \"Tails\", ten times, replacement set TRUE.Note: event labels strings (text), enter function vector; .e. \"quotes\"Note: lines code, see output got ran code. worry sequence heads tails different output; expected generating random sample.Note: want get output , add line code script prior loading library. set.seed() function can put number time run randomisation get number.\nSampling simply choosing selecting something - \nrandomly choosing one possible options; heads tails. \nexamples ‘sampling’ include randomly selecting participants, randomly\nchoosing stimuli present given trial, randomly\nassigning participants condition e.g.drug placebo…etc.\n\nReplacement putting sampled option back ‘pot’ \npossible options. example, first turn randomly sample\nHEADS options HEADS TAILS \nreplacement, meaning next turn \ntwo options ; HEADS TAILS. Sampling without\nreplacement means remove option subsequent\nturns. say first turn randomly sample HEADS \noptions HEADS TAILS without replacement. Now second turn\noption TAILS ‘randomly’ sample . \nthird turn without replacement options.\n\nreplacement means putting option back next turn \nturn possible outcome options.\n\nwant use sampling replacement coin toss scenario? sure set replacement FALSE (change last argument TRUE FALSE) run code . code stop working 2 coin flips. want sample replacement want options available sampling - run options quickly since 10 flips.far code returns outcomes 10 flips; either heads tails. want count many 'heads' can simply sum heads. However, heads number, make life easier can re-label events (.e. flips) 0 tails 1 heads. Now run code can pipe sample sum() function total 1s (heads) 10 flips.Run line code number times, notice output?Note: event labels now numeric, need vector.Note: 0:1 means numbers 0 1 increments 1. basically, 0 1.ouptut line changes every time run code randomly sampling 10 coin flips time. clear, get answer 6 example, means 6 heads, turn, 4 tails. running code basically demonstrating sampling distribution created.\nsampling distribution shows probability drawing sample\ncertain characteristics population; e.g. probability\n5 heads 10 flips, probability 4 heads 10 flips, \nprobability X heads 10 flips coin.\n\nNow order create full accurate sampling distribution scenario need replicate 10 flips large number times - .e. replications. replications reliable estimates. 10000 replications 10 coin flips. means flip coin 10 times, count many heads, save number, repeat 10000 times. slow way demonstrated , just running line noting outcome time. use replicate() function.Copy line code script run .exactly said saving 10000 outputs (counts heads) dataframe called heads10k (k shorthand thousand).reiterate, sum heads (.e., number times got heads) 10000 replications now stored vector heads10k. look heads10k, shown box , series 10000 numbers 0 10 indicating number heads, specifically 1s, got set 10 flips.Now, order complete distribution need :Convert vector (list numbers heads counts) data frame (tibble) can work . numbers stored column called heads.group results number possible heads; .e. group times got 5 heads together, times got 4 heads together, etc.Finally, work probability heads result, (e.g., probability 5 heads), totaling number observations possible result (e.g., 5 heads) submitting probability formula (number outcomes event divided possible outcomes)\nnumber times got specific number heads (e.g., 5 heads) divided total number outcomes (.e., number replications - 10000).\nnumber times got specific number heads (e.g., 5 heads) divided total number outcomes (.e., number replications - 10000).can carry steps using following code:Copy code script run .now discrete probability distribution number heads 10 coin flips. Use View() function look data10k variable. now see heads outcome, total number occurrences 10000 replications (n) plus probability outcome (p).\nTable 4.1: sampling distribution number heads 10 flips coin. p = probability obtaining number heads 10000 replications 10 flips coin\nuseful visualize distribution:\nFigure 4.1: Probability Distribution Number Heads 10 Flips\nanalysis, probability getting 5 heads 10 flips 0.2401. remember, surprised get slightly different value. Ten thousand replications lot huge amount compared infinity. run analysis replications numbers become stable, e.g. 100K.Note possible number heads 10 flips related one another, summing probabilities different number heads give total 1. different looked earlier cards events unrelated . , can use information start asking questions probability obtaining 2 less Heads 10 flips? Well, probability getting heads (10 flips) distribution 0.0007, probability getting 1 head 0.0082, probability getting 2 heads 0.0465, probability 2 less Heads distribution simply sum values: 0.0554. Pretty unlikely !Quickfire QuestionsLook probability values corresponding number coin flips created data10k sample distribution (use View() see ):Choose following options, wanted calculate probability getting 4, 5 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, wanted calculate probability getting 4, 5 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, wanted calculate probability getting 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, wanted calculate probability getting 6 heads 10 coin flips : multiply individual probabilities togethersum individual probabilities togetherChoose following options, distribution created : continuousdiscreteChoose following options, distribution created : continuousdiscrete\nthink , can’t get 5.5 heads 2.3 heads, can\nget whole numbers, 2 heads 5 heads. means data \ndistribution discrete. (Don’t confused one functions\nsaying continuous)\n\nfind probability getting say 4, 5, 6 heads 10 coin\nflips, combining related scenarios together, therefore need\nfind individual probabilities getting 4, 5 6 heads 10\ncoin flips, sum probabilities together get appropriate\nprobability obtaining 4, 5 6 heads. 6 \nheads, just sum probabilities 6, 7, 8, 9 10 heads get \nprobability 6 heads.\n\nsure summing multiplying probabilities? \ngood way remember, coin flip examples pack\ncards examples earlier, scenarios \nrelated summing probabilities, scenarios \nseparate multiplying probabilities.\nRelated scenarios usually asking \nprobability ‘either / ’ scenarios occuring,\nwhereas separate scenarios usually ask \nprobability one scenario ‘’ another scenario \noccuring.\n\nsample distribution data10k already completed\nfirst part calculation (finding individual\nprobabilities n heads 10 coin flips), need sum\nrequired probabilities together!\n","code":"\nlibrary(\"tidyverse\")\nsample(c(\"HEADS\", \"TAILS\"), 10, TRUE) ##  [1] \"TAILS\" \"HEADS\" \"TAILS\" \"HEADS\" \"TAILS\" \"TAILS\" \"TAILS\" \"HEADS\" \"HEADS\"\n## [10] \"TAILS\"\nset.seed(1409)\nsample(0:1, 10, TRUE) %>% sum() ## [1] 5\nheads10k <- replicate(10000, sample(0:1, 10, TRUE) %>% sum())   ##  int [1:10000] 7 4 5 6 2 6 2 6 8 5 ...\ndata10k <- tibble(heads = heads10k) %>%       # creating a tibble/data frame\n                group_by(heads) %>%           # group by number of possibilities\n                summarise(n = n(), p=n/10000) # count occurences of possibility,\n                                              # & calculate probability (p) of\n                                              # each\nggplot(data10k, aes(heads,p)) + \n  geom_col(fill = \"skyblue\") + \n  labs(x = \"Number of Heads\", y = \"Probability of Heads in 10 flips (p)\") +\n  theme_bw() +\n  scale_x_discrete(limits=0:10)## Warning: Continuous limits supplied to discrete scale.\n## i Did you mean `limits = factor(...)` or `scale_*_continuous()`?"},{"path":"revisiting-probability-distributions.html","id":"the-binomial-distribution---creating-a-discrete-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.2.3 The Binomial Distribution - Creating a Discrete Distribution","text":"Great, now learning probabilities distributions work. However, wanted calculate probability 8 heads 10 coin flips, go entire procedure time. Instead, dichotomous outcome, \"heads tails\", can establish probabilities using binomial distribution - effectively just created. can look R help page binomial distribution (type ?dbinom directly console) understand use walk essentials .use 3 functions work binomial distribution ask questions asked :dbinom() - density function. function gives probability x successes (e.g., heads) given size (e.g., number trials) probability success prob single trial (0.5, assume flipping fair coin - Heads Tails)dbinom() - density function. function gives probability x successes (e.g., heads) given size (e.g., number trials) probability success prob single trial (0.5, assume flipping fair coin - Heads Tails)pbinom() - cumulative probability function. function gives probability getting number successes certain cut-point given size prob. questions probability 5 heads less example. sums probability 0, 1, 2, 3, 4, 5 heads.pbinom() - cumulative probability function. function gives probability getting number successes certain cut-point given size prob. questions probability 5 heads less example. sums probability 0, 1, 2, 3, 4, 5 heads.qbinom() - quantile function. function inverse pbinom gives x-axis value (including value) summation probabilities greater equal given probability p, given size prob. words, many heads need probability p = 0.0554qbinom() - quantile function. function inverse pbinom gives x-axis value (including value) summation probabilities greater equal given probability p, given size prob. words, many heads need probability p = 0.0554Let's look functions turn little. thing keep mind probability every event likelihood occurring distribution. trying look numbers come mean us.","code":""},{"path":"revisiting-probability-distributions.html","id":"dbinom---the-density-function","chapter":"4 Revisiting Probability Distributions","heading":"4.2.4 dbinom() - The Density Function","text":"Using dbinom() function can create probabilities possible outcomes two possibilities outcome trial - e.g., heads tails, cats dogs, black red. going stick coin flip idea. showing code obtaining 3 heads 10 flips:possible outcomes heads (0:10) 10 flips:plot probability possible outcomes 10 flips look like :\nFigure 4.2: Probability Distribution Number Heads 10 Flips\ndbinom (density binom) function takes format dbinom(x, size, prob), arguments give :x number 'heads' want know probability . Either single one, 3 series 0:10.size number trials (flips) ; case, 10 flips.prob probability 'heads' one trial. chance 50-50 probability state 0.5 .5Now say wanted know probability 6 heads 10 flips. change first argument code used 3 heads, :probability 6 heads, using dbinom() p = 0.2050781. compare value data10k value 6 see similar quite . dbinom() uses lot replications 10000 used simulation.terms visualising just calculated, p = 0.2050781 height green bar plot .\nFigure 4.3: Probability Distribution Number Heads 10 Flips probability 6 10 Heads highlighted green\nQuickfire QuestionsTo three decimal places, probability 2 heads 10 flips? \nwant know probability 2 heads 10 flips.\n\nX therefore 2;\n\nSize therefore 10;\n\nprobability outcomes trial stays \n.5.\n\ndbinom(2, 10, 0.5) = .04394531 \nrounded = .044\n","code":"\ndbinom(3, 10, 0.5)\ndbinom(0:10, 10, 0.5)\ndbinom(6, 10, 0.5)## [1] 0.2050781"},{"path":"revisiting-probability-distributions.html","id":"pbinom---the-cumulative-probability-function","chapter":"4 Revisiting Probability Distributions","heading":"4.2.5 pbinom() - The Cumulative Probability Function","text":"wanted know probability including 3 heads 10 flips? asked similar questions . can either use dbinom outcome 3 heads sum results:can use pbinom() function; known cumulative probability distribution function cumulative density function. first argument give cut-value including value want know probability (3 heads). , , tell many flips want probability heads single trial.Copy line script run :probability including 3 heads 10 flips 0.172. visualization, done calculated cumulative probability lower tail distribution (lower.tail = TRUE; shown green ) cut-3:\nFigure 4.4: Probability Distribution Number Heads 10 Flips probability 0 3 Heads highlighted green - lower.tail = TRUE\npbinom function gives us cumulative probability outcomes including cut-. wanted know probability outcomes including certain value? Say want know probability 7 heads 10 coin flips. code :explain code little.First, switch lower.tail call TRUE FALSE tell pbinom() want lower tail distribution time (left including cut-), want upper tail, right cut-. results cumulative probability upper tail distribution cut-value (shown green ).Next specify cut-instead stating 7 might expect, even though want 7 , specify cut-6 heads. ? set cut-'6' working discrete distribution, lower.tail = TRUE includes cut-(6 ) whereas lower.tail = FALSE everything cut-including cut-(7 ).short, want upper tail using discrete distribution set cut-value (x) one lower number interested . wanted know 7 heads set cut-6.\nFigure 4.5: Probability Distribution Number Heads 10 Flips probability 7 Heads highlighted green - lower.tail = FALSE\n\nconfusing part people find concept \nlower.tail. look distribution, say \nbinomial, find lot high bars middle \ndistribution smaller bars far left right \ndistribution. Well far left right distribution called\ntail distribution - tend \nextremity distribution taper like …..well like tail.\nlot time talk left right tails \npbinom() function ever considers data relation \nleft side distribution - calls \nlower.tail.\n\nLet’s consider lower.tail = TRUE. default,\ndon’t state lower.tail = ... \nconsidered want. lower.tail = TRUE means \nvalues left value including \nvalue state. binomial distribution say give \nprobability 5 heads less, set\nlower.tail = TRUE counting summing \nprobability 0, 1, 2, 3, 4 5 heads. can check \ndbinom(0:5, 10, .5) %>% sum().\n\nHowever, say give probability 7 heads, \nneed lower.tail = FALSE, consider \nright-hand side tail, also, need set code \npbinom(6, 10, .5, lower.tail = FALSE). 6 7?\npbinom() function, \nlower.tail = FALSE, starts value plus one \nvalue state; always considers value state \npart lower.tail say 6, includes 6\nlower.tail gives 7, 8, 9 10 \nupper tail. said 7 lower.tail = FALSE, \ngive 8, 9 10. tricky worth keeping \nmind using pbinom() function. \nremember, can always check using\ndbinom(7:10, 10, .5) %>% sum() seeing whether \nmatches pbinom(6, 10, 0.5, lower.tail=FALSE) \npbinom(7, 10, 0.5, lower.tail=FALSE)\nQuickfire QuestionsUsing format shown pbinom() function, enter code determine probability including 5 heads 20 flips, assuming probability 0.5: Using format shown pbinom() function, enter code determine probability including 5 heads 20 flips, assuming probability 0.5: two decimal places, probability obtaining including 50 heads 100 flips? two decimal places, probability obtaining including 50 heads 100 flips? \n\nlooking calcuate probability 5 less heads\n(x) 20 flips (size), \nprobability ‘heads’ one trial (prob) remaining \n. need lower.tail call \ncalculating cumulative probability lower tail \ndistribution?\n\n\nlooking calcuate probability 5 less heads\n(x) 20 flips (size), \nprobability ‘heads’ one trial (prob) remaining \n. need lower.tail call \ncalculating cumulative probability lower tail \ndistribution?\n\n\nlooking calculate probability 51 heads\n(x), 100 flips (size), \nprobability ‘heads’ one trial (prob) remaining \n(0.5). need lower.tail call\ncalculating cumulative probability upper tail\ndistribution? Remember, looking \nlower.tail, value heads enter \npbinom() included final\ncalculation, e.g. entering\npbinom(3, 100, lower.tail = FALSE) give \nprobability 4 heads. instead looking \nlower.tail, entering\npbinom(3, 100, lower.tail = TRUE) give \nprobability 3 heads.\n\n\nlooking calculate probability 51 heads\n(x), 100 flips (size), \nprobability ‘heads’ one trial (prob) remaining \n(0.5). need lower.tail call\ncalculating cumulative probability upper tail\ndistribution? Remember, looking \nlower.tail, value heads enter \npbinom() included final\ncalculation, e.g. entering\npbinom(3, 100, lower.tail = FALSE) give \nprobability 4 heads. instead looking \nlower.tail, entering\npbinom(3, 100, lower.tail = TRUE) give \nprobability 3 heads.\n\n\ncode first one :\npbinom(5, 20, 0.5) \npbinom(5, 20, 0.5, lower.tail = TRUE)\n\n\ncode first one :\npbinom(5, 20, 0.5) \npbinom(5, 20, 0.5, lower.tail = TRUE)\n\n\ncode second one :\npbinom(50, 100, 0.5, lower.tail = FALSE), giving answer\n.46. Remember can confirm :\ndbinom(51:100, 100, 0.5) %>% sum()\n\n\ncode second one :\npbinom(50, 100, 0.5, lower.tail = FALSE), giving answer\n.46. Remember can confirm :\ndbinom(51:100, 100, 0.5) %>% sum()\n","code":"\ndbinom(0:3, 10, 0.5) %>% sum()## [1] 0.171875\npbinom(3, 10, 0.5, lower.tail = TRUE)  ## [1] 0.171875\npbinom(6, 10, 0.5, lower.tail = FALSE) ## [1] 0.171875"},{"path":"revisiting-probability-distributions.html","id":"qbinom---the-quantile-function","chapter":"4 Revisiting Probability Distributions","heading":"4.2.6 qbinom() - The Quantile Function","text":"qbinom() function inverse pbinom() function. Whereas pbinom() supply outcome value x get tail probability, qbinom() supply tail probability get outcome value (approximately) cuts tail probability. Think rephrase questions pbinom() ask qbinom() question. Worth noting though qbinom() approximate discrete distribution \"jumps\" probability discrete outcomes (.e. can probability 2 heads 3 heads 2.5 heads).see two functions inverses one another. code probability 49 heads less 100 coin flipsThis tells us probability p = 0.4602054. now put probability (stored p1) qbinom get back started. E.g.can stated number heads required obtain probability 0.4602054 49. qbinom() function useful want know minimum number successes (‘heads’) needed achieve particular probability. time cut-specify number ‘heads’ want probability want.example say want know minimum number ‘heads’ 10 flips result 5% heads success rate (probability .05), use following code.dealing lower tail, telling us lower tail probability .05, expect 2 heads 10 flips. However, important note discrete distribution, value exact. can see :exactly p = .05 close. data working discrete distribution qbinom() gives us closest category boundary.\nfound lot students asking qbinom() \nworks inputting two different probabilities \narguments qbinom(). Let us try make things\nclearer.\n\nFirst, useful remember inverse \npbinom(): pbinom() gives tail\nprobability associated x, qbinom()\ngives closest x cuts specified tail\nprobability. understand pbinom() try reversing\nquestion see helps understand\nqbinom().\n\nqbinom() function set \nqbinom(p, size, prob). First , used\nprob previous two functions, dbinom()\npbinom(), represents probability success\nsingle trial (probability ‘heads’ one coin\nflip, prob = .5). Now, prob represents \nprobability success one trial, whereas\np represents tail probability want know .\nfunction gives value x yield \nprobability asked . give qbinom() tail\nprobability p = .05, 10 flips, probability success\none flip prob = .5. tells answer 2, meaning\ngetting 2 flips 10 trials probability roughly\n.05.\n\nqbinom() also uses lower.tail argument\nworks similar fashion pbinom().\nQuickfire QuestionsType box, maximum number heads associated tail probability 10% (.1) 17 flips: \nanswer 6 code :\n\nqbinom(0.1, 17, 0.5, lower.tail = TRUE)\n\nRemember want overall probability 10% (p\n= .1), 17 flips go (size = 17), \nprobability heads one flip .5 (prob = .5). \nwant maximum number lower tail, \nlower.tail TRUE.\nKeep mind: trying get understanding every value distribution probability existing distribution. probability may large, meaning , bell-shaped distributions looked , value middle distribution, probability might rather low, meaning tail, ultimately every value distribution probability.","code":"\np1 <- pbinom(49, 100, .5, lower.tail = TRUE)\np1## [1] 0.4602054\nqbinom(p1, 100, .5, lower.tail = TRUE)## [1] 49\nqbinom(.05, 10, 0.5, lower.tail = TRUE) ## [1] 2\npbinom(2, 10, .5, lower.tail = TRUE) ## [1] 0.0546875"},{"path":"revisiting-probability-distributions.html","id":"continuous-data-and-normal-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.3 Continuous Data and Normal Distribution","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"continuous-data-properties","chapter":"4 Revisiting Probability Distributions","heading":"4.3.1 Continuous Data Properties","text":"previous section, seen can use distribution estimate probabilities determine cut-values (play important part hypothesis testing later chapters), looked discrete binomial distribution. Many variables encounter continuous tend show normal distribution (e.g., height, weight, IQ).say interested height population psychology students, estimate 146cm 194cm. plotted continuous, normal distribution, look like:\nFigure 4.6: Normal Distribution height Psychology students (black line). Green line represents mean. Blue line represent 1 Standard Deviation mean. Yellow line represents 2 Standard Deviation mean. Red line represents 3 Standard Deviation mean.\nfigure shows hypothetical probability density heights ranging 146cm 194cm population Psychology students (black curve). data normally distributed following properties:Properties Normal distribution1. distribution defined mean standard deviation: mean (\\(\\mu\\)) describes center, therefore peak density, distribution. largest number people population terms height. standard deviation (\\(\\sigma\\)) describes much variation mean distribution - figure, standard deviation distance mean inflection point curve (part curve changes upside-bowl shape right-side-bowl shape).2. Distribution symmetrical around mean: mean lies middle distribution divides area curve two equal sections - get typical bell-shaped curve.3. Total area curve equal 1: add probabilities (densities) every possible height, end value 1.4. mean, median mode equal: good way check given dataset normally distributed calculate measure central tendency see approximately (normal distribution) (skewed distribution).5. curve approaches, never touches, x axis: never probability 0 given x axis value.6. normal distribution follows Empirical Rule: Empirical Rule states 99.7% data within normal distribution falls within three standard deviations (\\(\\pm3\\sigma\\)) mean, 95% falls within two standard deviations (\\(\\pm2\\sigma\\)), 68% falls within one standard deviation (\\(\\pm\\sigma\\)).Continuous data can take precise specific value scale, e.g. 1.1, 1.2, 1.11, 1.111, 1.11111. Many variables encounter Psychology :continuous opposed discrete.tend show normal distribution.look similar - bell-shaped curve - plotted.","code":""},{"path":"revisiting-probability-distributions.html","id":"estimating-from-the-normal-distribution","chapter":"4 Revisiting Probability Distributions","heading":"4.3.2 Estimating from the Normal Distribution","text":"Unlike coin flips, outcome normal distribution just 50/50 ask create normal distribution complicated binomial distribution estimated previous section. Instead, just binomial distribution (distributions) functions allow us estimate normal distribution ask questions distribution. :dnorm() - Density function normal distributionpnorm() - Cumulative Probability function normal distributionqnorm() - Quantile function normal distributionYou might thinking look familiar. fact work similar way binomial counterparts. unsure function works remember can call help typing console, example, ?dnorm ?dnorm().\nQuickfire QuestionsType box binomial counterpart dnorm()? Type box binomial counterpart dnorm()? Type box binomial counterpart pnorm()? Type box binomial counterpart pnorm()? Type box binomial counterpart qnorm()? Type box binomial counterpart qnorm()? \ncounterpart functions start letter, d, p, q, \njust distribution name changes, binom,\nnorm, t - though haven’t quite come across\nt-distribution yet.\n\n\ndbinom() binomial equivalent \ndnorm()\n\n\ndbinom() binomial equivalent \ndnorm()\n\n\npbinom() binomial equivalent \npnorm()\n\n\npbinom() binomial equivalent \npnorm()\n\n\nqbinom() binomial equivalent \nqnorm()\n\n\nqbinom() binomial equivalent \nqnorm()\n\nalso rnorm() rbinom() \nlook another time.\n","code":""},{"path":"revisiting-probability-distributions.html","id":"dnorm---the-density-function","chapter":"4 Revisiting Probability Distributions","heading":"4.3.3 dnorm() - The Density Function","text":"Using dnorm(), like dbinom, can plot normal distribution. time however need:x, vector quantiles (words, series values x-axis - think max min distribution want plot)mean dataand standard deviation sd data.use IQ example. actually disagreement whether IQ continuous data degree depend measurement use. IQ however definitely normally distributed assume continuous purposes demonstration. Many Psychologists interested studying IQ, perhaps terms heritability, interested controlling IQ studies rule effect (e.g., clinical autism studies).","code":""},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT1","chapter":"4 Revisiting Probability Distributions","heading":"4.3.3.1 Task 1: Standard Deviations and IQ Score Distribution","text":"Copy code new script run . Remember need call tidyverse library first.code creates plot showing normal distribution IQ scores (M = 100, SD = 15) ranging 40 160. values considered typical general population.First set range IQ values 40 160Then plot distribution IQ_data, M = 100 SD = 15\nFigure 4.7: Distribution IQ scores mean = 100, sd = 15\npart code need change alter SD plot? mean = 100sd = 15(40, 160)Now copy edit code plot distribution mean = 100 sd = 10, visually compare two plots.\nThinking Cap PointWhat changing standard deviation (sd) shape distribution? Spend minutes changing code various values running , discussing group answer following questions:happens shape distribution change sd 10 20? distribution gets narrowerthe distribution gets widerWhat happens shape distribution change sd 10 20? distribution gets narrowerthe distribution gets widerWhat happens shape distribution change sd 10 5? distribution gets narrowerthe distribution gets widerWhat happens shape distribution change sd 10 5? distribution gets narrowerthe distribution gets widerWhat small large standard deviation sample tell data collected?small large standard deviation sample tell data collected?\nChanging SD 10 20 means larger standard deviation \nwider distribution.\n\nChanging SD 10 5 means smaller standard deviation \nnarrower distribution.\n\nSmaller SD results narrower distribution meaning data\nless spread ; larger SD results wider distribution meaning\ndata spread .\n\nnote Standard Deviation:\n\nknow lectures can estimate data two\nways: point-estimates spread estimates. mean point-estimate\ncondenses data one data point - tells \naverage value data tells nothing spread\ndata . standard deviation however spread estimate \ngives estimate spread data mean - \nmeasure standard deviation mean.\n\nimagine looking IQ scores test 100 people \nget mean 100 SD 5. means vast majority \nsample IQ around 100 - probably fall within\n1 SD mean, meaning participants IQ\n95 105.\n\nNow test find mean 100 SD 20, \nmeans data much spread . take 1 SD approach\nparticipants IQ 80 \n120.\n\none sample tight range IQs sample \nwide range IQs. , point-estimate spread\nestimate data can tell shape sample\ndistribution.\n\nfar good! example told dnorm() values limit range rest; said give us range 40 160 IQ scores. However, plot another way telling dnorm() sequence, range, values want much precision want .","code":"\nIQ_data <- tibble(IQ_range = c(40, 160))\n\nggplot(IQ_data, aes(IQ_range)) + \n  stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n  labs(x = \"IQ Score\", y = \"probability\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT2","chapter":"4 Revisiting Probability Distributions","heading":"4.3.3.2 Task 2: Changing Range and Step Size of The Normal Distribution","text":"Copy code script run .\nplot standard Normal Distribution -4 4 steps 0.01. also stated mean 0 sd 1.\nFigure 4.8: Normal Distribution Mean = 0 SD = 1\nQuickfire QuestionsFill box show type create tibble containing column called ND_range values ranging -10 10 steps .05:ND_data <- Now know change, try plotting normal distribution following attributes:range -10 10 steps 0.05,range -10 10 steps 0.05,mean 0,mean 0,standard deviation 1.standard deviation 1.Compare new plot original one created. change distribution? Distribution widensNo change distributionDistribution narrowsCompare new plot original one created. change distribution? Distribution widensNo change distributionDistribution narrows\nchange distribution write:\nND_data <- tibble(ND_range = seq(-10, 10, 0.05))\n\nHowever, comparing plots, whilst plot may look\nthinner, distribution changed. change appearance \ndue range sd values extended \n-4 4 -10 10. density values within values \nchanged however see, clearly second plot,\nvalues beyond -3 3 unlikely.\nRemember, every value probability distribution able use dnorm() function get visual representation probability values change normal distribution. values probable. values less probable. key concept comes thinking significant differences later.However, know, one important difference continuous discrete probability distributions - number possible outcomes. discrete probability distributions usually finite number outcomes take probability. instance, 5 coin flips, 5 possible outcomes number heads: 0, 1, 2, 3, 4, 5. binomial distribution exact finite outcomes, can use dbinom() get exact probability outcome.contrast, truly continuous variable, number possible outcomes infinite, 0.01 also .0000001 .00000000001 arbitrary levels precision. rather asking probability single value, ask probability range values, equal area curve (black line plots ) range values., leave dnorm() now move onto looking establishing probability range values using Cumulative Probability function","code":"\nND_data <- tibble(ND_range = seq(-4, 4, 0.01))\nggplot(ND_data, aes(ND_range)) + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  labs(x = \"SD units\", y = \"probability\", title = \"The Normal Distribution\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"pnorm---the-cumulative-probability-function","chapter":"4 Revisiting Probability Distributions","heading":"4.3.4 pnorm() - The Cumulative Probability Function","text":"Just dnorm() works like dbinom(), pnorm() works just like pbinom(). , pnorm(), given mean sd data, returns cumulative density function (cumulative probability) given probability (p) lies specified cut-point , unless course lower.tail = FALSE specified case cut-point .OK, English people can understand, means pnorm() function tells probability obtaining given value lower set lower.tail = TRUE. Contrastingly, pnorm() function tells probability obtaining given value higher set lower.tail = FALSE.use height give concrete example. Say test sample students (M = 170cm, SD = 7) want calculate probability given student 150cm shorter following:Remember, lower.tail = TRUE means lower including value XTRUE default actually need declare itThis tells us finding probability someone 150cm shorter class p = 0.0021374. Stated differently, expect proportion students 150cm shorter 0.21% (can convert probability proportion multiplying probability 100). small probability suggests pretty unlikely find someone shorter 150cm class. mainly small standard deviation distribution . Think back said earlier narrow standard deviations round mean!Another example might , probability given student 195 cm taller? , set following code:tells us finding probability someone 195cm taller class 0.02%. , really unlikely.notice something different cut-example using dbinom() function looking cut-? might ? discuss second first quick task.","code":"\npnorm(150, 170, 7, lower.tail = TRUE)\npnorm(195, 170, 7, lower.tail = FALSE)"},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT3","chapter":"4 Revisiting Probability Distributions","heading":"4.3.4.1 Task 3: Calculating Cumulative Probability of Height","text":"Edit pnorm() code calculate probability given student 190cm taller.three decimal places, Task 3, probability student 190cm taller class? \nanswer .002. See solution code end \nchapter.\n\nkey thing difference need \nspecify cut-point pbinom() (discussed \npreclass activity) pnorm() functions values\nx, .e. \nlower.tail = FALSE.\n\ndiscrete data, say number coin flips result \nheads, wanted calculate probability \nx, apply pbinom() \nspecify cut-point x-1 \ninclude x calculation. example, calculate\nprobability 4 ‘heads’ occuring 10 coin flips, \nspecify pbinom(3, 10, 0.5, lower.tail = FALSE) \nlower.tail includes value state.\n\ncontinuous data, however, height, applying\npnorm() therefore can specify cut-point simply\nx. example, \ncut-point 190, mean 170 standard deviation 7, can\nwrite pnorm(190, 170, 7, lower.tail = FALSE). way \nthink setting x 189 continuous\nscale, want values greater 190, also\ninclude possible values 189 190. Setting\nx 190 starts 190.0000000…001.\n\ntricky difference pbinom() \npnorm() recall easily, best include explanation\npoint portfolio help carry correct analyses \nfuture!\n","code":""},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT4","chapter":"4 Revisiting Probability Distributions","heading":"4.3.4.2 Task 4: Using Figures to Calculate Cumulative Probability","text":"look distribution :\nFigure 4.9: Normal Distribution Height probability people 185cm highlighted purple, mean = 170cm SD = 7\nUsing information figure, mean SD , calculate probability associated shaded area.\nalready mean standard deviations input \npnorm(), look shaded area obtain cut-\npoint. lower.tail call set according\nshaded area?\nQuickfire QuestionTo three decimal places, cumulative probability shaded area Task 4? \nanswer p = .016. See solution code end \nchapter correct code.\n\nRemember, lower.tail set FALSE want \narea right.\npnorm() great telling us probability obtaining specific value greater distribution, given mean standard deviation distribution. significance come clearer coming chapters key point mind progress understanding analyses. leave now look last function normal distribution.","code":""},{"path":"revisiting-probability-distributions.html","id":"qnorm---the-quantile-function","chapter":"4 Revisiting Probability Distributions","heading":"4.3.5 qnorm() - The Quantile Function","text":"Using qnorm() can inverse pnorm(), instead finding cumulative probability given set values (cut-value), can find cut-value given desired probability. example, can use qnorm() function ask maximum IQ person bottom 10% IQ distribution (M = 100 & SD = 15)?Note: first need convert 10% probability dividing 10010% = 10 / 100 = 0.1.anyone IQ 80.8 lower bottom 10% distribution. rephrase , person bottom 10% distribution max IQ value 80.8.recap, calculated inverse cumulative density function (inverse cumulative probability) lower tail distribution, cut-probability 0.1 (10%), illustrated purple :\nFigure 4.10: Normal Distribution Height bottom 10% heights highlighted purple\n, English people can understand, means qnorm() function tells maximum value person can maintain given probability set lower.tail = TRUE. Contrastingly, pnorm() function tells minimum value person can maintain given probability set lower.tail = FALSE.","code":"\nqnorm(0.1, 100, 15) "},{"path":"revisiting-probability-distributions.html","id":"Ch4InClassQueT5","chapter":"4 Revisiting Probability Distributions","heading":"4.3.5.1 Task 5: Using pnorm() and qnorm() to find probability and cut-off values","text":"Calculate lowest IQ score student must top 5% distribution.Calculate lowest IQ score student must top 5% distribution.challenging: Using appropriate normal distribution function, calculate probability given student IQ 105 110, normal distribution mean = 100, sd = 15.challenging: Using appropriate normal distribution function, calculate probability given student IQ 105 110, normal distribution mean = 100, sd = 15.\nPart 1: Remember include lower.tail call \nrequired! unsure, visualise trying find\n(.e. lowest IQ score can top 5%) sketching \nnormal distribution curve. may help reverse question\nsound like previous example.\n\nPart 2: second part, function, necessarily\nqnorm(), gives one value, looking \nseparate calculation IQ. combine two\nvalues, summing subtracting ? less\nlikely students IQ falls range \ncut-? try sketching trying \nachieve.\nQuickfire QuestionsTo one decimal place, enter answer Task 5 part 1: lowest IQ score student must top 5% distribution? one decimal place, enter answer Task 5 part 1: lowest IQ score student must top 5% distribution? two decimal places, enter answer Task 5 part 2: probability student IQ 105 110, normal distribution mean = 100, sd = 15? two decimal places, enter answer Task 5 part 2: probability student IQ 105 110, normal distribution mean = 100, sd = 15? \n\nquestion can rephrased value give 95% \ndistribution - answer 124.7. See solution code\nTask 5 Question 1 end chapter.\n\n\nquestion can rephrased value give 95% \ndistribution - answer 124.7. See solution code\nTask 5 Question 1 end chapter.\n\n\nuse pnorm() establish probability\nIQ 110. use pnorm() \nestablish probability IQ 105. answer difference\ntwo probabilities p = .12. See solution\ncode Task 5 Question 2 end chapter.\n\n\nuse pnorm() establish probability\nIQ 110. use pnorm() \nestablish probability IQ 105. answer difference\ntwo probabilities p = .12. See solution\ncode Task 5 Question 2 end chapter.\n","code":""},{"path":"revisiting-probability-distributions.html","id":"practice-your-skills-4","chapter":"4 Revisiting Probability Distributions","heading":"4.4 Practice Your Skills","text":"order complete exercise, first download assignment .Rmd file need edit - titled GUID_Ch4_PracticeSkills_Probabilities.Rmd. can downloaded within zip file link . downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top. Download Assignment .zip file .Now open assignment .Rmd file within RStudio. see code chunk 10 tasks. Follow instructions edit code chunk. often entering code single value based skills learnt current chapter well previous chapters.","code":""},{"path":"revisiting-probability-distributions.html","id":"topic-probabilities","chapter":"4 Revisiting Probability Distributions","heading":"4.4.1 Topic: Probabilities","text":"recapped expanded understanding probability, including number binom norm functions well basic ideas probability. need skills complete following exercises.starting, checkThe .Rmd file saved folder computer access manually set folder working directory. assessments ask save format GUID_Ch4_PracticeSkills_Probabilities.Rmd GUID replaced GUID. Though practice exercise may good practice .Note: complete code chunks replacing NULL (except library chunk appropriate code just entered). answers require coding. require code, can enter answer either code, mathematical notation, actual single value. Pay attention number decimal places required.","code":""},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueLib","chapter":"4 Revisiting Probability Distributions","heading":"4.4.2 Load in the Library","text":"good chance need tidyverse library point exercise load code chunk :Basic probability binomial distribution questionsBackground Information: conducting auditory discrimination experiment participants listen series sounds determine whether sound human . trial participants hear one brief sound (100 ms) must report whether sound human (coded 1) (coded 0). sounds either: person, animal, vehicle, tone, type sound equally likely appear.","code":"\n# hint: something to do with library() and tidyverse"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT1","chapter":"4 Revisiting Probability Distributions","heading":"4.4.3 Task 1","text":"single trial, probability sound person? Replace NULL t1 code chunk either mathematical notation single value. entering single value, give answer two decimal places.","code":"\nt1 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT2","chapter":"4 Revisiting Probability Distributions","heading":"4.4.4 Task 2","text":"sequence 4 trials, one trial sound, sampled replacement, probability following sequence sounds: animal, animal, vehicle, tone? Replace NULL t2 code chunk either mathematical notation single value. entering single value, give answer three decimal places.","code":"\nt2 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT3","chapter":"4 Revisiting Probability Distributions","heading":"4.4.5 Task 3","text":"sequence four trials, one trial sound, without replacement, probability following sequence sounds: person, tone, animal, person? Replace NULL t3 code chunk either mathematical notation single value.","code":"\nt3 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT4","chapter":"4 Revisiting Probability Distributions","heading":"4.4.6 Task 4","text":"Replace NULL code, using appropriate binomial distribution function, determine probability hearing exactly 17 'tone' trials sequence 100 trials. Assume probability tone single trial 1 4. Store output t4.","code":"\nt4 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT5","chapter":"4 Revisiting Probability Distributions","heading":"4.4.7 Task 5","text":"Replace NULL code using appropriate binomial distribution function determine probability hearing 30 'vehicle' trials sequence 100 trials. Assume probability vehicle trial one trial 1 4. Store output t5.\nHint: want upper lower tails distribution?","code":"\nt5 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT6","chapter":"4 Revisiting Probability Distributions","heading":"4.4.8 Task 6","text":"block experiment contained 100 trials, enter line code run 10000 replications one block, summing many living sounds heard replication. Code 1 living sounds (person/animal) 0 non living sounds (vehicle/tone) assume probability living sound given trial \\(p = .5\\).Normal Distribution QuestionsPreviously, Chapter 2, looked ageing research project investigating differences visual processing speed younger (M = 22 years) older adults (M = 71 years). One check experiment, prior analysis, make sure older participants show signs mild cognitive impairment (early symptoms Alzheimer's disease). , carry battery cognitive tests screen symptoms. One tests D2 test attention target cancellation task (.e., participants cross letter d's two dashes line letters). designed test peoples' selective sustained attention visual scanning speed. results test give single score Concentration Performance participant. key piece information analysis distributions D2 test scores typically normally distributed (M = 100, SD = 10).","code":"\nt6 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT7","chapter":"4 Revisiting Probability Distributions","heading":"4.4.9 Task 7","text":"Replace NULL code using appropriate function determine probability given participant D2 score 90 lower? Store output t7","code":"\nt7 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT8","chapter":"4 Revisiting Probability Distributions","heading":"4.4.10 Task 8","text":"Replace NULL code using appropriate function determine probability given participant D2 score 120 ? Store output t8","code":"\nt8 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT9","chapter":"4 Revisiting Probability Distributions","heading":"4.4.11 Task 9","text":"Replace NULL code using appropriate function(s) determine difference scores cut top 5% bottom 5% distribution? Store output t9.","code":"\nt9 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"Ch4AssignQueT10","chapter":"4 Revisiting Probability Distributions","heading":"4.4.12 Task 10","text":"Finally, participant says worried heard Concentration Performance bottom 2% scores distribution, maximum D2 score can ? Replace NULL single value two decimal places. enter code. Store t10Job Done - Activity Complete!Well done, finished! Now go check answers solutions end chapter. looking check questions looking single value answer , questions asking code code give answer alternative variations code allowed (e.g., including lower.tail = TRUE including default). Remember single value coded answer spelling matters, replicate() replicat(). alternative answers, means submitted one options return answer.Lastly, keep mind main point probability, interested determining probability given value distribution! .","code":"\nt10 <- NULL"},{"path":"revisiting-probability-distributions.html","id":"solutions-to-questions-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"revisiting-probability-distributions.html","id":"continuous-data-and-normal-distribution-tasks","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1 Continuous Data and Normal Distribution Tasks","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"task-1-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.1 Task 1","text":"First set range IQ values 40 160Then plot distribution IQ_data, M = 100 SD = 10\nFigure 4.11: Distribution IQ scores mean = 100, sd = 10\nReturn Task","code":"\nlibrary(tidyverse)\n\nIQ_data <- tibble(IQ_range = c(40, 160))\n\nggplot(IQ_data, aes(IQ_range)) + \n  stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n  labs(x = \"IQ Score\", y = \"probability\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"task-2-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.2 Task 2","text":"\nFigure 4.12: Normal Distribution shown scale -10 10, mean = 0, sd = 1\nReturn Task","code":"\nND_data <- tibble(ND_range = seq(-10,10,0.05))\n\nggplot(ND_data, aes(ND_range)) + \n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  labs(x = \"SD units\", y = \"probability\", title = \"The Normal Distribution\") +\n  theme_classic()"},{"path":"revisiting-probability-distributions.html","id":"task-3-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.3 Task 3","text":"Key thing set lower.tail FALSE calculate area. using pnorm() state actual cut-even using lower.tail = FALSE. using pbinom() state cut-minus one using lower.tail = FALSEReturn Task","code":"\npnorm(190, 170, 7, lower.tail = FALSE)## [1] 0.002137367"},{"path":"revisiting-probability-distributions.html","id":"task-4-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.4 Task 4","text":"highlighted area 185cm .Key thing set lower.tail FALSE calculate area cut-.Return Task","code":"\npnorm(185, 170, 7, lower.tail = FALSE)## [1] 0.01606229"},{"path":"revisiting-probability-distributions.html","id":"task-5-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.1.5 Task 5","text":"Question 1 - lowest IQ score student must top 5% distribution.Question 2 - calculate probability given student IQ 105 110, normal distribution mean = 100, sd = 15.Return Task","code":"\nqnorm(0.95, 100, 15, lower.tail = TRUE)## [1] 124.6728\npnorm(105, 100, 15, lower.tail = FALSE) - \n  pnorm(110, 100, 15, lower.tail = FALSE)## [1] 0.1169488"},{"path":"revisiting-probability-distributions.html","id":"practice-your-skills-5","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2 Practice Your Skills","text":"","code":""},{"path":"revisiting-probability-distributions.html","id":"load-in-the-library","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.1 Load in the Library","text":"Return Task","code":"\nlibrary(tidyverse)"},{"path":"revisiting-probability-distributions.html","id":"task-1-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.2 Task 1","text":"probability sound person 0.75Return Task","code":"\nt1 <- 3/4\nt1 <- .75"},{"path":"revisiting-probability-distributions.html","id":"task-2-4","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.3 Task 2","text":"probability sequence sounds 0.004Return Task","code":"\nt2 <- (1/4) * (1/4) * (1/4) * (1/4)\nt2 <- .004"},{"path":"revisiting-probability-distributions.html","id":"task-3-4","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.4 Task 3","text":"probability sequence sounds 0The reason replacement repeat person trial happen.Return Task","code":"\nt3 <- (1/4) * (1/3) * (1/2) * (0/1)\nt3 <- 0"},{"path":"revisiting-probability-distributions.html","id":"task-4-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.5 Task 4","text":"Assuming probability tone given trial 1 4, probability hearing 17 'tone' trials sequence 100 trials 0.0165156Return Task","code":"\nt4 <- dbinom(17, 100, 1/4)"},{"path":"revisiting-probability-distributions.html","id":"task-5-3","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.6 Task 5","text":"answered using either pbinom() dbinom(). trick remember set cut-depending function used.scenario, probability hearing 30 'vehicle' trials sequence 100 trials 0.149541Return Task","code":"\nt5 <- pbinom(29, 100, 1/4, lower.tail = FALSE)\nt5 <- dbinom(30:100, 100, 1/4) %>% sum()"},{"path":"revisiting-probability-distributions.html","id":"task-6-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.7 Task 6","text":"appropriate code :look output see something like following. Remember numbers vary due random sampling. showing first 10 values 10000Return Task","code":"\nt6 <- replicate(10000, sample(0:1, 100, TRUE, c(.5,.5)) %>% sum())##  int [1:10000] 44 54 47 52 54 45 54 48 48 55 ..."},{"path":"revisiting-probability-distributions.html","id":"task-7-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.8 Task 7","text":"probability given participant D2 score 90 lower 0.1586553Return Task","code":"\nt7 <- pnorm(90, 100, 10, lower.tail = TRUE)"},{"path":"revisiting-probability-distributions.html","id":"task-8-2","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.9 Task 8","text":"probability given participant D2 score 120 0.0227501Return Task","code":"\nt8 <- pnorm(120, 100, 10, lower.tail = FALSE)"},{"path":"revisiting-probability-distributions.html","id":"task-9","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.10 Task 9","text":"difference scores cut top bottom 5% distribution 32.8970725Return Task","code":"\nt9 <- qnorm(.95, 100, 10) - qnorm(.05, 100, 10)"},{"path":"revisiting-probability-distributions.html","id":"task-10","chapter":"4 Revisiting Probability Distributions","heading":"4.5.2.11 Task 10","text":"maximum D2 score can situation 79.46Return TaskChapter Complete!","code":"\nt10 <- 79.46"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"nhst-binomial-test-and-one-sample-t-test","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5 NHST: Binomial test and One-Sample t-test","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"overview-5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.1 Overview","text":"chapter introduce Null Hypothesis Significance Testing (NHST): quantitative research collect sample, calculate summary statistic sample, use probability establish likelihood statistic occurring given certain situations. aim draw inferences sample population.However concepts ideas hard grasp first take playing around data times help get better understanding . , demonstrate ideas , start introducing commonly used tests approaches reproducible science, look data related sleep. study look , explore NHST , one team members makes use well known task Psychology, Posner Paradigm: Woods et al. (2009). clock focus selective attention primary insomnia: experimental study using modified Posner paradigm .chapter, activities, :Introduce testing hypothesis null hypothesis significance testing (NHST).Learn Binomial tests.Learn One-sample t-tests.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"brief-introduction-to-nhst","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.2 Brief Introduction to NHST","text":"short introduction concept NHST dive practical aspects. main idea NHST testing significant difference two values - say two means moment. null hypothesis states difference two means (groups) interest. , test difference means two groups trying determine probability finding difference size found, larger, sample using experiment, actually real difference two groups population.say ran experiment collected sample : experiment, two groups, B, calculated difference means two groups D_diff = 7.39. Putting terms Null Hypothesis (H_0) now wanting know: probability finding difference means -7.39 (larger) real difference two groups population? order test question (Null Hypothesis) need compare observed difference distribution possible differences see likely observed difference distribution - extreme values, .e., large differences groups, located tails distribution.p-value probability finding difference equal greater one found sample difference population. Thus, say p-value p = .017. indicates small probability finding difference equal greater population difference. obtained p-value also smaller standard cut-use Psychology \\(p <= .05\\). reject null hypothesis suggest significant difference two groups.following resource Daniel Lakens helpful deepening understanding p-value recommend taking look : Understanding common misconceptions p-values","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"background-of-data-sleep","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.3 Background of data: Sleep","text":"Woods colleagues (2009) interested attention people poor sleep (Primary Insomnia - PI) tuned towards sleep-related stimuli attention people normal sleep (NS). Woods et al. hypothesised participants poor sleep attentive images related lack sleep (.e., alarm clock showing 2AM) participants normal sleep . test hypothesis, authors used modified Posner paradigm, shown Figure 1 paper, images alarm clock acted cue valid invalid trials, symbol ( .. ) target.can seen Figure 3 Woods et al. found , valid trials, whilst Primary Insomnia participants faster responding target, suggesting slight increase attention sleep related cue compared Normal Sleepers, difference groups. contrast, invalid trials, poor sleep participants expected distracted cue, authors indeed find significant difference groups consistent alternative hypothesis \\(H_{1}\\). Woods et al., concluded poor sleepers (Primary Insomnia participants) slower respond target invalid trials, compared Normal Sleepers, due attention Primary Insomnia participants drawn misleading cue (alarm clock) invalid trials. increased attention sleep-related cue led overall slower response target invalid trials.Now, imagine looking replicate finding Woods et al. (2009). pilot study, test recruitment procedures, gather data 22 Normal Sleepers. common use control participants pilot (case NS participants) plentiful population experimental group (case PI participants) saves using participants PI group may harder obtain long run.gathering data, want check recruitment process: Whether able draw sample normal sleepers similar sample drawn Woods et al. keep things straightforward, allowing us understand analyses better, look valid trials today, NS participants, effect perform test groups conditions.Participants Normal Sleepers (NS)?data 22 participants collected pilot study. mean reaction time valid trials (milliseconds) shown right hand column, valid_rt.\nTable 5.1: Pilot Data 22 Participants Sleep-Related Posner Paradigm. ID shown participant column mean reaction time (ms) valid trails shown valid_rt column.\nlook Woods et al (2009) Figure 3 see , valid trials, mean reaction time NS participants 590 ms SD = 94 ms. , part pilot study, want confirm 22 participants gathered indeed Normal Sleepers. use mean SD Woods et al., confirm . Essentially asking participants pilot responding similar fashion NS participants original study.using NHST working null hypothesis (\\(H_{0}\\)) alternative hypothesis (\\(H_{1}\\)). Thinking experiment makes logical sense think terms null hypothesis (\\(\\mu1 = \\mu2\\)). phrase hypothesis : \"hypothesise significant difference mean reaction times valid trials modified Posner task participants pilot study participants original study Woods et al.\"actually ways test null hypothesis. Today show two : tasks 1-3 use binomial test tasks 4-8 use one-sample t-test\nBinomial test simple test \nconverts participants either cut-\npoint, e.g. mean value, looking probability finding \nnumber participants cut-.\n\nOne-sample t-test similar compares\nparticipants cut-, compares mean standard\ndeviation collected sample ideal mean standard\ndeviation. comparing difference means, divided standard\ndeviation difference (measure variance), can\ndetermine sample similar ideal mean.\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-binomial-test","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4 The Binomial Test","text":"Binomial test one \"basic tests\" null hypothesis testing uses little information. binomial test used study two possible outcomes (success failure) idea probability success . sound familiar work Chapter 4 Binomial distribution.Binomial test tests observed result different expected. example, number heads series coin flips different expected. case chapter, want test whether normal sleepers giving reaction times different measured Woods et al. following tasks take process.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4.1 Task 1: Creating a Dataframe","text":"First need create tibble data can work .Enter data 22 participants displayed tibble store ns_data. one column showing participant number (called participant) another column showing mean reaction time (called valid_rt).ns_data <- tibble(participant = c(NULL,NULL,...), valid_rt = c(NULL,NULL,...))type value copy paste hint .\ncan use code structure replace NULL values:\n\nns_data <- tibble(participant = c(NULL,NULL,...), valid_rt = c(NULL,NULL,...))\n\nvalues : 631.2, 800.8, 595.4, 502.6, 604.5, 516.9, 658.0,\n502.0, 496.7, 600.3, 714.6, 623.7, 634.5, 724.9, 815.7, 456.9, 703.4,\n647.5, 657.9, 613.2, 585.4, 674.1\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT2","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4.2 Task 2: Comparing Original and New Sample Reaction Times","text":"next step establish many participants pilot study mean original study Woods et al.original study mean reaction time valid trials 590 ms. Store value woods_mean.Now write code calculate number participants new sample (ns_data created Task 1) mean reaction time greater original paper's mean. Store single value n_participants.function nrow() may help .nrow() similar count() n(), nrow() returns number single value tibble.sure whatever method use end single value, tibble. may need use pull() pluck().\nPart 1\n\nwoods_mean <- value\n\nPart 2\n\nways achieve . couple try\n\nns_data %>% filter(x ? y) %>% count() %>% pull(?)\n\n\n\nns_data %>% filter(x ? y) %>% summarise(n = ?) %>% pull(?)\n\n\n\nns_data %>% filter(x ? y) %>% nrow()\n\n\n\ndim[] %>% pluck()\nQuickfire QuestionsThe number participants mean reaction time valid trials greater original paper : 6101616","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.4.3 Task 3: Calculating Probability","text":"final step binomial test compare value Task 2, 16 participants, hypothetical cut-. work assumption mean reaction time original paper, .e. 590 ms, good estimate population good sleepers (NS). true new participant tested .5 chance mean reaction time (\\(p = .5\\) participant).phrase another way, expected number participants cut-\\(.5 \\times N\\), \\(N\\) number participants, \\(.5 \\times 22\\) = 11 participants.\n* Calculate probability observing least 16 participants 22 participants valid_rt greater Woods et al (2009) mean value.\n* hint: looked similar questions Chapter 4 using dbinom() pbinom()\n* hint: key thing asking obtaining X successes. need think back cut-offs lower.tails.\nThink back Chapter 4 used binomial distribution. \nquestion can phrased , probability obtaining X \nsuccesses Y trials, given expected probability Z.\n\nmany Xs? (see question)\n\nmany Ys? (see question)\n\nprobability either \nmean/cut-? (see question)\n\ncan use dbinom() %>% sum() maybe \npbinom()\nQuickfire QuestionsUsing Psychology standard \\(\\alpha = .05\\), think NS participants responding similar fashion participants original paper? Select appropriate answer: NoYesAccording Binomial test accept reject null hypothesis set start test? RejectAccept\nprobability obtaining 16 participants mean reaction\ntime greater cut-590 ms p = .026. smaller\nfield norm p = .05. can say , using \nbinomial test, new sample appears significantly different \nold sample significantly larger number participants\ncut-(M = 590ms) expected new sample\nold sample responding similar fashion. \ntherefore reject null hypothesis!\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-one-sample-t-test","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5 The One-Sample t-test","text":"binomial test null hypothesis testing suggested significant difference mean reaction times valid trials modified Posner task participants pilot study participants original study Woods et al. However, binomial test use available information data participant simply classified mean original paper, .e. yes . Information magnitude discrepancy mean discarded. information really interesting important however wanted maintain information need use One-sample \\(t\\)-test.One-sample \\(t\\)-test, test null hypothesis \\(H_0: \\mu = \\mu_0\\) :\\(H_0\\) symbol null hypothesis,\\(\\mu\\) (pronounced mu - like m) unobserved population mean. unobserved true mean possible participants. know value. best guess mean sample 22 participants use mean . substitute value formula, call \\(\\bar{X}\\) (pronounced X-bar), instead \\(\\mu\\).\\(\\mu_0\\) (mu-zero) mean compare (alternative population sample mean constant). us mean original paper observed 590 ms.calculating test statistic \\(t\\) comes \\(t\\)-distribution - distribution lectures. formula calculate observed test statistic \\(t\\) one-sample \\(t\\)-test :\\[t = \\frac{\\mu - \\mu_0}{s\\ / \\sqrt(n)}\\]\\(s\\) standard deviation sample collected,\\(n\\) number participants sample., testing null hypothesis \\(H_0: \\bar{X} =\\) 590. formula one-sample \\(t\\)-test becomes:\\[t = \\frac{\\bar{X} - \\mu_0}{s\\ / \\sqrt(n)}\\]Now just need fill numbers.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.1 Task 4: Calculating the Mean and Standard Deviation","text":"Calculate mean standard deviation valid_rt 22 participants (.e., participant data).Store mean ns_data_mean store standard deviation ns_data_sd. Make sure store single values!\ncode, replace NULL code find \nmean, m, ns_data.\n\nns_data_mean <- summarise(NULL) %>% pull(NULL)\n\nReplace NULL code find standard deviation,\nsd, ns_data.\n\nns_data_sd <- summarise(NULL) %>% pull(NULL)\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.2 Task 5: Calculating the Observed Test Statistic","text":"Task 4, found \\(\\bar{X}\\), sample mean, 625.464 ms, \\(s\\), sample standard deviation, 94.307 ms. Now, keeping mind \\(n\\) number observations/participants sample, \\(\\mu_0\\) mean Woods et al. (2009):Use One-sample t-test formula compute observed test statistic. Store answer t_obs .t_obs <- (x - y)/(s/sqrt(n))Quickfire QuestionsAnswering question help task also need numbers substitute formula:mean Woods et al. (2009) 595590580585, number participants sample : (type numbers) .Remember solutions end chapter stuck. check correct without looking solutions though - observed \\(t\\)-value t_obs, two decimal places, 1.661.761.861.96\nRemember BODMAS /PEDMAS given one operation \ncalculate. (.e. Brackets/Parenthesis, Orders/Exponents, Division,\nMultiplication, Addition, Subtraction)\n\nt_obs <- (sample mean - woods mean) / (sample standard deviation / square root n)\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT6","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.3 Task 6: Comparing the Observed Test Statistic to the t-distribution using pt()","text":"Now need compare t_obs t-distribution determine likely observation (.e. test statistic) null hypothesis difference. need use pt() function.Use pt() function get \\(p\\)-value two-tailed test \\(\\alpha\\) level set .05. test \\(n - 1\\) degrees freedom, \\(n\\) number observations contributing sample mean \\(\\bar{X}\\). Store \\(p\\) value variable pval.reject null?Hint: pt() function works similar pbinom() pnorm().Hint: want p-value two-tailed test, multiply pt() two.\nRemember get help can enter ?pt \nconsole.\n\npt() function works similar pbinom()\npnorm():\n\npval <- pt(test statistic, df, lower.tail = FALSE) * 2\n\nUse absolute value test statistic;\n.e. ignore minus signs.\n\nRemember, df equal n-1.\n\nUse lower.tail = FALSE wanting know\nprobability obtaining value higher one got.\n\nReject null field standard p < .05\n","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT7","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.4 Task 7: Comparing the Observed Test Statistic to the t-distribution using t.test()","text":"Now done hand, try using t.test() function get result. Take moment read documentation function typing ?t.test console window. need store t-test output dataframe, check p-value matches pval Task 6.structure t.test() function t.test(column_of_data, mu = mean_to_compare_against)\nfunction requires vector, table, first argument.\ncan use pull() function pull \nvalid_rt column tibble ns_data \npull(ns_data, valid_rt).\n\nalso need include mu \nt.test(), mu equal mean \ncomparing .\nQuickfire QuestionsTo make sure understanding output t-test, try answer following questions.three decimal places, type p-value t-test Task 7 three decimal places, type p-value t-test Task 7 One-sample t-test significantnot significantAs One-sample t-test significantnot significantThe outcome binomial test one sample t-test produce samea different answerThe outcome binomial test one sample t-test produce samea different answer","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch6PreClassQueT8","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.5.5 Task 8: Drawing Conclusions about the new data","text":"Given results, conclude similar 22 participants original participants Woods et al (2009) whether managed recruit sleepers similar study?Think test used available information?Also, reliable finding two tests give different answers?given thoughts end chapter.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"practice-your-skills-6","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6 Practice Your Skills","text":"activity uses open data Experiment 1 Mehr, Song, Spelke (2016). exercise taken Open Stats Lab Trinity University, US.order complete tasks need download data .csv file .Rmd file, need edit, titled Ch5_PracticeSkills_Template.Rmd. can downloaded within zip file link . downloaded unzipped, create new folder use working directory; put data file .Rmd file folder set working directory folder drop-menus top. Download Exercises .zip file .Now open .Rmd file within RStudio. see code chunk task. Follow instructions edit code chunk. exercises section guide data wrangling, run One-Sample t-tests published data, visualise data. Thus, can check published paper see able replicate results.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"does-music-convey-social-information-to-infants","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.1 Does Music Convey Social Information to Infants?","text":"Parents often sing children , even infants, children listen look parents singing. Research Mehr, Song, Spelke (2016) sought explore psychological function music parents infants, examining hypothesis particular melodies convey important social information infants. Specifically, melodies convey information social affiliation.authors argue melodies shared within social groups. Whereas children growing one\nculture may exposed certain songs infants (e.g., “Rock--bye Baby”), children growing \ncultures (even groups within culture) may exposed different songs. Thus, novel person (someone infant never seen ) sings familiar song, may signal infant new person member social group.test hypothesis, researchers recruited 32 infants parents complete \nexperiment. first visit lab, parents taught new lullaby (one neither infants heard ). experimenters asked parents sing new lullaby child every day next 1-2 weeks.Following 1-2 week exposure period, parents infant returned lab complete\nexperimental portion study. Infants first shown screen side--side videos \ntwo unfamiliar people, silently smiling looking infant. researchers\nrecorded looking behavior (gaze) infants ‘baseline’ phase. Next, one one, two unfamiliar people screen sang either lullaby parents learned different lullaby (lyrics rhythm, different melody). Finally, infants saw silent video used baseline, researchers recorded looking behavior infants ‘test’ phase. details experiment’s methods, please refer Mehr et al. (2016) Experiment 1.exercise looking baseline data test trial data gazing proportion specifically.starting check:.csv file saved folder computer manually set folder working directory..csv file saved folder computer manually set folder working directory..Rmd file saved folder .csv files..Rmd file saved folder .csv files.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT0","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.2 Load in the data","text":"Call tidyverse library() load data (socialmelodies_exp1.csv) store object melodydata.View dataIt always good idea familiarise layout data just loaded . can using glimpse() View() Console window. Note, analyse variables. Try find variables relevant study description .Tasks:Now data loaded, tidyverse attached, viewed data, now try complete following tasks. Go tasks change NULL question asks make sure file knits end fully reproducible code.","code":"\nlibrary(\"tidyverse\")\n\nmelodydata <- NULL"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.3 Task 1 - Filter data","text":"data file includes variables 5 experiments reported paper. want\nanalyze data Experiment 1. Using filter() function, create new data frame melodydata_exp1 contains data Experiment 1 (variable exp1 value 1 indicates data fro Experiment 1). Hint: new data frame 32 observations.","code":"\nmelodydata_exp1 <- NULL"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT2","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.4 Task 2 - Select data","text":"noted , using variables melodydata_exp1 order get better overview variables particularly interested , want create object contains variables focusing .context:\n* First, want show infants' looking behavior differ chance \nbaseline trial (Baseline_Proportion_Gaze_to_Singer). words, infants show attentional bias prior hearing unfamiliar others sign song.\n* Second, want examine whether proportion infants' looking\nbehaviour toward singer familiar melody (Test_Proportion_Gaze_to_Singer) higher chance test phase.Thus, create new data frame called melodydata_exp1_reduced contains variables id, Baseline_Proportion_Gaze_to_Singer, Test_Proportion_Gaze_to_Singer.Hint: new data frame contains 32 observations 3 variables.","code":"\nmelodydata_exp1_reduced <- NULL"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.5 Task 3 - Testing baseline gazing proportion","text":"Perform One-sample t-test examine whether proportion time spent looking person singing familiar song baseline differ chance (0.5).","code":"\nt.test(pull(NULL, NULL), mu = NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.6 Task 4 - Testing test trial gazing proportion","text":"Now, perform One-sample t-test examine whether proportion infants' looking\nbehaviour toward singer familiar melody higher chance test phase\n(0.5).","code":"\nt.test(pull(NULL, NULL), mu = NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.7 Task 5 - Gathering data into a different format","text":"want create boxplot depict proportion time infants spent looking singer familiar song baseline test trials. However, can , need slightly change format melodydata_exp1_reduced data frame: need represent cases (individuals) twice figure, need reorganise data gaze proportions one variable (Proportion), separate variable indicating whether belongs baseline test trial (TrialType). Put differently, participant represented two rows; one row baseline proportion another row test trial proportion.Store new data frame object melodydata_exp1_wide.Hint: need pivot_longer function step.\nHint: new data frame 64 observations 3 variables (id, TrialType, Proportion).","code":"\nmelodydata_exp1_wide <- melodydata_exp1_reduced %>% \n  pivot_longer(cols = NULL,\n               names_to = NULL,\n               values_to = NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT6","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.8 Task 6 - Visualise baseline and test trial data","text":"Generate boxplot depict proportion time infants spent looking singer \nfamiliar song baseline test trials.Turn legend using guides() needed x-axis tells trial .","code":"\nggplot(NULL)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"Ch5PracticeSkillsT7","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.6.9 Task 7 - Compare your analyses with the analyses reported in the published paper","text":"Finally, check results section Experiment 1 published paper compare results analyses ones reported paper: Mehr, Song, Spelke (2016). find?Well done, finished! Make sure knit .Rmd file. successfully replicated analyses data visualisation published paper. Check answers solutions end chapter, .","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"solutions-to-questions-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-binomial-test-1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1 The Binomial Test","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-1-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1.1 Task 1","text":"Return Task","code":"\nns_data <- tibble(participant = 1:22,\n                  valid_rt = c(631.2,800.8,595.4,502.6,604.5,\n                               516.9,658.0,502.0,496.7,600.3,\n                               714.6,623.7,634.5,724.9,815.7,\n                               456.9,703.4,647.5,657.9,613.2,\n                               585.4,674.1))"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-2-5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1.2 Task 2","text":"Giving n_participants value 16Return Task","code":"\nwoods_mean <- 590\n\nn_participants <- ns_data %>%\n  filter(valid_rt > woods_mean) %>%\n  nrow()"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-3-5","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.1.3 Task 3","text":"can use density function:, cumulative probability function:, plug numbers directly code:, finally, remembering need specify value lower minimum participant number lower.tail = FALSE.better practice use first two solutions, pull values straight ns_data, run risk entering error code plug values manually.Return Task","code":"\nsum(dbinom(n_participants:nrow(ns_data), nrow(ns_data), .5))## [1] 0.0262394\npbinom(n_participants - 1L, nrow(ns_data), .5, lower.tail = FALSE)## [1] 0.0262394\nsum(dbinom(16:22,22, .5))## [1] 0.0262394\npbinom(15, 22, .5, lower.tail = FALSE)## [1] 0.0262394"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"the-one-sample-t-test-1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2 The One-Sample t-test","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-4-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.1 Task 4","text":"ns_data_mean use summarise() calculate mean pull() value.ns_data_sd use summarise() calculate sd pull() value.NOTE: print screen wanted \"\\n\" end line symbol print different linesReturn Task","code":"\n# the mean\nns_data_mean <- ns_data %>%\n  summarise(m = mean(valid_rt)) %>%\n  pull(m)  \n\n# the sd\nns_data_sd <- ns_data %>%\n  summarise(sd = sd(valid_rt)) %>%\n  pull(sd)\ncat(\"The mean number of hours was\", ns_data_mean, \"\\n\")\ncat(\"The standard deviation was\", ns_data_sd, \"\\n\")## The mean number of hours was 625.4636 \n## The standard deviation was 94.30693"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-5-4","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.2 Task 5","text":"Giving t_obs value 1.7638067Return Task","code":"\nt_obs <- (ns_data_mean - woods_mean) / (ns_data_sd / sqrt(nrow(ns_data)))"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-6-3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.3 Task 6","text":"using values straight ns_data, multiplying 2 two-tailed test, following:Giving pval 0.0923092But can also get answer plugging values - though method runs risk error better using first calculation values come straight ns_data. :Giving pval 0.0923092Return Task","code":"\npval <- pt(abs(t_obs), nrow(ns_data) - 1L, lower.tail = FALSE) * 2L\npval2 <- pt(t_obs, 21, lower.tail = FALSE) * 2"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-7-3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.4 Task 7","text":"t-test run follows, output shown :Return Task","code":"\nt.test(pull(ns_data, valid_rt), mu = woods_mean)## \n##  One Sample t-test\n## \n## data:  pull(ns_data, valid_rt)\n## t = 1.7638, df = 21, p-value = 0.09231\n## alternative hypothesis: true mean is not equal to 590\n## 95 percent confidence interval:\n##  583.6503 667.2770\n## sample estimates:\n## mean of x \n##  625.4636"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-8-3","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.2.5 Task 8","text":"According one-sample t-test participants responding similar manner participants original study, , may inclined assume recruitment process pilot experiment working well.However, according binomial test participants responding differently original sample. test result take finding?Keep mind binomial test rough categorises participants yes . one-sample t-test uses much available data degree give accurate answer. However, fact two tests give really different answers may give reason question whether results stable potentially look gather larger sample get accurate representation population.Return Task","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"practice-your-skills-7","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3 Practice Your Skills","text":"","code":""},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"load-in-the-data-1","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.1 Load in the data","text":"Return Task","code":"\nlibrary(\"tidyverse\")\n\nmelodydata <- read_csv(\"socialmelodies_exp1.csv\")"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-1---filter-data","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.2 Task 1 - Filter data","text":"Return Task","code":"\nmelodydata_exp1 <- melodydata %>%\n  filter(exp1 == 1)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-2---select-data","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.3 Task 2 - Select data","text":"Return Task","code":"\nmelodydata_exp1_reduced <- melodydata_exp1 %>%\n  select(id, Baseline_Proportion_Gaze_to_Singer, Test_Proportion_Gaze_to_Singer)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-3---testing-baseline-gazing-proportion","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.4 Task 3 - Testing baseline gazing proportion","text":"Return Task","code":"\nt.test(pull(melodydata_exp1_reduced, Baseline_Proportion_Gaze_to_Singer), mu = 0.5)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-4---testing-test-trial-gazing-proportion","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.5 Task 4 - Testing test trial gazing proportion","text":"Return Task","code":"\nt.test(pull(melodydata_exp1_reduced, Test_Proportion_Gaze_to_Singer), mu = 0.5)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-5---gathering-data-into-a-different-format","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.6 Task 5 - Gathering data into a different format","text":"Return Task","code":"\nmelodydata_exp1_wide <- melodydata_exp1_reduced %>% \n  pivot_longer(cols = Baseline_Proportion_Gaze_to_Singer:Test_Proportion_Gaze_to_Singer,\n               names_to = \"TrialType\",\n               values_to = \"Proportion\")"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-6---visualise-baseline-and-test-trial-data","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.7 Task 6 - Visualise baseline and test trial data","text":"Return Task","code":"\nggplot(data = melodydata_exp1_wide, \n       aes(x = TrialType, \n           y = Proportion, \n           fill = TrialType)) + \n  geom_boxplot() +\n  guides(fill = FALSE)"},{"path":"nhst-binomial-test-and-one-sample-t-test.html","id":"task-7---compare-your-analyses-with-the-analyses-reported-in-the-published-paper","chapter":"5 NHST: Binomial test and One-Sample t-test","heading":"5.7.3.8 Task 7 - Compare your analyses with the analyses reported in the published paper","text":"looking results section see two reported one-sample t-tests published paper produced part exercise. addition, Fig 2a published paper resemble . Well done!Return Task","code":""},{"path":"nhst-two-sample-t-test.html","id":"nhst-two-sample-t-test","chapter":"6 NHST: Two-Sample t-test","heading":"6 NHST: Two-Sample t-test","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"overview-6","chapter":"6 NHST: Two-Sample t-test","heading":"6.1 Overview","text":"previous chapter looked situation gathered one sample data (one group participants) compared one group known value, e.g. standard value. extension design gather data two samples. Two-sample designs common Psychology often want know whether difference groups particular variable. Today look scenario closely \\(t\\)-test analysis.Comparing Means Two SamplesFirst thing note different types two-sample designs depending whether two groups independent (e.g., different participants different conditions) (e.g., participants different conditions). today's chapter focus independent samples, typically means observations two groups unrelated - usually meaning different people. next chapter, examine cases observations two groups pairs (paired samples) - often people, also matched-pairs design.","code":""},{"path":"nhst-two-sample-t-test.html","id":"background-of-data-speech-as-indicator-of-intellect","chapter":"6 NHST: Two-Sample t-test","heading":"6.2 Background of data: Speech as indicator of intellect","text":"lab revisiting data Schroeder Epley (2015), first encountered part homework Chapter 5. can take look Psychological Science article :Schroeder, J. Epley, N. (2015). sound intellect: Speech reveals thoughtful mind, increasing job candidate's appeal. Psychological Science, 26, 277--891.abstract article explains different experiments conducted (specifically looking dataset Experiment 4, courtesy Open Stats Lab):\"person's mental capacities, intellect, observed directly instead inferred indirect cues. predicted person's intellect conveyed strongly cue closely tied actual thinking: voice. Hypothetical employers (Experiments 1-3b) professional recruiters (Experiment 4) watched, listened , read job candidates' pitches hired. evaluators (employers) rated candidate competent, thoughtful, intelligent heard pitch rather read , result, favorable impression candidate interested hiring candidate. Adding voice written pitches, trained actors (Experiment 3a) untrained adults (Experiment 3b) read , produced results. Adding visual cues audio pitches alter evaluations candidates. conveying one's intellect, important one's voice, quite literally, heard.\"recap Experiment 4, 39 professional recruiters Fortune 500 companies evaluated job pitches M.B.. candidates (Masters Business Administration) University Chicago Booth School Business. methods results appear pages 887--889 article want look specifically details. original data, wide format, can found Open Stats Lab website later self-directed learning. Today however, working modified version \"tidy\" format can downloaded . unsure tidy format, refer back activities Chapter 2.","code":""},{"path":"nhst-two-sample-t-test.html","id":"the-two-sample-t-test","chapter":"6 NHST: Two-Sample t-test","heading":"6.3 The Two-Sample t-test","text":"overall goal today learn running \\(t\\)-test -subjects data, well learning analysing actual data along way. , task today reproduce figure results article (p. 887-888). two packages need tidyverse, used lot, broom, new , become friend. One main functions use broom broom::tidy() - incredibly useful function converts output inferential test R combination text lists, really hard work - technically called objects - tibble familiar can use much easily. show use function today ask use coming chapters.","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT1","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.1 Task 1: Evaluators","text":"Open new R Markdown file use code call broom tidyverse library.Note: Order important calling multiple libraries - two libraries function named thing, R use function library loaded last. recommend calling libraries order tidyverse called last functions library used often.file called evaluators.csv contains demographics 39 raters. downloading unzipping data, course setting working directory, load information file store tibble called evaluators.file called evaluators.csv contains demographics 39 raters. downloading unzipping data, course setting working directory, load information file store tibble called evaluators.Now, use code :\ncalculate overall mean standard deviation age evaluators.\ncount many male many female evaluators study.\nNow, use code :calculate overall mean standard deviation age evaluators.count many male many female evaluators study.Note: Probably easier task separate lines code. NAs data need include call na.rm = TRUE.\nRemember load libraries need!\n\nAlso make sure ’ve downloaded saved data folder\n’re working .\n\ncan use summarise() count() \npipeline group_by() complete task.\n\nanalysing number male female evaluators, isn’t\ninitially clear ‘1’ represents males ‘2’ represents\nfemales.\n\ncan use recode() convert numeric names \nindicate something meaningful. look ?recode\nsee can work use . ’ll help use\nmutate() create new variable recode()\nnumeric names evaluators.\n\nwebsite also incredibly useful one save anytime\nneed use recode():\nhttps://debruine.github.io/posts/recode/\n\nanalysis future reproducible analyses, ’s good\nidea make representations clearer others.\nQuickfire QuestionsFill answers check calculations correct:mean age evaluators study? Type answer one decimal place: standard deviation age evaluators study? Type answer two decimal places: many participants noted female: many participants noted male: Thinking Cap PointThe paper claims mean age evaluators 30.85 years (SD = 6.24) 9 male 30 female evaluators. agree? might differences?\npaper claimed 9 males. However, looking \nresults can see 4 males, 5 NA entries making rest\nparticipant count. looks like NA male entries \ncombined! information might clear person\nre-analysing data.\n\n’s important reproducible data analyses \nothers examine. another pair eyes examining data can\nbeneficial spotting discrepancies - allows \ncritical evaluation analyses results improves quality \nresearch published. reason emphasize \nimportance conducting replication studies!\n","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT2","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.2 Task 2: Ratings","text":"now going calculate overall intellect rating given evaluator. break bit, going calculate intellectual evaluators (raters) thought candidates overall, depending whether evaluators read listened candidates' resume pitches. calculated averaging ratings competent, thoughtful intelligent evaluator held within ratings.csv.Note: looking ratings individual candidates; looking overall ratings evaluator. bit confusing makes sense stop think little. can think terms \"raters rate differently depending whether read listen resume pitch\".combine overall intellect rating overall impression ratings overall hire ratings evaluator, ready found ratings.csv. end new tibble - call ratings2 - structure:eval_id shows evaluator ID. evaluator different ID. 1's evaluator.Category shows scale rating - intellect, hire, impressionRating shows overall rating given evaluator given scale.condition shows whether evaluator listened (e.g., evaluators 1, 2 3), read (e.g., evaluator 4) resume.following steps describe create tibble, might want bash without reading first. trick data analysis data wrangling first think want achieve - end goal - function need use. know want end - table - now get ?Steps 1-3 calculate new intellect rating. Steps 4 + 5 combine rating information.Load data found ratings.csv tibble called ratings.Load data found ratings.csv tibble called ratings.filter() relevant variables (thoughtful, competent, intelligent) new tibble (call like - solutions use iratings), calculate mean Rating evaluator.filter() relevant variables (thoughtful, competent, intelligent) new tibble (call like - solutions use iratings), calculate mean Rating evaluator.Add new column called Category every entry word intellect. tells us every number tibble intellect rating.Add new column called Category every entry word intellect. tells us every number tibble intellect rating.Now create new tibble called ratings2 filter just impression hire ratings original ratings tibble. Next, bind tibble tibble created step 3 bring together intellect, impression, hire ratings, ratings2.Now create new tibble called ratings2 filter just impression hire ratings original ratings tibble. Next, bind tibble tibble created step 3 bring together intellect, impression, hire ratings, ratings2.Join ratings2 evaluator tibble created Task 1. Keep necessary columns shown arrange Evaluator Category.Join ratings2 evaluator tibble created Task 1. Keep necessary columns shown arrange Evaluator Category.forget use hints solution end Chapter stuck. Take one step time.\n\nMake sure ’ve downloaded saved data folder\n’re working .\n\n\nMake sure ’ve downloaded saved data folder\n’re working .\n\n\nfilter(Category %% c()) might work use\ngroup_by() summarize() calculate mean\nRating evaluator.\n\n\nfilter(Category %% c()) might work use\ngroup_by() summarize() calculate mean\nRating evaluator.\n\n\nUse mutate() create new column.\n\n\nUse mutate() create new column.\n\n\nbind_rows() Chapter 2 help combine\nvariables two separate tibbles.\n\n\nbind_rows() Chapter 2 help combine\nvariables two separate tibbles.\n\n\nUse inner_join() common column \ntibbles. select() arrange() help \n, .\n\n\nUse inner_join() common column \ntibbles. select() arrange() help \n, .\n","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT3","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.3 Task 3: Creating a Figure","text":"recap, now ratings2 contains overall Rating score evaluator three Category (within: hire, impression, intellect) depending condition evaluator (: listened read). Great! Now information need replicate Figure 7 article (page 888), shown :\nFigure 6.1: Figure 7 Schroeder Epley (2015) try replicate.\nReplace NULLs create basic version figure.Thinking Cap PointImprove Figure: improve plot. geom_() options try? bar charts informative something else better? add change labels plot? change colours figure?Next, look possible solution see modern way presenting information. new functions solution play understand . Remember layering system, remove lines see happens. Note solution Figure shows raw data points well means condition; gives better impression true data just showing means can misleading. can continue exploration visualisations reading paper later chance: Weissberger et al., 2015, Beyond Bar Line Graphs: Time New Data Presentation ParadigmFilling code create basic figure shown:\nFigure 6.2: basic solution Figure 7\nalternatively, modern presentation data:\nFigure 6.3: possible alternative Figure 7\n","code":"\ngroup_means <- group_by(ratings2, NULL, NULL) %>%\n  summarise(Rating = mean(Rating))\n\nggplot(group_means, aes(NULL, NULL, fill = NULL)) +\n  geom_col(position = \"dodge\")\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(Rating = mean(Rating))## `summarise()` has grouped output by 'condition'. You can override using the\n## `.groups` argument.\nggplot(group_means, aes(Category, Rating, fill = condition)) + \n  geom_col(position = \"dodge\")\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(Rating = mean(Rating))## `summarise()` has grouped output by 'condition'. You can override using the\n## `.groups` argument.\nggplot(ratings2, aes(condition, Rating, color = condition)) +\n  geom_jitter(alpha = .4) +\n  geom_violin(aes(fill = condition), alpha = .1) +\n  facet_wrap(~Category) +\n  geom_point(data = group_means, size = 2) +\n  labs(x = \"Category\", y = \"Recruiters' Evaluation of Candidates\") +\n  coord_cartesian(ylim = c(0, 10), expand = FALSE) +\n  guides(color = \"none\", fill = \"none\") +\n  theme_bw()"},{"path":"nhst-two-sample-t-test.html","id":"Ch6InClassQueT4","chapter":"6 NHST: Two-Sample t-test","heading":"6.3.4 Task 4: t-tests","text":"Brilliant! far checked descriptives visualisations, last thing now check inferential tests; t-tests. still ratings2 stored Task 2. tibble, reproduce t-test results article time show run t-test. can refer back lectures understand maths -subjects t-test, essentially measure difference means variance means.paragraph paper describing results (p. 887):\"pattern evaluations professional recruiters replicated pattern observed Experiments 1 3b (see Fig. 7). particular, recruiters believed job candidates greater intellect---competent, thoughtful, intelligent---listened pitches (M = 5.63, SD = 1.61) read pitches (M = 3.65, SD = 1.91), t(37) = 3.53, p < .01, 95% CI difference = [0.85, 3.13], d = 1.16. recruiters also formed positive impressions candidates---rated likeable positive less negative impression ---listened pitches (M = 5.97, SD = 1.92) read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p < .01, 95% CI difference = [0.55, 3.24], d = 0.94. Finally, also reported likely hire candidates listened pitches (M = 4.71, SD = 2.26) read pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p < .01, 95% CI difference = [0.41, 3.24], d = 0.86.\"going run t-tests Intellect, Hire Impression; time comparing evaluators overall ratings listened group versus overall ratings read group see significant difference two conditions: .e., evaluators listened pitches give significant higher lower rating evaluators read pitches.terms hypotheses, phrase null hypothesis tests significant difference overall ratings {insert trait} scale evaluators listened resume pitches evaluators read resume pitches (\\(H_0: \\mu_1 = \\mu2\\)). Alternatively, state significant difference overall ratings {insert trait} scale evaluators listened resume pitches evaluators read resume pitches (\\(H_1: \\mu_1 \\ne \\mu2\\)).clarify, going run three -subjects t-tests total; one intellect ratings; one hire ratings; one impression ratings. show run t-test intellect ratings ask remaining two t-tests .run analysis intellect ratings need function t.test() use broom::tidy() pull results t-test tibble. , show create group means run t-test intellect. Run lines look .First calculate group means:can call look typing:Now just look intellect ratings need filter new tibble:run actual t-test tidy table.\nt.test() requires two vectors input\npull() pull single column tibble, e.g. Rating intellect\ntidy() takes information test turns tibble. Try running t.test without piping tidy() see differently.\nt.test() requires two vectors inputpull() pull single column tibble, e.g. Rating intellecttidy() takes information test turns tibble. Try running t.test without piping tidy() see differently.Now lets look intellect_ttibble created (assuming piped tidy()):\nTable 6.1: t-test output intellect condition.\ntibble, intellect_t, can see ran Two Sample t-test (meaning -subjects) two-tailed hypothesis test (\"two.sided\"). mean listened condition, estimate1, 5.635, whilst mean read condition, estimate2 3.648 - compare means group_means sanity check. overall difference two means 1.987. degrees freedom test, parameter, 37. observed t-value, statistic, 3.526, significant p-value, p.value, p = 0.0011, lower field standard Type 1 error rate \\(\\alpha = .05\\).know lectures, t-test presented t(df) = t-value, p = p-value. , t-test written : t(37) = 3.526, p = 0.001.Thinking interpretation finding, effect significant, can reject null hypothesis significant difference mean ratings listened resumes read resumes, intellect ratings. can go say overall intellect ratings listened resume significantly higher (mean diff = 1.987) read resumes, t(37) = 3.526, p = 0.001, accept alternative hypothesis. suggest hearing people speak leads evaluators rate candidates intellectual merely read words written.Now Try:Running remaining t-tests hire impression. Store tibbles called hire_t impress_t respectively.Running remaining t-tests hire impression. Store tibbles called hire_t impress_t respectively.Bind rows intellect_t, hire_t impress_t create table three t-tests called results. look like :Bind rows intellect_t, hire_t impress_t create table three t-tests called results. look like :\nTable 6.2: Output three t-tests\nQuickfire QuestionsCheck results hire. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):\nMean estimate1 (listened condition) = \nMean estimate2 (read condition) = \nt() = , p = \nLooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSE\nCheck results hire. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):Mean estimate1 (listened condition) = Mean estimate1 (listened condition) = Mean estimate2 (read condition) = Mean estimate2 (read condition) = t() = , p = t() = , p = Looking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSELooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSECheck results impression. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):\nMeanestimate1 (listened condition) = \nMean estimate2 (read condition) = \nt() = , p = \nLooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSE\nCheck results impression. Enter mean estimates t-test results (means t-value 2 decimal places, p-value 3 decimal places):Meanestimate1 (listened condition) = Meanestimate1 (listened condition) = Mean estimate2 (read condition) = Mean estimate2 (read condition) = t() = , p = t() = , p = Looking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSELooking result, True False, result significant \\(\\alpha = .05\\)? TRUEFALSE\nt-test answers following structure:\n\nt(degrees freedom) = t-value, p = p-value,\n\n:\n\ndegrees freedom = parameter,\n\nt-value = statistic,\n\np-value = p.value.\n\nRemember result p-value lower (.e. smaller) \nequal alpha level said significant.\nrecap, looked data Schroeder Epley (2015), descriptives inferentials, plotted figure, confirmed , paper, significant differences three rating categories (hire, impression intellect), listened condition receiving higher rating read condition rating. , interpretation people rate higher hear speak resume opposed just reading resume!","code":"\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(m = mean(Rating), sd = sd(Rating))## `summarise()` has grouped output by 'condition'. You can override using the\n## `.groups` argument.\ngroup_means\nintellect <- filter(ratings2, Category == \"intellect\")\nintellect_t <- t.test(intellect %>% \n                        filter(condition == \"listened\") %>% \n                        pull(Rating),\n                      intellect %>% \n                        filter(condition == \"read\") %>% \n                        pull(Rating),\n                      var.equal = TRUE) %>%\n  tidy()"},{"path":"nhst-two-sample-t-test.html","id":"students-versus-welchs-t-tests","chapter":"6 NHST: Two-Sample t-test","heading":"6.4 Student's versus Welch's t-tests","text":"-subjects t-test comes two versions: one assuming equal variance two groups (Student's t-test) another making assumption (Welch's t-test). Student's t-test considered common standard t-test field, advisable use Welch's t-test instead.blog post \"Always use Welch's t-test instead Student's t-test\" Daniel Lakens shows groups equal variance tests return finding. However, assumption equal variance violated, .e. groups unequal variance, Welch's test produces accurate finding, based data.important often final decision whether assumptions \"held\" \"violated\" subjective; .e. researcher fully decide. Nearly data show level unequal variance (perfectly equal variance across multiple conditions actually revealing fraudulent data). Researchers using Student's t-test regularly make judgement whether variance across two groups \"equal enough\". , blog shows always better run Welch's t-test analyse -subjects design ) Welch's t-test assumption equal variance, b) Welch's t-test gives accurate results variance equal, c) Welch's t-test performs exactly Student t-test variance equal across groups.short, Welch's t-test takes level ambiguity (may called \"researcher degree freedom\") analysis makes analysis less open bias subjectivity. , now , unless stated otherwise, run Welch's t-test.practice easy run Welch's t-test, can switch tests shown:\n* run Student's t-test set var.equal = TRUE\n* run Welch's t-test set var.equal = FALSEThis run Student's t-test:run Welch's t-test:two ways know run Welch's t-test :output says ran Welch Two Sample t-testThe df likely decimal places Welch's t-test whereas whole number Student's t-test.Always run Welch's t-test -subjects design using R!","code":"\nt.test(my_dv ~ my_iv, data = my_data, var.equal = TRUE)\nt.test(my_dv ~ my_iv, data = my_data, var.equal = FALSE)"},{"path":"nhst-two-sample-t-test.html","id":"practice-your-skills-8","chapter":"6 NHST: Two-Sample t-test","heading":"6.5 Practice Your Skills","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"single-dose-testosterone-administration-impairs-cognitive-reflection-in-men.","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.1 Single-dose testosterone administration impairs cognitive reflection in men.","text":"order complete exercise, first download assignment .Rmd file need edit assignment: titled GUID_Ch6_PracticeYourSkills.Rmd. can downloaded within zip file link . downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top. Download Assignment .zip file .assignment using real data following paper:Nave, G., Nadler, ., Zava, D., Camerer, C. (2017). Single-dose testosterone administration impairs cognitive reflection men. Psychological Science, 28, 1398--1407.full data documentation can found Open Science Framework repository, assignment just use .csv file zipped folder: CRT_Data.csv. may also want read paper, least part, help fully understand analysis times unsure. article's abstract:nonhumans, sex steroid testosterone regulates reproductive behaviors fighting males mating. humans, correlational studies linked testosterone aggression disorders associated poor impulse control, neuropsychological processes work poorly understood. Building dual-process framework, propose mechanism underlying testosterone's behavioral effects humans: reduction cognitive reflection. largest study behavioral effects testosterone administration date, 243 men received either testosterone placebo took Cognitive Reflection Test (CRT), estimates capacity override incorrect intuitive judgments deliberate correct responses. Testosterone administration reduced CRT scores. effect remained controlled age, mood, math skills, whether participants believed received placebo testosterone, effects 14 additional hormones, held CRT questions isolation. findings suggest mechanism underlying testosterone's diverse effects humans' judgments decision making provide novel, clear, testable predictions.critical findings presented p. 1403 paper heading \"influence testosterone CRT performance\". task today attempt try reproduce main results paper.Note: unable get exact results authors necessarily mean wrong! authors might wrong, might left important details. Present find.starting lets check:.csv file saved folder computer manually set folder working directory..csv file saved folder computer manually set folder working directory..Rmd file saved folder .csv files. assessments ask save format GUID_Ch6_PracticeYourSkills.Rmd GUID replaced GUID. Though formative assessment, may good practice ..Rmd file saved folder .csv files. assessments ask save format GUID_Ch6_PracticeYourSkills.Rmd GUID replaced GUID. Though formative assessment, may good practice .","code":""},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT1A","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.2 Task 1A: Libraries","text":"today's exercise need tidyverse broom packages. Enter code t1A code chunk load libraries.","code":"\n## load in the tidyverse and broom packages"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT1B","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.3 Task 1B: Loading in the data","text":"Use read_csv() replace NULL t1B code chunk load data stored data file CRT_Data.csv. Store data variable crt. change file name data file.","code":"\ncrt <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT2","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.4 Task 2: Selecting only relevant columns","text":"look crt. three variables crt need find extract order perform t-test: subject ID number (hint: participant unique number); independent variable (hint: participant possibility one two treatments coded 1 0); dependent variable (hint: test specifically looks answers people get correct). Identify three variables. might help look first sentences heading \"influence testosterone CRT performance\" Figure 2a paper guidance correct variables.identified important three columns, replace NULL t2 code chunk select three columns crt store tibble crt2.Check work: correct, crt2 tibble 3 columns 243 rows.Note: remainder assignment use crt2 main source tibble crt.","code":"\ncrt2 <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT3","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.5 Task 3: Verify the number of subjects in each group","text":"Participants section article contains following statement:243 men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = 125) placebo (n = 118).t3 code block , replace NULLs lines code calculate:number men Treatment. tibble called cond_counts containing column called Treatment showing two groups column called n shows number men group.number men Treatment. tibble called cond_counts containing column called Treatment showing two groups column called n shows number men group.total number men sample. single value, tibble, stored n_men.total number men sample. single value, tibble, stored n_men.know answer tasks already. Make sure code gives correct answer!Now replace strings statements , using inline R code, reproduces sentence paper exactly shown . words, statement , anywhere says \"(code )\", replace string (including quotes), inline R code. clarify, looking .Rmd file see R code, looking knitted file, see values. Look back Chapter 1 unsure use inline code.Hint: One solution something cond_counts similar filter() pull() exercises Chapter.\"(code )\" men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = \"(code )\") placebo (n = \"(code )\").","code":"\ncond_counts <- NULL\nn_men <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT4","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.6 Task 4: Reproduce Figure 2a","text":"Figure 2A original paper:\nFigure 6.4: Figure 2A Nave, Nadler, Zava, Camerer (2017) replicate\nWrite code t4 code chunk reproduce version Figure 2a - shown . create plot, replace NULL make tibble called crt_means mean standard deviation number CorrectAnswers group.Use crt_means source data plot.Hint: need check recode() get labels treatments right. Look back Chapter 2 hint use recode(). check short resource examples use recode(): Recode short tutorial.worry including error bars (unless want ) line indicating significance plot. however make sure pay attention labels treatments y-axis scale label. Reposition x-axis label Figure. can use colour, like.","code":"\ncrt_means <- NULL\n\n## TODO: add lines of code using ggplot"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT5","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.7 Task 5: Interpreting your Figure","text":"Always good slight recap point make sure following analysis. Replace NULL t5 code chunk number statement best describes data calculated plotted thus far. Store single value answer_t5:Testosterone group (M = 2.10, SD = 1.02) appear fewer correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 2.10, SD = 1.02) appear fewer correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear fewer correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 1.66, SD = 1.18) appear fewer correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 2.10, SD = 1.02) appear correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.Testosterone group (M = 2.10, SD = 1.02) appear correct answers average Placebo group (M = 1.66, SD = 1.18) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgments correct response.","code":"\nanswer_t5 <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT6","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.8 Task 6: t-test","text":"Now calculated descriptives study need run inferentials. t6 code chunk , replace NULL line code run t-test taking care make sure output table Placebo mean Estimate1 (group 0) Testosterone mean Estimate2 (group 1). Assume variance equal use broom::tidy() sweep store results tibble called t_table.","code":"\nt_table <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"Ch6AssignQueT7","chapter":"6 NHST: Two-Sample t-test","heading":"6.5.9 Task 7: Reporting results","text":"t7A code chunk , replace NULL line code pull df t_table. must single value stored t_df.t7B code chunk , replace NULL line code pull t-value t_table. Round three decimal places. must single value stored t_value.t7C code chunk , replace NULL line code pull p-value t_table. Round three decimal places. must single value stored p_value.t7D code chunk , replace NULL line code calculate absolute difference mean number correct answers Testosterone group Placebo group. Round three decimal places. must single value stored t_diff.completed t7A t7D accurately, knitted, one statements produce accurate coherent summary results. t7E code chunk , replace NULL number statement best summarises data study. Store single value answer_t7eThe testosterone group performed significantly better ( fewer correct answers) placebo group, t() = , p = .testosterone group performed significantly worse ( fewer correct answers) placebo group, t() = , p = .testosterone group performed significantly better ( correct answers) placebo group, t() = , p = .testosterone group performed significantly worse ( fewer correct answers) placebo group, t() = , p = .Well done, finished! Now go check answers solutions end chapter. looking check resulting output answers submitted exactly output solution - example, remember single value coded answer. alternative answers, means submitted one options return answer.","code":"\nt_df <- NULL\nt_value <- NULL\np_value <- NULL\nt_diff <- NULL\nanswer_t7e <- NULL"},{"path":"nhst-two-sample-t-test.html","id":"solutions-to-questions-5","chapter":"6 NHST: Two-Sample t-test","heading":"6.6 Solutions to Questions","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"the-two-sample-t-test-1","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1 The Two-Sample t-test","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"task-1-5","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1.1 Task 1","text":"mean age evaluators 30.9The standard deviatoin age evaluators 6.24There 4 males e_count %>% filter(sex_names == \"female\") %>% pull(n) females, 5 people stating sex.Return Task","code":"\nlibrary(\"tidyverse\")\nlibrary(\"broom\") # you'll need broom::tidy() later\n\nevaluators <- read_csv(\"evaluators.csv\")\n\nevaluators %>%\n  summarize(mean_age = mean(age, na.rm = TRUE))\n\nevaluators %>%\n  count(sex)\n\n# If using `recode()`:\nevaluators %>%\n  count(sex) %>%\n  mutate(sex_names = recode(sex, \"1\" = \"male\", \"2\" = \"female\"))"},{"path":"nhst-two-sample-t-test.html","id":"task-2-6","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1.2 Task 2","text":"load dataFirst pull ratings associated intellectNext calculate means evaluatorMutate Category variable. way can combine 'impression' 'hire' single table useful!combine information one single tibble.Return Task","code":"\nratings <- read_csv(\"ratings.csv\")\niratings <- ratings %>%\n  filter(Category %in% c(\"competent\", \"thoughtful\", \"intelligent\"))\nimeans <- iratings %>%\n  group_by(eval_id) %>%\n  summarise(Rating = mean(Rating))\nimeans2 <- imeans %>%\n  mutate(Category = \"intellect\")\nratings2 <- ratings %>%\n  filter(Category %in% c(\"impression\", \"hire\")) %>%\n  bind_rows(imeans2) %>%\n  inner_join(evaluators, \"eval_id\") %>%\n  select(-age, -sex) %>%\n  arrange(eval_id, Category)"},{"path":"nhst-two-sample-t-test.html","id":"task-4-5","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.1.3 Task 4","text":"First calculate group means:can call look typing:Now just look intellect ratings need filter new tibble:run actual t-test tidy table.\nt.test() requires two vectors input\npull() pull single column tibble, e.g. Rating intellect\ntidy() takes information test turns table. Try running t.test without piping tidy() see differently.\nt.test() requires two vectors inputpull() pull single column tibble, e.g. Rating intellecttidy() takes information test turns table. Try running t.test without piping tidy() see differently.Now repeat HIRE IMPRESSIONAnd ImpressionBefore combining one table showing three t-testsReturn Task","code":"\ngroup_means <- ratings2 %>%\n  group_by(condition, Category) %>%\n  summarise(m = mean(Rating), sd = sd(Rating))## `summarise()` has grouped output by 'condition'. You can override using the\n## `.groups` argument.\ngroup_means\nintellect <- filter(ratings2, Category == \"intellect\")\nintellect_t <- t.test(intellect %>% filter(condition == \"listened\") %>% pull(Rating),\n                      intellect %>% filter(condition == \"read\") %>% pull(Rating),\n                      var.equal = TRUE) %>%\n  tidy()\nhire <- filter(ratings2, Category == \"hire\")\nhire_t <- t.test(hire %>% filter(condition == \"listened\") %>% pull(Rating),\n                      hire %>% filter(condition == \"read\") %>% pull(Rating),\n                 var.equal = TRUE) %>%\n  tidy()\nimpress <- filter(ratings2, Category == \"impression\")\nimpress_t <- t.test(impress %>% filter(condition == \"listened\") %>% pull(Rating),\n                    impress %>% filter(condition == \"read\") %>% pull(Rating),\n                    var.equal = TRUE) %>%\n  tidy()\nresults <- bind_rows(\"hire\" = hire_t, \n                     \"impression\" = impress_t,\n                     \"intellect\" = intellect_t, .id = \"id\")\n\nresults"},{"path":"nhst-two-sample-t-test.html","id":"practice-your-skills-9","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2 Practice Your Skills","text":"","code":""},{"path":"nhst-two-sample-t-test.html","id":"task-1a-libraries","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.1 Task 1A: Libraries","text":"Return Task","code":"\nlibrary(broom)\nlibrary(tidyverse)"},{"path":"nhst-two-sample-t-test.html","id":"task-1b-loading-in-the-data","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.2 Task 1B: Loading in the data","text":"Use read_csv() read data!Return Task","code":"\ncrt <- read_csv(\"data/06-s01/homework/CRT_Data.csv\")\ncrt <- read_csv(\"CRT_Data.csv\")"},{"path":"nhst-two-sample-t-test.html","id":"task-2-selecting-only-relevant-columns","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.3 Task 2: Selecting only relevant columns","text":"key columns :IDTreatmentCorrectAnswersCreating crt2 tibble 3 columns 243 rows.Return Task","code":"\ncrt2 <- select(crt, ID, Treatment, CorrectAnswers)"},{"path":"nhst-two-sample-t-test.html","id":"task-3-verify-the-number-of-subjects-in-each-group","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.4 Task 3: Verify the number of subjects in each group","text":"Participants section article contains following statement:243 men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = 125) placebo (n = 118).t3 code block , replace NULLs lines code calculate:number men Treatment. tibble/table called cond_counts containing column called Treatment showing two groups column called n shows number men group.number men Treatment. tibble/table called cond_counts containing column called Treatment showing two groups column called n shows number men group.total number men sample. single value, tibble/table, stored n_men.total number men sample. single value, tibble/table, stored n_men.know answer tasks already. Make sure code gives correct answer!cond_counts, :alternativelyFor n_men, :alternativelySolution:formatted inline R code :`r n_men` men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = `r cond_counts %>% filter(Treatment == 1) %>% pull(n)`) placebo (n = `r cond_counts %>% filter(Treatment == 0) %>% pull(n)`).give:243 men (mostly college students; demographic details, see Table S1 Supplemental Material available online) randomly administered topical gel containing either testosterone (n = 125) placebo (n = 118).Return Task","code":"\ncond_counts <- crt2 %>% group_by(Treatment) %>% summarise(n = n())\ncond_counts <- crt2 %>% count(Treatment)\nn_men <- crt2 %>% summarise(n = n()) %>% pull(n)\nn_men <- nrow(crt2)"},{"path":"nhst-two-sample-t-test.html","id":"task-4-reproduce-figure-2a","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.5 Task 4: Reproduce Figure 2A","text":"produce good representation Figure 2A following approach:\nFigure 6.5: representation Figure 2A\nReturn Task","code":"\ncrt_means <- crt2 %>% \n  group_by(Treatment) %>% \n  summarise(m = mean(CorrectAnswers), sd = sd(CorrectAnswers)) %>%\n  mutate(Treatment = recode(Treatment, \"0\" = \"Placebo\", \"1\" = \"Testosterone Group\"))\n\nggplot(crt_means, aes(Treatment, m, fill = Treatment)) + \n  geom_col() + \n  theme_classic() + \n  labs(x = \"CRT\", y = \"Number of Correct Answers\") +\n  guides(fill = \"none\") +\n  scale_fill_manual(values = c(\"#EEEEEE\",\"#AAAAAA\")) +\n  coord_cartesian(ylim = c(1.4,2.4), expand = TRUE)"},{"path":"nhst-two-sample-t-test.html","id":"task-5-interpreting-your-figure","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.6 Task 5: Interpreting your Figure","text":"Option 3 correct answer given :Testosterone group (M = 1.66, SD = 1.18) appear fewer correct answers average Placebo group (M = 2.10, SD = 1.02) Cognitive Reflection Test suggesting testosterone fact inhibit ability override incorrect intuitive judgements correct response.Return Task","code":"\nanswer_t5 <- 3"},{"path":"nhst-two-sample-t-test.html","id":"task-6-t-test","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.7 Task 6: t-test","text":"need pay attention order using first approach, making sure 0 group entered first. put Placebo groups Estimate1 output. reality change values, key thing pass code someone, expect Placebo Estimate1, need make sure coded way.Alternatively, use known formula approach shown . state DV ~ IV say name tibble data = .... just need make sure columns state DV IV actually tibble!Return Task","code":"\nt_table <- t.test(crt2 %>% filter(Treatment == 0) %>% pull(CorrectAnswers),\n                  crt2 %>% filter(Treatment == 1) %>% pull(CorrectAnswers),\n                  var.equal = TRUE) %>%\n  tidy()\nt_table <- t.test(CorrectAnswers ~ Treatment, data = crt2, var.equal = TRUE) %>% tidy()"},{"path":"nhst-two-sample-t-test.html","id":"task-7-reporting-results","chapter":"6 NHST: Two-Sample t-test","heading":"6.6.2.8 Task 7: Reporting results","text":"degrees freedom (df) found parameterAn alternative option follows, using pull() method. work B D wellThe t-value found statisticThe p-value found p.valueThe absolute difference two means can calculated follows:completed t7A t7D accurately, knitted, Option 4 stated suchThe testosterone group performed significantly worse (0.438 fewer correct answers) placebo group, t(241) = 3.074, p = 0.002and therefore correct answer!Return TaskChapter Complete!","code":"\nt_df <- t_table$parameter\nt_df <- t_table %>% pull(parameter)\nt_value <- t_table$statistic %>% round(3)\np_value <- t_table$p.value %>% round(3)\nt_diff <- (t_table$estimate1 - t_table$estimate2) %>% round(3) %>% abs()\nanswer_t7e <- 4"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"nhst-paired-sample-t-test-and-nonparametric-tests","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7 NHST: Paired-Sample t-test and Nonparametric tests","text":"","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"overview-7","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.1 Overview","text":"previous chapters looked one-sample t-tests -samples (two-sample) t-tests. chapter continue look paired-sample t-test (sometimes called dependent sample within-subject t-test). paired-sample t-test statistical procedure used determine whether mean difference two sets observations matched participants zero.tests, paired-sample t-test two competing hypotheses: null hypothesis alternative hypothesis.null hypothesis assumes true mean difference paired samples zero: \\[H_0: \\mu1 = \\mu_2\\].alternative hypothesis assumes true mean difference paired samples equal zero: \\[H_1: \\mu1 \\ne \\mu_2\\].chapter going look running paired-sample t-test, begin little work checks data need perform prior analysis.Assumptions testsSo far focused skills data-wrangling, visualisations, probability, now moving towards actual analysis stage research. However, know lectures, tests, particularly parametric tests, make number assumptions data tested, , responsible researcher, need check assumptions \"held\" violation assumptions may make results invalid.t-tests assumptions change two-sample paired-sample designs (one-sample matched-pairs designs can thought within-subjects designs).assumptions two-sample t-test :data points independent.variance across groups/conditions equal.dependent variable must continuous (interval/ratio).dependent variable normally distributed.assumptions paired-sample t-test :participants appear conditions/groups.dependent variable must continuous (interval/ratio).dependent variable normally distributed.beginning analysis, using data-wrangling skills, must check see data deviates assumptions, whether contains outliers, order assess quality results. assumption violation present, may want use nonparametric tests instead.chapter :Run assumption checksAnalyse experiment paired-sample design.Understand run nonparametric tests.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"the-paired-sample-t-test","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2 The Paired-Sample t-test","text":"now covered one-sample t-test -subjects/two-sample t-test, talked little assumptions -subjects t-test. now going expand looking final t-test need cover, paired-sample/within-subject t-test; used participants conditions, two groups people closely matched various demographics age, IQ, verbal acuity, etc.exploring paired-sample t-test also look checking assumptions t-tests. refer back earlier activities know many assumptions different t-tests largely similar (apart equal variance, example), looking assumption check can apply tests.activity look replication Furnham (1986) School Psychology & Neuroscience, University Glasgow, carried 2016-2017. worth familiarising original study point information regarding concepts study, essential order complete exercises: Furnham, . (1986), Robustness Recency Effect: Studies Using Legal Evidence. explain little study carrying tasks check assumptions analyse data.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"data-set-juror-decision-making","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.1 Data Set: Juror Decision Making","text":"research question experiment : order information affect juror judgments guilt innocence? Thus, overall aim original experiment investigate whether decision jury member makes innocence guilt defendant influenced something simple crucial evidence presented trial. experiment participants (Level 2 Psychology students) listened series recordings recreated 1804 trial man known Joseph Parker accused assuming two identities marrying two women; .e. bigamy. participant listened recordings evidence, presented prosecution defense witnesses, asked judge guilty thought Mr. Parker 14 different time points experiment scale 1 9: 1 innocent 9 guilty. 14 time points came immediately certain pieces evidence.manipulation experiment order evidence altered half participants received one order evidence half received second order evidence. Key order change time critical piece evidence presented. critical evidence proved defendant innocent. Middle group heard evidence Timepoint 9 trial whereas Late group heard evidence Timepoint 13. opportunity look data due course , today's exercise, focus Late group.exercise, task analyse data examine whether participants' ratings guilt significantly changed presentation critical evidence Late condition. critical evidence, proved defendant's innocence, desired effect see significant drop ratings guilt hearing evidence (Timepoint 13) compared (Timepoint 12). words, hypothesised significant decrease ratings guilt, caused presentation critical evidence, Timepoint 12 Timepoint 13.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT1","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.2 Task 1: Load the data","text":"Download data experiment .Unzip data save folder access set folder working directory.Open new R Markdown document.Today need broom tidyverse libraries. Load order. Remember order load libraries matters.Using read_csv(), load data experiment contained GuiltJudgements.csv store tibble called ratings.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT2","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.3 Task 2: Wrangle the data","text":", interested 75 participants Late group Timepoints 12 (rating key evidence) 13 (rating key evidence). look ratings see Timepoints wide format (columns 1 14 - different timepoint) Evidence column contains Middle group well. wrangling make data look like shown Table 7.1 . steps follows:filter() participants Late condition.filter() participants Late condition.select() Timepoints 12 13.select() Timepoints 12 13.rename() Timepoints Twelve Thirteen numerical names hard deal .rename() Timepoints Twelve Thirteen numerical names hard deal .pivot_longer() gather data structure. (Note: first four rows shown example).pivot_longer() gather data structure. (Note: first four rows shown example).one pipe store tibble called lates. tibble, lates 150 rows 4 columns.one pipe store tibble called lates. tibble, lates 150 rows 4 columns.Check table looks like table .\nTable 7.1: table look Task 2\n\n\nneed specify column want filter , stating\nvariable (.e. Late) column ‘equal ’\n(.e. ‘==’)\n\n\nneed specify column want filter , stating\nvariable (.e. Late) column ‘equal ’\n(.e. ‘==’)\n\n\ntwo columns representing Timepoints 12 13,\ntwo columns need keep order identify \nparticipant group. Use table guide.\n\n\ntwo columns representing Timepoints 12 13,\ntwo columns need keep order identify \nparticipant group. Use table guide.\n\n\nrenaming, first state new variable name \ndesignate old variable name.\n.e. rename(data, new_column_name = old_column_name). \nold column number, put backticks, e.g. Five = backtick 5\nbacktick (sure use `s).\n\n\nrenaming, first state new variable name \ndesignate old variable name.\n.e. rename(data, new_column_name = old_column_name). \nold column number, put backticks, e.g. Five = backtick 5\nbacktick (sure use `s).\n\n\nstructure shown two new columns: Timepoint\nGuiltRating, created columns Twelve\nThirteen. state new column names using\npivot_longer(), well columns used create\n. Think completing :\ncols = X:Y, names_to = \"\", values_to = \"\"\n\n\nstructure shown two new columns: Timepoint\nGuiltRating, created columns Twelve\nThirteen. state new column names using\npivot_longer(), well columns used create\n. Think completing :\ncols = X:Y, names_to = \"\", values_to = \"\"\nQuickfire QuestionsTo check completed Task correctly, enter appropriate values boxes.\ndataset :  columns  rows  participants.\ndataset :  columns  rows  participants.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT3","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.4 Task 3: Look at the histogram for normality","text":"now going show start checking data terms assumptions normality. creating visualisations data thinking visualisations tell us data.\nFirst create histogram two timepoints see individual distributions appear normally distributed.Use data visualisation skills plot histogram TimePoint. two histograms side--side one figure set histogram binwidth something reasonable experiment.\n\nggplot() + geom_?\n\n\nggplot() + geom_?\n\n\nhistogram requires state aes(x) \naes(x, y). examining differences guilt\nrating scores across participants. column lates\n‘x’? categorical Independent\nVariable.\n\n\nhistogram requires state aes(x) \naes(x, y). examining differences guilt\nrating scores across participants. column lates\n‘x’? categorical Independent\nVariable.\n\n\nbinwidth argument can specify within\ngeom_histogram() \ngeom_historgram(binwidth = ...). Think appropriate\nbinwidth. guilt rating scale runs 1 9 increments \n1.\n\n\nbinwidth argument can specify within\ngeom_histogram() \ngeom_historgram(binwidth = ...). Think appropriate\nbinwidth. guilt rating scale runs 1 9 increments \n1.\n\n\nused something like facet_?() display \ncategorical variables (.e. Timepoint) according \ndifferent levels contains. need specify variable want\nuse, using ~ variable name.\n\n\nused something like facet_?() display \ncategorical variables (.e. Timepoint) according \ndifferent levels contains. need specify variable want\nuse, using ~ variable name.\n\n\nBeyond point, can think adding appropriate labels\ncolor like.\n\n\nBeyond point, can think adding appropriate labels\ncolor like.\n","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.5 Task 4: A boxplot of outliers","text":"can also check outliers different conditions. Outliers can obviously create skewed data make mean value misleading.Create boxplot Timepoint GuiltRating check outliers.\n\ntime using ggplot() create boxplot,\nneed specify ‘x’, discrete/categorical\nvariable, ‘y’, continuous variable.\n\n\ntime using ggplot() create boxplot,\nneed specify ‘x’, discrete/categorical\nvariable, ‘y’, continuous variable.\n\n\ngeom_boxplot() - see Chapter 3 example.\n\n\ngeom_boxplot() - see Chapter 3 example.\nQuickfire QuestionsHow many outliers see? 0123too many countRemember outliers normally represented dots stars beyond whiskers boxplot. see solution data contains outlier. deal outliers today, good able spot moment. worth thinking deal outliers. numerous methods replacing given value removing participants. Remember though decision, deal outliers, deviation normality, considered written advance part preregistration protocol.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT5","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.6 Task 5: The violin plot","text":"Boxplots histograms tell slightly different information data, discuss minute, developing skills plotting interpreting . already introduced new type figure can combine information boxplot histogram one figure. using violin plot can created using geom_violin() function.Take code written boxplot (Task 4) add geom_violin another layer. may need rearrange code (.e., boxplot violin plot functions) violin plots appear underneath boxplot. Think layers visualisation!\n\nggplot() works layers - first layer (.e.,\nfirst plot call) underneath second layer. means \nget boxplot showing top violin plot, violin must come\nfirst (.e., need call geom_violin() call\ngeom_boxplot())\n\n\nggplot() works layers - first layer (.e.,\nfirst plot call) underneath second layer. means \nget boxplot showing top violin plot, violin must come\nfirst (.e., need call geom_violin() call\ngeom_boxplot())\n\n\nembellished figure little solution \ncan look basics sorted. Things like adding \nwidth call boxplot, alpha call \nviolin.\n\n\nembellished figure little solution \ncan look basics sorted. Things like adding \nwidth call boxplot, alpha call \nviolin.\nsee violin plot relates histogram created earlier? head, rotate histograms bars pointing left imagine mirror-image (making two-sided histogram). look similar violin plot. sure able still locate outlier Thirteen condition.HOLD !!!know lectures assumptions paired-sample t-test, dealing within-subjects design, paired t-test, normality actually determined based difference two conditions. , fact check distribution scores difference two conditions normally distributed. looking terms separate conditions, degree show process visualisations, really follow paired-sample t-test need look normality difference. code create violin boxplot visualisation difference two conditions now able understand code.look output, code, think whether scores difference two conditions normally distributed.Note: code , outliers appear red circles individual data points appear blue Xs. Can see controlling ? important step? say second, worth thinking minute two.\nFigure 7.1: violin boxplot showing distribution scores difference Thirteen Twelve conditions. Individual participant data show blue stars. Positive values indicate rating Thirteen condition higher rating Twelve condition. Oultiers show red circles.\nThinking Cap PointWe now checked assumptions, sort still need make decision regarding normality. look figures created, spend minutes thinking whether data normally distributed .Also, think data shows outliers, individual conditions show outlier. Lastly, important code different presentations outliers individual data points.\ns\ndata normally distributed ?\n\nlooking nice symmetry around top bottom\nhalf violin boxplot. looking median (thick\nblack line roughly middle boxplot), whiskers\nboxplot roughly equal length. Also, want violin \nbulge middle figure taper top\nbottom. also want just one bulge violin\n(symbolising scores ). Two bulges violin may\nindicative two different response patterns samples within \none dataset.\n\nNow need make judgement normality. Remember real\ndata never textbook curve normal distribution\n. always bit messier degree \njudgement call needed need use test compare \nsample distribution normal distribution. Tests \nKolmogorov-Smirnov Shapiro-Wilks tests sometimes recommended\ndetermine violation normal distribution. However, last\ntwo tests reliable depending sample size often \nrevert back judgement call. really important \nsteps documented, future researcher can check confirm \nprocess.\n\nOverall, data looks normally distributed - least visually.\n\nOutliers vs Data Points?\n\ncareful using geom_jitter() show\nindividual data points. can overcomplicate figure \nlarge number participants might make sense include\nindividual data points can just create noisy figure. \nsecond point want make sure outliers \nconfused individual data points; outliers data points, \ndata points outliers. , order confuse data \noutliers, need make sure properly controlling colors\nshapes different information figure. sure explain\nfigure legend element shows.\n\nLastly, noted plotted original\nboxplots individual conditions, Thirteen group\noutlier. However, now plot difference boxplot \nsee outliers. important reinforces plot\ncorrect data order check assumptions. assumptions \npaired-sample t-test based difference \nscores individual participants. see \ncalculate difference scores outlier, even though \noriginal data point outlier. fine within consideration \nassumption - assumption looks difference. \nsaid, might also want check original outlier \nacceptable value rating scale (e.g., 1 9) \nwild value come bad data entry (e.g., \nrating 13; say rating condition got mixed \nsomehow).\nGreat. run data checks assumptions paired-sample t-test. checked data normal distribution, using violin plot boxplot, looking skewed data outliers. understanding data, arguments field, whilst rating scale used called ordinal, many, including Furnham treat interval. Finally, spotted issues code suggest participant give response conditions, can check descriptives - conditions n = 75.check also run descriptives start understanding relationship two levels interest: Timepoint 12 Timepoint 13.","code":"\nlates %>% \n  spread(Timepoint, GuiltRating) %>%\n  mutate(diff = Thirteen - Twelve) %>%\n  ggplot(aes(x = Evidence, y = diff)) +\n  geom_violin() +\n  geom_boxplot(fill = \"red\", \n               width = .5, \n               alpha = .1, \n               outlier.colour = \"red\") +\n  geom_jitter(color = \"blue\", \n              width = .1, \n              shape = 4) + \n  theme_classic()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.7 Task 6: Calculating Descriptives","text":"Calculate mean, standard deviation, Lower Upper values 95% Confidence Interval (CI) levels Independent Variable (two time points). need also calculate n() - see Chapter 2 - Standard Error complete task. Store data tibble called descriptives.solution shows values obtain, sure go first. sure confirm groups 75 people !Answering questions may help calculating CIs:Quickfire QuestionsFrom options, equation use calculate LowerCI? mean - 1.96 * sdmean * 1.96 - semean - 1.96 * semean * 1.96 - sdFrom options, equation use calculate UpperCI? mean + 1.96 * sdmean * 1.96 + semean + 1.96 * semean * 1.96 + sd\ngroup_by() categorical column\nTimepoint. column want compare groups\n.\n\nsummarise()\n\nDifferent calculations can used within \nsummarise() function long calculated \norder require . example, first need calculate\nparticipant number, n = n(), standard\ndeviation, sd = sd(variable), order calculate \nstandard error, se = sd/sqrt(n), required \ncalculate Confidence Intervals.\n\n95% Confidence Interval, need calculate LowerCI\nUpperCI using appropriate formula. Remember \nmean + 1.96se mean - 1.96se. \ndon’t include mean just calculating much higher \nlower mean CI . want actual interval.\n\nLet’s think task. ’re looking calculate 95%\nConfidence Interval normally distributed data. \nrequire z-score tells many standard deviations \nmean. 95% area normal distribution curve lies\nwithin 1.96 standard deviations mean; .e. 1.96 SD \nmean.\n\nlooking calculate 99% Confidence Interval \ninstead use z-score 2.576. takes account greater area\nnormal distribution curve away \nmean (.e., closer tail ends curve), resulting higher\nz-score.\n","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT7","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.8 Task 7: Visualising Means and Descriptives","text":"Using data descriptives, produce plot visualises mean 95% Confidence Intervals.\nOne way basic barplot, shown previous chapters, error bars indicating 95% CI.\nadd error bars add layer like .\nFeel free embellish figure see fit.\nshown couple options solution look try adjusting, tried plot.\nOne way basic barplot, shown previous chapters, error bars indicating 95% CI.add error bars add layer like .Feel free embellish figure see fit.shown couple options solution look try adjusting, tried plot.\nrecommend using geom_col()\n\nRemember add (+) geom_errorbar() line \ncode! Don’t pipe .\n\ncode error bars, aesthetic, aes(),\nallows set min max values error bars - \nmax min CIs.\n\nposition = \"dodge\" \nposition = position_dodge() \nposition = position_dodge(width = .9). number\nways use position call thing.\nImportant remember: mentioned previous labs, barplots informative . Going ahead research, keep mind look use plots incorporate good indication distribution/spread individual data points well, needed. Barplots give good representation categorical counts like chi-square test, much ordinal interval data likely spread.Thinking Cap PointNow descriptives look need think tell us - really interpret . First thing think back hypothesis every interpretation phrased around hypothesis. hypothesised significant decrease ratings guilt, caused presentation critical evidence, Timepoint 12 Timepoint 13. think significant difference two time points? evidence ? Think overlap confidence intervals! Remember key thing stage subjective impression - \"appears might ....\" words effect.","code":"\ngeom_errorbar(aes(ymin = LowerCI, ymax = UpperCI),\n              position = \"dodge\", width = .15)"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT8","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.9 Task 8: The paired-sample t-test","text":"Now checked assumptions ran descriptives, last thing need perform paired-sample t-test test differences two time points.perform paired-sample t-test use t.test function Chapter 6. However, time add argument, paired = TRUE, tells code \"yes, paired-sample t-test\".Perform paired-sample t-test guilt ratings crucial time points (Twelve Thirteen) participants Late group. Store data (e.g., tidy) tibble called results.\nwork earlier chapters know two ways use \nt.test() function. two options :\n\nformula approach\n\nt.test(x ~ y, data, paired = TRUE/FALSE, alternative =\"two.sided\"/\"greater\"/\"less\")\n\nx columns containing DV \ny typically grouping variable (.e., \nindependent variable).\n\nvector approach\n\n\nt.test(data %>% filter(condition1) %>% pull(data),data %>% filter(condition2) %>% pull(data), paired = TRUE)\n\n\nt.test(data %>% filter(condition1) %>% pull(data),data %>% filter(condition2) %>% pull(data), paired = TRUE)\n\n\npull Twelve Thirteen\ncolumns pass condition1 condition2,\ncan use: lates %>% pull(Twelve) \nlates %>% pull(Thirteen).\n\n\npull Twelve Thirteen\ncolumns pass condition1 condition2,\ncan use: lates %>% pull(Twelve) \nlates %>% pull(Thirteen).\n\nRegardless method\n\n\nforget state paired = TRUE run\n-subjects t-test\n\n\nforget state paired = TRUE run\n-subjects t-test\n\n\n’ve calculated results, don’t forget \ntidy() - can add using pipe!\n\n\n’ve calculated results, don’t forget \ntidy() - can add using pipe!\n\n\ndon’t quite understand use tidy() yet,\nrun t.test() without tidy() see \nhappens!\n\n\ndon’t quite understand use tidy() yet,\nrun t.test() without tidy() see \nhappens!\n\n\nNote: options running t-test \ngive result. difference whether t-value \npositive negative. Remember vector approach allows \nstate condition 1 condition 2. formula approach\njust runs conditions alphabetically.\n\n\nNote: options running t-test \ngive result. difference whether t-value \npositive negative. Remember vector approach allows \nstate condition 1 condition 2. formula approach\njust runs conditions alphabetically.\nThinking Cap PointNow look output test within tibble results. group, spend time breaking results can see. understand values mean come ? may match knowledge lectures. significant difference ? write best know sure.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"Ch7InClassQueT9","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.2.10 Task 9: The Write-up","text":"Fill blanks complete paragraph, summarising results study. need refer back information within results descriptives get correct answers make sure understand output t-test.Enter values two decimal places present absolute t-value.solutions contain completed version paragraph compare .\"paired-sample t-testone-sample t-testbetween-samples t-testmatched-pairs t-test ran compare change guilt ratings (M = , SD = ) (M = , SD = ) crucial evidence heard. significantnon-significant difference found (t() = , p = .05> .05= .001< .001) Timepoint 13 average rating  units lower Timepoint 12. tells us critical evidence influence rating guilt jury membersthat critical evidence influence rating guilt jury membersthat critical evidence rating guilt jury members unconnectedsomething quite sure right now, best check!`\n\nt-tests take following format: t(df) = t-value, p =\np-value\n\n\nt-tests take following format: t(df) = t-value, p =\np-value\n\n\nresults states degrees freedom \nparameter, t-value \nstatistic.\n\n\nresults states degrees freedom \nparameter, t-value \nstatistic.\n\n\nestimate mean difference ratings \nTimepoints Twelve Thirteen.\n\n\nestimate mean difference ratings \nTimepoints Twelve Thirteen.\n\n\nconf.low conf.high values \n95% Confidence Intervals mean difference two\nconditions. haven’t included write-\n. written something like, “difference\ntwo groups (M = -1.76, 95% CI = [-2.19, -1.33])”.\n\n\nconf.low conf.high values \n95% Confidence Intervals mean difference two\nconditions. haven’t included write-\n. written something like, “difference\ntwo groups (M = -1.76, 95% CI = [-2.19, -1.33])”.\nTwo tips:write future report, can make write-reproducible well using output tibbles calling specific columns. example, t(`r results$parameter`) = `r results$statistic %>% abs()`, p < .001, knitted become t(74) = 8.23, p < .001. , code can prevent mistakes write-ups! However, working rounding p-values can tricky offered code solutions show .write future report, can make write-reproducible well using output tibbles calling specific columns. example, t(`r results$parameter`) = `r results$statistic %>% abs()`, p < .001, knitted become t(74) = 8.23, p < .001. , code can prevent mistakes write-ups! However, working rounding p-values can tricky offered code solutions show .Another handy function writing round() function putting values given number decimal places. example wanted round absolute t-value two decimal places might results %>% pull(statistic) %>% abs() %>% round(2) give t = 8.23. maybe want three decimal places: results$statistic %>% abs() %>% round(3) give t = 8.232. really handy function follows format round(value_to_round, number_of_decimal_places).Another handy function writing round() function putting values given number decimal places. example wanted round absolute t-value two decimal places might results %>% pull(statistic) %>% abs() %>% round(2) give t = 8.23. maybe want three decimal places: results$statistic %>% abs() %>% round(3) give t = 8.232. really handy function follows format round(value_to_round, number_of_decimal_places).Excellent work! can see performing t-test small part entire process: wrangling data, calculating descriptives, plotting data check distributions assumptions major part analysis process. past chapters, building skills able see put good use now moved onto complex data analysis. Running inferential part usually just one line code.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"practice-your-skills-10","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.3 Practice Your Skills","text":"Using data , perform paired-sample t-test Middle group crucial evidence presented Timepoint 9. Save current .Rmd file, make copy , rename accordingly, go steps Middle group. Since exercise close example worked chapter, added solutions end. Feel free check peers discuss difference ratings guilt, caused presentation critical evidence, Timepoint 8 Timepoint 9?","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"non-parametric-tests","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4 Non-Parametric tests","text":"previous chapters really focusing -subjects within-subjects t-tests fall category parametric tests. One main things know tests fair number assumptions need check first make sure results valid. looked main body chapter, get practice progress, one question might , assumptions met (\"violated\"\" termed)?options ? tests known non-parametric tests fewer assumptions parametric tests can run quite quickly using approach seen t-tests. non-parametric \"t-tests\" generally require assumption normality tend work either medians data (opposed mean values) rank order data - .e., highest value, second highest, lowest - opposed actual value.just like slightly different versions t-tests different non-parametric tests -subjects designs within-subjects designs:Mann-Whitney U-test non-parametric equivalent -subjects t-testThe Wilcoxon Signed-Ranks Test non-parametric equivalent within-subjects t-test.Note: Mann-Whitney Wilcoxon Signed-Ranks tests now bit antiquated designed done hand computer processing power limited. However, still used Psychology still see older papers, worth seeing one action least.example, concerned data far normally distributed, might use Mann-Whitney Wilcoxon Signed-Ranks Test depending design. run Mann-Whitney U-test can try Wilcoxon Signed-Ranks Test time uses function - just matter saying paired = TRUE.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"our-scenario","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.1 Our Scenario","text":"Aim: examine influence perceived reward problem solving.Procedure: 14 Participants 2 groups (7 per group) asked solve difficult lateral thinking puzzle. One group offered monetary reward completing quick possible. One group offered nothing; just internal joy getting task completed correct.Task: participants asked solve following puzzle. \"Man walks bar asks glass water. barman shoots gun. man smiles, says thanks, leaves. ?\"IV: Reward group vs. Reward groupDV: Time taken solve puzzle measured minutes.Hypothesis: hypothesise participants given monetary incentive solving puzzle solve puzzle significantly faster, measured minutes solve puzzle, given incentive.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"assumption-check","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.2 Assumption check","text":"data boxplot data try visualise happening data.\n\nTable 7.2: Table showing time taken complete puzzle Reward Reward groups\n\nFigure 7.2: Boxplots showing time taken solve puzzle two conditions, Reward vs Reward. Outliers represented solid blue dots.\nLooking boxplots potentially issues skew data (see Reward group particular) conditions showing least one outlier. convinced assumption normality held run Mann-Whitney U-test - non-parametric equivalent two-sample t-test (.e., independent groups) - require assumption normal data.","code":"## Warning: The `<scale>` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n## of ggplot2 3.3.4.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was\n## generated."},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"the-descriptives","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.3 The descriptives","text":"Next, always, look descriptives well make subjective, descriptive inference pattern results. One thing note Mann-Whitney analysis based \"rank order\" data regardless group. code table put data order lowest highest added rank order column. used rank() function create ranks setting ties.method = \"average\". go case can read ?rank.\nTable 7.3: Table showing time taken complete puzzle Reward Reward groups rank order times.\ntable descriptives dataset code used create .\nTable 7.4: Descriptive (N, Medians Mean Ranks) two groups (Reward vs Reward) time taken solve puzzle.\nBased figure descriptive data, can suggest appears real difference two groups terms time taken solve puzzle. group offered reward slightly higher spread data reward group. However, medians mean ranks comparable.","code":"\nscores_rnk <- scores %>%\n  arrange(Time) %>%\n  mutate(ranks = rank(Time, ties.method = \"average\"))\nByGrp <- group_by(scores_rnk, Group) %>%\n  summarise(n_Pp = n(),\n            MedianTime = median(Time),\n            MeanRank = mean(ranks))"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"running-the-inferential-test","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.4 Running the inferential test","text":"now run Mann-Whitney U-test see difference two groups significant . , somewhat confusingly, us wilcox.test() function. code analysis current data (tibble scores, DV column Time, IV column Group) shown . works just like t-test() function can use either vectors formula approach.Note:couple additional calls function can read using ?wilcox.test() approach.just easily used scores_rnk tibble wilcox.test() opposed scores. using scores show need put ranks wilcox.test() function, create runs analysis. created run descriptives.output test tidied tibble using tidy()\nTable 7.5: Output Mann-Whitney U-test\nmain statistic (test-value) Mann-Whitney U-test called U-value shown table statistic; .e. U = 19 can see results difference non-significant p = 0.535.Note:eagle-eyed spot test actually says Wilcoxon rank sum test. fine. Mann-Whitney U-test calculated sum ranks (shown table ). Wilcoxon rank sum test just , sum ranks. U-value created summed ranks.mistake Wilcoxon rank sum test mentioned - -subjects - Wilcoxon Signed-Ranks test within-subjects mentioned . different tests.However, one thing note U unstandardised value - meaning dependent values sampled compared U values look magnitude one effect versus another. second thing note U-value wilcox.test() return different U-value depending condition stated Group 1 Group 2.Compare outputs two tests switched order conditions:Version 1:Version 2:U-value two tests , Version 1, U = 30 Version 2, U = 19. may seem odd, actually test correct. However, strictly speaking U-value smaller two-values given different outputs. U-value calculated. groups U-value one checked significance smaller two.U unstandardised value, present Mann-Whitney U-test usually also give Z-statistic, standardised version U-value. also present effect size, commonly r.Z r can calculated follows:Z = \\(\\frac{U - \\frac{N1 \\times N2}{2}}{\\sqrt\\frac{N1 \\times N2 \\times (N1 + N2 + 1)}{12}}\\)Z = \\(\\frac{U - \\frac{N1 \\times N2}{2}}{\\sqrt\\frac{N1 \\times N2 \\times (N1 + N2 + 1)}{12}}\\)r = \\(\\frac{Z}{\\sqrt(N1 + N2)}\\)r = \\(\\frac{Z}{\\sqrt(N1 + N2)}\\)Putting formulas coded format look like :write-written :time taken solve problem Reward group (n = 7, Mdn Time = 9.02, Mean Rank = 8.29) reward group (n = 7, Mdn Time = 8.05, Mean Rank = 6.71) compared using Mann-Whitney U-test. significance difference found, U = 19, Z = -0.703, p = 0.535, r = -0.188","code":"\nresult <- wilcox.test(Time ~ Group, \n                      data = scores, alternative = \"two.sided\", \n            exact = TRUE, correct = FALSE) %>%\n  tidy()\nresult_v1 <- wilcox.test(scores %>% filter(Group == \"Reward\") %>% pull(Time),\n                         scores %>% filter(Group == \"No Reward\") %>% pull(Time),\n                         data = scores, alternative = \"two.sided\",\n                         exact = TRUE, correct = FALSE) %>%\n  tidy()\nresult_v2 <- wilcox.test(scores %>% filter(Group == \"No Reward\") %>% pull(Time),\n                         scores %>% filter(Group == \"Reward\") %>% pull(Time),\n                         data = scores, alternative = \"two.sided\",\n                         exact = TRUE, correct = FALSE) %>%\n  tidy()\nU <- result$statistic\nN1 <- ByGrp %>% filter(Group == \"Reward\") %>% pull(n_Pp)\nN2 <- ByGrp %>% filter(Group == \"No Reward\") %>% pull(n_Pp)\nZ <- (U - ((N1*N2)/2))/ sqrt((N1*N2*(N1+N2+1))/12)\nr <- Z/sqrt(N1+N2)"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"last-point-on-calculating-u-and-reporting-the-test","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.4.5 Last point on calculating U and reporting the test","text":"final write-know, codes, U = 19 smallest U-value test. However put alternative U, U = 30, calculated Z got Z = 0.703, opposed Z = -0.703. standardised statistic value just opposite polarity (either positive negative). fine though can look medians mean ranks make sure interpreting data correctly.However, watch writing test presenting correct U-value - remembering technically present smallest two U-values (refer back Version 1 Version 2 using wilcox.test()) . Fortunately run analyses figure smaller U (though wanted). quicker way using formula:\\(U1 + U2 = N1 \\times N2\\)U1 U-value wilcox.test() functionN1 number people one group (technically matter group) N2 number people group.actually know U-values ran tests; U1 = 30 U2 = 19, know two groups N1 = 7 N2 = 7. put numbers formula get\\(U1 + U2 = N1 \\times N2\\)=> \\(30 + 19 = 7 \\times 7\\)=> \\(49 = 49\\)sides equal 49. say know one U-values; course know Ns. Well can quickly figure U-value based :\\(U2 = (N1 \\times N2) - U1\\)example, know U1 = 19, N1 = 7 N = 7 :\\(U2 = (7 \\times 7) - 19\\)=> \\(U2 = (49) - 19\\)=> \\(U2 = 30\\)just present smallest two U-values, case U = 19.Hopefully now decent understanding Mann-Whitney test. also try running Wilcoxon Signed-Ranks Test well though might read little present . similar Mann-Whitney though able get .Oh, last last point, remember test ? Mann-Whitney -subjects within-subjects, Wilcoxon Signed-Ranks test ? know different designs, ? Well, silly memory aid : name -subjects designs, know, independent designs. Add fact late great singer Whitney Houston starred \"Bodyguard\" maintaining right freedom independence. whenever get stuck knowing test , remember Whitney wanted independence \"Bodyguard\" ok. say good memory aid!","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"solutions-to-questions-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5 Solutions to Questions","text":"find solutions questions Activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"the-paired-sample-t-test-1","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1 The Paired-Sample t-test","text":"","code":""},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-1-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.1 Task 1","text":"Return Task","code":"\nlibrary(broom)\nlibrary(tidyverse)\n\nratings <- read_csv(\"GuiltJudgements.csv\")"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-2-7","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.2 Task 2","text":"carried correctly, lates 150 rows 4 columns. comes 75 participants giving two responses - TimePoint 12 TimePoint 13.Return Task","code":"\nlates <- ratings %>%\n  filter(Evidence == \"Late\") %>% \n  select(Participant, Evidence, `12`, `13`) %>% \n  rename(Twelve = `12`, Thirteen = `13`) %>%\n  pivot_longer(cols = Twelve:Thirteen, \n               names_to = \"Timepoint\", \n               values_to = \"GuiltRating\")"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-3-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.3 Task 3","text":"\nFigure 7.3: Potential Solution Task 3\nReturn Task","code":"\nlates %>% \n  ggplot(aes(GuiltRating)) +\n  geom_histogram(binwidth = 1) +\n facet_wrap(~Timepoint) +\n  labs(x = \"GuiltRating\", y = NULL) +\ntheme_bw()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-4-6","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.4 Task 4","text":"Task asks boxplot. added additional functions tidy figure bit might want play .\nFigure 7.4: Potential Solution Task 4\ncan see one outlier Thirteen condition. represented single dot far whiskers boxplot.Return Task","code":"\nlates %>% \n  ggplot(aes(x = Timepoint,\n             y = GuiltRating)) + \n  geom_boxplot() +\n  scale_y_continuous(breaks = c(1:9)) + \n  coord_cartesian(xlim = c(.5, 2.5), ylim = c(1,9), expand = TRUE) +\n  theme_bw()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-5-5","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.5 Task 5","text":"added color necessary:\nFigure 7.5: Potential Solution Task 5\ncan still see outlier top figure solid black dot.even add geom_jitter data points:\nFigure 7.6: Alternative Potential Solution Task 5\nReturn Task","code":"\nlates %>% \n  ggplot(aes(x=Timepoint,y=GuiltRating))+\n  geom_violin(aes(fill = Timepoint), alpha = .2) + \n  geom_boxplot(width = 0.5) +\n  scale_y_continuous(breaks = c(1:9)) + \n  coord_cartesian(ylim = c(1,9), expand = TRUE) +\n  theme_bw()\nlates %>% \n  ggplot(aes(x=Timepoint,y=GuiltRating))+\n  geom_violin(aes(fill = Timepoint), alpha = .2) + \n  geom_boxplot(width = 0.5) +\n  geom_jitter(aes(fill = Timepoint), width = .1, alpha = .2) + \n  scale_y_continuous(breaks = c(1:9)) + \n  coord_cartesian(ylim = c(1,9), expand = TRUE) +\n  theme_classic()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-6-4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.6 Task 6","text":"show following data:\nTable 7.6: Descriptive data current study\nReturn Task","code":"\ndescriptives <- lates %>% \n  group_by(Timepoint) %>%\n  summarise(n = n(),\n            mean = mean(GuiltRating),\n            sd = sd(GuiltRating),\n            se = sd/sqrt(n),\n            LowerCI = mean - 1.96*se,\n            UpperCI = mean + 1.96*se)\nknitr::kable(descriptives, align = \"c\", caption = \"Descriptive data for the current study\")"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-7-4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.7 Task 7","text":"basic barplot 95% Confidence Intervals.embellished figure little can mess around code see bit .\nFigure 7.7: Possible Solution Task 7\nOne thing watch code scale_y_continuous() function helps us set length tick marks (-) y-axis. Rather oddly, set limits = ... values ylim = ... coord_cartesian() figure behave oddly may disappear. coord_cartesian() zoom function must set within limits scale, set scale_y_continuous().One thing watch code scale_y_continuous() function helps us set length tick marks (-) y-axis. Rather oddly, set limits = ... values ylim = ... coord_cartesian() figure behave oddly may disappear. coord_cartesian() zoom function must set within limits scale, set scale_y_continuous().alternative way display just means errorbars use pointrange approach. image shows 95% CIAn alternative way display just means errorbars use pointrange approach. image shows 95% CI\nFigure 7.8: Alternative Solution Task 7\nReturn Task","code":"\nggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + \n  geom_col(colour = \"black\") +\n  scale_fill_manual(values=c(\"#999000\", \"#000999\")) +\n  scale_x_discrete(limits = c(\"Twelve\",\"Thirteen\")) +\n  labs(x = \"Timepoint of Evidence\", y = \"GuiltRating\") +\n  guides(fill=\"none\") +\n  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI),\n                position = \"dodge\", width = .15) +\n  scale_y_continuous(breaks = c(1:9), limits = c(0,9)) +\n  coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) +\n  theme_classic()\nggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + \n  geom_pointrange(aes(ymin = LowerCI, ymax = UpperCI))+\n  scale_x_discrete(limits = c(\"Twelve\",\"Thirteen\")) +\n  labs(x = \"Timepoint of Evidence\", y = \"GuiltRating\") +\n  guides(fill=\"none\")+\n  scale_y_continuous(breaks = c(1:9), limits = c(0,9)) +\n  coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) +\n  theme_bw()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-8-4","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.8 Task 8","text":"Remember set paired = TRUE run within-subjects t-testAlternatively, using filter() pull() functions make force given condition first condition. , , forcing condition Thirteen first condition values match approach. forced condition Twelve first condition difference t-value change polarity (positive negative vice versa).reason two outputs formula (top) method (x ~ y) actually process second approach, just sure first condition. second approach (bottom) just makes clearer.Note: conf.low conf.high values 95% Confidence Intervals mean difference two conditions. written something like, \"difference two groups (M = -1.76, 95% CI = [-2.19, -1.33])\".Return Task","code":"\nresults <- t.test(GuiltRating ~ Timepoint, \n                  data = lates, \n                  paired = TRUE, \n                  alternative = \"two.sided\") %>% tidy()\nresults <- t.test(lates %>% filter(Timepoint == \"Thirteen\") %>% pull(GuiltRating),\n                  lates %>% filter(Timepoint == \"Twelve\") %>% pull(GuiltRating),\n                  paired = TRUE, \n                  alternative = \"two.sided\") %>% tidy()"},{"path":"nhst-paired-sample-t-test-and-nonparametric-tests.html","id":"task-9-1","chapter":"7 NHST: Paired-Sample t-test and Nonparametric tests","heading":"7.5.1.9 Task 9","text":"potential write-study follows:\"paired-samples t-test ran compare change guilt ratings (M = 5.8, SD = 1.5) (M = 4.04, SD = 1.93) crucial evidence heard. significant difference found (t(74) = 8.23, p = 4.7113406^{-12}) Timepoint 13 average rating 1.76 units lower Timepoint 12. tells us critical evidence influence rating guilt jury members.\"Working rounding p-valuesWhen rounding p-values less .001, rounding give value 0 technically wrong - probability low 0. , according APA format, values less .001 normally written p < .001. create reader-friendly p-value, try something like following code:instead writing t(74) = 8.23, p = 4.7113406^{-12}, write t(74) = 8.23, p < .001The -line coding options look like:p = `r results %>% pull(p.value)` p = 4.7113406^{-12}&`r ifelse(results$p.value < .001, \"p < .001\", paste0(\"p = \", round(results$p.value,3)))` p < .001Return TaskChapter Complete!","code":"\nifelse(results$p.value < .001, \n       \"p < .001\", \n       paste0(\"p = \", round(results$p.value,3))) "},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"apes---alpha-power-effect-sizes-sample-size","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8 APES - Alpha, Power, Effect Sizes, Sample Size","text":"","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"overview-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.1 Overview","text":"now mainly spent time data-wrangling, understanding probability, visualising data, recently, running inferential tests, .e. t-tests. lectures, however, also started learn additional aspects inferential testing trying reduce certain types error analyses. balance minimising error inferential statisitcs focus chapter.First thing remember two types hypotheses Null Hypothesis Significance Testing (NHST) trying establish probability null hypothesis accepted. two hypotheses :null hypothesis states compared values equivalent , referring means, written : \\(H_0: \\mu_1 = \\mu_2\\)alternative hypothesis states compared values equivalent , referring means, written : \\(H_1: \\mu_1 \\ne \\mu_2\\).Now, decision hypothesis prone degree error , learn, two main types error worry Psychology :Type error - False Positives, error rejecting null hypothesis rejected (otherwise called alpha \\(\\alpha\\)). words, conclude real \"effect\" fact effect. field standard rate acceptable false positives \\(\\alpha = .05\\) meaning theory 1 20 studies may false positive.Type II error - False Negatives, error retaining null hypothesis false (otherwise called beta \\(\\beta\\)). words, conclude real \"effect\" fact one. field standard rate acceptable false negatives \\(\\beta = .2\\) meaning theory 1 5 studies may false negative.Adding ideas hypotheses errors, going look idea power learn long-run probability correctly rejecting null hypothesis fixed effect size fixed sample size; .e. correctly concluding effect real effect detect. Power calculated \\(power = 1-\\beta\\) directly related False Negative rate. field standard False Negatives \\(\\beta = .2\\) field standard power \\(power = 1 - .2 = .8\\), given effect size sample size (though papers, including Registered Reports often required power least \\(power >= .9\\)). , \\(power = .8\\) means majority studies find effect one detect, assuming study maintains rates error power.Unfortunately, however, psychological research criticised neglecting power \\(\\beta\\) planning studies resulting called \"underpowered\" \"low powered\" studies - meaning error rates higher think , power lower think , study unreliable. Note \\(\\beta\\) increases (false negative rate increases), power decreases; power false positive rates also related, though less directly. fact, low powered studies, combined undisclosed analytical flexibility publication bias, thought key issue replication crisis within field. may large number studies null hypothesis rejected , unpublished studies written find effect . turn, case, field becomes noisy unsure studies replicate. issues like led us redevelop courses really want understand power much possible.chapter power, error rates, effect sizes, sample sizes. learn:relationship power, alpha, effect sizes sample sizeshow calculate certain effect sizeshow determine appropriate sample sizes given scenariosand interpret power analyses.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"introduction-to-power","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2 Introduction to Power","text":"written selected material chapter give better understanding power interacts effect size, sample size, alpha. also suggest optional material can look play get rounder view.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"blog-post","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2.1 Blog post","text":"Read following blog Power: blog fictional conversation professor student importance power. Grab coffee read. worry reading additional papers unless want ; just blog fine get understanding. trying understand blog relationship sample size effect sizes, whether result study likely replicate based power original study.Power Dialogues PIGEE University Illinois.Using power design studyTo reiterate, power defined probability correctly rejecting null hypothesis fixed effect size fixed sample size. , power key decision design study, premise higher power planned study, better.Two relationships important understand:given sample size \\(\\alpha\\), power study higher effect looking assumed large effect opposed small effect; large effects easier detect., given effect size \\(\\alpha\\), power study higher increase sample size.relationships see , little control size effect trying detect (lives real world control), can instead increase power study increasing size sample (also reducing sources noise measurement error study). , planning study, good researcher consider following four key elements - APES:alpha (false positive rate - Type 1 error): commonly thought significance level; usually set \\(\\alpha = .05\\)power: probability rejecting null hypothesis given effect size sample size, \\(power = .8\\) usually cited minimum power aim based false negative rate set \\(\\beta = .2\\);effect size: size association difference trying detect;sample size: number observations (usually, participants, sometimes also stimuli) study.Note: power depends several variables, useful think power function varying value rather single fixed quantity.Now cool thing APES know three elements, can calculate fourth. reality, two common approaches designing study :determine appropriate sample size required reject null hypothesis, high probability, effect size interested . , decide \\(\\alpha\\), \\(power\\), effect size, calculate sample size required study. Generally, smaller assumed effect size, participants need, assuming power alpha held constant.determine smallest effect size can reliably detect given sample size. , know everything except effect size. example, say using open dataset know run 100 participants, add participants, want know minimum effect size detect dataset set \\(power\\) \\(\\alpha\\) field standards.Hopefully gives idea use power determine sample sizes studies - sample size just pulled thin air. approaches described called priori power analyses stating power level want (priori means ) study.However, may now thinking, everything connected, can use effect size study sample size determine power study run ? ! Well, can wrong . actually called Observed Post-Hoc power papers discourage calculating grounds effect size using true effect size population interested ; just effect size sample. indication power analysis misleading. Avoid . can read , , time like:  Lakens (2014) Observed Power, editor asks post-hoc power analyses. short, stick using priori power analyses approaches use determine required sample size achievable reliable effect size.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"video-about-power-and-sample-size","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2.2 Video about power and sample size","text":"now also watch short nonetheless highly informative video Daniel Lakens Power Sample Size. help consolidate points. shirt amazing!Power Analysis Sample Size Decisions Daniel Lakens","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"useful-links","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.2.3 Useful links","text":"Finally, number great webpages blogs help understand concepts chapter. think might good look . look understand chapter, come back really help progress becoming responsible researcher. deliberately giving number options everyone one analogy work best one paper make everything click place. example different person person variety explanations help.YouTube video Dan Quintana (University Oslo) showing use pwr package calculate power t-tests, correlations, one-way ANOVAs https://www.youtube.com/watch?v=ZIjOG8LTTh8A YouTube video Dan Quintana (University Oslo) showing use pwr package calculate power t-tests, correlations, one-way ANOVAs https://www.youtube.com/watch?v=ZIjOG8LTTh8A shiny app created Lisa Debruine (University Glasgow) guessing effect size two conditions http://shiny.psy.gla.ac.uk/guess/shiny app created Lisa Debruine (University Glasgow) guessing effect size two conditions http://shiny.psy.gla.ac.uk/guess/blog Daniel Lakens (Eindhoven University Technology) determining smallest effect size interested . often referred Smallest Effect Size Interest (SESOI) http://daniellakens.blogspot.com/2017/05/-power-analysis-implicitly-reveals.htmlA blog Daniel Lakens (Eindhoven University Technology) determining smallest effect size interested . often referred Smallest Effect Size Interest (SESOI) http://daniellakens.blogspot.com/2017/05/-power-analysis-implicitly-reveals.htmlAn interactive webpage Kristoffer Magnusson (Karolina Instituet, Stockholm) interpreting Cohen's d effect size https://rpsychologist.com/d3/cohend/interactive webpage Kristoffer Magnusson (Karolina Instituet, Stockholm) interpreting Cohen's d effect size https://rpsychologist.com/d3/cohend/shiny app Hause Lin (University Toronto) showing conversion one effect size another http://escal.site/shiny app Hause Lin (University Toronto) showing conversion one effect size another http://escal.site/Frontiers Psychology paper Daniel Lakens calculating various effect sizes t-tests ANOVAs https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/fullA Frontiers Psychology paper Daniel Lakens calculating various effect sizes t-tests ANOVAs https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/fullA blog Daniel Lakens Type Type II errors acceptable. short, justify everything http://daniellakens.blogspot.com/2019/05/justifying--alpha--minimizing-.htmlA blog Daniel Lakens Type Type II errors acceptable. short, justify everything http://daniellakens.blogspot.com/2019/05/justifying--alpha--minimizing-.htmlChapter 9 Ian Walker's \"Research Methods statistics\" availble read online University Library. short chapter hypothesis testing power really brining everything last chapters together.Chapter 9 Ian Walker's \"Research Methods statistics\" availble read online University Library. short chapter hypothesis testing power really brining everything last chapters together.forget Chapter 3 \"7 Deadly Sins Psychology: Manifesto Reforming Culture Scientific Practice\" good read topic power unreliable research. book available University Library can bought reputable bookshops online repositories.forget Chapter 3 \"7 Deadly Sins Psychology: Manifesto Reforming Culture Scientific Practice\" good read topic power unreliable research. book available University Library can bought reputable bookshops online repositories.really nice paper Marjan Bakker colleagues whether people put power analyses ethics proposals. nice introduction power results show many different ways researchers actually calculate sample sizes https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079A really nice paper Marjan Bakker colleagues whether people put power analyses ethics proposals. nice introduction power results show many different ways researchers actually calculate sample sizes https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079A paper Marcus Munafo colleagues mentioned many times might help read now concepts ideas famility now. https://www.nature.com/articles/s41562-016-0021A paper Marcus Munafo colleagues mentioned many times might help read now concepts ideas famility now. https://www.nature.com/articles/s41562-016-0021A paper Schafer Schwarz (2019) aimed helping people make meaningful interpretation effect sizes Psychology. also explores differences commonly found effect sizes sub-disciplines Psychology. https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/fullA paper Schafer Schwarz (2019) aimed helping people make meaningful interpretation effect sizes Psychology. also explores differences commonly found effect sizes sub-disciplines Psychology. https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/fullFinally, paper Brysbaert (2019) shows just many participants need variety common designs Psychology studies. shocked difference number participants needed compared number participants used published studies https://www.journalofcognition.org/article/10.5334/joc.72/Finally, paper Brysbaert (2019) shows just many participants need variety common designs Psychology studies. shocked difference number participants needed compared number participants used published studies https://www.journalofcognition.org/article/10.5334/joc.72/Hopefully given good basis understanding power, sample sizes, alpha, effect sizes. difficult concepts grasp take lot time thinking interacting really start sink . Hopefully however, nothing else, least come away idea number participants run study arbitrary decision fact relationship effect size want test level error (Type Type II) willing accept.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practical-apes-calculations","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3 Practical APES Calculations","text":"Hopefully now decent understanding least four APES need considered designing study: \\(\\alpha\\), \\(power\\), effect size sample size. going look calculating understanding elements. fully understand everything power complete chapter - believe us say many seasoned researchers struggle parts - just need get general gist always level acceptable error hypothesis testing trying minimise given effect size (.e., magnitude difference, relationship, association).jump bit now start running analyses help understanding alpha, power, effect sizes sample size! help understanding focus t-tests chapter know well previous chapters.Effect Sizes - Cohen's \\(d\\)number different effect sizes can choose calculate, common one t-tests Cohen's d: standardised difference two means (units SD) written d = effect-size-value. key point Cohen's d standardised difference, meaning can used compare studies regardless measurement made. Take example height differences men women estimated 5 inches (12.7 cm). effect size, unstandardised effect size every sample test, difference dependent measurement tools, measurement scale, errors contained within . using standardised effect size allows make comparisons across studies regardless measurement error. standardised terms, height difference considered medium effect size (d = .5) Cohen (1988, cited Schafer & Schwarz (2019)) defined representing \"effect likely visible naked eye careful observer\". Cohen (1988) fact stated three sizes Cohen's d people use guide:may wish read paper later different effect sizes psychology - Schafer Schwarz (2019) Meaningfulness Effect Sizes Psychological Research: Differences Sub-Disciplines Impact Potential Biases.One thing note formula Cohen's d slightly different depending type t-test used. even within type t-test formula can sometimes change depending read. chapter, go following formulas:One-sample t-test & within-subjects (paired-sample) t-test:\\[d = \\frac{t}{sqrt(N)}\\]-subjects (two-sample) t-test:\\[d = \\frac{2t}{sqrt(df)}\\]now try using formulas order calculate effect sizes given scenarios; work calculating power later chapter.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.1 Task 1: Effect size from a one-sample t-test","text":"run one-sample t-test discover significant effect, t(25) = 3.24, p = .003. Calculate d determine whether effect size small, medium large.\nUse appropriate formula one-sample\nt-tests.\n\ngiven t-value df (degrees freedom), still\nneed determine n calculate\nd.\n\nAccording Cohen (1988), effect size small (.2 .5),\nmedium (.5 .8) large (> .8).\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter, digits, many people run study: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test considered: smallmediumlarge","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT2","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.2 Task 2: Effect size from a two-sample t-test","text":"run two-sample t-test discover significant effect, t(30) = 2.9, p = .007. Calculate d determine whether effect size small, medium large.\nUse appropriate formula two-sample t-tests.\n\nremember df = (N-1) + (N-1) two-sample t-test.\n\nAccording Cohen (1988), effect size small (.2 .5),\nmedium (.5 .8) large (> .8).\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter, digits, many people run study: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test considered: smallmediumlarge","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT3","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.3 Task 3: Effect Size from a matched-sample t-test","text":"run paired-sample t-test ASD sample non-ASD sample discover significant effect t(39) = 2.1, p < .05. many people group? Calculate d determine whether effect size small, medium large.\nneed df value determine N.\n\nmatched pairs treated like paired-sample t-test two\nseparate groups.\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter, digits, many people group study. Note, total number participants: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test considered: smallmediumlarge\ndf paired-samples matched-pairs t-test calculated\ndf = N - 1.\nConversely, find total number participants:\nN = df + 1 N = 39 + 1 = 40.\n\nGiven matched-pairs t-test, design \nequal number participants group. Therefore 40 participants\ngroup.\n","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT4","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.4 Task 4: t-value and effect size for a between-subjects experiment","text":"run -subjects design study descriptives tell : Group 1, M = 10, SD = 1.3, n = 30; Group 2, M = 11, SD = 1.7, n = 30. Calculate t d -subjects experiment.\ncan calculate d (using appropriate\nformula -subjects experiment), need first calculate\nt using formula:\n\nt = (Mean1 - Mean2)/sqrt((var1/n1) + (var2/n2))\n\nvar stands variance formula. Variance\nstandard deviation, right? Variance measured\nsquared units. equation, require variance \ncalculate t standard deviation, \nneed remember \\(var = SD^2\\)\n(otherwise written \\(var = SD \\times SD\\).\n\nNow t-value, calculating d \nalso need degrees freedom. Think calculate\ndf -subjects experiment, taking n\nGroup 1 Group 2 account.\n\nRemember convention people report t \npositive. , convention also dictates d \nreported positive value.\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Enter correct t-value test, rounded two decimal places: codes appropriate calculation d instance:d = t/sqrt(N)d = 2t/sqrt(df)Based t-value , enter correct value d analysis rounded 2 decimal places: According Cohen (1988), effect size t-test described : smallmediumlargeNow comfortable calculating effect sizes, look using establish sample size required power. One thing realise progress true effect size population something know, need justify one design. clever approach laid Daniel Lakens blog previous section Smallest Effect Size Interest (SESOI) - set smallest effect interested ! can determined theoretical analysis, previous studies, pilot studies, rules thumb like Cohen (1988). However, also keep mind lower effect size, larger sample size need. Everything trade-!Power CalculationsWe going use function pwr.t.test() run calculations pwr library. really useful library functions various tests, just use t-tests right now.Remember information function pwr.t.test(), simply ?pwr.t.test console. can look webpages get idea (bad ideas spot erroneously calculate post-hoc power!):quick-R summary pwr package - https://www.statmethods.net/stats/power.htmlthe pwr package vignette -  https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.htmlFrom see pwr.t.test() takes series inputs:n: observations/participants, per group independent samples version, number subjects matched pairs paired one-sample designs.d: effect size interestsig.level \\(\\alpha\\)power \\(1-\\beta\\)type: type t-test; e.g. \"two.sample\", \"one.sample\", \"paired\"alternative: type hypothesis; \"two.sided\", \"one.sided\"works leave one principle. give info returns element missing. , example, say needed know many people per group need detect effect size low d = .4 power = .8, alpha = .05 two.sample (-subjects) t-test two.sided hypothesis test. :show following output, , look , tells need 99.0803248 people per condition.get whole people like conservative estimates actually run 100 per condition. lot people!!!One problem though output pwr.t.test() object easy work terms getting values reproducible. However, function purr::pluck() allows us pluck values objects. code look like thisSo call n_test get answer , saved single value easier work :use ceiling() funtion round whole people:Note: ceiling() better use round() dealing people always rounds . example, ceiling(1.1) gives 2. round() hand useful rounding effect size, example, two decimal places - e.g. d = round(.4356, 2) give d = 0.44We use approach pwr.t.test() %>% pluck() pwr.t.test() %>% pluck() %>% ceiling() throughout rest chapter get used .\nstart next task, need make sure loaded tidyverse.","code":"\npwr.t.test(d = .4,\n           power = .8,\n           sig.level = .05,\n           alternative = \"two.sided\",\n           type = \"two.sample\")## \n##      Two-sample t test power calculation \n## \n##               n = 99.08032\n##               d = 0.4\n##       sig.level = 0.05\n##           power = 0.8\n##     alternative = two.sided\n## \n## NOTE: n is number in *each* group\nn_test <- pwr.t.test(d = .4, \n                     power = .8,\n                     sig.level = .05,\n                     alternative = \"two.sided\",\n                     type = \"two.sample\") %>%\n  pluck(\"n\")\nn_test## [1] 99.08032\nn_test %>% ceiling()## [1] 100"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.5 Task 5: Sample size for standard power one-sample t-test","text":"Assuming smallest effect size interest Cohen's d d = .23, minimum number participants need one-sample t-test, assuming \\(power = .8\\), \\(\\alpha = .05\\), two-sided hypothesis?Using pipeline, store answer single value called sample_size (e.g., think pluck()) round nearest whole participant.\nUse list inputs kind checklist clearly\ndetermine inputs known unknown. can help enter \nappropriate values code.\n\nstructure pwr.t.test() similar\none shown except two.sample become one.sample\n\nalso need use pluck(\"n\") help obtain\nsample size %>% ceiling() round \nnearest whole participant.\nQuickfire QuestionsAnswer following question check answers. solutions end chapter check :Enter minimum number participants need one-sample t-test: ","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.6 Task 6: Effect size from a high power between-subjects t-test","text":"Assuming run -subjects t-test 50 participants per group want power .9, minimum effect size can reliably detect? Assume field standard \\(\\alpha = .05\\) alternative hypothesis settings (\"two-tailed\"). Using pipeline, store answer single value called cohens round two decimal places.\n, use list inputs kind checklist \nclearly determine inputs known unknown. can help \nenter values code.\n\nalso need use pluck() obtain Cohen’s\nd, round() value rounded two\ndecimal places.\n\nDon’t forget quotes using pluck().\n.e. pluck(\"value\") pluck(value)\nQuickfire QuestionsAnswer following questions check answers. solutions end chapter:Based information given, set type function? one.sampletwo.sampleBased output, enter minimum effect size can reliably detect test, rounded two decimal places: According Cohen (1988), effect size t-test smallmediumlargeSay run study find effect size determined d = .50. Given know power, select statement accurate: study sufficiently powered analysis indicates can reliably detect effect sizes smaller d = .65the study potentially underpowered analysis indicates can really reliably detect effect sizes larger d = .65","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8InClassQueT7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.3.7 Task 7: Power of Published Research","text":"Thus far used hypothetical situations - now go look paper Open Stats lab website called Music Convey Social Information Infants?. can download pdf look , determine power significant t-tests reported Experiment 1 Results section page 489. one-sample t-test paired-samples t-test consider, summarised . Assume testing power = .8, alpha = .05. Based calculations either stated effects underpowered?one-sample: t(31) = 2.96, p = .006paired t-test: t(31) = 2.42, p = .022\none-sample t-test paired t-test use formula \nCohen’s d.\n\ncalculate n: n = df + 1.\n\nCalculate achievable Cohens d studies calculate\nestablished Cohen’s d studies.\nThinking Cap PointBased found , think following questions discuss groups:t-tests believe potentially underpowered?think may ?Additional information discussion can found solutions end chapter.One caveat Tasks 6 7: keep mind looking single studies using one sample potentially huge number samples within population. degree variance true effect size within population regardless effect size one given sample. means little bit cautious making claims study. Ultimately higher power better can detect smaller effect sizes!Concluding remarksSo hopefully now starting see interaction alpha, power, effect sizes, sample size. always want high-powered studies depending size effect interested (small large), \\(\\alpha\\) level, determine number observations need make sure study well powered. Points note:Lowering \\(\\alpha\\) level (e.g., .05 .01) reduce power.Lowering effect size (e.g., .8 .2) reduce power.Increasing power (e.g., .8 .9) require participants.also possible increase power fixed sample size reducing sources noise study.high-powered study looking detect small effect size low alpha may require large number participants!","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practice-your-skills-11","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4 Practice Your Skills","text":"Lab 8: APES AssignmentIn order complete assignment first download assignment .Rmd file need edit assignment: titled GUID_PracticeSkills_Ch8.Rmd. can downloaded within zip file link. downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top Download assignment zip file .NOTE: nearly problems , need replace NULL value pipeline code computes value. Please pay special attention question asking output, e.g. value tibble; asked value output, make sure single value value stored tibble. Finally, altering code inside code blocks, please re-order rename code blocks (T1, T2, ... etc.).also recommended \"knit\" report able see accomplished spot potential errors. great thing close whole programme, restart , knit code. test whether remembered include essential elements, libraries, code.APES: Alpha, Power, Effect Size, Sample SizeIn chapter looking interplay four components Alpha, Power, Effect Size, Sample Size. important part experimental design understand help understand studies worth paying attention help design studies coming years know just many people run make effect find.starting check:.Rmd file saved working directory. assessments ask save format GUID_PracticeSkills_Ch8.Rmd GUID replaced GUID. Though formative assessment, may good practice .LibrariesYou need use tidyverse broom libraries assignment, load library code chunk .need use tidyverse broom libraries assignment, load library code chunk .Hint: library(package)Hint: library(package)Basic Calculations","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.1 Task 1","text":"set study power value \\(power = .87\\). two decimal places, Type II error rate study?Replace NULL T1 code chunk either single value, mathematical notation, error_rate returns actual value Type II error rate study. mathematical notation mean use appropriate formula insert actual values.","code":"\nerror_rate <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT2","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.2 Task 2","text":"run two-sample t-test discover significant effect, t(32) = 3.26, p < .05. Using appropriate formula, given chapter, calculate effect size t-test.Replace NULL T2 code chunk mathematical notation effect1 returns value effect size. round value.","code":"\neffect1 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT3","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.3 Task 3","text":"run paired-sample t-test discover significant effect, t(43) = 2.24, p < .05. Using appropriate formula, given chapter, calculate effect size t-test.Replace NULL T3 code chunk mathematical notation effect2 returns value effect size. round value.Using Power function","code":"\neffect2 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT4","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.4 Task 4","text":"Replace NULL T4 code chunk pipeline combining pwr.t.test(), pluck() ceiling(), determine many participants needed sufficiently power paired-samples t-test \\(power = .9\\) \\(d = .5\\)? Assume two-sided hypothesis \\(\\alpha = .05\\). Ceiling answer nearest whole participant store value participants.Replace NULL T4 code chunk pipeline combining pwr.t.test(), pluck() ceiling(), determine many participants needed sufficiently power paired-samples t-test \\(power = .9\\) \\(d = .5\\)? Assume two-sided hypothesis \\(\\alpha = .05\\). Ceiling answer nearest whole participant store value participants.Hint: Remember quotes pluckHint: Remember quotes pluck","code":"\nparticipants <- NULL "},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.5 Task 5","text":"Using pipeline similar Task 4, minimum effect size one-sample t-test study (two-tailed hypothesis) reliably detect given following details: \\(\\beta = .16, \\alpha = 0.01, n = 30\\). Round two decimal places replace NULL T5 code chunk store value effect3.Using pipeline similar Task 4, minimum effect size one-sample t-test study (two-tailed hypothesis) reliably detect given following details: \\(\\beta = .16, \\alpha = 0.01, n = 30\\). Round two decimal places replace NULL T5 code chunk store value effect3.Hint: Remember going round() ceiling()Hint: Remember going round() ceiling()","code":"\neffect3 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.6 Task 6","text":"Study 1You run -subjects study establish following descriptives: Group 1 (M = 5.1, SD = 1.34, N = 32); Group 2 (M = 4.4, SD = 1.27, N = 32). Replace NULL T6 code chunk following formula, substituting appropriate values, calculate t-value test. Calculate Group1 minus Group2. Store t-value tval. round tval include t = part formula.\\[ t = \\frac {{\\bar{x_{1}}} - \\bar{x_{2}}}{ \\sqrt {\\frac {{s_{1}}^2}{n_{1}} + \\frac {{s_{2}}^2}{n_{2}}}}\\]","code":"\ntval <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.7 Task 7","text":"Using tval calculated Task 6, calculate effect size study store d1 T7 code chunk , replacing NULL appropriate formula values. round d1.Using tval calculated Task 6, calculate effect size study store d1 T7 code chunk , replacing NULL appropriate formula values. round d1.Hint: Think -subjectsHint: Think -subjects","code":"\nd1 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.8 Task 8","text":"Assuming \\(power = .8\\), \\(\\alpha =.05\\) two-tailed hypothesis, based d1 value Task 7 smallest achievable effect size study, statements correct.smallest effect size study can determine d = .71. detected effect size, d1, larger study potentially suitably poweredThe smallest effect size study can determine d = .17. detected effect size, d1, larger study potentially suitably poweredThe smallest effect size study can determine d = .17. detected effect size, d1, smaller study potentially suitably poweredThe smallest effect size study can determine d = .71. detected effect size, d1, smaller study potentially suitably poweredReplace NULL T8 code chunk number statement true summary study. may help calculate store smallest achievable effect size study poss_d.Hint: use poss_d calculate smallest possible effect size study help answer question.","code":"\nposs_d <- NULL\n\nanswer_T8 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT9","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.9 Task 9","text":"Study 2Below paragraph results Experiment 4 Schroeder, J., & Epley, N. (2015). sound intellect: Speech reveals thoughtful mind, increasing job candidate's appeal. Psychological Science, 26, 877-891. saw paper previously can find details <href=\"https://sites.trinity.edu/osl/data-sets--activities/t-test-activities\", target = \"_blank\">Open Stats Lab.Recruiters believed job candidates greater intellect - competent, thoughtful, intelligent - listened pitches (M = 5.63, SD = 1.61, n = 21) read pitches (M = 3.65, SD = 1.91, n = 18), t(37) = 3.53, p < .01, 95% CI difference = [0.85, 3.13], d1 = 1.16. recruiters also formed positive impressions candidates - rated likeable positive less negative impression - listened pitches (M = 5.97, SD = 1.92) read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p < .01, 95% CI difference = [0.55, 3.24], d2 = 0.94. Finally, also reported likely hire candidates listened pitches (M = 4.71, SD = 2.26) read pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p < .01, 95% CI difference = [0.41, 3.24], d3 = 0.86.Using pwr.t.test() function, minimum effect size paper reliably detected? Test \\(power = .8\\) two-sided hypothesis. Use \\(\\alpha\\) stated paragraph smallest n stated; store value effect4 T9 code chunk . Replace NULL pipeline round effect size two decimal places.","code":"\neffect4 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT10","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.10 Task 10","text":"Given value effect4 calculated Task 9, stated alpha paragraph smallest n two groups, statements true.study enough power reliably detect effects size d3 larger.study enough power reliably detect effects size d1.study enough power reliably detect effects size d2 larger, d3.study enough power reliably detect effect sizes d1 lower.Replace NULL T10 code chunk number statement TRUE, storing single value answer_t10.","code":"\nanswer_t10 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"Ch8AssignQueT11","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.11 Task 11","text":"Last, least:Read following statements.general, increasing sample size increase power study.general, smaller effect sizes require fewer participants detect \\(power = .8\\).general, lowering alpha (.05 .01) decrease power study.Now look four summary statements validity statements , b c.Statements , b c TRUE.Statements c TRUE.Statements b c TRUE.None statements TRUE.Replace NULL T11 code chunk number statement correct, storing single value answer_t11.","code":"\nanswer_t11 <- NULL"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"the-pwr-package","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.4.12 The pwr package","text":"alternative solution Task 9 use pwr.t2n.test() function pwr package (Champely, 2020). allow enter n groups n1 n2 argument. use , entering n1 = 18, n2 = 21, alpha = .01, d drops just little, changing interpretation Task 10. Feel free try analysis see can figure alternative answer Task 10.Job Done - Activity Complete!Well done, finished! Now go check answers solution can found end chapter. looking check resulting output answers submitted exactly output solution - example, remember single value coded answer. alternative answers, means submitted one options return answer.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"solutions-to-questions-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5 Solutions to Questions","text":"find solutions questions activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practical-apes-calculations-1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1 Practical APES Calculations","text":"","code":""},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-1-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.1 Task 1","text":"Giving effect size d = 0.64 medium large effect size according Cohen (1988)Return Task","code":"\nd <- 3.24 / sqrt(25 +1)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-2-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.2 Task 2","text":"Giving effect size d = 1.06 large effect size according Cohen (1988)Return Task","code":"\nd <- (2*2.9) / sqrt(30)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-3-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.3 Task 3","text":"Giving N = 40 effect size d = 0.33. considered small effect size according Cohen (1988)Return Task","code":"\nN = 39 + 1\n\nd <- 2.1 / sqrt(N)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-4-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.4 Task 4","text":"Giving t-value t = 2.56 effect size d = 0.67.Remember convention people tend report t d positive values.Return Task","code":"\nt = (10 - 11)/sqrt((1.3^2/30) + (1.7^2/30))\n\nd = (2*t)/sqrt((30-1) + (30-1))"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-5-6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.5 Task 5","text":"Giving sample size n = 151Return Task","code":"\nlibrary(pwr)\n\nsample_size <- pwr.t.test(d = .23,\n                          power = .8, \n                          sig.level = .05, \n                          alternative = \"two.sided\", \n                          type = \"one.sample\") %>%\n  pluck(\"n\") %>% \n  ceiling()"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-6-5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.6 Task 6","text":"Giving Cohen's d effect size d = 0.65Return Task","code":"\ncohens <- pwr.t.test(n = 50,\n                    power = .9, \n                    sig.level = .05, \n                    alternative = \"two.sided\", \n                    type = \"two.sample\") %>% \n  pluck(\"d\") %>% \n  round(2)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-7-5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.1.7 Task 7","text":"Example 1Giving achievable effect size 0.51 found effect size 0.52.study seems ok authors achieve effect size low .51 found effect size .52Example 2Giving achievable effect size 0.51 found effect size 0.43.effect might reliable given effect size found much lower achievable effect size. issue researchers established sample size based previous effect size minimum effect size find important. effect size small .4 important powered studies level ran appropriate n ~52 babies (see ). Flipside course obtaining 52 babies easy; hence people consider Many Labs approach good way ahead.ONE CAVEAT making assumption study therefore flawed, keep mind one study using one sample potentially huge number samples within population. degree variance true effect size within population regardless effect size one given sample. means little bit cautious making claims study. Ultimately higher power better.calculate actual sample size required achieve power .8:Suggesting sample size n = 52 appropriate.Return Task","code":"\nach_d_exp1 <- pwr.t.test(power = .8, \n                         n = 32, \n                         type = \"one.sample\", \n                         alternative = \"two.sided\", \n                         sig.level = .05) %>% \n  pluck(\"d\") %>% \n  round(2) \n\nexp1_d <- 2.96/sqrt(31+1) \nach_d_exp2 <- pwr.t.test(power = .8, \n                         n = 32, \n                         type = \"paired\", \n                         alternative = \"two.sided\", \n                         sig.level = .05) %>% \n  pluck(\"d\") %>% \n  round(2) \n\nexp2_d <- 2.42/sqrt(31+1) \nsample_size <- pwr.t.test(power = .8,\n                          d = .4,\n                          type = \"paired\", \n                          alternative = \"two.sided\", \n                          sig.level = .05) %>%\n  pluck(\"n\") %>% \n  ceiling()"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"practice-your-skills-12","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2 Practice Your Skills","text":"Libraries","code":"\nlibrary(pwr)\nlibrary(tidyverse)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-1-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.1 Task 1","text":"Type II error rate study \\(\\beta\\) = 0.13.Return Task","code":"\nerror_rate <- 1 - .87"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-2-9","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.2 Task 2","text":"effect size d = 1.1525841Return Task","code":"\neffect1 <- (2*3.26)/sqrt(32)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-3-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.3 Task 3","text":"effect size d = 0.3376927Return Task","code":"\neffect2 <- 2.24/sqrt(43+1)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-4-8","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.4 Task 4","text":"Given detailed scenario, appropriate number participants n = 44Return Task","code":"\nparticipants <- pwr.t.test(power = .9,\n                           d = .5,\n                           sig.level = 0.05,\n                           type = \"paired\",\n                           alternative = \"two.sided\") %>% \n  pluck(\"n\") %>% \n  ceiling()"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-5-7","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.5 Task 5","text":"Given detailed scenario, able detect effect size d = 0.69Return Task","code":"\neffect3 <- power.t.test(power = 1-.16,\n                      n = 30,\n                      sig.level = 0.01,\n                      type = \"one.sample\",\n                      alternative = \"two.sided\") %>% \n  broom::tidy() %>% \n  pull(delta) %>% \n  round(2)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-6-6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.6 Task 6","text":"Given stated means standard deviations, t-value study t = 2.1448226Return Task","code":"\ntval <- (5.1 - 4.4) / sqrt((1.34^2/32) + (1.27^2/32))"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-7-6","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.7 Task 7","text":"Given t-value Task 6, effect size study d = 0.5447855.Return Task","code":"\nd1 <- (2*tval)/sqrt((32-1)+(32-1))"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-8-5","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.8 Task 8","text":"smallest effect size study can determine d = 0.71. detected effect size, d1, smaller (d1 = 0.5447855) study suitably powered.Given outcome, 4th statement suitable answer - answer_T8 = 4.Return Task","code":"\nposs_d <- pwr.t.test(power = .8,\n                     n = 32,\n                     sig.level = 0.05,\n                     type = \"two.sample\",\n                     alternative = \"two.sided\") %>% \n  pluck(\"d\") %>% \n  round(2)\n\nanswer_T8 <- 4"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-9-2","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.9 Task 9","text":"smallest stated n n = 18 stated \\(\\alpha\\) \\(\\alpha\\) = .01Given details, minimum effect size paper reliably detected d = 1.198Return Task","code":"\neffect4 <- pwr.t.test(power = .8,\n                      n = 18,\n                      sig.level = .01,\n                      alternative = \"two.sided\",\n                      type = \"two.sample\") %>% \n  pluck(\"d\") %>% \n  round(3)"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-10-1","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.10 Task 10","text":"study enough power detect effect sizes d1 lower answer_t10 = 4However, worth keeping mind looking one study drew one sample population samples. means always uncertainty true effect size difference association - taking different sample may given different effect size. , comparison making entirely valid see reminder always think power planning studies rather search criticism.Return Task","code":"\nanswer_t10 <- 4"},{"path":"apes---alpha-power-effect-sizes-sample-size.html","id":"task-11","chapter":"8 APES - Alpha, Power, Effect Sizes, Sample Size","heading":"8.5.2.11 Task 11","text":"general, increasing sample size increase power study whereas lowering alpha (.05 .01) decrease power study. , statements c, answer_t11 = 2.Return TaskChapter Complete!","code":"\nanswer_t11 <- 2"},{"path":"correlations.html","id":"correlations","chapter":"9 Correlations","heading":"9 Correlations","text":"","code":""},{"path":"correlations.html","id":"overview-9","chapter":"9 Correlations","heading":"9.1 Overview","text":"read Miller Haden (2013) part Correlations Basics reading, correlations used detect quantify relationships among numerical variables. short, measure two variables correlation analysis tells whether related manner - positively (.e., one increases increases) negatively (.e., one decreases increases). Schematic examples three extreme bivariate relationships (.e., relationship two (bi) variables) shown :\nFigure 9.1: Schematic examples extreme bivariate relationships\nHowever, unfortunately, may heard people say lots critical things correlations:prove causalitythey suffer bidirectional problem (vs B B vs )relationship found may result unknown third variable (e.g. murders ice-creams)Whilst true must aware , correlations incredibly useful commonly used measure field, put using designing study uses correlations. fact, can used variety areas. examples include:looking reading ability IQ scores Miller Haden Chapter 11, also look today;exploring personality traits voices (see Phil McAleer's work Voices);traits faces (see Lisa DeBruine's work);brain-imaging analysis looking activation say amygdala relation emotion faces (see Alex Todorov's work Princeton);social attidues, implicit explicit, Helena Paterson discuss Social lectures semester;whole variety fields simply measure variables interest.actually carry correlation simple show little : can use cor.test() function. harder part correlations really wrangling data (learned first part book) interpreting data (focus chapter). , going run correlations, showing one, asking perform others give good practice running interpreting relationships two variables.Note: dealing correlations better refer relationships predictions. correlation, X predict Y, really regression look later book. correlation, can say X Y related way. Try get correct terminology please feel free pull us say wrong thing class. easy slip tongue make!chapter :Introduce Miller Haden book, using throughout rest bookShow thought process researcher running correlational analysis.Give practice running writing correlation.","code":""},{"path":"correlations.html","id":"correlations-basics","chapter":"9 Correlations","heading":"9.2 Correlations Basics","text":"majority basics activities first part chapters now involve reading chapter two Miller Haden (2013) trying couple tasks. excellent free textbook use introduce General Linear Model, model underlies majority analyses seen see year - e.g., t-tests, correlations, ANOVAs (Analysis Variance), Regression related part General Linear Model (GLMs).start chapter introduction correlations.","code":""},{"path":"correlations.html","id":"read","chapter":"9 Correlations","heading":"9.2.1 Read","text":"ChapterRead Chapters 10 11 Miller Haden (2013).chapters really short, give good basis understanding correlational analysis. Please note, Chapter 10 might know terminology yet, e.g. ANOVA means Analysis Variance GLM means General Linear Model (Reading Chapter 1 Miller Haden might help). go depth terms coming chapters.","code":""},{"path":"correlations.html","id":"watch","chapter":"9 Correlations","heading":"9.2.2 Watch","text":"VisualisationHave look visualisation correlations Kristoffer Magnusson: https://rpsychologist.com/d3/correlation/.read Miller Haden Chapter 11, use visualisation page visually replicate scatterplots Figures 11.3 11.4 - use sample 100. , visually replicate scatterplots Figure 11.5. time change correlation, pay attention shared variance (overlap two variables) see changes changing level relationship two variables. greater shared variance, stronger relationship.Also, try setting correlation r = .5 moving single dot see one data point, potential outlier, can change stated correlation value two variables","code":""},{"path":"correlations.html","id":"play","chapter":"9 Correlations","heading":"9.2.3 Play","text":"Guess correlationNow well versed interpreting scatterplots (scattergrams) go online app guessing correlation: https://www.rossmanchance.com/applets/GuessCorrelation.html.basic application allows see good recognising different correlation strengths scatterplots. recommend click \"Track Performance\" tab can keep overview overall bias underestimate overestimate correlation.just bit fun? Well, yes stats actually fun, , serves purpose helping determine correlations see data real, help see correlations published research match told. know examples one data point can lead misleading relationship even might considered medium strong relationship may actually limited relevance real world. One needs mention Anscombe's Quartet reminded importance visualising data.Anscombe (1973) showed four sets bivariate data (X,Y) exact means, medians, relationships:\nTable 9.1: Four bivariate datasets means, medians, standard deviations, relationships\nCan look different plotted:\nFigure 9.2: Though datasets correlation r = .82, plotted four datasets look different. Grp standard linear relationship pearson correlation suitable. Grp II appear non-linear relationship non-parametric analysis appropriate. Grp III shows linear relationship (approaching r = 1) outlier lowered correlation coefficient. Grp IV shows relationship two variables (X, Y) oultier inflated correlation higher.\nclear example visualise data rely just numbers. can read Anscombe's Quartet time, wikipedia (https://en.wikipedia.org/wiki/Anscombe%27s_quartet) offering good primer data used example.","code":"## `geom_smooth()` using formula = 'y ~ x'"},{"path":"correlations.html","id":"correlations-application","chapter":"9 Correlations","heading":"9.3 Correlations Application","text":"going jump straight one! get used running correlation use examples read Miller Haden (2013), Chapter 11, looking relationship four variables: reading ability, intelligence, number minutes per week spent reading home, number minutes per week spent watching TV home. great example correlation works best unethical manipulate variables measuring exist environment appropriate.Click download data today.","code":""},{"path":"correlations.html","id":"Ch10InClassQueT1","chapter":"9 Correlations","heading":"9.3.1 Task 1 - The Data","text":"downloading data folder, unzip folder, set working directory appropriately, open new R Markdown, load Miller Haden data (MillerHadenData.csv), storing tibble called mh. always, solutions end chapter.Note 1: Remember reality store data name like, make easier demonstrator debug handy use names.Note 2: find instructions tasks sparser now , compared earlier book. want really push develop skills learnt previously. worry though, always help, get stuck, ask!Note 3: see additional data set vaping. use later chapter, can ignore now.\nHint 1: going need following libraries: tidyverse,\nbroom\n\nHint 2: mh <- read_csv()\n\nRemember , book, ALWAYS ask use\nread_csv() load data. Avoid using functions\nread.csv(). Whilst functions similar,\n.\n\nlook data mh - showed number ways . Miller Haden, five columns:Participant (Participant),Reading Ability (Abil),Intelligence (IQ),Number minutes spent reading home per week (Home),Number minutes spent watching TV per week (TV).exercises focus relationship Reading Ability IQ, practice can look relationships .probable hypothesis today Reading Ability increases Intelligence (see issue causality direction?). phrasing alternative hypothesis (\\(H_1\\)) formally, hypothesise reading ability school children, measured standardized test, intelligence, measured standardized test, show linear positive relationship. hypothesis test today, remember always state null hypothesis (\\(H_0\\)) relationship reading ability IQ.First, however, must check assumptions correlation tests. stated hypothesise linear relationship, means going looking Pearson's product-moment correlation often just shortened Pearson's correlation symbolised letter r.main assumptions need check Pearson's correlation, look turn, :data interval, ratio, ordinal?data point participant variables?data normally distributed variables?relationship variables appear linear?spread homoscedasticity?","code":""},{"path":"correlations.html","id":"Ch10InClassQueT2","chapter":"9 Correlations","heading":"9.3.2 Task 2 - Interval or Ordinal","text":"Assumption 1: Level MeasurementThinking Cap PointIf going run Pearson correlation need interval ratio data. alternative correlation, non-parametric equivalent Pearson correlation Spearman correlation can run ordinal, interval ratio data. type data ?Check thinking: type data analysis probably ratiointervalordinalnominal data continuousdiscrete unlikely true zero\nvariables continuous?\n\ndifference 1 2 scale equal \ndifference 2 3?\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT3","chapter":"9 Correlations","heading":"9.3.3 Task 3 - Missing Data","text":"Assumption 2: Pairs DataAll correlations must data point participant two variables correlated. make sense - correlate empty cell! Check data point columns participant. , try answering question Task 3. discussion solutions end chapter.Note: can check missing data visual inspection - literally using eyes. missing data point show NA, short applicable, available, answer. alternative use .na() function. can really handy lots data visual inspection just take long. example ran following code:look output function, FALSE tells data-point cell. .na() asks cell NA; empty. cell empty come back TRUE. cells data , showing FALSE. wanted ask opposite question, data cell, write !.na() read \"NA\". exclamation mark ! turns question opposite.looks like everyone data columns test skills little whilst . Answer following questions:missing data represented cell tibble? empty cellNAa large numberdon't knowWhich code leave just participants missing Reading Ability data mh:\nfilter(mh, .na(Ability)filter(mh, .na(Abil)filter(mh, !.na(Ability)filter(mh, !.na(Abil)code leave just participants missing Reading Ability data mh: filter(mh, .na(Ability)filter(mh, .na(Abil)filter(mh, !.na(Ability)filter(mh, !.na(Abil)\nfilter(dat, .na(variable)) versus filter(dat,\n!.na(variable))\n","code":"\nis.na(mh)"},{"path":"correlations.html","id":"Ch10InClassQueT4","chapter":"9 Correlations","heading":"9.3.4 Task 4 - Normality","text":"Assumption 3: shape dataThe remaining assumptions best checked visualisations. use histograms check data (Abil IQ) normally distributed, use scatterplot (scattergrams) IQ function Abil check whether relationship linear, homoscedasticity, without outliers!alternative use z-scores check outliers cut-usually set around \\(\\pm2.5SD\\). using mutate function (e.g. mutate(z = (X - mean(X))/SD(X))), today just use visual checks.Create following figures discuss outputs group. discussion solutions end chapter.histogram Ability histogram IQ.\nnormally distributed?\nnormally distributed?scatterplot IQ (IQ) function Ability (Abil).\nsee outliers?\nrelationship appear linear?\nspread appear ok terms homoscedasticity?\nsee outliers?relationship appear linear?spread appear ok terms homoscedasticity?\nggplot(mh, aes(x = )) + geom_histogram()\n\nggplot(mh, aes(x = , y = )) + geom_point()\n\nNormality: something keep mind 25\nparticipants, ‘normal’ expect relationships \n\nhomoscedasticity spread data points around \n(imaginary) line best fit equal sides along line; \nopposed narrow one point wide others.\n\nRemember judgement calls!\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT5","chapter":"9 Correlations","heading":"9.3.5 Task 5 - Descriptives","text":"Descriptives correlationA key thing keep mind scatterplot actually descriptive correlation. Meaning article, report, use scatterplot determine type correlation use, also describe potential relationship regards hypothesis. always expect see scatterplot write-type analysisThinking Cap PointLooking scatterplot created Task 4, spend couple minutes describing relationship Ability IQ terms hypothesis. Remember descriptive analysis stage, nothing confirmed. relationship appear predicted hypothesis? discussion solutions end chapter.\nHint 1: hypothesised reading ability intelligence \npositively correlated. see scatterplot?\n\nHint 2: Keep mind subjective stage.\n\nHint 3: Remember talk relationship \nprediction. correlational work, regression.\n\nHint 4: Can say something strength (weak, medium,\nstrong) direction (positive, negative)?\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT6","chapter":"9 Correlations","heading":"9.3.6 Task 6 - Pearson or Spearman?","text":"correlationFinally run correlation using cor.test() function. Remember help function can type ?cor.test console window. cor.test() function requires:name tibble column name Variable 1the name tibble column name Variable 2the type correlation want run: e.g. \"pearson\", \"spearman\"type NHST tail want run: e.g. \"one.sided\", \"two.sided\"example, data stored dat wanting two-sided pearson correlation variables (columns) X Y, :dat$X means column X tibble dat. dollar sign ($) way indexing, calling , specific column.Based answers Task 5, spend couple minutes deciding correlation method use (e.g., Pearson Spearman) type NHST tail set (e.g., two.sided one.sided). Now, run correlation IQ Ability save tibble called results (hint: broom::tidy()). solution end chapter.\nHint 1: data looked reasonably normal linear method\n?\n\nHint 2: results <- cor.test(mh$Abil……, method = …..,\nalternative….) %>% tidy()\n","code":"cor.test(dat$X, dat$Y, method = \"pearson\", alternative = \"two.sided\")"},{"path":"correlations.html","id":"Ch10InClassQueT7","chapter":"9 Correlations","heading":"9.3.7 Task 7 - Interpretation","text":"Interpreting CorrelationYou now tibble called results gives output correlation Reading Ability IQ school children measured Miller Haden (2013) Chapter 11. left now interpret output correlation.Look results. Locate correlation value, e.g. results %>% pull(estimate), answer following questions. discussion can found end chapter.direction relationship Ability IQ : positivenegativeno relationshipThe strength relationship Ability IQ : strongmediumweakBased \\(\\alpha = .05\\) relationship Ability IQ : significantnot significantBased output, given hypothesis reading ability school children, measured standardized test, intelligence, standardized test, positively correlated, can say hypothesis: supportedis supportedis provenis proven\nHint1: Y increases X increases relationship \npositive. Y increases X decreases relationship \nnegative. change Y X changes \nrelationship\n\nHint2: Depending field correlation values greater \n.5 strong; .3 .5 medium, .1 .3 small.\n\nHint3: field standard says less .05 significant.\n\nHint4: Hypotheses can supported supported, never\nproven.\n","code":""},{"path":"correlations.html","id":"Ch10InClassQueT8","chapter":"9 Correlations","heading":"9.3.8 Task 8 - The Matrix","text":"Great, far set hypothesis correlation, checked assumptions, run correlation interpreted appropriately. can see running correlation easy bit. lot analyses getting data order, checking assumptions, interpreting output hard part.now walked one analysis can always go run Miller Haden dataset. six total run watch Multiple Comparisons - False Positive Error rate (Type 1 error rate) inflated chance finding significant effect inflated simply running numerous tests. Alternatively, another data set want run correlation first want show something can handy want view lots correlations .Advanced 1: Matrix ScatterplotsAbove ran one correlation wanted different correlation edit cor.test() line run . However, lots variables dataset, get quick overview patterns, one thing might want run correlations time create matrix scatterplots one time. can functions Hmisc library - already installed Boyd Orr Labs. use Miller Haden data still tibble called mh.First, need get rid Participant column want correlate anything. tell us anything. Copy run line codeNow run following line. pairs() function Hmisc library creates matrix scatterplots can use view relationships one time.\nFigure 9.3: Matrix Correlation plots Miller Haden (2013) data\nrcorr() function creates matrix correlations p-values. watch , accepts data matrix format. Run following two lines code.running lines, spend minutes answering following questions. first table outputted correlation values second table p-values.tables look symmetrical around blank diagonal?strongest positive correlation?strongest negative correlation?\nHint1: hint, just cheeky test make sure \nread correlation chapter Miller Haden, like asked \n! :-) unsure answers, solutions end\nchapter.\n","code":"\nlibrary(\"Hmisc\")\nlibrary(\"tidyverse\")\nmh <- read_csv(\"MillerHadenData.csv\") %>% select(-Participant)\npairs(mh)\nmh_mx <- as.matrix(mh)\nrcorr(mh_mx, type = \"pearson\")##       Abil   IQ  Home    TV\n## Abil  1.00 0.45  0.74 -0.29\n## IQ    0.45 1.00  0.20  0.25\n## Home  0.74 0.20  1.00 -0.65\n## TV   -0.29 0.25 -0.65  1.00\n## \n## n= 25 \n## \n## \n## P\n##      Abil   IQ     Home   TV    \n## Abil        0.0236 0.0000 0.1624\n## IQ   0.0236        0.3337 0.2368\n## Home 0.0000 0.3337        0.0005\n## TV   0.1624 0.2368 0.0005"},{"path":"correlations.html","id":"Ch10InClassQueT9","chapter":"9 Correlations","heading":"9.3.9 Task 9 - Attitudes to Vaping","text":"Advanced 2: Attitudes towards VapingGreat work far! Now really want see can . mentioned earlier, data folder another file called VapingData.csv. data comes data set looking implicit explicit attitudes towards vaping.Explicit attitudes measured via questionnaire higher scores indicated positive attitude towards vaping.Implicit attitudes measured Implicit Association Test (IAT) using images Vaping Kitchen utensils associating positive negative words.IAT works principle associations go together (congruent, e.g. warm sun) quicker respond associations go together (incongruent, e.g. warm ice). can read procedure later date Noba Project https://nobaproject.com/modules/research-methods--social-psychology good description procedure section \"Subtle/Nonsconscious Research Methods\".exercise, need know \"Block 3\" experiment tested reaction times accuracy towards congruent associations, pairing positive words Kitchen utensils negative words Vaping. \"Block 5\" experiment tested reaction times accuracy towards incongruent associations, pairing positive words Vaping negative words Kitchen Utensils. , reaction times longer \"Block 5\" \"Block 3\" people considered hold view Vaping negative (.e., congruent associations quicker incongruent associations). However, reaction times quicker \"Block 5\" \"Block 3\" people considered hold view Vaping positive (.e., incongruent associations quicker congruent associations). difference reaction times \"Block 5\" \"Block 3\" called participants IAT score.Load data VapingData.csv analyse test hypothesis Implicit Explicit attitudes towards Vaping positively related. pointers, hints tips. , solutions walk step, help try steps first. Think step. Sometimes need think backwards, want tibble look like. can steps. nothing new.loading data, start looking data. 8 columns. Reaction times Accuracy scores Blocks 3 5 well Explicit Vaping Questionnaire scores, Gender Age, participant.Accuracy calculated proportion go 1. Participants entered data might made mistake. Remove participants accuracy greater 1 either Block 3 Block 5 unclear accuracy values.also want participants paying attention best remove anybody whose average accuracy score across Blocks 3 5 less 80%. Note - value arbitrary wanted, experiment, use relaxed strict cut-based studies guidance. Note decisions set start research part pre-registration part Registered Report. Finally, instance, remember, values proportions percentages (80% .8).Now create IAT score participants subtracting Block 3 reaction times (RT) away Block 5 reaction times e.g \\(Block5 - Block3\\). Use information understand scores relate attitudes.Create descriptives summary number people, mean RT Vaping Questionnaire Score. might averages useful? averages always useful correlations?Check assumptions correlations descriptives, thinking compares hypothesis.Run appropriate correlation based assumptions interpret output.\n\nlibraries might include tidyverse broom\n\n\nlibraries might include tidyverse broom\n\n\nHint Step 1: read_csv()\n\n\nHint Step 1: read_csv()\n\n\nHint Step 2: filter(Accuracy < 1 Accuracy <= 1 \nAccuracy > 1 Accuracy >= 1)\n\n\nHint Step 2: filter(Accuracy < 1 Accuracy <= 1 \nAccuracy > 1 Accuracy >= 1)\n\n\nHint Step 3: average accuracy: mutate(data, name = (1 + 2)/2)\n%>% filter(name > …)\n\n\nHint Step 3: average accuracy: mutate(data, name = (1 + 2)/2)\n%>% filter(name > …)\n\n\nHint Step 4: RT: mutate(data, nom = 1 - 2)\n\n\nHint Step 4: RT: mutate(data, nom = 1 - 2)\n\n\nHint Step 5: descriptives <- summarise()\n\n\nHint Step 5: descriptives <- summarise()\n\n\nHint Step 6: assumptions type data, normality, linear\nrelationship, homoscedasicity, data points everyone!\n\n\nHint Step 6: assumptions type data, normality, linear\nrelationship, homoscedasicity, data points everyone!\n\n\nHint Step 7: results <- cor.test(method = “pearson”)?\n\n\nHint Step 7: results <- cor.test(method = “pearson”)?\nExcellent work, thought Explicit Implicit attitudes towards Vaping?","code":""},{"path":"correlations.html","id":"Ch9PracticeSkills","chapter":"9 Correlations","heading":"9.4 Practice Your Skills","text":"purpose exercise test ability run interpret correlation , shown chapter, time get think skills learnt previously. hard work data wrangling; running actual analysis, much like t-tests, straightforward. Remember refer back previous chapters stuck.order complete tasks need download data .csv file .Rmd file, need edit, titled GUID_L2_Ch9_PracticeSkills.Rmd. can downloaded within zip file link. Download Exercises .zip file .zip file, also find activity html file can check table figure reproduced tasks 8 9.downloaded unzipped create new folder use working directory; put data file .Rmd file folder set working directory folder drop-menus top. .Rmd file currently knit errors code good test can perform time time make sure still errors code. Obviously mean answers correct; just means code error free.previous exercises number code chunks already set . code chunks require entering number entering adjusting code practiced chapters. Follow instructions edit code chunk. often entering code based covered point.Background\nbackdrop exercise following study:Dawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). wealthier people think people wealthier, matters: social sampling attitudes redistribution. Psychological Science, 26, 1389-1400. Available link VPN switched onThe abstract Dawtry, Sutton Sibley (2015) reads:present studies provide evidence social-sampling processes lead wealthier people oppose redistribution policies. samples American Internet users, wealthier participants reported higher levels wealth social circles (Studies 1a 1b). associated, turn, estimates higher mean wealth wider U.S. population, greater perceived fairness economic status quo, opposition redistribution policies. Furthermore, mods large-scale, nationally representative New Zealand survey revealed low levels neighborhood-level socioeconomic deprivation - objective index wealth within participants' social circles - mediated relation income satisfaction economic status quo (Study 2). findings held controlling relevant variables, including political orientation perceived self-interest. Social-structural inequalities appear combine social-sampling processes shape different political attitudes wealthier poorer people.summarised Open Stats Lab asIn research, Dawtry, Sutton, Sibley (2015) wanted examine people differ assessments increasing wealth inequality within developed nations. Previous research reveals people desire society overall level wealth high wealth spread somewhat equally across society. However, support approach income distribution changes across social strata. particular, wealthy people tend view society already wealthy thus satisfied status quo, less likely support redistribution. paper Dawtry et al., (2015) sought examine case. authors propose one reason wealthy people tend view current system fair social-circle comprised wealthy people, biases perceptions wealth, leads overestimate mean level wealth across society.test hypothesis, authors conducted study 305 participants, recruited online participant pool. Participants reported annual household income, income level within social circle, income entire population. Participants also rated perception level equality/inequality across social circle across society, level satisfaction perceived fairness current system (measured using two scales), attitudes toward redistribution wealth (measured using four-item scale), political preference.Open Stats Lab activity paper already set can work later skills development. use amended version pay close attention instructions assignment specifically task asks. Today running correlation measure Fairness Satisfaction versus measure Support Redistribution.","code":""},{"path":"correlations.html","id":"solutions-to-questions-8","chapter":"9 Correlations","heading":"9.5 Solutions to Questions","text":"find solutions questions activities chapter. look giving questions good try speaking tutor issues.","code":""},{"path":"correlations.html","id":"correlations-application-1","chapter":"9 Correlations","heading":"9.5.1 Correlations Application","text":"","code":""},{"path":"correlations.html","id":"task-1-9","chapter":"9 Correlations","heading":"9.5.1.1 Task 1","text":"Loading data two libraries neededGood point remind :\nuse read_csv() load data\norder libraries read important. conflicts terms libraries last library loaded functions using.\nuse read_csv() load datathe order libraries read important. conflicts terms libraries last library loaded functions using.Return Task","code":"\nlibrary(\"broom\")\nlibrary(\"tidyverse\")\nmh <- read_csv(\"MillerHadenData.csv\")"},{"path":"correlations.html","id":"task-2-10","chapter":"9 Correlations","heading":"9.5.1.2 Task 2","text":"Actually information within textbook unclear whether data interval ordinal accepted make case arguments. quick Google search show just many people think IQ interval think ordinal. terms Reading Ability, probably know enough information scale make clear judgement least ordinal well interval.Return Task","code":""},{"path":"correlations.html","id":"task-3-9","chapter":"9 Correlations","heading":"9.5.1.3 Task 3","text":"Missing data represented NA. stands Available good way improving Scottish accent. example, \"number\" can replied \"NA!\".Missing data represented NA. stands Available good way improving Scottish accent. example, \"number\" can replied \"NA!\".want keep everybody whole dataset score Ability use:want keep everybody whole dataset score Ability use:Alternatively, want keep everybody whole dataset score Ability use:Remember need store output step, really something like mh <- filter(mh, !.na(Abil))Return Task","code":"\nfilter(mh, !is.na(Abil))\nfilter(mh, is.na(Abil))"},{"path":"correlations.html","id":"task-4-9","chapter":"9 Correlations","heading":"9.5.1.4 Task 4","text":"Reading ability data appears normal expected 25 participants. Hard say close normality something look participants.\nFigure 9.4: Histogram showing distribution Reading Ability Scores Miller Haden (2013)\nIQ data appears normal expected 25 participants\nFigure 9.5: Histogram showing distribution IQ Scores Miller Haden (2013)\nrelationship reading ability IQ scores appears appears linear clear outliers. Data also appears homeoscedastic.added geom_smooth() function help clarify line best fit (also know regression line slope).\nFigure 9.6: Scatterplot IQ scores function Reading Ability Miller Haden (2013) data\nReturn Task","code":"\nggplot(mh, aes(x = Abil)) + \n  geom_histogram(binwidth = 5) +\n  theme_bw()\nggplot(mh, aes(x = IQ)) + \n  geom_histogram(binwidth = 5) +\n  theme_bw()\nggplot(mh, aes(x = Abil, y = IQ)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw()## `geom_smooth()` using formula = 'y ~ x'"},{"path":"correlations.html","id":"task-5-8","chapter":"9 Correlations","heading":"9.5.1.5 Task 5","text":"Based scatterplot might suggest reading ability scores increase, IQ scores also increase appear data inline hypothesis two variables positively correlated. appears medium strength relationship.Return Task","code":""},{"path":"correlations.html","id":"task-6-7","chapter":"9 Correlations","heading":"9.5.1.6 Task 6","text":"going run pearson correlation argue data interval relationship linear.correlation run follows - tidying nice usable table.output table look follows:\nTable 9.2: correlation output Reading Ability IQ relationship.\nReturn Task","code":"\nresults <- cor.test(mh$Abil, \n                    mh$IQ, \n                    method = \"pearson\", \n                    alternative = \"two.sided\") %>% \n  broom::tidy()"},{"path":"correlations.html","id":"task-7-7","chapter":"9 Correlations","heading":"9.5.1.7 Task 7","text":"Task 6 output:correlation value (r) stored estimateThe degrees freedom (N-2) stored parameterThe p-value stored p.valueAnd statistic t-value associated analysis correlations use t-distribution (Chapters 6, 7 8) determine probability outcome.Remember can use pull() function get individual values shown .can use information write-following using inline coding accuracy:pearson correlation found reading ability intelligence positively correlated medium strong relationship, (r(`r df`) = `r correlation`, p = `r pvalue`). can say hypothesis supported appears relationship reading ability IQ reading ability increases intelligence.knitted read :pearson correlation found reading ability intelligence positively correlated medium strong relationship, (r(23) = 0.45, p = 0.024). can say hypothesis supported appears relationship reading ability IQ reading ability increases intelligence.Return Task","code":"\npvalue <- results %>% \n  pull(p.value) %>% \n  round(3)\n\ndf <- results %>% \n  pull(parameter)\n\ncorrelation <- results %>% \n  pull(estimate) %>% \n  round(2)"},{"path":"correlations.html","id":"task-8-6","chapter":"9 Correlations","heading":"9.5.1.8 Task 8","text":"table looks across diagonal correlation e.g. Abil vs Abil shown, correlation Abil vs Home correlation Home vs Abil.strongest positive correlation number minutes spend reading home (Home) Reading Ability (abil), r(23) = .74, p < .001.strongest negative correlation number minutes spend reading home (Home) minutes spent watching TV per week (TV), r(23) = -.65, p < .001.Return Task","code":""},{"path":"correlations.html","id":"task-9-3","chapter":"9 Correlations","heading":"9.5.1.9 Task 9","text":"Step 1Reading Vaping Data using read_csv()Steps 2 4The main wrangle parts 2 4Step 5It always worth thinking averages informative .\nKnowing average explicit attitude towards vaping well informative.\ncontrast, using ordinal scale people use whole scale average may just tell middle scale using - already know really informative. always worth thinking descriptives calculating.\nKnowing average explicit attitude towards vaping well informative.contrast, using ordinal scale people use whole scale average may just tell middle scale using - already know really informative. always worth thinking descriptives calculating.Step 6A couple visual checks normality histograms\nFigure 9.7: Histogram showing distribution Scores Vaping Questionnaire (Explicit)\n\nFigure 9.8: Histogram showing distribution IAT Reaction Times (Implicit)\ncheck relationship reaction times IAT scores Vaping Questionnaire.Remember , often, scatterplot considered descriptive correlation, hence see including journal articles support stated relationship.scatterplot can used make descriptive claims direction relationship, strength relationship, whether linear , check outliers homeoscedasticity.\nFigure 9.9: scatterplot showing relationship implicit IAT reaction times (x) explicit Vaping Questionnaire Scores (y)\nquick look data reveals people Vaping Questionnaire score IAT score. correlation works people score factors remove score one factors.Step 7The analysis:extracting values:inline coding:Testing hypothesis relationship beween implicit explicit attitudes towards vaping, pearson correlation found significant relationship IAT reaction times (implicit attitude) answers Vaping Questionnaire (explicit attitude), r(`r df`) = `r correlation`, p = `r pvalue`. Overall suggests direct relationship implicit explicit attitudes relating Vaping hypothesis supported; reject null hypothesis.appears knitted:Testing hypothesis relationship beween implicit explicit attitudes towards vaping, pearson correlation found significant relationship IAT reaction times (implicit attitude) answers Vaping Questionnaire (explicit attitude), r(143) = -0.1043797, p = 0.2115059. Overall suggests direct relationship implicit explicit attitudes relating Vaping hypothesis supported; reject null hypothesis.Remember though r-values p-values often rounded three decimal places, appropriate write :Testing hypothesis relationship beween implicit explicit attitudes towards vaping, pearson correlation found significant relationship IAT reaction times (implicit attitude) answers Vaping Questionnaire (explicit attitude), r(143) = -.104, p = .212. Overall suggests direct relationship implicit explicit attitudes relating Vaping hypothesis supported; reject null hypothesis.","code":"\ndat <- read_csv(\"VapingData.csv\")\ndat <- dat %>% \n  filter(IAT_BLOCK3_Acc <= 1) %>%\n  filter(IAT_BLOCK5_Acc <= 1) %>%\n  mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %>%\n  filter(IAT_ACC > .8) %>%\n  mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT)\ndescriptives <- dat %>% summarise(n = n(),\n                          mean_IAT_ACC = mean(IAT_ACC),\n                          mean_IAT_RT = mean(IAT_RT),\n                          mean_VPQ = mean(VapingQuestionnaireScore, \n                                          na.rm = TRUE))\nggplot(dat, aes(x = VapingQuestionnaireScore)) + \n  geom_histogram(binwidth = 10) +\n  theme_bw()## Warning: Removed 11 rows containing non-finite values (`stat_bin()`).\nggplot(dat, aes(x = IAT_RT)) + \n  geom_histogram(binwidth = 10) +\n  theme_bw()\nggplot(dat, aes(x = IAT_RT, y = VapingQuestionnaireScore)) + \n  geom_point() + \n  theme_bw()## Warning: Removed 11 rows containing missing values (`geom_point()`).\ndat <- dat %>% \n  filter(!is.na(VapingQuestionnaireScore)) %>% \n  filter(!is.na(IAT_RT))\nresults <- cor.test(dat$VapingQuestionnaireScore, \n                    dat$IAT_RT, \n                    method = \"pearson\") %>% \n  broom::tidy()\ncorrelation <- results %>% \n  pull(estimate)\n\ndf <- results %>% \n  pull(parameter)\n\npvalue <- results %>% \n  pull(p.value)"},{"path":"correlations.html","id":"practice-your-skills-13","chapter":"9 Correlations","heading":"9.5.2 Practice Your Skills","text":"Check work solution tasks : Chapter 9 Practice Skills Solution.Return Task","code":""},{"path":"correlations.html","id":"additional-material-1","chapter":"9 Correlations","heading":"9.6 Additional Material","text":"additional material might help understand correlations bit additional ideas.","code":""},{"path":"correlations.html","id":"checking-for-outliers-with-z-scores","chapter":"9 Correlations","heading":"Checking for outliers with z-scores","text":"briefly mentioned activities use z-scores check outliers, instead visual inspection. covered lectures works interval ratio dataset, just ones correlations, demonstrate using IQ data Miller Haden. First, lets get just IQ data.z-score just standardised value based mean (\\(\\mu\\)) standard deviation (SD \\(\\sigma\\), proonounced \"sigma\") sample comes . formula z-score :\n\\[z = \\frac{x - \\mu}{\\sigma}\\]Using z-scores away effectively converting data onto Normal Distribution. , known cut-offs Normal Distribution (e.g., 68% data \\(\\pm1SD\\), etc - See Chapter 4), can use information determine value outlier. , convert data within variable respective z-score values fall cut-considered outlier.give us following data (showing first 6 rows):\nTable 9.3: Raw IQ data z-scored IQ data Miller Haden\nrun data see whole range z-scores ranging -2.1053138 1.9858948. class said use cut-\\(\\pm2.5SD\\) can see range IQ z-scores value , outliers. confirm following code:run code, can see 0 outliers. also demonstrates however must stipulate cut-offs advance - otherwise might fall foul adjusting data fit predictions hiding decision making numerous researcher degrees freedom exist. example, say run analysis see outliers find result want/expect/believe . questionable research practice now start adjusting z-score cut-different values see difference makes. instance, said cut-stringent \\(\\pm2SD\\) found 1 outlier.two take-aways :spot outliers using z-scores.must set exclusion criteria advance running analysis.","code":"\nmh_IQ <- mh %>% \n  select(IQ)\nmh_IQ_z <- mh_IQ %>%\n  mutate(z_scores = (IQ - mean(IQ))/sd(IQ))\nmh_IQ_z %>% filter(abs(z_scores) > 2.5) %>% count() %>% pull(n)"},{"path":"correlations.html","id":"a-different-approach-to-making-a-correlation-table","chapter":"9 Correlations","heading":"A different approach to making a correlation table","text":"activities, Task 8, looked making table correlation functions. bit messy actually alternative approach can use, useful additional functions. makes use corrr package may need install onto laptop want follow along.code can try explore . main functions correlate(), shave(), fashion():correlate() runs correlations can changed pearson, spearman, etc. quiet argument just removes information reminders dont really need. Switch FALSE see happens. nice aspect function default use complete rows can alter .shave() can used convert one set correlation values showing test NAs. example, bottom half table just reflects top half table can convert one half. default convert upper half table.fashion() can used tidy table terms removing NAs, leading zeros, setting number decimal places.code look like :look ouput get nice clean correlation matrix table shown .bit nicer approach one shown Task 8, gives control output correlation matrix table. However, downside approach, reason use approach show p-values easily. fact, show need bit work. can read using corrr package webpage author package: https://drsimonj.svbtle.com/exploring-correlations--r--corrr","code":"\nlibrary(corrr)\n\nmh <- read_csv(\"MillerHadenData.csv\")\n\nmh %>% \n  select(-Participant) %>%\n  correlate(method = \"pearson\", quiet = TRUE) %>%\n  shave() %>%\n  fashion(decimals = 3, leading_zeros = FALSE)"},{"path":"correlations.html","id":"comparing-correlations","chapter":"9 Correlations","heading":"Comparing Correlations","text":"One step often overlooked working correlations data set numerous variables, compare whether correlations significantly different . example, say looking whether relationship height attractiveness males females. run two correlations find one stronger relationship . Many try conclude means significantly stronger relationship one gender . However tested. can tested though. quickly show , paper also help discussion: Diedenhofen Musch (2015) cocor: Comprehensive Solution Statistical Comparison Correlations. PLOS ONE 10(6): e0131499. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121945As always need libraries first. need install cocor package laptop.Now going walk three different examples just show work different experimental designs. need match values code values text make full sense examples. use examples work faces, voices personality traits encounter later book.Comparing correlation -subjects designs (.e., independent-samples)Given scenario males voice pitch height (r(28) = .89) female voice pitch height (r(28) = .75) can say difference correlations significant?correlations come two different groups - male voices female voices - used cocor.indep.groups() function.Note: df groups 28, means N 30 groups (based N = df-2)Gives output :actually get two test outputs function: Fisher's Z Zou's Confidence Interval. one commonly used Fisher's Z look one. p-value comparison two correlations, p = .1, greater p = .05, tells us reject null hypothesis significant difference two correlations similar magnitude - .e. correlation similar groups.report outcome along lines , \"despite correlation male voice pitch height found stronger relationship female voices, Fisher's Z suggested significant difference (Z = 1.65, p = .1)\"Within-Subjects (.e., dependent samples) common variableWhat scenario 30 participants rating faces scales trust, warmth likability, want know relationship trust warmth (r = .89) significantly different relationship trust likability (r = .8).data comes participants, crossover/overlap traits - .e. one trait appears correlations interest - use cocor.dep.groups.overlap(). comparison also need know relationship remaining correlation warmth likability (r = .91).Note: order input correlations matters. See ?cocor.dep.groups.overlap() help.Gives output :test actually produces output lot tests; many probably know whether use . However, can run analysis choose just specific test adding argument function: test = steiger1980 test = pearson1898 example. probably two common.run analysis using just Pearson's Z (1898)Gives output :can see output, test significant (p = .047) suggesting significant difference correlation trust warmth (r = .89) correlation trust likeability (r = .8).Within-Subjects (.e., dependent samples) common variableOk last scenario. 30 participants rating faces scales trust, warmth, likability, attractiveness, want know relationship trust warmth (r = .89) significantly different relationship likability attractiveness (r = .93).correlations interest crossover variables come participants, example use cocor.dep.groups.nonoverlap().Note: order need correlations comparisons:trust likability (.88)trust attractiveness (.91)warmth likability (.87)warmth attractiveness (.92)Note: order input correlations matters. See ?cocor.dep.groups.nonoverlap() help.Gives output :can see output, test non-significant (p = .253) suggesting significant difference correlation trust warmth (r = .89) correlation likability attractiveness (r = .8).End Chapter!","code":"\nlibrary(cocor)\nlibrary(tidyverse)\ncompare1 <- cocor.indep.groups(r1.jk = .89, \n                               r2.hm = .75, \n                               n1 = 30, \n                               n2 = 30)## \n##   Results of a comparison of two correlations based on independent groups\n## \n## Comparison between r1.jk = 0.89 and r2.hm = 0.75\n## Difference: r1.jk - r2.hm = 0.14\n## Group sizes: n1 = 30, n2 = 30\n## Null hypothesis: r1.jk is equal to r2.hm\n## Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided)\n## Alpha: 0.05\n## \n## fisher1925: Fisher's z (1925)\n##   z = 1.6496, p-value = 0.0990\n##   Null hypothesis retained\n## \n## zou2007: Zou's (2007) confidence interval\n##   95% confidence interval for r1.jk - r2.hm: -0.0260 0.3633\n##   Null hypothesis retained (Interval includes 0)\ncompare2 <- cocor.dep.groups.overlap(r.jk = .89, \n                                     r.jh = .8, \n                                     r.kh = .91, \n                                     n = 30)## \n##   Results of a comparison of two overlapping correlations based on dependent groups\n## \n## Comparison between r.jk = 0.89 and r.jh = 0.8\n## Difference: r.jk - r.jh = 0.09\n## Related correlation: r.kh = 0.91\n## Group size: n = 30\n## Null hypothesis: r.jk is equal to r.jh\n## Alternative hypothesis: r.jk is not equal to r.jh (two-sided)\n## Alpha: 0.05\n## \n## pearson1898: Pearson and Filon's z (1898)\n##   z = 1.9800, p-value = 0.0477\n##   Null hypothesis rejected\n## \n## hotelling1940: Hotelling's t (1940)\n##   t = 2.4208, df = 27, p-value = 0.0225\n##   Null hypothesis rejected\n## \n## williams1959: Williams' t (1959)\n##   t = 2.4126, df = 27, p-value = 0.0229\n##   Null hypothesis rejected\n## \n## olkin1967: Olkin's z (1967)\n##   z = 1.9800, p-value = 0.0477\n##   Null hypothesis rejected\n## \n## dunn1969: Dunn and Clark's z (1969)\n##   z = 2.3319, p-value = 0.0197\n##   Null hypothesis rejected\n## \n## hendrickson1970: Hendrickson, Stanley, and Hills' (1970) modification of Williams' t (1959)\n##   t = 2.4208, df = 27, p-value = 0.0225\n##   Null hypothesis rejected\n## \n## steiger1980: Steiger's (1980) modification of Dunn and Clark's z (1969) using average correlations\n##   z = 2.2476, p-value = 0.0246\n##   Null hypothesis rejected\n## \n## meng1992: Meng, Rosenthal, and Rubin's z (1992)\n##   z = 2.2410, p-value = 0.0250\n##   Null hypothesis rejected\n##   95% confidence interval for r.jk - r.jh: 0.0405 0.6061\n##   Null hypothesis rejected (Interval does not include 0)\n## \n## hittner2003: Hittner, May, and Silver's (2003) modification of Dunn and Clark's z (1969) using a backtransformed average Fisher's (1921) Z procedure\n##   z = 2.2137, p-value = 0.0268\n##   Null hypothesis rejected\n## \n## zou2007: Zou's (2007) confidence interval\n##   95% confidence interval for r.jk - r.jh: 0.0135 0.2353\n##   Null hypothesis rejected (Interval does not include 0)\ncompare3 <- cocor.dep.groups.overlap(r.jk = .89, \n                                     r.jh = .8, \n                                     r.kh = .91, \n                                     n = 30,\n                                     test = \"pearson1898\")## \n##   Results of a comparison of two overlapping correlations based on dependent groups\n## \n## Comparison between r.jk = 0.89 and r.jh = 0.8\n## Difference: r.jk - r.jh = 0.09\n## Related correlation: r.kh = 0.91\n## Group size: n = 30\n## Null hypothesis: r.jk is equal to r.jh\n## Alternative hypothesis: r.jk is not equal to r.jh (two-sided)\n## Alpha: 0.05\n## \n## pearson1898: Pearson and Filon's z (1898)\n##   z = 1.9800, p-value = 0.0477\n##   Null hypothesis rejected\ncompare5 <- cocor.dep.groups.nonoverlap(r.jk = .89, \n                                        r.hm = .93, \n                                        r.jh = .88, \n                                        r.jm = .91, \n                                        r.kh = .87, \n                                        r.km = .92, \n                                        n = 30,\n                                        test = \"pearson1898\")## \n##   Results of a comparison of two nonoverlapping correlations based on dependent groups\n## \n## Comparison between r.jk = 0.89 and r.hm = 0.93\n## Difference: r.jk - r.hm = -0.04\n## Related correlations: r.jh = 0.88, r.jm = 0.91, r.kh = 0.87, r.km = 0.92\n## Group size: n = 30\n## Null hypothesis: r.jk is equal to r.hm\n## Alternative hypothesis: r.jk is not equal to r.hm (two-sided)\n## Alpha: 0.05\n## \n## pearson1898: Pearson and Filon's z (1898)\n##   z = -1.1424, p-value = 0.2533\n##   Null hypothesis retained"},{"path":"simple-regression.html","id":"simple-regression","chapter":"10 Simple Regression","heading":"10 Simple Regression","text":"chapter, working real data using regression explore question whether relationship statistics anxiety engagement course activities. hypothesis students anxious statistics less likely engage course-related activities. avoidance behaviour ultimately responsible lower performance students (although examining assessment scores activity).going analyse data STARS Statistics Anxiety Survey, administered students third-year statistics course Psychology University Glasgow. responses anonymised associating responses student arbitrary ID number (integer).STARS survey (Cruise, Cash, & Bolton, 1985) 51-item questionnaire, response 1 5 scale, higher numbers indicating greater anxiety.Cruise, R. J., Cash, R. W., & Bolton, D. L. (1985). Development validation instrument measure statistical anxiety. Proceedings American Statistical Association, Section Statistical Education, Las Vegas, NV.measure engagement course, use data Moodle usage analytics. course term, eight optional weekly -line sessions students attend extra support. variable n_weeks psess.csv file tells many (eight) given student attended.hypothesis greater anxiety reflected lower engagement. Answer following question.hypothesis correct positivenoa negative correlation students' mean anxiety levels n_weeks.","code":""},{"path":"simple-regression.html","id":"regression-a1","chapter":"10 Simple Regression","heading":"10.1 Activity 1: Setup","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Regression\".Download L3_stars.csv psess.csv save folder. Make sure change file name .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads pwr, broom, see, performance, report tidyverse using library() function.Load two CSV datasets variables called stars engage using read_csv().","code":""},{"path":"simple-regression.html","id":"regression-a2","chapter":"10 Simple Regression","heading":"10.2 Activity 2: Tidy the data","text":"Take look datasets loaded .next thing need calculate mean anxiety score student (recall individual students identified ID variable).Recall difference wide tidy data. wide data, row represents individual case, observations case separate columns; tidy data, row represents single observation, observations grouped together cases based value variable (data, ID variable).STARS data currently widetidy format.calculate means, need use pivot_longer() restructure STARS data appropriate \"tidy\" format; .e., looks like table .Write run code tidy STARS data, store resulting table stars2.","code":""},{"path":"simple-regression.html","id":"regression-a3","chapter":"10 Simple Regression","heading":"10.3 Activity 3: Calculate mean anxiety for each student","text":"Now got data tidy format, use summarise() group_by() calculate mean anxiety scores (mean_anxiety) student (ID). Store resulting table variable named stars_means.","code":""},{"path":"simple-regression.html","id":"regression-a4","chapter":"10 Simple Regression","heading":"10.4 Activity 4: Join the datasets together","text":"order perform regression analysis, combine data stars_means engage using inner_join(). Call resulting table joined. look like :","code":""},{"path":"simple-regression.html","id":"regression-a5","chapter":"10 Simple Regression","heading":"10.5 Activity 5: Calculate descriptives for the variables overall","text":"also useful calculate descriptives statistics sample overall can check sample scores expecting (e.g., comparable previous studies samples?). also useful write-.Run code. Read line ensure understand calculated.","code":"\ndescriptives <- joined %>%\n  summarise(mean_anx = mean(mean_anxiety, na.rm = TRUE),\n            sd_anx = sd(mean_anxiety, na.rm = TRUE),\n            mean_weeks = mean(n_weeks, na.rm = TRUE),\n            sd_weeks = sd(n_weeks, na.rm = TRUE))"},{"path":"simple-regression.html","id":"regression-a6","chapter":"10 Simple Regression","heading":"10.6 Activity 6: Visualisations","text":"Now variables one place, write code reproduce exact scatterplot (using ggplot2).\nFigure 10.1: Scatteplot mean anxiety attendance\nAccording scatterplot, apparent relationshipas anxiety increases, engagement decreasesas anxiety increases, engagement increases","code":""},{"path":"simple-regression.html","id":"regression-a7","chapter":"10 Simple Regression","heading":"10.7 Activity 7: Run the regression","text":"lm() function Base R main function estimate Linear Model (hence function name lm). lm() uses formula syntax seen , .e., DV ~ predictor.Use lm() function predict n_weeks (DV) mean_anxiety (predictor). Store result call lm() variable mod. see results, use summary(mod).Answer following questions model. may wish refer lecture notes help answer questions.estimate y-intercept model, rounded three decimal places, three decimal places, GLM model \\(Y_i = \\beta_0 + \\beta_1 X_i + e_i\\), \\(\\beta_1\\) three decimal places, unit increase anxiety, n_weeks decreases two decimal places, overall F-ratio model? overall model significant? YesNoWhat proportion variance model explain? summary table, estimate intercept.summary table, estimate mean_anxiety, .e., slope.summary table, also estimate mean_anxiety, slope much decreases just remove - sign.summary table, F-ratio noted F-statistic.overall model p.value .001428 less .05, therefore significant.variance explained determined R-squared, simply multiple 100 get percent. always use adjusted R-squared value.","code":"\nmod <- lm(n_weeks ~ mean_anxiety, joined)\nmod_summary <- summary(mod)"},{"path":"simple-regression.html","id":"regression-a8","chapter":"10 Simple Regression","heading":"10.8 Activity 8: Assumption checking","text":"check assumptions run regression now check whether anything concerned . covered lecture, assumptions regression :outcome/DV interval/ratio level dataThe predictor variable interval/ratio categorical (two levels)values outcome variable independent (.e., score come different participant)predictors non-zero varianceThe relationship outcome predictor linearThe residuals normally distributedThere homoscedasticity (homogeneity variance, residuals)Assumptions 1-3 nice easy. know data design study. Assumption 4 simply means spread data - example, point running regression age variable participants 20 years old. can check using scatterplot created Activity 4 can see assumption met, indeed spread scores.rest assumptions, going use functions packages see performance make life whole lot easier.packages installed machine yet, can go ahead install typing install.packages(\"NAME PACKAGE\") console . working server, packages already installed.First, can use check_model() produce range assumption test visualisations. Helpfully, function also provides brief explanation looking plot - functions R helpful!get error message Failed error:  ‘package called ‘qqplotr’’, install package qqplotr, need load using library(), check_model() uses background.\n(#fig:check_model)Visual assumption checks\nAssumption 5, linearity, plot suggests perfect looks pretty good.already noted, good visualise assumption checks just relying statistics can problematic, can sensitive small large sample sizes. However, can also reassuring statistical test back intuitions plot.Assumption 6, normality residuals, plot suggest residuals might normal, can check check_normality() runs Shapiro-Wilk test.result confirms residuals normally distributed, something likely exacerbated relatively small sample size. feeling confident, can see might resolve , core aims chapter conclude sample continue.multiple ways can transform data deal non-normality, can find information data transformation Appendix .First, need get sense issue dependent variable, case n_weeks. simple histogram shows DV normal distribution, instead, looks like uniform distribution.important remember assumptions regression residuals normally distributed, raw data, however, transforming DV can help.transform uniform distribution normal distribution, going use unif2norm function faux package (may need install).code uses mutate() create new variable n_weeks_transformed result transformation.notice histogram transformed variable still look amazing, remember residuals, raw data matters. re-run regression transformed data check model , things looking much better.worth saying point transformation use, whether works, can bit trial--error.homoscedasticity, plot looks mostly fine, can double check check_heteroscedasticity() result confirms data met assumption.","code":"\ncheck_model(mod)\ncheck_normality(mod)## Warning: Non-normality of residuals detected (p = 0.008).\nggplot(joined, aes(x = n_weeks)) +\n  geom_histogram(binwidth = 1)\nlibrary(faux)\njoined <- mutate(joined, \n                 n_weeks_transformed = unif2norm(n_weeks))\nggplot(joined, aes(x = n_weeks_transformed)) +\n  geom_histogram()\nmod_transformed <- lm(n_weeks_transformed ~ mean_anxiety, joined)\ncheck_normality(mod_transformed)\ncheck_model(mod_transformed)## OK: residuals appear as normally distributed (p = 0.107).\ncheck_heteroscedasticity(mod)## OK: Error variance appears to be homoscedastic (p = 0.542)."},{"path":"simple-regression.html","id":"regression-a9","chapter":"10 Simple Regression","heading":"10.9 Activity 9: Power and effect size","text":"First can calculate minimum effect size able detect given sample size design study using pwr.f2.test(). usual, fill information set effect size argument, case f2, NULL.u - Numerator degrees freedom. number coefficients model (minus intercept)\nv - Denominator degrees freedom. calculated v=n-u-1, n number participants\nf2 - effect size - solving effect size, parameter left NULL\nsig.level - significance level study\npower - power level studyBased power analysis, minimum effect size able detect rounded 2 decimal places? According Cohen's guidelines, SmallMediumLarge effect.formula calculate observed f2, must manually using formula lecture.observed effect size larger minimum effect size detect? Yes, study sufficiently poweredNo, study underpowered","code":"\npwr.f2.test(u = 1, v = 35, f2 = NULL, sig.level = .05, power = .8)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)"},{"path":"simple-regression.html","id":"regression-a10","chapter":"10 Simple Regression","heading":"10.10 Activity 10: Write-up","text":"two ways can use R help write-. first inline coding like done previously, second use report package. one use entirely nice options.need manually calculate p-value inline coding extract lm() model. Run code .Now, copy paste code white-space knit document.simple linear regression performed engagement (M = 4.54, SD = 0.56) outcome variable statistics anxiety (M = 2.08, SD = 0.56) predictor variable. results regression indicated model significantly predicted course engagement (F(1, 35) = 11.99, p < .001, Adjusted R2 = 0.23, f2 = .63), accounting 23% variance. Anxiety significant predictor (β = -2.17, p < 0.001.\n)second option uses report. output functions tend usable without editing particularly first learning write-stats can useful kind template (also see different ways reporting stats).Running report() output summary results copy past Word document.","code":"\nf <-mod_summary$fstatistic\nmod_p <- pf(f[1], f[2], f[3], lower=FALSE) A simple linear regression was performed with engagement (M = `r descriptives$mean_weeks %>% round(2)`, SD = `r descriptives$sd_anx %>% round(2)`) as the outcome variable and statistics anxiety (M = `r descriptives$mean_anx %>% round(2)`, SD = `r descriptives$sd_anx %>% round(2)`) as the predictor variable. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3]`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f2 = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Anxiety was a significant predictor (β = `r mod$coefficients[2] %>% round(2)`, p < `r mod_p %>% round(3)`.\n)\nreport(mod)## We fitted a linear model (estimated using OLS) to predict n_weeks with\n## mean_anxiety (formula: n_weeks ~ mean_anxiety). The model explains a\n## statistically significant and moderate proportion of variance (R2 = 0.26, F(1,\n## 35) = 11.99, p = 0.001, adj. R2 = 0.23). The model's intercept, corresponding\n## to mean_anxiety = 0, is at 9.06 (95% CI [6.32, 11.80], t(35) = 6.71, p < .001).\n## Within this model:\n## \n##   - The effect of mean anxiety is statistically significant and negative (beta =\n## -2.17, 95% CI [-3.45, -0.90], t(35) = -3.46, p = 0.001; Std. beta = -0.51, 95%\n## CI [-0.80, -0.21])\n## \n## Standardized parameters were obtained by fitting the model on a standardized\n## version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n## computed using a Wald t-distribution approximation."},{"path":"simple-regression.html","id":"regression-sols","chapter":"10 Simple Regression","heading":"10.11 Solutions to Activities","text":"","code":""},{"path":"simple-regression.html","id":"regression-a1sol","chapter":"10 Simple Regression","heading":"10.11.1 Activity 1","text":"** Click tab see solution **","code":"\nlibrary(\"pwr\")\nlibrary(\"broom\")\nlibrary(\"see\")\nlibrary(\"performance\")\nlibrary(\"report\")\nlibrary(\"tidyverse\")\nstars <- read_csv(\"L3_stars.csv\")\nengage <- read_csv(\"psess.csv\")"},{"path":"simple-regression.html","id":"regression-a2sol","chapter":"10 Simple Regression","heading":"10.11.2 Activity 2","text":"** Click tab see solution **","code":"\nstars2 <- pivot_longer(data = stars, names_to = \"Question\", values_to = \"Score\",cols = Q01:Q51) %>%\n  arrange(ID)"},{"path":"simple-regression.html","id":"regression-a3sol","chapter":"10 Simple Regression","heading":"10.11.3 Activity 3","text":"** Click tab see solution **","code":"\nstars_means <- stars2 %>%\n  group_by(ID) %>%\n  summarise(mean_anxiety = mean(Score, na.rm = TRUE),\n            min = min(Score), \n            max = min(Score),\n            sd = sd(Score))"},{"path":"simple-regression.html","id":"regression-a4sol","chapter":"10 Simple Regression","heading":"10.11.4 Activity 4","text":"** Click tab see solution **","code":"\njoined <- inner_join(stars_means, engage, \"ID\")"},{"path":"simple-regression.html","id":"regression-a6sol","chapter":"10 Simple Regression","heading":"10.11.5 Activity 6","text":"** Click tab see solution **","code":"\nggplot(joined, aes(mean_anxiety, n_weeks)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")+\n  theme_minimal()"},{"path":"multiple-regression.html","id":"multiple-regression","chapter":"11 Multiple Regression","heading":"11 Multiple Regression","text":"previous chapter, looked simple regressions - predicting outcome variable using one predictor variable. chapter, expand look scenarios predict outcome using one predictor model - hence, multiple regression.DataThere currently much debate (hype) surrounding smartphones effects well-, especially regard children teenagers. looking data recent study English adolescents:Przybylski, . & Weinstein, N. (2017). Large-Scale Test Goldilocks Hypothesis. Psychological Science, 28, 204--215.\nlarge-scale study found support \"Goldilocks\" hypothesis among adolescents: \"just right\" amount screen time, amount less amount associated lower well-. huge survey study: data contain responses 120,000 participants!Fortunately, authors made data study openly available, allows us dig deeper results. exercise, look whether relationship screen time well-modulated participants' (self-reported) gender.dependent measure used study Warwick-Edinburgh Mental Well-Scale (WEMWBS). 14-item scale 5 response categories, summed together form single score ranging 14-70.Przybylski & Weinstein's page study Open Science Framework, can find participant survey asks large number additional questions (see page 14 WEMWBS questions pages 4-5 questions screen time). Within page can also find raw data; however, purpose exercise, using local pre-processed copies data provide.Przybylski Weinstein looked multiple measures screen time, focusing smartphone use. found decrements well-started appear respondents reported one hour weekly smartphone use. question: negative association hours use well-(beyond one-hour point) differ boys girls?Note analysis, :continuous\\(^*\\) DV, well-;continuous\\(^*\\) DV, well-;continuous\\(^*\\) predictor, screen time;continuous\\(^*\\) predictor, screen time;categorical predictor, gender.categorical predictor, gender.\\(^*\\)variables quasi-continuous, inasmuch discrete values possible. However, sufficient number discrete categories can treat effectively continuous.want estimate two slopes relating screen time well-, one girls one boys, statistically compare slopes. problem seems simultaneously like situation run regression (estimate slopes) also one need t-test (compare two groups). expressive power regression allows us within single model.","code":""},{"path":"multiple-regression.html","id":"mulregression-a1","chapter":"11 Multiple Regression","heading":"11.1 Activity 1: Set-up","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Multiple Regression\".Download wellbeing.csv, participant_info.csv screen_time.csv save Chapter folder. Make sure change file names .server, avoid number issues restarting session - click Session - Restart RDelete default R Markdown welcome text insert new code chunk loads pwr, see, performance, report, tidyverse using library() function.Load CSV datasets variables called pinfo, wellbeing screen using read_csv().","code":""},{"path":"multiple-regression.html","id":"mulregression-a2","chapter":"11 Multiple Regression","heading":"11.2 Activity 2: Look at the data","text":"Take look resulting tibbles pinfo, wellbeing, screen. wellbeing tibble information WEMWBS questionnaire; screen information screen time use weekends (variables ending ) weekdays (variables ending wk) four types activities: using computer (variables starting Comph; Q10 survey), playing video games (variables starting Comp; Q9 survey), using smartphone (variables starting Smart; Q11 survey) watching TV (variables starting Watch; Q8 survey). want information variables, look items 8-11 pages 4-5 PDF version survey OSF website.variable corresponding gender located table named pinfowellbeingscreen variable called .variable corresponding gender located table named pinfowellbeingscreen variable called .WEMWBS data longwide format, contains observations  participants  items.WEMWBS data longwide format, contains observations  participants  items.Individual participants dataset identified variable named  [sure type name exactly, including capitalization]. variable allow us link information across three tables.Individual participants dataset identified variable named  [sure type name exactly, including capitalization]. variable allow us link information across three tables.Run summary() three data-sets. missing data points? YesNoRun summary() three data-sets. missing data points? YesNo","code":""},{"path":"multiple-regression.html","id":"mulregression-a3","chapter":"11 Multiple Regression","heading":"11.3 Activity 3: Compute the well-being score for each respondent","text":"WEMWBS well-score simply sum items.Write code create new table called wemwbs, two variables: Serial (participant ID), tot_wellbeing, total WEMWBS score.\"pivot\" table wide longgroup_by(); summarise(tot_wellbeing = ...)Sanity check: Verify scores fall 14-70 range. Przybylski Weinstein reported mean 47.52 standard deviation 9.55. Can reproduce values?summarise(), min(), max()Now visualise distribution tot_wellbeing histogram using ggplot2.geom_histogram()distribution well-scores symmetricnegatively skewedpositively skewed.","code":"\nggplot(wemwbs, aes(tot_wellbeing)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"multiple-regression.html","id":"mulregression-a4","chapter":"11 Multiple Regression","heading":"11.4 Activity 4: Visualise the relationship","text":"take quick look relationship screen time (four different technologies) measures well-. code .Run code try explain words line code (remember, pronounce %>% \"\"). may find easier look tables produced.\nFigure 11.1: Relationship wellbeing screentime usage technology weekday\ngraph makes evident smartphone use 1 hour per day associated increasingly negative well-. Note combined tables using inner_join(), include data observations across wemwbs screen2 tables.next step, going focus smartphone/well-relationship.","code":"\nscreen_long <- screen %>%\n  pivot_longer(names_to = \"var\", values_to = \"hours\", -Serial) %>%\n  separate(var, c(\"variable\", \"day\"), \"_\")\nscreen2 <- screen_long %>%\n  mutate(variable = dplyr::recode(variable,\n               \"Watch\" = \"Watching TV\",\n               \"Comp\" = \"Playing Video Games\",\n               \"Comph\" = \"Using Computers\",\n               \"Smart\" = \"Using Smartphone\"),\n     day = dplyr::recode(day,\n              \"wk\" = \"Weekday\",\n              \"we\" = \"Weekend\"))\ndat_means <- inner_join(wemwbs, screen2, \"Serial\") %>%\n  group_by(variable, day, hours) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\nggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~variable, nrow = 2)"},{"path":"multiple-regression.html","id":"mulregression-a5","chapter":"11 Multiple Regression","heading":"11.5 Activity 5: Smartphone and well-being for boys and girls","text":"analysis, going collapse weekday weekend use smartphones.Create new table, smarttot, mean number hours per day smartphone use participant, averaged weekends/weekdays.need filter dataset include smartphone use technologies.also need group results participant ID (.e., Serial).final data-set two variables: Serial (participant) tothours.need use data-set screen2 .filter() group_by() summarise()Next, create new tibble called smart_wb includes (filters) participants smarttot used smartphone one hour per day week, combine (join) table information wemwbs pinfo.**filter() inner_join() another inner_join()","code":""},{"path":"multiple-regression.html","id":"mulregression-a6","chapter":"11 Multiple Regression","heading":"11.6 Activity 6: Mean-centering variables","text":"continuous variables regression, often sensible transform mean centering. mean center predictor X simply subtracting mean (X_centered = X - mean(X)). two useful consequences:model intercept reflects prediction \\(Y\\) mean value predictor variable, rather zero value unscaled variable;model intercept reflects prediction \\(Y\\) mean value predictor variable, rather zero value unscaled variable;interactions model, lower-order effects can given interpretation receive ANOVA (main effects, rather simple effects).interactions model, lower-order effects can given interpretation receive ANOVA (main effects, rather simple effects).categorical predictors two levels, become coded -.5 .5 (mean two values 0).Use mutate add two new variables smart_wb: thours_c, calculated mean-centered version tothours predictor; male_c, recoded -.5 female .5 male.create male_c need use if_else(male == 1, .5, -.5) can read code \"variable male equals 1, recode .5, , recode -.5\".Finally, recode male male_c factors, R knows treat real numbers.","code":""},{"path":"multiple-regression.html","id":"mulregression-a7","chapter":"11 Multiple Regression","heading":"11.7 Activity 7: Visualise the relationship","text":"Reverse-engineer plot. Calculate mean well-scores combination male tothours, create scatterplot plot includes separate regression lines gender.may find useful refer Data Visualisation chapter.group_by() variables summarise()colour = variable_you_want_different_colours_for\nFigure 11.2: Relationship mean wellbeing smartphone use gender\nWrite interpretation plot plain English.Girls show lower overall well-compared boys. addition, slope girls appears negative boys; one boys appears relatively flat.suggests negative association well-smartphone use stronger girls.","code":""},{"path":"multiple-regression.html","id":"mulregression-a8","chapter":"11 Multiple Regression","heading":"11.8 Activity 8: Running the regression","text":"Now going see statistical support interpretation graph.data smart_wb, use lm() function calculate multiple regression model:\\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\)\\(Y_i\\) well-score participant \\(\\);\\(X_{1i}\\) mean-centered smartphone use variable participant \\(\\);\\(X_{2i}\\) gender (-.5 = female, .5 = male);\\(X_{3i}\\) interaction smartphone use gender (\\(= X_{1i} \\times X_{2i}\\))lm() function R main function estimate Linear Model (hence function name lm). function takes format :lm(dv ~ iv, data = my_data) simple linear regression (used previous chapter)lm(dv ~ iv1 + iv2, data = my_data) multiple linear regression; see just adds predictors model.Note: code lm(dv ~ iv1 + iv2, data = my_data) add predictors iv1 iv2. provide contributions predictor explain outcome variable. However, also interested interaction two predictors, use * instead +, .e., lm(dv ~ iv1 * iv2, data = my_data). Try see output changes.\nquestion trying answer , want look interaction gender (male_c) smartphone use time (thours_c) predict outcome variable well-(tot_wellbeing). Use lm() function run multiple regression. use summary() view results store object called mod_summary().R formulas look like : y ~ + b + :b :b means interactionThe interaction smartphone use gender shown variable thours_cmale_cthours_c:male_c, interaction significantnonsignificant \\(\\alpha = .05\\) level.interaction smartphone use gender shown variable thours_cmale_cthours_c:male_c, interaction significantnonsignificant \\(\\alpha = .05\\) level.2 decimal places, proportion variance well-scores overall model explain? 2 decimal places, proportion variance well-scores overall model explain? p-value overall model fit < 2.2e-16. significant? YesNoThe p-value overall model fit < 2.2e-16. significant? YesNoWhat reasonable interpretation results? smartphone use harms girls boyssmartphone use harms boys girlsthere evidence gender differences relationship smartphone use well-beingsmartphone use negatively associated wellbeing girls boysWhat reasonable interpretation results? smartphone use harms girls boyssmartphone use harms boys girlsthere evidence gender differences relationship smartphone use well-beingsmartphone use negatively associated wellbeing girls boys","code":""},{"path":"multiple-regression.html","id":"mulregression-a9","chapter":"11 Multiple Regression","heading":"11.9 Activity 9: Assumption checking","text":"Now time test pesky assumptions. assumptions multiple regression simple regression one additional assumption, multicollinearity, idea predictor variables highly correlated.outcome/DV interval/ratio level dataThe predictor variable interval/ratio categorical (two levels)values outcome variable independent (.e., score come different participant)predictors non-zero varianceThe relationship outcome predictor linearThe residuals normally distributedThere homoscedasticity (homogeneity variance, residuals)Multicollinearity: predictor variables highly correlatedFrom work done far know assumptions 1 - 4 met can use functions performance package check rest, like simple linear regression chapter.One difference used check_model() previously rather just letting run tests wants, going specify tests, stop throwing error. word warning - assumptions tests take longer usual run, big dataset. first line code run assumption tests save object, calling object name display plots.\nFigure 4.11: Assumption plots\nassumption 5, linearity, already know looking scatterplot relationship linear, residual plot also confirms .assumption 6, normality residuals, residuals look good plots provides excellent example often better visualise rely statistics use check_normality() calls Shapiro-Wilk test:tells us residuals normal, despite fact plots look almost perfect. large sample sizes, deviation perfect normality can flagged non-normal.assumption 7, homoscedasticity, plot missing reference line - fun fact, took us several days lives asking help Twitter figure . reason line dataset large creates memory issue need create plot using code developers package see provided us Twitter. default code try draw confidence intervals around line causes memory issue, code removes se = FALSE.Please note datasets extra step, good example comes programming, matter long , always problem come across asking help part process.\nFigure 11.3: Adjusted homogeneity plot produce reference line\nlike normality, plot perfect pretty good another example visualisation better running statistical tests see significant result run:assumption 8, linearity, plot looks fine, also used grouped scatterplots look .Finally, assumption 9, multicollinearity, plot also indicates issues can also test statistically using check_collinearity().Essentially, function estimates much variance coefficient “inflated” linear dependence predictors, .e., predictor actually adding unique variance model, just really strongly related predictors. can read . Thankfully, VIF affected large samples like tests.various rules thumb, converge VIF 2 - 2.5 one predictor problematic.","code":"\nassumptions <- check_model(mod, check = c(\"vif\", \"qq\", \"normality\", \"linearity\", \"homogeneity\"))\nassumptions\ncheck_normality(mod)## Warning: Non-normality of residuals detected (p < .001).\nggplot(assumptions$HOMOGENEITY, aes(x, y)) +\n    geom_point2() +\n    stat_smooth(\n      method = \"loess\",\n      se = FALSE,\n      formula = y ~ x,\n    ) +\n    labs(\n      title = \"Homogeneity of Variance\",\n      subtitle = \"Reference line should be flat and horizontal\",\n      y = expression(sqrt(\"|Std. residuals|\")),\n      x = \"Fitted values\"\n    ) \ncheck_homogeneity(mod)## Warning: Variances differ between groups (Bartlett Test, p = 0.000).\ncheck_collinearity(mod)"},{"path":"multiple-regression.html","id":"mulregression-a10","chapter":"11 Multiple Regression","heading":"11.10 Activity 10: Power and effect size","text":"Finally, calculate power effect size using pwr package. run power analysis previous chapter simple regressions,Using code power analysis calculate minimum effect size reliably observe given sample size design 99% power. Report 2 decimal places Using code power analysis calculate minimum effect size reliably observe given sample size design 99% power. Report 2 decimal places observed effect size study 2 decimal places? observed effect size study 2 decimal places? study sufficiently powered? YesNoIs study sufficiently powered? YesNo","code":""},{"path":"multiple-regression.html","id":"mulregression-a11","chapter":"11 Multiple Regression","heading":"11.11 Activity 11: Making predictions","text":"successfully constructed linear model relating wel-gender smartphone use time. However, one last thing might want quickly show make prediction using predict() function. One way use , though see solution chapter alternatives, :newdata tibble new observations/values X (e.g. male_c /thours_c) want predict corresponding Y values (tot_wellbeing). try now.Make tibble two columns, one called male_c one called thours_c - exactly spelt model. Give male_c value -0.5 (girls) give thours_c value 4. code making tibble using tibble() function tibble(male_c = value, thours_c = value). Store tibble object newdata.Now put tibble, newdata, predict() function run regression model mod, predict(NAME MODEL, NAME TIBBLE).Now, based output try answer question:one decimal place, predicted well-rating girl smartphone use 4 hours - ","code":"\npredict(mod, newdata)"},{"path":"multiple-regression.html","id":"mulregression-a12","chapter":"11 Multiple Regression","heading":"11.12 Activity 12: Write-up","text":"simple regression, can use inline coding report() function help write-. First, copy paste code white-space knit document. Note p-values entered manually APA p < .001 formatting.continuous predictors mean-centered deviation coding used categorical predictors. results regression indicated model significantly predicted course engagement (F(3, 7.1029^{4}) = 2450.89, p < .001, Adjusted R2 = 0.09, f2 = .63), accounting 9% variance. Total screen time significant negative predictor well-scores (β = -0.77, p < .001, gender (β = 5.14, p < .001, girls lower well-scores boys. Importantly, significant interaction screen time gender (β = 0.45, p < .001), smartphone use negatively associated well-girls boys.\nNow, can use report() produce automated summary. , need editing may useful aid interpretation reporting.","code":"All continuous predictors were mean-centered and deviation coding was used for categorical predictors. The results of the regression indicated that the model significantly predicted course engagement (F(`r mod_summary$fstatistic[2]`, `r mod_summary$fstatistic[3] %>% round(2)`) = `r mod_summary$fstatistic[1] %>% round(2)`, p < .001, Adjusted R2 = `r mod_summary$adj.r.squared %>% round(2)`, f^2^ = .63), accounting for `r (mod_summary$adj.r.squared %>% round(2))*100`% of the variance. Total screen time was a significant negative predictor of wellbeing scores (β = `r mod$coefficients[2] %>% round(2)`, p < .001, as was gender (β = `r mod$coefficients[3] %>% round(2)`, p < .001, with girls having lower wellbeing scores than boys. Importantly, there was a significant interaction between screentime and gender (β = `r mod$coefficients[4] %>% round(2)`, p < .001), smartphone use was more negatively associated with wellbeing for girls than for boys. \nreport(mod)## We fitted a linear model (estimated using OLS) to predict tot_wellbeing with\n## thours_c and male_c (formula: tot_wellbeing ~ thours_c * male_c). The model\n## explains a statistically significant and weak proportion of variance (R2 =\n## 0.09, F(3, 71029) = 2450.89, p < .001, adj. R2 = 0.09). The model's intercept,\n## corresponding to thours_c = 0 and male_c = -0.5, is at 44.87 (95% CI [44.78,\n## 44.96], t(71029) = 1001.87, p < .001). Within this model:\n## \n##   - The effect of thours c is statistically significant and negative (beta =\n## -0.77, 95% CI [-0.82, -0.73], t(71029) = -32.96, p < .001; Std. beta = -0.15,\n## 95% CI [-0.16, -0.15])\n##   - The effect of male c [0.5] is statistically significant and positive (beta =\n## 5.14, 95% CI [5.00, 5.28], t(71029) = 72.25, p < .001; Std. beta = 0.54, 95% CI\n## [0.52, 0.55])\n##   - The interaction effect of male c [0.5] on thours c is statistically\n## significant and positive (beta = 0.45, 95% CI [0.38, 0.52], t(71029) = 12.24, p\n## < .001; Std. beta = 0.09, 95% CI [0.08, 0.11])\n## \n## Standardized parameters were obtained by fitting the model on a standardized\n## version of the dataset. 95% Confidence Intervals (CIs) and p-values were\n## computed using a Wald t-distribution approximation."},{"path":"multiple-regression.html","id":"mulregression-sols","chapter":"11 Multiple Regression","heading":"11.13 Solutions to Activities","text":"","code":""},{"path":"multiple-regression.html","id":"mulregression-a3sol","chapter":"11 Multiple Regression","heading":"11.13.1 Activity 3","text":"","code":"\nwemwbs <- wellbeing %>%\n  pivot_longer(names_to = \"var\", values_to = \"score\", -Serial) %>%\n  group_by(Serial) %>%\n  summarise(tot_wellbeing = sum(score))\n# sanity check values\nwemwbs %>% summarise(mean = mean(tot_wellbeing),\n                     sd = sd(tot_wellbeing),\n                     min = min(tot_wellbeing), \n                     max = max(tot_wellbeing))"},{"path":"multiple-regression.html","id":"mulregression-a5sol","chapter":"11 Multiple Regression","heading":"11.13.2 Activity 5","text":"","code":"\nsmarttot <- screen2 %>%\n  filter(variable == \"Using Smartphone\") %>%\n  group_by(Serial) %>%\n  summarise(tothours = mean(hours))\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") "},{"path":"multiple-regression.html","id":"mulregression-a6sol","chapter":"11 Multiple Regression","heading":"11.13.3 Activity 6","text":"","code":"\nsmart_wb <- smarttot %>%\n  filter(tothours > 1) %>%\n  inner_join(wemwbs, \"Serial\") %>%\n  inner_join(pinfo, \"Serial\") %>%\n  mutate(thours_c = tothours - mean(tothours),\n         male_c = ifelse(male == 1, .5, -.5),\n         male_c = as.factor(male_c),\n         male = as.factor(male))"},{"path":"multiple-regression.html","id":"mulregression-a7sol","chapter":"11 Multiple Regression","heading":"11.13.4 Activity 7","text":"","code":"\nsmart_wb_gen <- smart_wb %>%\n  group_by(tothours, male) %>%\n  summarise(mean_wellbeing = mean(tot_wellbeing))\nggplot(smart_wb_gen, aes(tothours, mean_wellbeing, color = male)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_color_discrete(name = \"Gender\", labels = c(\"Female\", \"Male\"))+\n  scale_x_continuous(name = \"Total hours smartphone use\") +\n  scale_y_continuous(name = \"Mean well-being score\")"},{"path":"multiple-regression.html","id":"mulregression-a8sol","chapter":"11 Multiple Regression","heading":"11.13.5 Activity 8","text":"","code":"\nmod <- lm(tot_wellbeing ~ thours_c * male_c, smart_wb)\n# alternatively: \n# mod <- lm(tot_wellbeing ~ thours_c + male_c + thours_c:male_c, smart_wb)\nmod_summary <- summary(mod)"},{"path":"multiple-regression.html","id":"mulregression-a9sol","chapter":"11 Multiple Regression","heading":"11.13.6 Activity 9","text":"","code":"\nqqPlot(mod$residuals)"},{"path":"multiple-regression.html","id":"mulregression-a10sol","chapter":"11 Multiple Regression","heading":"11.13.7 Activity 10","text":"","code":"\npwr.f2.test(u = 3, v = 71029, f2 = NULL, sig.level = .05, power = .99)\nf2 <- mod_summary$adj.r.squared/(1 - mod_summary$adj.r.squared)"},{"path":"multiple-regression.html","id":"mulregression-a11sol","chapter":"11 Multiple Regression","heading":"11.13.8 Activity 11","text":"Note: value gender -0.5 0.5 recoded categorical variable. reason, value gender entered within quotes.","code":"\nnewdata <- tibble(male_c = \"-0.5\", thours_c = 4)\npredict(mod, newdata)"},{"path":"one-way-anova.html","id":"one-way-anova","chapter":"12 One-way ANOVA","heading":"12 One-way ANOVA","text":"","code":""},{"path":"one-way-anova.html","id":"background-intrusive-memories","chapter":"12 One-way ANOVA","heading":"12.1 Background: Intrusive memories","text":"lecture textbooks worked calculating ANOVA hand order gain conceptual understanding. However, run ANOVA, typically computer calculations . chapter, show run one-factor ANOVA using afex package post-hoc tests using package called emmeans.example using data experiment 2 James, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J. R., Milton, . L., & Holmes, E. . (2015). Computer game play reduces intrusive memories experimental trauma via reconsolidation-update mechanisms. Psychological Science, 26, 1201-1215.abstract paper follows:Memory traumatic event becomes consolidated within hours. Intrusive memories can flash back repeatedly mind's eye cause distress. investigated whether reconsolidation - process memories become malleable recalled - can blocked using cognitive task whether approach can reduce unbidden intrusions. predicted reconsolidation reactivated visual memory experimental trauma disrupted engaging visuospatial task compete visual working memory resources. showed intrusive memories virtually abolished playing computer game Tetris following memory-reactivation task 24 hr initial exposure experimental trauma. Furthermore, memory reactivation playing Tetris required reduce subsequent intrusions (Experiment 2), consistent reconsolidation-update mechanisms. simple, non-invasive cognitive-task procedure administered emotional memory already consolidated (.e., > 24 hours exposure experimental trauma) may prevent recurrence intrusive memories emotional events.","code":""},{"path":"one-way-anova.html","id":"anova-a1","chapter":"12 One-way ANOVA","heading":"12.2 Activity 1: Set-up","text":"following:Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"One-way ANOVA\".Download James_Holmes_Expt 2_DATA.csv save chapter folder.server, avoid number issues restarting session - click Session - Restart RIn new code chunk, type run code loads pwr, lsr, car, broom, afex, emmeans, performance tidyverse using library() function load data object named dat using read_csv(). working machine may need install afex emmeans always install packages university machines.Add (hint: mutate) column dat called subjectthat equals row_number() act participant ID currently missing data set.","code":""},{"path":"one-way-anova.html","id":"anova-a2","chapter":"12 One-way ANOVA","heading":"12.3 Activity 2: Data wrangling","text":"lot columns data set need analysis names variable also long difficult work .Create new object called dat2 just three columns need - use select() select columns subject, Condition, Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_DiaryUse rename() rename Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary intrusionsSee can one pipelineHint: new_name = old_name","code":""},{"path":"one-way-anova.html","id":"anova-a3","chapter":"12 One-way ANOVA","heading":"12.4 Activity 3: Numbers and factors","text":"addition names variables long, levels Condition named 1,2,3,4 R think numbers rather category. going overwrite column Condition column recodes numbers factor. Copy paste code Markdown run .really important step. forget recode variables factors R treats numbers, lot things work. Trust us, spent lot time trying figure wrong forgot step!information, conditions 1, 2, 3, 4 conditions \"-task control\", \"Reactivation plus Tetris\", \"Tetris \", \"Reactivation \", respectively, experiment. add meaningful labels later visualise data.","code":"\ndat2 <- dat2 %>%\n  mutate(Condition = as.factor(Condition))"},{"path":"one-way-anova.html","id":"anova-a4","chapter":"12 One-way ANOVA","heading":"12.5 Activity 4: Create summary statistics","text":"Next want calculate descriptive statistics. really interested scores experimental group rather overall.Create object called sum_datthat contains mean, standard deviation standard error number intrusions grouped Condition.Use pipe achieve one pipeline.table four columns, Condition, mean, sd, se.##Activity 5: Visualisation {#anova-a5}Now can visualise data. original paper use bar plot, reproduce later, now use better plot gives us information data.Create violin-boxplot number intrusions y-axis condition x-axis (see data visualisation chapter info).Change labels x-axis something informative (hint: scale_x_discrete(labels = c(\"label names\"))\nFigure 12.1: Number intrusions condition\ncan see plot outliers groups. information present bar plot, good idea use , reproduce anyway. code shows produce bar plot presented paper. Try figure bit code plot (remember use help documentation function) see happens change values argument.\nFigure 12.2: Bar plot instrusions condition\n","code":"## \n## * Use group_by(some_grouping_variable) %>% summarise(...)\n## * standard error = sd/sqrt(n) =  sd/sqrt(length(some_variable_name)\nggplot(sum_dat, aes(x = Condition, y = mean, fill = Condition))+\n  stat_summary(fun = \"mean\", geom = \"bar\", show.legend = FALSE)+\n  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.25)+\n  scale_y_continuous(limits = c(0,7), \n                     breaks = c(0,1,2,3,4,5,6,7), \n                     name = \"Intrusive-Memory Frequency (Mean for the Week\")+\n  scale_x_discrete(labels = c(\"No-task control\", \"Reactivation plus Tetris\", \"Tetris only\", \"Reactivation only\"))"},{"path":"one-way-anova.html","id":"anova-a6","chapter":"12 One-way ANOVA","heading":"12.6 Activity 6: One-way ANOVA","text":"Now can run one-way ANOVA using aov_ez afex package save object mod. well running ANOVA, aov_ez function also conducts Levene's test homogeneity variance can test final assumption.aov_ez() likely produce messages look like errors, worry , just letting know done.Copy paste code run view results ANOVA using anova(mod).Just like t-tests correlations, can use tidy() make output easier work .Run code transform output. worry warning message, just telling know automatically rename columns keep original names.term = independent variablenum.Df = degrees freedom effectden.Df = degrees freedom residualsMSE = Mean-squared errorsstatistic = F-statisticges = effect sizep.value = p.valueYou refer lecture textbooks information variable means calculated.overall effect Condition significant? YesNoWhat F-statistics 2 decimal places? According rules thumb, effect size SmallMediumLarge\nquite effect sizes around used research.\nlearned Cohen’s D previous chapter using\npartial eta-squared. Psychometrica\nresource can help transforming one effect size another. also\nprovides nice table (section 16) helps interpreting\ndifferent effect sizes. Take look!\n","code":"\nmod <- aov_ez(id = \"subject\", # the column containing the subject IDs\n              dv = \"intrusions\", # the dependent variable\n              between = \"Condition\", # the between-subject variable\n              es = \"pes\", # sets effect size to partial eta-squared\n              type = 3, # this affects how the sum of squares is calculated, set this to 3\n              include_aov = TRUE,\n              data = dat2)\nanova(mod)\nmod_output <- (mod$anova_table) %>% tidy()## Warning in tidy.anova(.): The following column names in ANOVA output were not\n## recognized or transformed: num.Df, den.Df, MSE, ges"},{"path":"one-way-anova.html","id":"anova-a7","chapter":"12 One-way ANOVA","heading":"12.7 Activity 7: Assumption checking","text":"may wondering yet checked assumptions. Well, unlike t-tests correlations, order test assumptions need use model created aov_ez(), assess point. one-way independent ANOVA, assumptions Student's t-test:DV interval ratio dataThe observations independentThe residuals normally distributedThere homogeneity variance groupsWe know 1 2 met design study. test 3, done can look QQ-plot residuals test normality Shapiro-Wilk test. residuals stored one components mod. access specify mod$aov$residuals.\nFigure 12.3: qq-plot model residuals\nthings note assumption test results. First, look p-value Shapiro-Wilk test - 4.252e-06. Whenever see e end number means R using scientific notation. Scientific notation way writing large small numbers. number e negative means number divided 10 power six. Put simply, move decimal place six places left get standard number. reporting p-values results section, use scientific notation, instead round 3 decimal places.value 4.252e-06? .00425242.52.000004252If want R round make easier read, use code save object, tidy round p.value. Just remember APA style never write \"p = 0\", instead, write \"p < .001\" (p never equal actual zero, can just , , small).second thing note qq-plot Shapiro-Wilk test clear assumption normality met. problem? Well, Field et al. (2009) say sample sizes group equal ANOVA robust violations normality homogeneity variance. also good discussion bit technical. can check many participants condition using count():Thankfully sample sizes equal OK proceed ANOVA. clear whether normality checked original paper.last assumption, can test homogeneity variance Levene's test function test_levene() afex. code simple, just need supply ANOVA model created earlier mod.results Levene's test show assumption homogeneity variance also met. paper indicate might case specifies ANOVAs assume equal variance, however, results ANOVA reported identical results correction made although post-hoc tests Welch tests (can tell degrees freedom adjusted whole numbers).Whilst might seem confusing - imagine might wondering point assumption testing given seems ignored - showing three reasons:reassure sometimes data can fail meet assumptions still OK use test. put statistical terms, many tests robust mild deviations normality unequal variance, particularly equal sample sizes.reassure sometimes data can fail meet assumptions still OK use test. put statistical terms, many tests robust mild deviations normality unequal variance, particularly equal sample sizes.critical thinking point, remind just piece research published mean perfect always evaluate whether methods used appropriate.critical thinking point, remind just piece research published mean perfect always evaluate whether methods used appropriate.reinforce importance pre-registration decisions made advance, /open data code analyses can reproduced exactly avoid ambiguity exactly done. example, given equal sample sizes difference variance groups extreme, looks like still appropriate use ANOVA decisions justification decisions transparent.reinforce importance pre-registration decisions made advance, /open data code analyses can reproduced exactly avoid ambiguity exactly done. example, given equal sample sizes difference variance groups extreme, looks like still appropriate use ANOVA decisions justification decisions transparent.##Activity 8: Post-hoc tests {#anova-a8}post-hoc comparisons, mentioned, paper appears computed Welch t-tests, mention multiple comparison correction. reproduce results using t.test contrasts.example, compare condition 1 (control group) condition 2 (reactivation plus tetris group) run:\nCondition four levels, can’t just specify\nintrusions ~ Condition t-test compares two groups\nwouldn’t know four compare first \nfilter data use new function droplevels(). ’s\nimportant remember comes R two things \nconsider, data can see underlying structure \ndata. code use filter() select \nconditions 1 2 can compare . However, doesn’t\nchange fact R “knows” Condition four\nlevels - doesn’t matter two levels don’t \nobservations , underlying structure still says \nfour groups. droplevels() tells R remove unused\nlevels factor. Try running code without\ndroplevels() see happens.\nHowever, quicker better way allows apply correction multiple comparisons easily use emmeans() computes possible pairwise comparison t-tests applies correction p-value.First, use emmeans() run comparisons can pull contrasts use tidy() make easier work .Run code . conditions significantly different ? comparisons different ones reported paper now correction multiple comparisons applied?\ninquisitive amongst may noticed mod \nlist 5 seemingly contains thing three times:\nanova_table, aov Anova. \nreasons behind differences complex go detail \ncourse (see \ninfo) simple version anova_table \nAnovause one method calculating results (type 3 sum\nsquares) aov uses different method (type 1 sum \nsquares). ’s important purposes need use\nanova_table view overall results (replicate \nresults papers) aovto run follow-tests \nget access residuals (lm() factorial\nANOVA). always, precision attention detail key.\n","code":"\nqqPlot(mod$aov$residuals)\nshapiro.test(mod$aov$residuals)## [1] 11 60\n## \n##  Shapiro-Wilk normality test\n## \n## data:  mod$aov$residuals\n## W = 0.87739, p-value = 4.252e-06\nshapiro <- shapiro.test(mod$aov$residuals) %>% #run the test\n  tidy() %>% # tidy the output\n  mutate(p.value = round(p.value, digits = 3)) # overwrite the p-value with one rounded to 3 decimal places\ndat2 %>% count(Condition)\ntest_levene(mod)## Warning: Functionality has moved to the 'performance' package.\n## Calling 'performance::check_homogeneity()'.## Warning: Variances differ between groups (Levene's Test, p = 0.039).\ndat2 %>%\n  filter(Condition %in% c(\"1\", \"2\")) %>%\n  droplevels() %>% \n  t.test(intrusions ~ Condition, data = .)\nmod_pairwise <-emmeans(mod, pairwise ~ Condition, adjust = \"bonferroni\")\nmod_contrasts <- mod_pairwise$contrasts %>% tidy()"},{"path":"one-way-anova.html","id":"anova-a9","chapter":"12 One-way ANOVA","heading":"12.8 Activity 9: Power and effect size","text":"Finally, can replicate power analysis using pwr.anova.test.basis effect size d = 1.14 Experiment 1, assumed large effect size f = 0.4. sample size 18 per condition required order ensure 80% power detect difference 5% significance level.already got effect size overall ANOVA, however, also really calculate Cohen's D using cohensD lsr pairwise comparisons. code little complicated need separately comparison, bind together add mod_contrasts - just make sure understand bits code need change run different data.\noptions data don’t meet assumptions ’s\nreally appropriate continue regular one-way ANOVA? \nalways, multiple options judgement call.\n1. run non-parametric test, Kruskal-Wallis \n-subject designs Friedman test within-subject\ndesigns. 2. normality problem, try transforming \ndata. Field et al. (2009) good section data transformation. 3.\nuse bootstrapping, something cover \ncourse . , Field et al. (2009) covers although \nlittle complicated.\n","code":"\npwr.anova.test(k = 4, f = .4, sig.level = .05, power = .8)## \n##      Balanced one-way analysis of variance power calculation \n## \n##               k = 4\n##               n = 18.04262\n##               f = 0.4\n##       sig.level = 0.05\n##           power = 0.8\n## \n## NOTE: n is number in each group\nd_1_2 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(1,2)) %>% \n                   droplevels())\nd_1_3 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(1,3)) %>%\n                   droplevels()) \nd_1_4 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(1,4)) %>%\n                   droplevels())\nd_2_3 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(2,3)) %>% \n                   droplevels())\nd_2_4 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(2,4)) %>% \n                   droplevels())\nd_3_4 <- cohensD(intrusions ~ Condition, \n                 data = filter(dat2, Condition %in% c(3,4)) %>%\n                   droplevels())\npairwise_ds <- c(d_1_2,d_1_3,d_1_4,d_2_3,d_2_4,d_3_4)\nmod_contrasts <- mod_contrasts %>%\n  mutate(eff_size = pairwise_ds)"},{"path":"one-way-anova.html","id":"anova-a10","chapter":"12 One-way ANOVA","heading":"12.9 Activity 10: Write-up","text":"code replicates write-paper, although changed Welch t-test pairwise comparisons emmeans.Second, critically, 7-day diary postintervention, significant difference groups overall intrusion frequency daily life, F(3, 68) = 3.79, p = 0.014, ηp2 = 0.14. Planned comparisons demonstrated relative -task control group, reactivation-plus-Tetris group, t(68) = 3.04, p = 0.02, d = 1, experienced significantly fewer intrusive memories; finding replicated Experiment 1. Critically, predicted reconsolidation theory, reactivation-plus-Tetris group significantly fewer intrusive memories Tetris-group, t(68) = -1.89, p = 0.38, d = 0.84, well reactivation-group, t(68) = -2.78, p = 0.04, d = 1.11. , significant differences -task control group reactivation-group, t(68) = 0.26, p = 1, -task control group Tetris-group, t(68) = 1.15, p = 1","code":"Second, and critically, for the 7-day diary postintervention, there was a significant difference between groups in overall intrusion frequency in daily life, F(`r mod_output$num.Df`, `r mod_output$den.Df`) = `r mod_output$statistic %>% round(2)`, p = `r mod_output$p.value %>% round(3)`, ηp2 = .`r mod_output$ges %>% round(2)`. Planned comparisons demonstrated that relative to the no-task control group, only those in the reactivation-plus-Tetris group, t(`r mod_contrasts$df[1]`) = `r mod_contrasts$statistic[1] %>% round(2)`, p = `r mod_contrasts$adj.p.value[1] %>% round(2)`, d = `r mod_contrasts$eff_size[1] %>% round(2)`, experienced significantly fewer intrusive memories; this finding replicated Experiment 1. The reactivation-plus-Tetris group had significantly fewer intrusive thoughts than the reactivation-only group, t(`r mod_contrasts$df[5]`) = `r mod_contrasts$statistic[5] %>% round(2)`, p = `r mod_contrasts$adj.p.value[5] %>% round(2)`, d = `r mod_contrasts$eff_size[5] %>% round(2)`. Further, there were no significant differences between the reactivation-plus-Tetris group and the Tetris-only group, t(`r mod_contrasts$df[4]`) = `r mod_contrasts$statistic[4] %>% round(2)`, p = `r mod_contrasts$adj.p.value[4] %>% round(2)`, d = `r mod_contrasts$eff_size[4] %>% round(2)`, the no-task control group and the reactivation-only group, t(`r mod_contrasts$df[3]`) = `r mod_contrasts$statistic[3] %>% round(2)`, p = `r mod_contrasts$adj.p.value[3] %>% round(2)`, or between the no-task control group and the Tetris-only group, t(`r mod_contrasts$df[2]`) = `r mod_contrasts$statistic[2] %>% round(2)`, p = `r mod_contrasts$adj.p.value[2] %>% round(2)`"},{"path":"one-way-anova.html","id":"anova-sols","chapter":"12 One-way ANOVA","heading":"12.10 Solutions to Activities","text":"line find solutions tasks. look giving tasks good try !","code":""},{"path":"one-way-anova.html","id":"anova-1sol","chapter":"12 One-way ANOVA","heading":"12.10.1 Activity 1","text":"** Click tab see solution **","code":"library(\"pwr\")\nlibrary(\"lsr\")\nlibrary(\"car\")\nlibrary(\"broom\")\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibr\\ry(\"performance\")\nlibrary(\"tidyverse\")\ndat <- read_csv(\"James Holmes_Expt 2_DATA.csv\")%>%\n  mutate(subject = row_number())"},{"path":"one-way-anova.html","id":"anova-a2sol","chapter":"12 One-way ANOVA","heading":"12.10.2 Activity 2","text":"** Click tab see solution **","code":"\ndat2 <- dat%>%\n  select(subject,Condition,Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary)%>%\n  rename(intrusions = Days_One_to_Seven_Image_Based_Intrusions_in_Intrusion_Diary)"},{"path":"one-way-anova.html","id":"anova-a4sol","chapter":"12 One-way ANOVA","heading":"12.10.3 Activity 4","text":"** Click tab see solution **","code":"\nsum_dat<-dat2%>%\n  group_by(Condition)%>%\n  summarise(mean = mean(intrusions),\n            sd = sd(intrusions),\n            se = sd/sqrt(length(intrusions)))"},{"path":"one-way-anova.html","id":"anova-a5sol","chapter":"12 One-way ANOVA","heading":"12.10.4 Activity 5","text":"** Click tab see solution **","code":"\nggplot(dat2, aes(x = Condition, y = intrusions))+\n  geom_violin(trim = FALSE)+\n  geom_boxplot(width = .2)+\n  scale_x_discrete(labels = c(\"No-task control\", \"Reactivation plus Tetris\", \"Tetris only\", \"Reactivation only\"))"},{"path":"factorial-anova.html","id":"factorial-anova","chapter":"13 Factorial ANOVA","heading":"13 Factorial ANOVA","text":"chapter, continuing previous chapter ANOVA going look example Factorial ANOVA. learn interpreting lectures, now, just focus code.going reproduce analysis Experiment 3 Zhang, T., Kim, T., Brooks, . W., Gino, F., & Norton, M. . (2014). \"present\" future: unexpected value rediscovery. Psychological Science, 25, 1851-1860..help understand data working , abstract:Although documenting everyday activities may seem trivial, four studies reveal creating records present generates unexpected benefits allowing future rediscoveries. Study 1, used time-capsule paradigm show individuals underestimate extent rediscovering experiences past curiosity provoking interesting future. Studies 2 3, found people particularly likely underestimate pleasure rediscovering ordinary, mundane experiences, opposed extraordinary experiences. Finally, Study 4 demonstrates underestimating pleasure rediscovery leads time-inconsistent choices: Individuals forgo opportunities document present prefer rediscovering moments future engaging alternative fun activity. Underestimating value rediscovery linked people’s erroneous faith memory everyday events. documenting present, people provide opportunity rediscover mundane moments may otherwise forgotten.experiment 2 x 2 mixed design:first IV time (time1, time2) within-subjectsThe second IV type event (ordinary vs. extraordinary) -subjects factor * DV use interest","code":""},{"path":"factorial-anova.html","id":"factorial-a1","chapter":"13 Factorial ANOVA","heading":"13.1 Activity 1: Set-up","text":"Open R Studio set working directory chapter folder. Ensure environment clear.Open new R Markdown document save working directory. Call file \"Factorial ANOVA\".Download Zhang et al. 2014 Exp 3.csv extract files folder.server, avoid number issues restarting session - click Session - Restart RIf working computer, install package rcompanion. Remember install packages university computers, already installed.Type run code loads pwr, rcompanion, lsr, car, broom, afex, emmeans tidyverse using library() function.Run code load data wrangle format need. need write code , make sure can understand line - good way code uses pipes (%>%) highlight run line progressively can see builds . Line--line code:Reads data fileSelect three columns needAdds column subject IDsTidies dataRecodes values Condition numeric text labelsRecodes values time easier read/write","code":"\nfactorial <- read_csv(\"Zhang et al. 2014 Exp 3.csv\")%>%\n  select(Condition, T1_Predicted_Interest_Composite, T2_Actual_Interest_Composite)%>%\n  mutate(subject = row_number())%>%\n  pivot_longer(names_to = \"time\",values_to = \"interest\", cols =       c(\"T1_Predicted_Interest_Composite\",\"T2_Actual_Interest_Composite\"))%>%\n  mutate(Condition = dplyr::recode(Condition, \"1\" = \"Ordinary\", \"2\" = \"Extraordinary\"))%>%\n  mutate(time = dplyr::recode(time, \"T1_Predicted_Interest_Composite\" = \"time1_interest\",\n                       \"T2_Actual_Interest_Composite\" = \"time2_interest\")) %>%\n  mutate(Condition = as.factor(Condition)) %>% \n  mutate (time= as.factor(time))"},{"path":"factorial-anova.html","id":"factorial-a2","chapter":"13 Factorial ANOVA","heading":"13.2 Activity 2: Descriptive statistics","text":"Calculate descriptive statistics (mean SD) interest Condition time (hint: need group_by() two variables) store object named sum_dat_factorial. known cells means.","code":""},{"path":"factorial-anova.html","id":"factorial-a3","chapter":"13 Factorial ANOVA","heading":"13.3 Activity 3: Violin-boxplots","text":"going produce two kinds plots visualise data. First, produce violin-boxplots can see distribution data.Write code produces violin-boxplots scores group.\nHint 1: need add second IV first call ggplot fill argument (aes(x,y,fill)).\nHint 2: need add position = position_dodge(.9) geom_boxplot get plots align.\nHint 1: need add second IV first call ggplot fill argument (aes(x,y,fill)).Hint 2: need add position = position_dodge(.9) geom_boxplot get plots align.need replicate exact colour scheme used , see can play around settings whatever colour scheme think works best.\nFigure 13.1: Violin-boxplot condition time\n","code":""},{"path":"factorial-anova.html","id":"factorial-a4","chapter":"13 Factorial ANOVA","heading":"13.4 Activity 4: Interaction plots","text":"Now going produce interaction plot makes easier see IVs interacting, requires ggplot2 functions come across yet. Rather using raw data dat_factorial, use means produced sum_dat_factorial. type plot requires two geoms, one draw points, one draw lines connect .plot reproduces plot used paper.Run code play around looks changing arguments e.g., colour, line-type, theme.\nFigure 13.2: Interaction plot\n","code":"\nggplot(sum_dat_factorial, aes(x = time, y = mean, group = Condition, shape = Condition)) +\n  geom_point(size = 3) +\n  geom_line(aes(linetype = Condition))+\n  scale_x_discrete(labels = c(\"Time 1\", \"Time 2\"))+\n  theme_classic()"},{"path":"factorial-anova.html","id":"factorial-a5","chapter":"13 Factorial ANOVA","heading":"13.5 Activity 5: ANOVA","text":"Complete code run factorial ANOVA. Remember need specify IVs one -subjects one within-subjects. Look help documentation aov_ez find .Complete code run factorial ANOVA. Remember need specify IVs one -subjects one within-subjects. Look help documentation aov_ez find .Save ANOVA model object called mod_factorialSave ANOVA model object called mod_factorialPull anova table, can either mod_factorial$anova_table anova(mod_factorial) result. Save object named factorial_output make sure used tidy().Pull anova table, can either mod_factorial$anova_table anova(mod_factorial) result. Save object named factorial_output make sure used tidy().Look results. Remember information read p-values scientific notation.main effect condition significant? YesNoIs main effect time significant? YesNoIs two-way interaction significant? YesNo","code":"\nmod_factorial <- aov_ez(id = \"NULL\",\n               data = NULL, \n               between = \"NULL\", \n               within = \"NULL\",\n               dv = \"NULL\", \n               type = 3,\n               es = \"NULL\") \nfactorial_output <- NULL"},{"path":"factorial-anova.html","id":"factorial-a6","chapter":"13 Factorial ANOVA","heading":"13.6 Activity 6: Assumption checking","text":"assumptions factorial ANOVA one-way ANOVA.DV interval ratio dataThe observations independentThe residuals normally distributedThere homogeneity variance groupsAs , know assumption 2 met design study. Assumption 1 throws interesting issue problem ordinal data. Ordinal data kind data comes Likert scales , common psychology. problem ordinal data interval ratio data, fixed number values can take (values Likert scale) claim distance values equal (difference strongly agree agree difference agree neutral?).Technically, use ANOVA analyse ordinal data - almost everyone . people argue multiple Likert scale items averaged (case study) averaged data normally distributed, problem. minority (actually correct) argue use non-parametric methods complicated tests ordinal regression type data. Whichever route choose, understand data able justify decision.test assumption 3, extract residuals model (mod_factorial$lm$residuals), create qq-plot conduct Shapiro-Wilk test.test assumption 3, extract residuals model (mod_factorial$lm$residuals), create qq-plot conduct Shapiro-Wilk test.residuals normally distributed? YesNoNo, given sample probably acceptable proceedAre residuals normally distributed? YesNoNo, given sample probably acceptable proceedFor final assumption, can use test_levene() test homogeneity variance.Conduct Levene's test. assumption 4 met? YesNo","code":""},{"path":"factorial-anova.html","id":"factorial-a7","chapter":"13 Factorial ANOVA","heading":"13.7 Activity 7: Post-hoc tests","text":"interaction significant, follow post-hoc tests using emmeans() determine comparisons significant. overall interaction significant, conduct additional tests.emmeans() requires specify aov object, factors want contrast. interaction, use notation pairwise ~ IV1 | IV2 specify multiple comparison correction want apply. Finally, can use tidy() tidy output contrasts save tibble.Run code view results.Note two factors, also reverse order IVs. , get results contrasting time 1 time 2 condition. Instead, look difference ordinary extraordinary events time point.Run code look output contrast_factorial contrasts_factorial2 carefully making sure understand interpret results. find useful refer interaction plot made earlier.main effects (condition time) two levels, need post-hoc tests determine conditions differ , however, one factors three levels use emmeans() calculate contrast main effects, like one-way ANOVA.Finally, calculate effect size pairwise comparisons need individually using 'cohensD()fromlsr.  * Run code add effect sizes tocontrasts_factorialandcontrasts_factorial2`.","code":"\n# run the tests\nposthoc_factorial <- emmeans(mod_factorial, \n                             pairwise ~ time| Condition, \n                             adjust = \"bonferroni\")\n# tidy up the output of the tests\ncontrasts_factorial <- posthoc_factorial$contrasts %>%\n  tidy()\nposthoc_factorial2 <- emmeans(mod_factorial, \n                             pairwise ~ Condition| time, \n                             adjust = \"bonferroni\") \ncontrasts_factorial2 <- posthoc_factorial2$contrasts %>%\n  tidy()\nd_extra_t1_t2 <- cohensD(interest ~ time, \n                         data = (filter(factorial, Condition == \"Extraordinary\") %>% droplevels())) \nd_ord_t1_t2 <- cohensD(interest ~ time, \n                         data = (filter(factorial, Condition == \"Ordinary\") %>% droplevels())) \nCondition_ds <- c(d_extra_t1_t2, d_ord_t1_t2)\ncontrasts_factorial <- contrasts_factorial %>%\n  mutate(eff_size = Condition_ds)\nd_time1_extra_ord <- cohensD(interest ~ Condition, \n                         data = (filter(factorial, time == \"time1_interest\") %>% droplevels())) \nd_time2_extra_ord <- cohensD(interest ~ Condition, \n                         data = (filter(factorial, time == \"time2_interest\") %>% droplevels()))\ntime_ds <- c(d_time1_extra_ord, d_time2_extra_ord)\ncontrasts_factorial2 <- contrasts_factorial2 %>%\n  mutate(eff_size = time_ds)"},{"path":"factorial-anova.html","id":"factorial-a8","chapter":"13 Factorial ANOVA","heading":"13.8 Activity 8: Write-up","text":"p-values < .001 entered manually. way get R produce formatting overly complicated purposes. want push , look papaja package.values partial eta-squared match analysis reported paper. figured yet - know, please get touch!replaced simple effects main paper pairwise comparisons.First need calculate descriptives main effect time earlier.Copy paste white-space.conducted repeated measures ANOVA interest dependent measure found main effect time, F(1, 128) = 25.88, p < .001, ηp2 = 0.044; anticipated interest Time 1 (M = 4.2), SD = 1.12)) lower actual interest Time 2 (M = 4.69, SD = 1.19).also observed interaction time type experience, F(1, 128) = 4.445, p = 0.04, ηp2 = 0.008. Pairwise comparisons revealed ordinary events, predicted interest Time 1 (M = 4.04, SD = 1.09) lower experienced interest Time 2 (M = 4.73, SD = 1.24), t(128) = -5.05, p < .001, d = 0.59. Although predicted interest extraordinary events Time 1 (M = 4.36, SD = 1.13) lower experienced interest Time 2 (M = 4.65, SD = 1.14), t(128) = -2.12, p < .001, d = 0.25 , magnitude underestimation smaller ordinary events.","code":"\ntime_descrip <- factorial %>% \n  group_by(time) %>%\n  summarise(mean_interest = mean(interest, na.rm = TRUE),\n            sd_interest = sd(interest, na.rm = TRUE),\n            min = mean(interest) - qnorm(0.975)*sd(interest)/sqrt(n()),\n            max = mean(interest) + qnorm(0.975)*sd(interest)/sqrt(n()))We conducted the same repeated measures ANOVA with interest as the dependent measure and again found a main effect of time, F(`r factorial_output$num.Df[2]`, `r factorial_output$den.Df[2]`) = `r factorial_output$statistic[2] %>% round(2)`, p < .001, ηp2 = `r factorial_output$ges[2] %>% round(3)`; anticipated interest at Time 1 (M = `r time_descrip$mean_interest[1] %>% round(2)`), SD = `r time_descrip$sd_interest[1]%>% round(2)`)) was lower than actual interest at Time 2 (M = `r time_descrip$mean_interest[2]%>% round(2)`, SD = `r time_descrip$sd_interest[2]%>% round(2)`).We also observed an interaction between time and type of experience, F(`r factorial_output$num.Df[3]`, `r factorial_output$den.Df[3]`) = `r factorial_output$statistic[3] %>% round(3)`, p = `r factorial_output$p.value[3] %>% round(2)`, ηp2 = `r factorial_output$ges[3] %>% round(3)`. Pairwise comparisons revealed that for ordinary events, predicted interest at Time 1 (M = `r sum_dat_factorial$mean[3]%>% round(2)`, SD = `r sum_dat_factorial$sd[3]%>% round(2)`) was lower than experienced interest at Time 2 (M = `r sum_dat_factorial$mean[4]%>% round(2)`, SD = `r sum_dat_factorial$sd[4]%>% round(2)`), t(`r contrasts_factorial$df[2]%>% round(2)`) = `r contrasts_factorial$statistic[2]%>% round(2)`, p < .001, d = `r contrasts_factorial$eff_size[2]%>% round(2)`. Although predicted interest for extraordinary events at Time 1 (M = `r sum_dat_factorial$mean[1]%>% round(2)`, SD = `r sum_dat_factorial$sd[1]%>% round(2)`) was lower than experienced interest at Time 2 (M = `r sum_dat_factorial$mean[2]%>% round(2)`, SD = `r sum_dat_factorial$sd[2]%>% round(2)`), t(`r contrasts_factorial$df[1]%>% round(2)`) = `r contrasts_factorial$statistic[1]%>% round(2)`, p < .001, d = `r contrasts_factorial$eff_size[1]%>% round(2)` , the magnitude of underestimation was smaller than for ordinary events."},{"path":"factorial-anova.html","id":"factorial-a9","chapter":"13 Factorial ANOVA","heading":"13.9 Activity 9: Transforming data","text":"chapter decided violation assumption normality ok replicate results paper. happy violation extreme? One option deal normality transform data.various options can transform data going use Tukeys Ladder Powers transformation. finds power transformation makes data fit normal distribution closely possible type transformation.Run code. use mutate() add new variable data-set, interest_tukey going transformed DV. function transformTukey() rcompanion package. Setting plotit = TRUE automatically create qqPlots histograms can immediately visualise new variable.Now transformed DV can re-run ANOVA new variable.Notice changed pattern ANOVA results, p-values main effects interactions slightly different overall conclusions remain . likely violations normality quite mild large sample size, however, transformation can confident results may always case transformed ANOVA violations extreme.","code":"\nfactorial <- factorial %>%\n  mutate(interest_tukey = transformTukey(interest, plotit=TRUE))\ntukey_factorial <- aov_ez(id = \"subject\",\n               data = factorial, \n               between = \"Condition\", \n               within = \"time\",\n               dv = \"interest_tukey\", \n               type = 3)\ntukey_factorial"},{"path":"factorial-anova.html","id":"practice-your-skills-14","chapter":"13 Factorial ANOVA","heading":"13.10 Practice Your Skills","text":"Two-Factor ANOVA: Perspective-Taking Language ComprehensionIn order complete exercise, download .Rmd file need edit titled GUID_Level2B_PracticeSkills.Rmd. can downloaded within zip file link. downloaded unzipped create new folder use working directory; put .Rmd file folder set working directory folder drop-menus top. Download Practice Skills Exercise .zip file .Background: Perspective-Taking Language ComprehensionFor exercise, looking real data Experiment 2 Keysar, Lin, Barr (2003), \"Limits Theory Mind Use Adults\", Cognition, 89, 29--41. study used eye-tracking investigate people's ability take another's perspective communication game. (data analysing, real, appear original report.)communication game participants played follows: participant sat table opposite confederate participant (person lab pretended naive participant). two participants upright set shelves (see figure ). participants played game real participant assigned role \"matcher\" confederate role \"director\". director given picture goal state grid, showing objects needed arranged. However, director allowed touch objects. get objects proper places, director needed give instructions matcher move objects. example, director might say, \"take red box top corner move bottom row,\" matcher perform action. matcher's eye movements tracked listened interpreted director's instructions.\nFigure 13.3: Director-Matcher Viewpoints Keysar, Lin, Barr (2003)\ninvestigate perspective taking, instructions given director actually scripted beforehand order create certain ambiguities. objects grid, red box, mutually visible participants (.e., visible sides grid). However, objects, like brush green candle, occluded director's view; matcher see , reason believe director knew contents occluded squares, thus reason expect ever refer . However, sometimes director refer mutually visible object using description also happened match one hidden objects. instance, director might instruct matcher \"pick small candle.\" Note director, small candle purple candle. given matcher see grid one two conditions: Experimental condition, matcher saw additional green candle hidden box even smaller purple candle (see middle panel figure). object called \"competitor\" matched description intended referent (purple candle). Baseline condition, green candle replaced object match director's description, apple. ambiguous situations provided main data experiment, total eight different grids experiment presented analogous situations example .previous eye-tracking study authors found presence competitors severely confused matchers, suggesting people surprisingly egocentric---found hard ignore \"privileged\" information interpreting another person's speech. example, director said \"pick small candle,\" spent far time looking hidden green candle hidden apple, even though neither one objects, hidden, viable referent. refer difference looking time 'egocentric interference effect'.Experiment 2 Keysar, Lin, Barr aimed follow finding. previous article, matcher reason believe director merely ignorant identity hidden objects. happen matcher given reason believe director actually false belief hidden object? example, matcher experience less egocentric interference reason think director thought hidden candle toy truck?test , half participants randomly assigned false belief condition, matcher led believe director false belief identity hidden object; half participated ignorance condition, previous experiments, led believe director simply know hidden squares.40 participants experiment, 20 false belief condition, 20 ignorance condition. also equal number male female participants study. spoil plot bit, Keysar, Lin Barr find effect condition looking time. However, consider sex potential moderating variable. Thus, explore effects ignorance vs. false belief egocentric interference, broken sex matcher.starting lets check:.csv file saved folder computer manually set folder working directory..csv file saved folder computer manually set folder working directory..Rmd file saved folder .csv files. Save format GUID_Level2B_PracticeSkills.Rmd GUID replaced GUID..Rmd file saved folder .csv files. Save format GUID_Level2B_PracticeSkills.Rmd GUID replaced GUID.","code":""},{"path":"factorial-anova.html","id":"Ch13AssignQueT1A","chapter":"13 Factorial ANOVA","heading":"13.10.1 Task 1A: Libraries","text":"today's exercise need tidyverse afex packages. Enter code t1A code chunk load libraries.","code":"\n# load in the packages"},{"path":"factorial-anova.html","id":"Ch13AssignQueT1B","chapter":"13 Factorial ANOVA","heading":"13.10.2 Task 1B: Loading in the data","text":"Use read_csv() replace NULL t1B code chunk load data stored datafile keysar_lin_barr_2003.csv. Store data variable dat.Take look data (dat) console using glimpse() View(), just display typing name. see following columns:simplified things original experiment collapsing baseline vs. experimental conditions single DV. DV, egocentric interference, average difference looking time participant (milliseconds per trial) hidden competitors (e.g., small candle) versus hidden non-competitors (e.g., apple). larger number, egocentric interference participant experienced.","code":"\ndat <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT2","chapter":"13 Factorial ANOVA","heading":"13.10.3 Task 2: Calculate cell means","text":"Today going focus just main analysis write-, assumptions, always check assumptions hold justify decisions.One elements need write-descriptives. want start creating summary statistics four conditions. Remember, two factors (sex condition) 2 levels (sex: female vs. male; condition: false belief vs. ignorance) give four conditions, summary table, four cells created factorially combining sex condition.Replace NULL t2 code chunk create four cells created factorially combining sex condition, calculating mean standard deviation cell.\nStore descriptives tibble called cell_means\nCall column mean m column standard deviation sd.\ntable four rows four columns shown values replacing XXs\nFollow case spelling exactly.\nStore descriptives tibble called cell_meansCall column mean m column standard deviation sd.table four rows four columns shown values replacing XXsFollow case spelling exactly.","code":"\ncell_means <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT3","chapter":"13 Factorial ANOVA","heading":"13.10.4 Task 3: Marginal means for sex","text":"also need descriptives just look means given factor; marginal means - means levels one factor regardless factor.Replace NULL t3 code chunk calculate marginal means standard deviations factor sex.\nStore descriptives tibble marg_sex\nCall column mean m column standard deviation sd.\ntable two rows three columns shown values replacing XXs\nFollow case spelling exactly.\nStore descriptives tibble marg_sexCall column mean m column standard deviation sd.table two rows three columns shown values replacing XXsFollow case spelling exactly.","code":"\nmarg_sex <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT4","chapter":"13 Factorial ANOVA","heading":"13.10.5 Task 4: Marginal means for condition","text":"now condition.Replace NULL t4 code chunk calculate marginal means standard deviations factor, condition\nStore descriptives tibble marg_cond\nCall column mean m column standard deviation sd.\ntable two rows three columns shown values replacing XXs\nFollow case spelling exactly.\nStore descriptives tibble marg_condCall column mean m column standard deviation sd.table two rows three columns shown values replacing XXsFollow case spelling exactly.","code":"\nmarg_cond <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT5","chapter":"13 Factorial ANOVA","heading":"13.10.6 Task 5: Interaction plot","text":"finally going need plot. two factors, want show factors plot give reader much information possible save figure space. best way sort interaction plot. really lot easier looks requires think setting aes different conditions.Insert code t5 code chunk replicate figure shown .\nPay particular attention labels, axes dimensions, color background.\nNote figure must appear code knitted.\nPay particular attention labels, axes dimensions, color background.Note figure must appear code knitted.Note: figure nice figure really error bars including actual paper. Including error bars may help clarifying descriptive statistics see , although means different, huge overlap terms error bars may indicate overall effect.\nFigure 13.4: Replicate Figure\n","code":"\n# to do: something with ggplot to replicate the figure## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument."},{"path":"factorial-anova.html","id":"Ch13AssignQueT6","chapter":"13 Factorial ANOVA","heading":"13.10.7 Task 6: Recap Question 1","text":"Thinking information, one statements acceptable hypothesis interaction effect sex condition, one:t6 code chunk , replace NULL number statement best summarises analysis. Store single value answer_t6We hypothesised significant difference males females egocentric interference (mean looking time (msecs)) regardless condition.hypothesised significant difference participants false belief condition ignorance condition terms egocentric interference (mean looking time (msecs)) regardless sex participant.hypothesised significant interaction condition sex participant egocentric interference (mean looking time (msecs))hypothesised significant difference males females egocentric interference (mean looking time (msecs)) regardless condition significant difference participants false belief condition ignorance condition terms egocentric interference (mean looking time (msecs)) regardless sex participant.","code":"\nanswer_t6 <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT7","chapter":"13 Factorial ANOVA","heading":"13.10.8 Task 7: Recap Question 2","text":"Thinking information, one statements good description marginal means sex, one:t7 code chunk , replace NULL number statement best summarises analysis. Store single value answer_t7The female participants average longer looking time (M = 777.98, SD = 911.53) male participants (M = 555.04, SD = 707.81) may suggest significant main effect sex.female participants average shorter looking time (M = 777.98, SD = 911.53) male participants (M = 555.04, SD = 707.81) may suggest significant main effect condition.female participants average shorter looking time (M = 777.98, SD = 911.53) male participants (M = 555.04, SD = 707.81) may suggest significant main effect sex.female participants average longer looking time (M = 777.98, SD = 911.53) male participants (M = 555.04, SD = 707.81) may suggest significant main effect condition.","code":"\nanswer_t7 <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT8","chapter":"13 Factorial ANOVA","heading":"13.10.9 Task 8: Recap Question 3","text":"Thinking information, one statements good description marginal means condition, one:t8 code chunk , replace NULL number statement best summarises analysis. Store single value answer_t8The participants false belief group average longer looking time (M = 549.58, SD = 775.91) participants ignorance group (M = 749.58, SD = 861.23), may suggest significant main effect condition.participants false belief group average shorter looking time (M = 549.58, SD = 775.91) participants ignorance group (M = 749.58, SD = 861.23), may suggest significant main effect condition.participants false belief group average longer looking time (M = 549.58, SD = 775.91) participants ignorance group (M = 749.58, SD = 861.23), may suggest significant main effect sex.participants false belief group average shorter looking time (M = 549.58, SD = 775.91) participants ignorance group (M = 749.58, SD = 861.23), may suggest significant main effect sex.","code":"\nanswer_t8 <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT9","chapter":"13 Factorial ANOVA","heading":"13.10.10 Task 9: Running the Factorial ANOVA","text":"Great, looked descriptives thought effects might . need now run ANOVA using aov_ez() function. ANOVA going run two-way -subjects ANOVA conditions -subjects variables. may need refer back chapter look help aov_ez() see add second variable/factor.Replace NULL t9 code chunk run two-way -subjects ANOVA.\nLook chapter help guidance. need data, DV, two -subjects conditions, participant id.\nSet type type = 3\ntidy() output. nothing output store variable named mod (note: technically store list).\nsee red output code convert conditions factors automatically, set contrasts. fine.\nLook chapter help guidance. need data, DV, two -subjects conditions, participant id.Set type type = 3Do tidy() output. nothing output store variable named mod (note: technically store list).see red output code convert conditions factors automatically, set contrasts. fine.","code":"\nmod <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT10","chapter":"13 Factorial ANOVA","heading":"13.10.11 Task 10: Interpreting the ANOVA output Question","text":"Thinking information, one statements good summary outcome ANOVA, one:t10 code chunk , replace NULL number statement best summarises analysis. Store single value answer_t10There significant main effect sex, main effect condition interaction condition sex.significant main effect condition, main effect sex interaction condition sex.significant main effect sex condition significant interaction condition sex.significant main effect sex, significant main effect condition, significant interaction condition sex.","code":"\nanswer_t10 <- NULL"},{"path":"factorial-anova.html","id":"Ch13AssignQueT11","chapter":"13 Factorial ANOVA","heading":"13.10.12 Task 11: Report your results","text":"Write paragraph reporting findings.Finished!Well done! Check answers solutions .","code":""},{"path":"factorial-anova.html","id":"factorial-sols","chapter":"13 Factorial ANOVA","heading":"13.11 Solutions to Activities","text":"","code":""},{"path":"factorial-anova.html","id":"chapter-activities-1-9","chapter":"13 Factorial ANOVA","heading":"13.11.1 Chapter Activities 1-9","text":"","code":""},{"path":"factorial-anova.html","id":"factorial-a1sol","chapter":"13 Factorial ANOVA","heading":"13.11.1.1 Activity 1","text":"** Click tab see solution **","code":"\nlibrary(\"pwr\")\nlibrary(\"rcompanion\")\nlibrary(\"car\")\nlibrary(\"lsr\")\nlibrary(\"broom\")\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")"},{"path":"factorial-anova.html","id":"factorial-a2sol","chapter":"13 Factorial ANOVA","heading":"13.11.1.2 Activity 2","text":"** Click tab see solution **","code":"\nsum_dat_factorial<-factorial%>%\n  group_by(Condition, time)%>%\n  summarise(mean = mean(interest, na.rm = TRUE),\n            sd = sd(interest, na.rm = TRUE)\n            )"},{"path":"factorial-anova.html","id":"factorial-a3sol","chapter":"13 Factorial ANOVA","heading":"13.11.1.3 Activity 3","text":"** Click tab see solution **","code":"\nggplot(factorial, \n       aes(x = time , y = interest, fill = Condition))+\n  geom_violin(trim = FALSE, \n              alpha = .4)+\n  geom_boxplot(position = position_dodge(.9), \n               width = .2, \n               alpha = .6)+\n  scale_x_discrete(labels = c(\"Time 1\", \"Time 2\"))+\n  scale_fill_viridis_d(option = \"E\")+\n  stat_summary(fun = \"mean\", geom = \"point\",\n               position = position_dodge(width = 0.9)) +\n  stat_summary(fun.data = \"mean_se\", geom = \"errorbar\", width = .1,\n               position = position_dodge(width = 0.9)) +\n  theme_minimal()"},{"path":"factorial-anova.html","id":"factorial-a5sol","chapter":"13 Factorial ANOVA","heading":"13.11.1.4 Activity 5","text":"** Click tab see solution **","code":"\nmod_factorial <- aov_ez(id = \"subject\",\n               data = factorial, \n               between = \"Condition\", \n               within = \"time\",\n               dv = \"interest\", \n               type = 3) \nfactorial_output <- anova(mod_factorial) %>% tidy()\n# OR\nfactorial_output <- mod_factorial$anova_table %>% tidy()"},{"path":"factorial-anova.html","id":"factorial-a6sol","chapter":"13 Factorial ANOVA","heading":"13.11.1.5 Activity 6","text":"** Click tab see solution **","code":"\n# normality testing\nqqPlot(mod_factorial$lm$residuals)\nshapiro.test(mod_factorial$lm$residuals)\n# levene's test\ntest_levene(mod_factorial)"},{"path":"factorial-anova.html","id":"practice-your-skills-activities","chapter":"13 Factorial ANOVA","heading":"13.11.2 Practice Your Skills Activities","text":"","code":""},{"path":"factorial-anova.html","id":"task-1a-libraries-1","chapter":"13 Factorial ANOVA","heading":"13.11.2.1 Task 1A: Libraries","text":"** Click tab see solution **Return Task","code":"\nlibrary(afex)\nlibrary(tidyverse)"},{"path":"factorial-anova.html","id":"task-1b-loading-in-the-data-1","chapter":"13 Factorial ANOVA","heading":"13.11.2.2 Task 1B: Loading in the data","text":"** Click tab see solution **Return Task","code":"\ndat <- read_csv(\"keysar_lin_barr_2003.csv\")"},{"path":"factorial-anova.html","id":"task-2-calculate-cell-means-for-the-cell-means.","chapter":"13 Factorial ANOVA","heading":"13.11.2.3 Task 2: Calculate cell means for the cell means.","text":"** Click tab see solution **Return Task","code":"\ncell_means <- dat %>%\n  group_by(sex, condition) %>%\n  summarise(m = mean(looktime), sd = sd(looktime))## `summarise()` has grouped output by 'sex'. You can override using the `.groups`\n## argument."},{"path":"factorial-anova.html","id":"task-3-marginal-means-for-sex","chapter":"13 Factorial ANOVA","heading":"13.11.2.4 Task 3: Marginal means for sex","text":"** Click tab see solution **Return Task","code":"\nmarg_sex <- dat %>%\n  group_by(sex) %>%\n  summarise(m = mean(looktime), sd = sd(looktime))"},{"path":"factorial-anova.html","id":"task-4-marginal-means-for-condition","chapter":"13 Factorial ANOVA","heading":"13.11.2.5 Task 4: Marginal means for condition","text":"** Click tab see solution **Return Task","code":"\nmarg_cond <- dat %>%\n  group_by(condition) %>%\n  summarise(m = mean(looktime), sd = sd(looktime))"},{"path":"factorial-anova.html","id":"task-5-interaction-plot","chapter":"13 Factorial ANOVA","heading":"13.11.2.6 Task 5: Interaction plot","text":"\n(#fig:t5_sol)produced similar figure\n** Click tab see solution **Return Task","code":"\nggplot(cell_means, aes(condition, m, shape = sex, group = sex, color = sex)) +\n  geom_line() +\n  geom_point(size = 3) +\n  labs(y = \"mean looking time (msecs)\") +\n  scale_y_continuous(limits = c(0, 1000)) + \n  theme_bw()"},{"path":"factorial-anova.html","id":"task-6-recap-question-1","chapter":"13 Factorial ANOVA","heading":"13.11.2.7 Task 6: Recap Question 1","text":"want alternative, null hypothesis . , acceptable hypothesis interaction effect sex condition :hypothesised significant interaction condition sex participant egocentric interference (mean looking time (msecs)).correct answer :Return Task","code":"\nanswer_t6 <- 3"},{"path":"factorial-anova.html","id":"task-7-recap-question-2","chapter":"13 Factorial ANOVA","heading":"13.11.2.8 Task 7: Recap Question 2","text":"good description marginal means sex :female participants average longer looking time (M = 777.98, SD = 911.53) male participants (M = 555.04, SD = 707.81) may suggest significant main effect sex.correct answer :Return Task","code":"\nanswer_t7 <- 1"},{"path":"factorial-anova.html","id":"task-8-recap-question-3","chapter":"13 Factorial ANOVA","heading":"13.11.2.9 Task 8: Recap Question 3","text":"good description marginal means condition :participants false belief group average shorter looking time (M = 549.58, SD = 775.91) participants ignorance group (M = 749.58, SD = 861.23), may suggest significant main effect condition.correct answer :Return Task","code":"\nanswer_t8 <- 2"},{"path":"factorial-anova.html","id":"task-9-running-the-factorial-anova","chapter":"13 Factorial ANOVA","heading":"13.11.2.10 Task 9: Running the factorial ANOVA","text":"** Click tab see solution **Return Task","code":"\nmod <- aov_ez(data = dat,\n                   dv = \"looktime\", \n                   id = \"subject\",\n                   type = 3,\n                   between = c(\"condition\", \"sex\"))\nknitr::kable(mod$anova_table)"},{"path":"factorial-anova.html","id":"task-10-interpreting-the-anova-output-question","chapter":"13 Factorial ANOVA","heading":"13.11.2.11 Task 10: Interpreting the ANOVA output Question","text":"good summary outcome ANOVA :significant main effect sex condition significant interaction condition sex.correct answer :Return Task","code":"\nanswer_t10 <- 3"},{"path":"factorial-anova.html","id":"task-11-report-your-results","chapter":"13 Factorial ANOVA","heading":"13.11.2.12 Task 11: Report your results","text":"definitive way write paragraph, essentially findings report main effects interaction, giving appropriate F outputs, e.g. F(1, 36) = .79, p = .38, give interpretation/qualification results using means standard deviations , e.g. looking time significantly different false belief task (M = X, SD = XX) Ignorance task (M = XX, SD = XX). Something along following appropriate:two-way -subjects factorial ANOVA conducted testing main effects interaction sex (male vs. female) condition (false belief vs. ignorance) average looking time (msecs) matching task. Results revealed significant interaction (F(1, 36) = .21, p = .647) suggesting modulation condition sex participant looking task. Furthermore, significant main effect sex (F(1, 36) = .64, p = .429) suggesting male (M = 555.04, SD = 707.81) female participants (M = 777.98, SD = 911.53) perform similarly task. Finally, significant main effect condition (F(1, 36) = .79, p = .38) suggesting whether participants given false belief scenario (M = 594.58, SD = 775.91) ignorance scenario (M = 794.58, SD = 861.23) overall impact performance.Return Task","code":""},{"path":"acknowledgements.html","id":"acknowledgements","chapter":"Acknowledgements","heading":"Acknowledgements","text":"book work many people, staff students within School Psychology & Neuroscience, University Glasgow. special mention, however, go following people: Stephanie Boyle, Molly Burr, Morgan Daniel, Amalia Gomoiu, Kate Haining, Jesse Klein, Rebecca Lai, Steven McNair, Shannon McNee, Jennifer Murch, Jack Taylor, Jaimie Torrance, Ana Skolaris & Hollie Sneddon.hugely appreciate comments help creating material contained within book.special shout-students worked book pointed errors made suggestions: helped us consistently increase quality book. Thanks.","code":""},{"path":"installing-r.html","id":"installing-r","chapter":"A Installing R","heading":"A Installing R","text":"Installing R RStudio usually straightforward. sections explain helpful YouTube video .","code":""},{"path":"installing-r.html","id":"installing-base-r","chapter":"A Installing R","heading":"A.1 Installing Base R","text":"Install base R. Choose download link operating system (Linux, Mac OS X, Windows).Mac, install latest release newest R-x.x.x.pkg link (legacy version older operating system). install R, also install XQuartz able use visualisation packages.installing Windows version, choose \"base\" subdirectory click download link top page. install R, also install RTools; use \"recommended\" version highlighted near top list.using Linux, choose specific operating system follow installation instructions.","code":""},{"path":"installing-r.html","id":"installing-rstudio","chapter":"A Installing R","heading":"A.2 Installing RStudio","text":"Go rstudio.com download RStudio Desktop (Open Source License) version operating system list titled Installers Supported Platforms.","code":""},{"path":"installing-r.html","id":"rstudio-settings","chapter":"A Installing R","heading":"A.3 RStudio Settings","text":"settings fix immediately updating RStudio. Go Global Options... Tools menu (⌘,), General tab, uncheck box says Restore .RData workspace startup. keep things around workspace, things get messy, unexpected things happen. always start clear workspace. also means never want save workspace exit, set Never. thing want save scripts.may also want change appearance code. Different fonts themes can sometimes help visual difficulties dyslexia.\nFigure .1: RStudio General Appearance settings\nmay also want change settings Code tab. Foe example, Lisa prefers two spaces instead tabs code likes able see whitespace characters. matter personal preference.\nFigure .2: RStudio Code settings\n","code":""},{"path":"installing-r.html","id":"installing-latex","chapter":"A Installing R","heading":"A.4 Installing LaTeX","text":"can install LaTeX typesetting system produce PDF reports RStudio. Without additional installation, able produce reports HTML PDF. course require make PDFs. generate PDF reports, additionally need install tinytex (Xie, 2023) run following code:","code":"\ntinytex::install_tinytex()"},{"path":"updating-r-rstudio-and-packages.html","id":"updating-r-rstudio-and-packages","chapter":"B Updating R, RStudio, and packages","heading":"B Updating R, RStudio, and packages","text":"time--time, updated version R, RStudio, packages use (e.g., ggplot) become available. Remember separate, different process come different considerations. recommend updating latest version three start academic year.","code":""},{"path":"updating-r-rstudio-and-packages.html","id":"updating-rstudio","chapter":"B Updating R, RStudio, and packages","heading":"B.1 Updating RStudio","text":"RStudio easiest component update. Typically, updates RStudio affect code, instead add new features, like spell-check upgrades RStudio can . usually little downside updating RStudio easy .Click Help - Check updates\nFigure B.1: Updating RStudio\nupdate available, prompt download can install usual.","code":""},{"path":"updating-r-rstudio-and-packages.html","id":"updating-packages","chapter":"B Updating R, RStudio, and packages","heading":"B.2 Updating packages","text":"Package developers occasionally release updates packages. typically add new functions package, fix amend existing functions. aware package updates may cause previous code stop working. tend happen minor updates packages, occasionally major updates, can serious issues developer made fundamental changes code works. reason, recommend updating packages beginning academic year (semester) - assessment deadline just case!update individual package, easiest way use install.packages() function, always installs recent version package.update multiple packages, indeed packages, RStudio provides helpful tools. Click Tools - Check Package Updates. dialogue box appear can select packages wish update. aware select packages, may take time unable use R whilst process completes.\nFigure B.2: Updating packages RStudio\nOccasionally, might problem packages seemingly refuse update, , rlang vctrs cause end trouble. packages likely every explicitly load, required beneath surface R things like knit Markdown files etc.try update package get error message says something like Warning install.packages : installation package ‘vctrs’ non-zero exit status perhaps Error loadNamespace(, c(lib.loc, .libPaths()), versionCheck = vI[[]]) :  namespace 'rlang' 0.4.9 loaded, >= 0.4.10 required one solution found manually uninstall package, restart R, install package new, rather trying update existing version. installr package also useful function uninstalling packages.","code":"\ninstall.packages(\"tidyverse\")\n# Load installr\nlibrary(installr)\n\n# Uninstall the problem package\nuninstall.packages(\"package_name\")\n\n# Then restart R using session - restart R\n# Then install the package fresh\n\ninstall.packages(\"package\")"},{"path":"updating-r-rstudio-and-packages.html","id":"updating-r","chapter":"B Updating R, RStudio, and packages","heading":"B.3 Updating R","text":"Finally, may also wish update R . key thing aware update R, just download latest version website, lose packages. easiest way update R cause huge headache use installr package. use updateR() function, series dialogue boxes appear. fairly self-explanatory full step--step guide available use installr, important bit select \"Yes\" asked like copy packages older version R.always, issues, please ask Teams book GTA session.","code":"\n# Install the installr package\ninstall.packages(\"installr\")\n\n# Load installr\nlibrary(installr)\n\n# Run the update function\nupdateR()"},{"path":"exporting-files-from-the-server.html","id":"exporting-files-from-the-server","chapter":"C Exporting files from the server","heading":"C Exporting files from the server","text":"using R server, may need export files share people submit assignments.First, make sure saved changes made file. clicking \"File - Save\", Ctrl + S, clicking save icon. changes saved, save icon greyed . new unsaved changes, able click icon.Select file download files pane (bottom right) ticking box next , click \"- Export\" save file computer.R installed, try open computer. , open Word, Endnote similar, may corrupt code. open file R R Studio installed.want double check file definitely right one submit assignment, can re-upload server open make sure answers .","code":""},{"path":"symbols.html","id":"symbols","chapter":"D Symbols","heading":"D Symbols","text":"\nFigure D.1: Image James Chapman/Soundimals\n","code":""},{"path":"license.html","id":"license","chapter":"License","heading":"License","text":"book licensed Creative Commons Attribution-ShareAlike 4.0 International License (CC--SA 4.0). free share adapt book. must give appropriate credit, provide link license, indicate changes made. adapt material, must distribute contributions license original.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
