[["index.html", "Level 2 Research Methods and Statistics Practical Skills Overview", " Level 2 Research Methods and Statistics Practical Skills Last Update: 2021-07-15 Overview Materials for the University of Glasgow School of Psychology Year 2 Research Methods and Statistics Practical Course. Authors: Phil McAleer, Carolina Kuepper-Tetzel &amp; Helena M. Paterson Aim: This course covers data skills such as R Markdown, data wrangling with tidyverse, and data visualisation with ggplot2. It also introduces statistical concepts such as permutation tests, Null Hypothesis Significance Testing (NHST), alpha, power, effect size, and sample size. Semester 2 focusses on correlations and the general linear model. Contact: This book is a living document and will be regularly checked and updated for improvements. Should you have any issues using the book or queries, please contact Phil McAleer. R Version: This book has been written with R version 4.1.0 (2021-05-18) Randomising Seed: In chapters that use some level of randomisation, where we have remembered, the seed is set as 1409. Cite as: McAleer, P., Kuepper-Tetzel, C., &amp; Paterson, H. M. (2021, July 14). Year 2 Research Methods and Statistics Practical Skills (Version 1.0.9001). Zenodo. http://doi.org/10.5281/zenodo.3822464 "],["foreword.html", "Foreword", " Foreword Welcome to the Level 2 Practical Lab Series Over the course of this years practical lab sessions, and this book, we will help you learn a whole host of skills and methods based around being a Psychologist. If you have completed Book 1 in the PsyTeachR series (https://psyteachr.github.io/) the first few labs will be familiar to you, with some additions. This is deliberate in order to refresh your knowledge and skills before moving on to more advanced topics later in this book. First, we will remind you how to work with R Markdown, before recapping the main functions we use for visualisation and data wrangling. From there we will build your understanding of probability before going on to using all these refreshed skills to analyse a variety of different experiments; all with the idea and mantra of being reproducibile in our approach throughout. By the end of the book we would hope that you are both competent and confident in all the knolwedge and skills we want you to have going forward, and we will be here to support you along the way. As you will see, this book requires a bit more self-directed learning than the first book; part of learning is trying things out yourself and recognising where you need help. That said, that does not mean we are not here to help and any time you are unsure, please, ask in class, on the forums, at our student office hours, or wherever you are most comfortable. And remember, when working through this book, this book is not about learning a software. We do not teach R, nor do you learn R. We teach data and analytical skills and knowledge within R, mainly within the tidyverse framework, and you are learning data and analytical skills and knowledge within R. This is an important point tohave in mind, as keeping the goals and tasks achievable will help you develop and will help you realise your achievements and accomplishments. Learning different skills, step-by-step, and integrating them together as we progress, will see you all advance in your learning! You can do this! We will support you! We will all learn together! "],["starting-with-r-markdown.html", "Lab 1 Starting with R Markdown 1.1 Overview 1.2 PreClass Activiy 1.3 InClass Activiy 1.4 Assignment 1.5 Solutions to Questions 1.6 InClass Comparison", " Lab 1 Starting with R Markdown 1.1 Overview A key goal of any researcher is to carry out an experiment and to tell others about it. One of the main ways we as Psychologists do this is through publication of a journal article. There are numerous ways that people combine different software to create a journal article, but a more recent innovation in the field that we want you to know about is creating reports and articles through R Markdown. If you like, you can see an example from a research team in our school in this recent PLOS article. A link within the article methods section (this one - https://osf.io/eb9dq/) allows you to see the one file that creates the whole manuscript. Obviously you wont be writing full journal articles just yet but you will use R Markdown throughout this lab series to do assignments. You could also use it in other subjects to write reports, or to make yourself a portfolio of hints, tips, and study aids as we suggest throughout the labs. Today, we will start by showing you some of skills in using R Markdown efficiently. In this lab you will learn: What is R Markdown? How to create an R Markdown file and knit it. How to add code and edit rules in your R Markdown file. How to format your text. 1.1.1 What is R Markdown? R Markdown (abbreviated as Rmd) is a great way to create dynamic documents through embedded chunks of code. These documents are self-contained and fully reproducible which makes it very easy to share. For more information about R Markdown feel free to have a look at their main webpage sometime: The R Markdown Webpage. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then knit it using the package knitr() to create your document as either a webpage (HTML), a PDF, or Word document (.docx). Explain This - We Knit what? Throughout the labs you will see little tabs that give more information, answers to quick questions, helpful hints, solutions to tasks, or suggestions for information you want to note down somewhere. You do not have to read them all and you will find they get less as the course progresses, but they might help you if you are stuck on something. Knit is what we say when we want to turn our R Markdown file into either a webpage, PDF, or a Word document. Often in the labs you will hear someone say, Have you tried knitting it? or What happens when you knit it? This simply means what happens when you try turning your file into a pdf or webpage. For any of the practical data assignments, one check to run before submitting is to knit your code to an html (webpage) file and then see if you can open that file in your browser. This doesnt check that your code is correct. It does however confirm that your code runs and has no critical issues in it that would stop your code from running. A very valuable check. 1.1.2 Advantages of using R Markdown The output is one file that includes figures, text and citations. No additional files are needed so its easy to keep all your work in one place. R code can be put directly into an R Markdown report so it is not necessary to keep your writing (e.g. a Word document) and your analysis (e.g. your R script) separate. Including the R code directly lets others see how you did your analysis - this is a good thing for science! It is both reproducible and transparent, key components of Open Science! You write your report in plain text, a non software-specific format that is easy to share, so its not necessary to learn any new coding such as HTM, but can create various outputs depending on what you need. 1.1.3 Creating an R Markdown (.Rmd) File In this lab youre going to create your own R Markdown document. Knowing how to do this will: help you navigate R Markdown, in turn with submitting homework assignments, and help you to create your own reports using it. If at any point you are unsure about how to do something remember to think about where you can get help. There is an R Markdown Cheatsheet on the top menu under Help &gt;&gt; Cheatsheets or do what we do, google it. For example, if I forget how to put words in bold, I could simply go to Google and type rmarkdown bold and no doubt get a lot of useful hints. There is nothing wrong with this. Nobody is expecting you to keep every function in your head; we all need reminders. You will find some elements stick in your head better than others. So remember, google is your friend! Quickfire Questions We have put questions throughout to help you test your knowledge. When you type in or choose the correct answer, the dashed box will change color and become solid. From the following options, why are we creating an R Markdown document instead of simply using an R script? R Markdown can combine report writing and analysis R Scripts can't run code Reproducible Science! Explain This Answer! So theres more than one answer to this question! R Markdown can combine report writing and analysis, providing open access for others to examine data, and create more Reproducible Science. But what about the incorrect answer? R Scripts do in fact run R code as you may remember from Level 1 labs. The key difference is that R Scripts cannot really be used for documentation and creating reports as easily - this is where R Markdown is used to ensure your code can be added to all the other information of your research and can be reproduced by others. 1.1.4 One last thing before beginning! Before working through the rest of this Lab you may want to watch the short videos on the Moodle pages and the Level 1 PsyTeachR Grassroots book reminding you about the skills we want you have already learnt through using R and RStudio, and the various file formats you will be working with. 1.2 PreClass Activiy Having read the Overview for this lab, and the reason behind using R, we are now going work on making a reproducible code. If you have a laptop, it is best to install R and Rstudio on that for you to use. The videos on Moodle and in the Grassroots book gives a reminder of how to install R and Rstudio. If you dont have it installed yet you can just read along today and try it out when you have access to a machine in the labs or online through a cloud, server or remote access. 1.2.1 Lets Begin Create a new R Markdown file (.Rmd) by opening Rstudio, and then on the top menu, selecting File &gt;&gt; New File &gt;&gt; R Markdown.... You should now see the following dialog box: Figure 1.1: Starting an R Markdown file Click Document on the left-hand panel and then give your document a Title. This is your file so call it what you want but make sure it is informative to you and your reader. Put your name or your student ID in the Author field as you are the author. For now we will focus on making an HTML output, so make sure that is selected as shown in Figure 1.1 then hit OK when you have done so. You should now have an .Rmd file open in Rstudio. The first thing you will see in your R Markdown file is a header section enclosed at the top and bottom by ---. Technically called the yaml header, this section lists the title, author, date and output format. The layout of the header is very precise and will look like that shown in Figure 1.2, which is currently set to output as HTML. Figure 1.2: An Rmd yaml header By default the file header includes the info shown in Figure 1.2 but there are many other options available. You can learn more about this in your spare time if you like through these links: http://rmarkdown.rstudio.com/html_document_format.html for .html options or http://rmarkdown.rstudio.com/pdf_document_format.html for .pdf options. BUT WAIT!! What if you spelt your name wrong? How would you change this? Explain This - I spelt my name wrong! The long way would be to close the file and start again. The shorter way would be to just correct the info in the header - just remember to keep between the quotes. E.g. Si Cologe instead of Untitled 1.2.2 Code Chunks Immediately below the header information you will see the default setup code chunk as shown in Figure 1.3. Most of the time, in this lab series, you will not edit the information in this chunk. Instead, you will add information, text, code, and chunks, below this chunk. Figure 1.3: The defualt setup code chunk In RMarkdown you can type any text you want directly in the document just as you would in a word document. However, if you want to include code you need to include it in one of these code chunks similar to Figure 1.3. Code chunks start with a line that contains three backwards apostrophes ` (these are called grave accents - often in the top-left of QWERTY keyboards), and then a set of curly brackets with the letter r inside: ```{r}``` You will always need both of these parts to create a code chunk: The three back ticks ` are the part of the Rmd file that says this is code being inserted into my document. The {r} part says that you are specifically including R code. The default setup code chunk provides some basic options for your R Markdown file for when it knits your work. As above, for now, it is best to leave this particular code chunk alone. Instead we will show you how to use R Markdown by editing the code chunks that come after this default chunk. The next code chunk in your file will look a bit like this: ```{r cars} summary(cars) ``` Within the curly brackets, on the first line of the chunk, the word cars is included after the letter r. This is simply the name or the label for the code chunk and it really could have been called anything. For example, you could have called this code chunk cars1 and a later chunk cars2 to show it was the first and second chunk relating to cars. Whilst it is always advisable you name your code chunks, you do not need to name them. However, if you do put in names for the chunks do not use the same name twice as this will cause your script to crash when you knit it, e.g. Do not use data and data; instead maybe use personality_data and participant_info or whatever makes sense to what you are doing in the chunk. OK? Different names for different chunks! They are all individual. Explain This - You can crash whilst knitting? Remember knitting just means converting or rendering your file as a pdf, webpage, etc. Crashing means that you had an error in your code that stopped your knitting from working or finishing. You can usually find the problem line of code from the error message youll see. The second line in the above code chunk is the R code we have written: summary(cars). In this case, we are just asking for a summary() of the inbuilt dataset cars. R has a lot of inbuilt datasets for you to practice on; cars is one of these. The third line closes off the code chunk, again with the three backwards apostrophes. This means that whatever is contained between the first and third lines will be the code that is run. Helpful Hint - Be sure to close the chunk When people are first starting out using R Markdown, a common issue is code not working because they have started the code chunk correctly, but have forgotten to close it at the bottom with the three backticks. Remember, three backticks to open, three backticks to close, and in our chunk we bind them. Quickfire Questions From the following options what was the name, or label, of the default setup code chunk (i.e. the first code chunk in an R Markdown file)? include r setup FALSE Explain This Answer If you look at the default setup code chunk you can see the code chunk has the name setup. include=FALSE is a rule which we will explain in a little bit. 1.2.3 Knitting Code Now would be a good time to try knitting your file to see what the code chunks do. You can do this using the Knit button at the top of the RStudio screen: Figure 1.4: The knit button. Clicking this will knit your file. When you click Knit it will ask you to save the file as an .Rmd file. Call the file L2Psych_Lab1_Preclass.Rmd and save it in a folder where you will keep all the information for this lab. When working in the Psychology labs or the University Library you need to save in a location or drive space that you have full access to and can save files to. The best one on campus is your M: drive. If using your own device then anywhere you can save the file should work. However, having a good folder structure will help you navigate the labs better. Helpful Hint - One folder for all your work It would be very beneficial to create a folder in your M: drive that will contain all your practical lab work for the rest of Level 2. Maybe something like Psychology_Level_2_Lab_Work and then have folders within that for each lab, e.g Lab1. The clearer the structure of these folders the easier it will be to find and use your files again! This is important as one thing we will keep telling you to do is LOOK BACK (politely) at what you previously did. A good way to think about this is if you have an exam, it isnt helpful to be told the location of your exam is Glasgow Uni (i.e. a large folder of many locations). Instead you would need to be told the specific building (a folder within your larger folder), but more specifically the room number in the building where your exam is taking place (the folder which you are working from). Couple of tips: Avoid spaces in file names and folder names. It can make life really complicated and is a bad habit to start with. Use underscores between words in filenames and folder names. Never call your folder R. This will crash your R and potentially lead you to having to reinstall both R and Rstudio. When Rstudio opens it looks for a folder called R which it expects to contain the software and libraries. If they arent there because it is now looking in a different folder with the same name, things go wrong. Sort of like the Sally-Anne False Belief task, but not as exciting. After saving the file, a webpage should appear. The first thing to notice is that some lines in the code chunks have disappeared: the ```{r} and the closing ``` in your code chunk have gone. Whenever you knit an R Markdown file these lines will disappear leaving only the code within. Youll also notice that the output of the code is also now showing in your webpage. In the next section we will show you how to control showing the output of your code, or not, through adding rules. Figure 1.5: The knitted summary output 1.2.4 Adding Code Chunk Rules and Options It can often be a good idea or even necessary to show the data or the outcome of a test in your report, for example if you were writing a report and wanted to include a table of results. But what if your code displayed a table that was 10,000 lines long? In that case we might want to not show the output and only show the code. You can do this by including a rule within the first line of your code chunk - your ```{r name, rule = option} line. You have already seen a rule before in the standard default chunk, the include rule, but there are a number of others. Lets look at some now: First, lets look at how to hide the output but show the code. Here, we use the results = \"hide\" rule: Figure 1.6: The results Rule Add this rule into your example code chunk, as shown above, and knit the file again. What happens? Note that there is a comma separating the name of the chunk and the rule. You should now see the code only and not the data. A key thing to note here is that your code is still running, it just isnt showing an output. For example, say your code said x &lt;- 2 + 2. With the results = \"hide\" rule, you would still be running that line of code, x being assigned as 4, but you just dont see the output. Alternatively, we can hide the code, but show the ouput by using the echo = FALSE rule: Figure 1.7: The echo Rule In your template Rmd file, the rule echo is set to FALSE meaning to show the figure and not the code. Change the rule in your code to echo and set it as TRUE, then knit the file again. What happens? Explain This - Why would I hide my code? Remember from Level 1 where we called in libraries to our environment. The echo = FALSE option is useful for commands like library() when you are just calling a package into the library but dont necessarily want to display that in your final report or in your final HTML file. Another example might be if you wanted to make a plot but didnt want to include the code, you just want to show the plot in your report. Next, say you want to hide both the code AND the output but still run the code. You can do this using the include rule: Figure 1.8: The include Rule Change the rule to your example code chunk, as shown above, to include = FALSE and then knit the file again. What happens? Note that here the code still runs. It just does not show you anything. Finally, you can use the eval rule which specifies whether or not you want the code chunk you have written to be evaluated when you knit the RMarkdown file. Evaluated means to run or carry out the code. Here, the eval = FALSE rule will stop the code from being evaluated. The code will be shown because there is no rule stopping it but there will be no output because it wont get evaluated because of the eval rule being FALSE. Figure 1.9: The eval Rule This might be useful in cases where you want to show the code relating to how you programmed your stimuli for an experiment, but you dont necessarily want it to run as part of the R Markdown file. We could probably do with a wee summary here: Table 1.1: Rules! Rules! Rules! Code Does Code Run Does Code Show Do Results Show eval = FALSE NO YES NO echo = TRUE YES YES YES echo = FALSE YES NO YES results = hide YES YES NO include = FALSE YES NO NO You can also mix and match rules to get the code/output to display as you want. It takes a little getting used to at first but if in doubt, just ask. Portfolio Point - autocompletes You can use RStudios autocomplete (the tab button) to see the different options for the different rules. For example, type include = and then hit the tab button on your keyboard. You should see the options of TRUE or FALSE. Autocomplete also works for a lot of functions you cant quite remember how to spell as well. gg-what? gg-{tab button} Ah yes, ggplot(). Quickfire Questions Youve got a large dataset of thousands of participants personality and happiness scores that you want to analyse and present in RMarkdown. You want to show the code you are running in your analysis but not show the output as this would be too much to display. Note that you want the code to run. Type in the box (e.g. rule = set) how you would set the results rule to do this? You create a plot of happiness versus neuroticism scores but you want to hide the code and only show the output. How can you do this? echo = TRUE include = FALSE code = HIDE echo = FALSE Explain This - I dont understand these answers The first answer should be results = hide as you want to show the code and run the code but not necessarily show the output of the code. In the second question, include = FALSE technically would hide the code, but this also hides the output! echo = FALSE allows you to still see your plot while hiding the code you want hidden. code = HIDE - if only it were that simple! Remember, the aim of these questions arent to help you memorise these codes (no one can do that!); theyre to help you gain a better understanding of how to apply these codes when you come across them in the future. True or False, writing echo = TRUE has the same effect on the output of a code chunk as if you had no echo rule at all: TRUE FALSE Explain This - Echo True or Not at all All of the code chunk rules have a default option. For example, echo, include, and eval are usually by default set to TRUE. As a result, if you dont set any echo rule, i.e. you dont specifically set echo = FALSE in your code chunk, then it is the same as setting echo = TRUE. So not specifying an option will give you the default setting for that option. True or False, there is no difference between setting results = \"hide\" and eval = FALSE as they both hide the output: TRUE FALSE Explain This - Whats the difference? With setting results = hide, the code is evaluated and results are produced but the output is hidden. With setting eval = FALSE, the code is not evaluated and therefore no results or output have been produced. If you need your output for a later part of the code then you would might use results = hide. If you dont need the output and just want to show the code as an example then you might use eval = FALSE. 1.2.5 Adding Inline Code An alternative way to add code to a report is through what is called using inline code. With inline code you dont use a code chunk. Instead the code appears inline with the text. Inline code can be inserted using a back-tick, then the letter r, followed by a space, then the code you want to include, then finally another back-tick. For example, writing `r 2 + 2` would return the answer 4 when you knit the file instead of showing the code. Remember, you do not do this inside a code chunk, you do this in line with your text, e.g.: We ran `r 2+2` people. Which when knitted becomes: We ran 4 people. So inline coding is really useful if you want to do calculations within your text or insert values into text, say from a dataframe, to make an informative sentence. We will look at more complex examples later in the labs but again this is a really useful tool for writing manuscripts through R Markdown the more comfortable you get with it. Quickfire Questions You need Two One Three back tick(s) to insert code chunks Why is this inline code, `{r} 6 * 8` , not going to show the calculated answer when you knit the file? Try editing the code line in Rmarkdown and knitting it to get it to work. You need a space between each back tick and the code Inline code cannot complete calcuations Curly brackets around the r are only needed for code chunks Explain This - Why are these answers correct? All code chunks start and end with three back-ticks. Inline coding does not use the curly brackets around the r. All you need for inline coding is a back-tick, r, space, code, and a final back-tick. 1.2.6 Formatting the R Markdown File The last thing we want to show you in this preclass activity is how to format your text. When youre not writing in code chunks you can format your document in lots of different ways just like you would in a Word document (or other expensive license-based software). The R Markdown cheatsheet provides lots of information about how to do this but we will show you a couple of things that you might want to try out. We can make text bold by including two ** (two asterisks) at the start and end of the text we want to present in bold font. For example: We ran **4 people**. Which when knitted becomes: We ran 4 people\". Now write some text in your Rmd file and put it in bold. Knit the file to check it worked. You could also try using italics by putting a single * (asterisk) at the start and end of the word or sentence. Try this now. Here is an example to help. We ran *4 people*. Which when knitted becomes: We ran 4 people\". Note: italics can be difficult to read for many people and as such we have tried to avoid using it in this book. If you find some italics, where it is not necessary, please let us know and claim your reward of a packet of minstrels. Yes, a whole packet! Finally, you might want to add headings and sub-headings to your file. For example, maybe you are writing a Psychology journal article and want to put in a header for the Introduction, Methods, Results, or Discussion sections. We do this using the # (hashtag) symbol as shown in Figure 1.10. Figure 1.10: Inputting different Header levels using #s Now, type the four main sections found in a Psychology journal article in your R Markdown file, typing each one in a separate line. These are mentioned above. Knit the file. What do these look like? Now add a different number of #s before each heading, with a space between the heading and the hashtag (e.g. # Introduction) and knit the file again. What do you notice about the different number of hashtags? Quickfire Questions If * puts words into italics, and ** puts words into bold, type in the box what might you put before (and technically after) a word to put it into italics with bold? True or False: The more #s you include, the smaller the header is: TRUE FALSE From the options, the most common order of headings found in a Psychology Journal are: Discussion, Introduction, Methods, Results Discussion, Results, Methods, Introduction Introduction, Methods, Results, Discussion Introduction, Results, Methods, Discussion Explain This - I dont get these answers If * at the start and end of the word puts it in italics (e.g. italics) and ** puts it in bold (e.g. bold), then putting three *** at the start and end will put it in italics with bold (e.g. italics-bold). It is true that the more #s you use, the smaller the heading is. Word and other document writers use different headings as well. Here, # gives the biggest heading, and it gets smaller and smaller with every extra #. Finally, in Psychology, the vast majority of journal articles are written in the format of: Introduction, Methods, Results, Discussion. This format does not always hold as some journals ask authors to use a different format, depending how much emphasis that journal (erroneously) likes to put on results over hypothesis and methods. We however teach the order stated above. The question and approach is always as important, if not more so, than the results! Which of course you know from learning about Registered Reports in the labs and lectures. Job Done - Activity Complete! Well done on working your way through this activity. Be sure to make notes for yourself, and to post any questions on the forums that you may have. See you in the lab! 1.3 InClass Activiy For all the inclass activities, you will need to have completed the PreClass Activities. You may also need some cheatsheets such as The R Markdown Cheatsheet. Everything you need to complete this inclass activity can be found in those documents. We will tell you which additional material might help where we think it is appropriate. 1.3.1 R Markdown and The Experimental Design Portfolio In the preclass activity, we asked you to start playing about with R Markdown. Now, in the lab, we are going to continue learning about R Markdown as creating your own files from scratch is a great start to creating reproducible science! We will also start you off in creating your own Experimental Design and Analysis Portfolio through R Markdown. The aim of this portfolio is to consolidate your learning in experimental design and analysis, allowing you to reflect back on how your learning has progressed. You should add to it whenever you think Oh that is a good tip! or That is something I want to remember! Do this after a lab or a lecture, when you are collating all your notes together. Your portfolio is for you, unless you choose to share it of course, and will not be assessed or marked in anyway. It is your learning aid to help you develop your understanding of research methods and analysis in Psychology. By the end of these labs you should be able to use your portfolio for exam revision, particularly if you incorporate lecture material, so its worth maintaining a clear structure to your portfolio! In this first lab, across 9 tasks, we will help you to understand how to structure and format R Markdown files; you can then apply what you learn here to your portfolio in your own time. We suggest having your portfolio open in every lab so you can add to it as you go along. Lets begin! Portfolio Point - Things you could include Throughout the semester you will see these Portfolio Points. They arent always necessary to complete the lab but sometimes they will be and sometimes they are just things you might consider putting in your Portfolio to remember. What you keep in your portfolio is up to you but here are some examples of the kind of things we would recommend you include: Key points about classic experiments their main goal, outcome, authors, year a top tip is to write a short summary after every paper you read, including the authors names to help you consolidate that information Aspects of your Reports designs and analyses what decisions you made and why; how they compare to other studies. Glossary points for R code functions For codes you find more challenging to understand the function of For codes you might use more frequently in future activities We are developing a glossary which you can send us items to include or get involved with. It is still in development but you can see it here https://psyteachr.github.io/glossary/. Reflection Points on what you have learned in your labs each week. 1.3.2 The Ponzo Illusion and Age The activities in this lab will make use of an open dataset. Explain This - What is an open dataset? An open dataset is made available for everyone to see and is stored on the internet for other researchers to use. In the PreClass activity, you saw an example of this at the very start in the PLOS One article. Many journals now ask researchers to make their data available or to post it somewhere accessible like the Open Science Framework. Interestingly, the art of making your data available was standard in classic older articles. The data we are using today comes from 1967. Sometime between then and more recent times, data started being made unavailable - closed. We believe all data should be made available and will encourage you to do that over the coming years. Transparent science is Open Science! The data we will use today is from a paper looking at the Ponzo illusion and Age: Leibowitz, H. W. &amp; Judisch, J. M. (1967). The Relation between Age and the Magnitude of the Ponzo Illusion. The American Journal of Psychology, 80(1), 105-109. It can be accessed on campus (University of Glasgow) through this link. Off campus you can sign in to read it through the University of Glasgow library if you are a student at Glasgow. The basics of the Ponzo illusion (Wikipedia page) is that two lines of the same size are viewed as being of different length based on surrounding information - like sleepers on a traintrack. See Figure 1 of Leibowitz and Judisch (1967) for an example (P106). The authors showed people two vertical lines surrounded by differing horizontal lines running at angles behind the main vertical lines. The authors varied the size of one of the vertical lines (left line) and asked the participants to judge which of the two vertical lines was bigger or longer; the left line (variable) or the right one (standard). The paper also tested how this illusion was influenced by age. For more info, see the paper. Operationalising the dependent variable, Leibowitz &amp; Judisch measured what size the left line had to be to be considered the same size as the standard line on the right. The data we will be using can be seen on page 107, and includes: Which Group participants were assigned to according to age, with each group being made of 10 participants of the same sex The Sex of the Group The Mean Age of the Group The Mean Length of the left vertical line 1.3.3 Task 1: Setting up Your R Markdown Portfolio As above our overall goal is to make a reproducible report summarising the data in the Leibowitz and Judisch (1967) paper. As we go along, remember to refer back to the PreClass activity and cheatsheets to help you. Lets begin! Create a new R Markdown document. Give it a title, e.g. My Psychology Research Methods Portfolio Enter your GUID or name as the author Set the output as HTML. Helpful Hint Throughout the labs you will see these Helpful Hints. Usually the solutions are nearby or at the end of the chapter to prevent temptation. In setting up this Rmd file, if you have followed these steps correctly, you will probably see a new R Markdown file with a header containing the title, author, date and output information as shown in the PreClass activity. If you dont see the document header, then youve probably created an R Script instead. Refer back to the PreClass activity and try again. Look further down the list of File options on the top menu. If stuck, speak to a tutor or ask on an appropriate forum. You can now remove the parts of the generic R Markdown code that we do not need; anything after the setup code chunk can be removed (see Figure 1.3). So anything after line 11 can be removed. Leave the first code chunk however - lines 8 to 10 - as these lines make R Markdown show code chunks unless otherwise specified - note the echo = TRUE. Portfolio Point - Code Chunk Reminders After the lab it might be handy to write a reminder somewhere in your portfolio about what a code chunk is. Writing it in your own notes somewhere accessible to you will mean you can find it more easily than searching through all the labs for the right information. This is something to keep in mind for any information you come across that you might need to recall later on! 1.3.4 Task 2: Give your Report a Heading We are going to start off your portfolio with creating a brief report on Leibowitz and Judisch (1967), so we should give it a heading. After the setup code chunk, give your report a heading, e.g. Lab 1 - The Magnitude of the Ponzo Illusion varies as a function of Age. Using hashtags, give this heading a Header 1 size. Helpful Hint - hashtags are not just for social media Remember that the fewer the number of hashtags the larger the heading size. 1.3.5 Task 3: Creating a Code Chunk We are going to need the data soon so best to bring it in at the start of our code. Set your working directory: Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Portfolio Point - Set Working Directory One of the most common issues we see with people using Rstudio is that they forget to set their working directory to the folder containing the data file they are working on. This means that when you try to knit or run a code line it wont work because Rstudio doesnt know where the data is. Remember to set your working directory at the start of each session, using Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Avoid using code to set your working directory as often this will only work on your machine and not others and is therefore not fully reproducible without editing the script. Download the data for this lab in a zip file by clicking this link. Unzip it and save it to the folder you are working in. Create a new code chunk in your R Markdown script, give this code chunk the name load_data. Copy and paste the code below into your code chunk. Spend a couple of minutes with a partner reminding yourself what the code does. The answer is in the hint below. Now, add or change the echo rule in your code chunk so that when you knit the file, the code will not be included in the final document. library(&quot;tidyverse&quot;) ponzo_data &lt;- read_csv(&quot;PonzoAgeData.csv&quot;) Knit the document now and see what the output looks like. It will ask you to save the file somewhere. Remember that on the Boyd Orr Lab PCs this is best done on your M: drive, given available space. Important: There is a good chance that, on the webpage that you have knitted, you will see either some warnings or messages. You can suppress these using the message and warning rules within the code chunks as well. Try this now - the PreClass Activities and the R-Markdown cheatsheet will help. Helpful Hint - what was all that? Hints: Step 4 - echo can equal TRUE or FALSE. Remember to separate rules in the code chunk with commas. E.g. {r, rule1 = FALSE, rule2 = TRUE} What does the code do? Line 1 loads the tidyverse packages and all associated packages e.g. dplyr, readr and ggplot2. You have used these in Level 1 Grassroots book - we will recap a lot of that in the coming labs. Line 2 loads in the data using the read_csv() function and stores it in ponzo_data. Important points to note: ponzo_data could have been called anything but best to call it something that makes it clear what it is. Only rule is no spaces in the name. ponzo_data and ponzo.data are acceptable, and different from each other. ponzo data is not acceptable and will crash the code. read_csv() is actually in the readr package and is available to you only after you have loaded in tidyverse through library(tidyverse). We will always tell you to use read_csv() to read in data from a csv file. There are other codes that load in data - one very similar one is read.csv(). They work differently. Only ever use read_csv() in your Psychology labs unless otherwise instructed. remember &lt;- essentially means assign this to that. Assigning the ponzo data to the table ponzo_data can actually can be written the other way around - read_csv(PonzoAgeData.csv) -&gt; ponzo_data - but convention usually puts it the way we have in the code. 1.3.6 Task 4: Writing your Report Lets start giving this brief report some information and structure as we would a full report. Underneath the code chunk you entered, put a new heading called Introduction and give it a Header 2 size. Next, do a little research with your group on the Ponzo Illusion and write a sentence or two describing how it works and what it tells us; include a citation to support your research. There is a link to the wikipedia page on the illusion at the top of this lab which might help. Finally, copy the text in the box below into your report and finish the text by putting the names of two hypotheses behind the illusion below the sentence in an ordered list style; i.e. 1 2, etc. The two hypotheses are The Framing hypothesis and The Perspective hypothesis. &quot;There are two underlying hypotheses that may explain the Ponzo Illusion. These are: ...&quot; Helpful Hint - Lists Lists can be tricky to begin with but are very straightforward once you know the key points. The list begins after a blank line after any text. If you start the list without leaving a blank line at the top it wont work. Each point starts with an asterisk (*) or by an integer and full-stop (e.g. 1.) You must have a space after the * or 1. before writing your point. Each point is a new line. To stagger points on a list (i.e. indent), leave 4 blank spaces (two tabs) and then put your * etc. Quickfire Question Here are a couple of questions to try out in your group to remind you about using citations: When writing a report, how would you cite: Papers with five authors on the first mention? Author 1, Author 2, Author 3, Author 4, &amp; Author 5 Author 1, Author 2, Author 3, Author 4, &amp; Author 5, Year Author 1 et al., Year Papers with five authors on the second mention? Author 1, Author 2, Author 3, Author 4, &amp; Author 5 Author 1, Author 2, Author 3, Author 4, &amp; Author 5, Year Author 1 et al., Year Papers with seven authors on the first mention? Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, &amp; Author 7 Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, &amp; Author 7, Year Author 1 et al., Year Papers with two authors in a citation? (Author 1 &amp; Author 2) (Author 1 et al., Year) Two papers in one paretheses? Order chronologically according to year, separated by a semi-colon Order alphabetically according to first author surname, separated by a semi-colon Two papers of the same author? Order chronologically according to year, separated by a semi-colon Order chronologically according to year, separated by a comma Order alphabetically by adding a letter to each year Those questions about citations may throw you a little. In late 2019, the APA Style Guide 7th edition was released. One of the biggest changes here was that for papers of three or more authors, the citation would only show the first author and the year, as opposed to listing all authors on first citation. Two author papers and single author papers still always show all authors. For more information on how to format citations, you can look at the highly recommended APA Style blog. 1.3.7 Task 5: Making Text Bold or Italicized Sometimes we want to add some emphasis to text. In your report, format the line There are two underlying hypotheses... in bold. Answering the below question might help you remember how. Quickfire Question Bold text and italicized text are created similarly, how do you create italicized text? * (before text) ** (before and after text) * (before and after text) ** (before text) Its a good idea to knit the file at this point to make sure the codes are all working correctly. 1.3.8 Task 6: Adding Links to the Data in your Methods Good practice in a Report is to include information about where we got the data from. Create a new heading below your list of the two hypotheses and call it Methods. Set it as Header 2 size. Below Methods write a new heading called Data and set it as Header 3 size. Underneath the Methods heading, copy and paste in the below sentence and turn the citation into an internet link to the paper. The data in this report was obtained from within the original paper, (Lebowitz and Judisch, 2016). Now knit your document again to make sure your formatting is working. Titles should be bigger than normal text and the list should be indented and have numbers at the start of each line. Helpful Hint - adding links? You can get the web address by following the link to the paper shown towards the beginning of this lab activity. Include the https part. Use the R Markdown cheatsheet to see how to insert links. It has something to do with square brackets [] and circular brackets () next to each other. 1.3.9 Task 7: Adding an Image to your Methods For certain studies, you may want to add an image to the Methods section, either of the stimuli, of the materials, or of the procedure. If you look at the R Markdown cheatsheet youll see that adding an image is very similar to adding a link, the only difference is the exclamation mark, !, beforehand. Surprising, I know! For now we will just add an image of the illusion taken from the internet to illustrate how to add images to our documents. Below the sentence you added for Task 6, add a new heading called Stimuli and set it as Header 3 size. Below the Stimuli heading, insert the image at the following web address: https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg Portfolio Point - A good methods section Remember that a good methods section will contain all the necessary information that would be required for another researcher to replicate your experiment exactly! It would normally be split into three or four sections including Ethics, Participants, Stimuli, and Procedure. This may sound very obvious but you would be surprised at how many Methods sections dont give enough information for replicating the study. Articles tend to have word counts - just like your assignments. Authors have tended to cut words where they can to fit in further discussion or more results. Methods sections have suffered as a result. But no more! 1.3.10 Task 8: Adding a Table to your Results Another benefit of R Markdown is that you can insert tables of results directly into your report without having to format them - though for aesthetics you will want to learn how to format tables eventually. But for now Create a new heading below your methods sentence, called Results and format it as Header 2 size. Add a new code chunk and give it the name table, and include the code shown below. The first part of the code my_table &lt;- group_by %&gt;% summarise creates the table and stores it in my_table. The second part of the code my_table calls the table. Calls means to display or to show me in this sense. Add an echo rule so that the code IS NOT included in the final document but the ouput table is included. my_table &lt;- group_by(ponzo_data, Sex) %&gt;% summarise(NofGroups=n(), mean_length = mean(ComparisonLength)) my_table Now, knit your document to see what you have produced. You should not see the above code, just the output table. 1.3.11 Task 9: Adding a Figure to your Results Nearly all research reports have a figure so we will want to add one as well. Underneath your table code chunk, add a new code chunk and give it the name plot. Add the below code to the chunk and set the include rule so that both the code and the plot are included in the final report. ggplot(ponzo_data, aes(x = Mean_Age, y = ComparisonLength, color = Sex)) + geom_point() Portfolio Point - to assign or to not assign You may notice above that we assigned our data in the table to my_table and then called my_table to show it. However, we didnt do that for the figure. We just put the code for the figure but did not assign it. Why? There is no great answer and you could assign both or not assign either, and we will chop and change throughout the labs to show you the difference but the tendency is to assign tables but not assign figures. Simply because we often are creating the figures to show them and therefor assigning them and then calling them requires more code. Tables on the other hand are often stored to work on later, so it makes sense to assign them. Again this is not a hard and fast rule and often we will assign figures but it just makes it quicker not to. If you ever do assign a figure remember to call it, or your figure wont be displayed! Again, knit your document to make sure it is working correctly. Below your table you should now have the ggplot code followed by the nice scatterplot. Group Discussion Point In your group, have a brief discussion about the figure to answer the following question. Based on the distibrution of the data, shown in the above Figure,  as age increases, people perceive a shorter vertical line to be of same length as the standard vertical line as age increases, people perceive a longer vertical line to be of same length as the standard vertical linge There is no relationship between Age and the Ponzo illusion This figure tells me nothing about the relationship between Age and the Ponzo illusion Helpful Hint What does each dot represent in the Figure, and what is the pattern of the dots? We will learn more about how to improve the visualisations as we progress but for now you have completed the bones of your first report! Compare your report to the one we have created to see if they match, which can be found at the end of this chapter using menu on the left (i.e. InClass Comparison) or click here to download the .Rmd file in a zip folder. Fix anything that is not formatted as in our template. Portfolio Point - The Power of R Markdown and the ggplot Package Here is a real-world scenario of why plotting in R Markdown can save a lot of effort. Say you carried out an experiment, made a figure of the results using an R Script, and wrote up the report using Microsoft Word. Then you realised you forgot to include 2 participants. To fix this, you would have to re-run the R script, make a new plot, save the plot, and then transfer that to your Word document. However, had you used R Markdown to begin with and both analysis and report were in the same place, then you can simply update the code within the document and a new figure will be created in the exact same place as the old one. Magic! The code above uses the ggplot2 package you used in Level 1. This is the main package we use for plots, figures, visualisations, or however you like to call them. It can be called into the library by itself, or is automatically called in when you call in the tidyverse package. Later this semester we will revist ggplot2 in more detail. For now, we are using it to make a scatterplot (geom_point) of Age (Mean_Age) and Comparison Length (ComparisonLength), and splitting the data for males and females. Job Done - Activity Complete! Great work! We have now created a rough layout of a report. The only section we are missing is the Discussion where you relate the information from previous research to what your study showed. Feel free to add one in your own time; read the short summary at the end of the actual paper to help get your thoughts together. We will talk more about the structuring of reports all throughout the year so you will have a great idea of how to write one by the end of Level 2. Well done on succesfully creating your own R Markdown file! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost the skills which you will need in future assignments. If you have any questions, please post them on the forums. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 1.4 Assignment This is a formative assignment. Instructions as to how to access this assignment will be made available on Moodle during the course. Please note though that this assignment will not be graded and does not count towards obtaining course credit or your overall grade. 1.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 1.5.1 InClass Activities 1.5.1.1 Task 2: Give your Report a Heading You should have used only one hashtag to give the biggest heading size. # Lab 1 - The magnitude of the Ponzo Illusion varies as a function of Age Return to Task 1.5.1.2 Task 3: Creating a Code Chunk The echo rule, warning rule and message rule should all be set to FALSE. As such, the start of the code chunk should look like: ```{r load_data, echo = FALSE, warning = FALSE, message = FALSE}``` Return to Task 1.5.1.3 Task 4: Writing your Report Task 4 is about setting a title to Header 2 style. This is done via two ## at the start of the line - before the word Introduction in this case but dont forget the space. ## Introduction Worth noting: In basic R Scripts, # at the start of the line would result in turning the line into a comment. Here, in R Markdown, # sets the header size much like a Word document header For the second part, create an ordered list by putting 1 followed by a . then a space before the first piece of information. A 2 then a . before the second, and so on. Note that lists will only work if there is a empty line above the list as well: 1. The Perspective Hypothesis 2. The Framing Hypothesis Return to Task 1.5.1.4 Task 5: Making Text Bold or Italicized To turn text to bold you need to put two ** at the start and end of the word or sentence you want as bold, e.g. **make me bold** Return to Task 1.5.1.5 Task 6: Adding Links to the Data in your Methods To set a header as Header 2 style use ## at the start of the line. To set a header as Header 3 style use ### at the start of the line. A link is created by putting the words you want to act as the link between [] and then the link immediately after in (). For example: [Lebowitz and Judisch (2016)](https://www.jstor.org/stable/1420548?seq=1#page_scan_tab_contents) 1.5.1.6 Task 7: Adding an Image to your Methods To set a header as Header 3 style use ### at the start of the line. An image is created by putting the words you want to act as the name of the image [] and then the link to the image immediately after in (). The key thing is to start with an exclamation mark !. For example: ![name](link) and therefore ![The Ponzo Illusion](https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg) Return to Task 1.5.1.7 Task 8: Adding a Table to your Results To set a header as Header 2 style use ## at the start of the line. The code chunk heading should read as follows: ```{r table, echo = FALSE}``` Return to Task 1.5.1.8 Task 9: Adding a Figure to your Results The code chunk heading should read as follows: ```{r plot, include = TRUE}``` Return to Task Chapter Complete! 1.6 InClass Comparison This section shows the output that would be expected if you were to follow the inclass activities correctly. Note: Headings in this comparison will appear one size smaller than if you were to knit the Rmd due to rendering. Do not worry if yours look a bit bigger, it is more that you have them as headers is the key part.Your output should match the output of knitting the .Rmd document found here Lab 1 - The Magnitude of the Ponzo Illusion varies as a function of Age Introduction The Ponzo Illusion is where There are two underlying hypotheses that may explain the Ponzo Illusion. These are: The Framing hypothesis The Perspective hypothesis Methods Data The data in this report was obtained from within the original paper, Lebowitz and Judisch (2016) Stimuli PonzoIllusion Results Sex NofGroups mean_length Female 15 3.574667 Male 26 3.606923 ggplot(ponzo_data, aes(x = Mean_Age, y = ComparisonLength, color = Sex)) + geom_point() Figure 1.11: You wont have a caption. We will cover that later! End of Comparison! "],["data-wrangling-a-key-skill.html", "Lab 2 Data-Wrangling: A Key Skill 2.1 Overview 2.2 PreClass Activity 2.3 InClass Activity 2.4 Test Yourself 2.5 Solutions to Questions 2.6 Additional Material", " Lab 2 Data-Wrangling: A Key Skill 2.1 Overview One of the key skills in any researchers toolbox is the ability to work with data. When you run an experiment you get lots of data in various files. For instance, it is not uncommon for an experimental software to create a new file for every participant you run and for each participants file to contain numerous columns and rows of data, only some of which are important. Being able to wrangle that data, manipulate it into different layouts, extract the parts you need, and summarise it, is one of the most important skills we will help you learn in the coming weeks. The next few labs are aimed at refreshing and consolidating your skills in working with data. This lab focuses on organizing data using the tidyverse package. Over the course of the activities, you will recap the main functions and how to use them, and we will use a number of real datasets to give you a wide range of exposure to what Psychology is about, and to reiterate that the same skills apply across different datasets. The skills dont change, just the data! There are some questions to answer as you go along to help build your skills: use the example code as a guide and check your answer against the solutions at the end of the chapter. Finally, remember to be pro-active in your learning, work together as a community, and if you get stuck, ask on the forums, google what you are trying to do (e.g. mutate table tidyverse), and use the cheatsheets or the Grassroots PsyTeacher book. The key cheatsheet for this activity is the Data Transformation Cheatsheet with dplyr. In this lab you will recap on: Data-Wrangling with the Wickham Six one-table verbs. Additional useful functions such as count, pivot_longer (to gether data) and joins Piping and making efficient codes. Note: This preclass is a bit of a read but it is important that you have all this information in the one place so you can quickly refer back to it. Also, you did a very similar task in the Grassroots book so it is about recapping more than learning afresh. But take your time to try to understand the information and be sure to ask any questions you have. Try first, then ask is our only rule! Portfolio Point - Getting Help Remember to open up your Portfolio that you created in Lab 1 so you can add useful information to it as you work through the tasks! Also summarising the information we give in this preclass, in your own words, is a great way to learn! You dont have to read all of these but they might help from time to time to explain parts further. For instance, do you remember how to get help on an R function in RStudio? In your Console window, you can call the help function (e.g. ?mutate) to view the reference page for each function. This example shows how to get help on the mutate() function within dplyr, which we will use in later labs. 2.2 PreClass Activity Revisiting Tabular Data From working with data previously you know that nearly all data in research methods is stored in two-dimensional tables, either called data-frames, tables or tibbles. There are other ways of storing data that you will discover in time but mainly we will be using tibbles (if you like more info, type vignette(\"tibble\") in the Console Window). A tibble is really just a table of data with columns and rows of information, and within the tibble you can get different types of data, i.e. double, integer, and character. Type of Data Description Double Numbers including decimals (e.g. 3.14) Integer Numbers without decimals (e.g. 3) Character Tends to contain letters or be words, but can be numbers (e.g. AB12, Data Rocks!) Note: Double and Integer can both be referred to as Numeric data, and you will see this word from time to time. For clarity, we will use Double as a term for any number with a decimal (e.g. 3.14) and Integer as a term for any whole number (no decimal, e.g. 3). Quickfire Questions What type of data would these most likely be: Male = Character Double Integer 7.15 = Character Double Integer 137 = Character Double Integer Portfolio Point - Data types and levels of measurement There are lots of different types of data as well as different levels of measurements and it can get very confusing. Its important to try to remember which is which because you can only do certain types of analyses on certain types of data and certain types of measurements. For instance, you cant take the average of characters just like you cant take the average of categorical data. Likewise, you can do any maths on double and integer data, just like you can on interval and ratio data. Integer data is funny in that sometimes it is ordinal and sometimes it is interval, sometimes you should take the median, sometimes you should take the mean. The main point is to always know what type of data you are using and to think about what you can and cannot do with them. 2.2.1 Revisiting the Wickham Six The main way we teach data-wrangling skills is by using the Wickham Six one-table verbs. These are part of the tidyverse package which we introduced to you in the first PsyTeachR book, and more specifically from the dplyr package that is contained within the tidyverse. These six verbs are often referred to as the Wickham Six one-table dplyr verbs as they perform actions on a single table of data. We will look at some of the basics again here but try to look back at the exercises in the Grassroots book to see how we used these verbs (functions) previously. The Wickham Six are: Function Description select() Include or exclude certain variables (columns) filter() Include or exclude certain observations/data (rows) mutate() Creates new variables (columns) arrange() Changes the order of observations (rows) group_by() Organises the observations (rows) into groups summarise() Create summary variables for groups of observations (rows) Portfolio Point - The Wickham Six You will use the Wickham Six very frequently for wrangling your data so this would definitely be something you should be making notes about - not just the names, but how they work and any particular nuances that you spot. Perhaps recreate the above table and add your own examples. 2.2.2 Learning to Wrangle: Is there a Chastity Belt on Perception Today we are going to be using data from this recent paper: Is there a Chastity Belt on Perception. You can read the full paper if you like, it is a nice representation of action/perception/cogntion working together, but we will summarise the paper for you. The main research question asks, does your ability to perform an action influence your perception? For instance, does your ability to hit a tennis ball influence how fast you perceive the ball to be moving? Or to phrase another way, do expert tennis players perceive the tennis ball moving slower than novice tennis players? This experiment does not use tennis players however, they used the Pong task: a computerised game in which participants aim to block moving balls with various sizes of paddles. A bit like a very classic retro arcade game. Participants tend to estimate the balls as moving faster when they have to block it with a smaller paddle as opposed to when they have a bigger paddle. You can read the paper to get more details if you wish but hopefully that gives enough of an idea to help you understand the wrangling we will do on the data. We have cleaned up the data a little to start with. Lets begin! Download the data as a zip file from this link and save it somewhere you have access. In the lab, use your M: drive. Set your working directory to the same folder as the data. Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Open a new script and copy and paste the two lines below. Here we are: loading the tidyverse library into our session and then loading in the data through the read_csv() function and storing it in the tibble called pong_data. library(&quot;tidyverse&quot;) pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) DO NOT install packages in the Boyd Orr labs; they are already there and just need called in through library(). However, If you are using your own computer and you havent previously installed the tidyverse package before, you will have to install it first, e.g. install.packages(tidyverse). If you have already installed tidyverse but it was a long time ago, it might be worth running some updates on the packages as you may have an old version that works differently. The easiest way to do this is through RStudio using the menu at the top - Tools &gt;&gt; Check for Package Updates. You can update packages individually or just run all updates. Tends to be better to just update all packages as many of the packages are linked, unless you specifically dont want to update a certain package. Help my data is not loading? The three most common mistakes we see are: Make sure you have spelt the data file name exactly as it is shown. Spaces and everything. Do not change the name of the csv file, fix your code instead. The reason being is that if you have a different name for your file than someone else then your code is not reproducible. We would say avoid using spaces in filenames you create, but as this is one created by another researcher and already has them, we will leave it as is and work with them. Remember when uploading data we use read_csv which has an underscore, whereas the data file itself will have a dot in its name, filename.csv. Check that the datafile is actually in the folder you have set as your working directory. Lets have a look at the pong_data and see how it is organized. Type View(pong_data) or glimpse(pong_data) in your Console window. Capital V and little g. In the dataset you will see that each row (observation) represents one trial per participant and that there were 288 trials for each of the 16 participants. The columns (variables) we have in the dataset are as follows: Variable Type Description Participant integer participant number JudgedSpeed integer speed judgement (1 = fast, 0 = slow) PaddleLength integer paddle length (pixels) BallSpeed integer ball speed (2 pixels/4ms) TrialNumber integer trial number BackgroundColor character background display colour HitOrMiss integer hit ball = 1, missed ball = 0 BlockNumber integer block number (out of 12 blocks) We will use this data to master our skills of the Wickham Six verbs, taking each verb in turn and looking at it briefly. You should develop your skills by setting yourself new challenges based on the ones we set. There are 6 verbs to work through and then after that we will briefly recap on two other functions before finishing with a quick look at pipes. Try everything out and let us know anything you cant quite get. 2.2.3 The select() Function - to keep only specific columns The select() function lets us pick out the variables within a dataset that we want to work with. For example, say in pong_data we wanted to only keep the columns Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, and HitOrMiss but we dont need BackgroundColor or BlockNumber. We can do this in two ways: We can tell the function what variables we want to include select(pong_data, Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss) Or we can do it the opposite way by excluding columns through -ColumnName approach (i.e. minus the ColumnName) select(pong_data, -BackgroundColor, -BlockNumber) In this latter example, -BackgroundColor means not BackgroundColor, so here you are saying all columns except BackgroundColor and BlockNumber. The minus sign is the crucial part! Task 1: Using the select() function Either by inclusion or exclusion, select only the columns Participant, PaddleLength, TrialNumber, BackgroundColor and HitOrMiss from pong_data. Did you know select() can also be used to reorder columns? Use select() to keep only the columns Participant, JudgedSpeed, BallSpeed, TrialNumber, and HitOrMiss but have them display in alphabetical order, left to right. Helpful Hint - selecting Have you remembered to include the dataset pong_data? Pay attention to upper/lower case letters and spelling! Think about how you first entered the column names as they appeared. But what happens if you change the order that you enter the column names? 2.2.4 The arrange() Function - to sort and arrange columns The arrange() function sorts the rows in the tibble according to what column you tell it to sort by. In this example we show how to the data arrange by one column e.g. by BallSpeed arrange(pong_data, BallSpeed) Or by multiple columns e.g. by BallSpeed (fastest first) and BackgroundColor arrange(pong_data, desc(BallSpeed), BackgroundColor) Explain this - where did desc come from? What does desc() do? desc() is how to sort by largest to smallest - i.e. descending order. Compare the output of the two lines above on the BallSpeed column. Does desc() also work for BackgroundColor? Task 2: Arranging Data with the arrange() function Arrange the data in pong_data by two variables: HitOrMiss (putting hits - 1 - first), and JudgedSpeed (fast judgement - 1 - first). 2.2.5 The filter() Function - to keep only parts of the data The filter() function lets us parse out a subset of the data, meaning we keep only parts of the data. For example, we might want to only keep the red BackgroundColor filter(pong_data, BackgroundColor == &quot;red&quot;) or only keep BallSpeed above 4 pixels filter(pong_data, BallSpeed &gt; 4) or only keep trials that match both the red BackgroundColor and BallSpeed above 4 pixels. Any trial that is not red background color or slower than 5 pixels will be removed. filter(pong_data, BackgroundColor == &quot;red&quot;, BallSpeed &gt; 4) This last example can also be written as follows. Two arguements or requirements separated by a comma is equivalent to an &amp; (ampersand - meaning and). filter(pong_data, BackgroundColor == &quot;red&quot; &amp; BallSpeed &gt; 4) Or say you want to keep specific Participant IDs. Say we want just the data from Participants 1, 3, 10, 14 and 16. We would write it as follows. The %in% is called group membership and means keep each of these Participants The c() creates a little container of items called a vector. filter(pong_data, Participant %in% c(&quot;1&quot;, &quot;3&quot;, &quot;10&quot;, &quot;14&quot;, &quot;16&quot;)) And finally, say you wanted to keep all Participants except Participant 7. Say the experiment didnt work for them and you want to remove them. You would write: you can read != (exclamation mark followed by equals) as does not equal. So Participant != \"7\" means keep all Participants where the values in Participant column are not 7. The exclamation mark can sometimes be used to negate the function that follows it. filter(pong_data, Participant != &quot;7&quot;) Task 3: Using the filter() Function Use filter() to extract all Participants that had a fast speed judgement, for speeds 2, 4, 5, and 7, but missed the ball. Store this remaining data in a variable called pong_fast_miss Helpful Hint There are three parts to this filter so it is best to think about them individually and then combine them. Filter all fast speed judgements (JudgedSpeed) Filter for the speeds 2, 4, 5 and 7 (BallSpeed) Filter for all Misses (HitOrMiss) You could do this in three filters where each one uses the output of the preceeding one, or remember that filter functions can take more than one arguement - see the example above. Also, because the JudgedSpeed and HitOrMiss are Integer you will need == instead of just =. And not Or. Mistakes with filter() The filter function is very useful but if used wrongly can give you very misleading findings. This is why it is very important to always check your data after you perform an action. Lets say you are working in comparative psychology and have run a study looking at how cats, dogs and horses perceive emotion. Lets say the data is all stored in the tibble animal_data and there is a column called animals that tells you what type of animal your participant was. Something like this: Participant ID animals Perceived Emotion Accuracy (%) 1 dog 80 2 dog 90 3 cat 10 4 dog 85 5 horse 100 6 cat 6 Ok, so imagine you wanted all the data from just cats filter(animal_data, animals == \"cat\") Exactly! But what if you wanted cats and dogs? filter(animal_data, animals == \"cat\", animals == \"dog\") Right? Wrong! This actually says give me everything that is a cat and a dog. But nothing is a cat and a dog, that would be weird - like a dat or a cog! What you actually want is everything that is either a cat or a dog, which is stated as: filter(animal_data, animals == \"cat\" | animals == \"dog\") The vertical line | is the symbol for or, just as &amp; is the symbol for and. TOP TIP: Always pay attention to what you want and most importantly to what your code produces. 2.2.6 The mutate() Function - for adding new columns The mutate() function lets us create a new variable in our dataset. For example, lets add a new column to pong_data in which the background color is represented by numbers, where red will be represented as 1, and blue will be represented as 2. pong_data &lt;- mutate(pong_data, BackgroundColorNumeric = recode(BackgroundColor, &quot;red&quot; = 1, &quot;blue&quot; = 2)) The code here is is a bit complicated but: BackgroundColorNumeric is the name of the new column you are adding to the tibble, BackgroundColor is the name of the original column in the tibble and the one to take information from, and 1 and 2 are the new codings of red and blue respectively. The mutate() function is also handy for making some calculations on or across columns in your data. For example, say you realise you made a mistake in your experiment where your participant numbers should be 1 higher for every participant, i.e. Participant 1 should actually be numbered as Participant 2, etc. You would do something like: mutate(pong_data, Participant = Participant + 1) Note here that the new column has the same name as the old column, i.e. Participant. In the resulting table, the Participant column will have the new values which will differ from the values in the original pong_data table. While it may seem like you have overwritten these values, in reality you have created a copy of the table with altered values, but you have not lost anything: the original values are still there in pong_data because you didnt store (assign) this action to pong_data. You didnt save the change basically. In general however it is good practice not to overwrite pong_data with a new version of pong_data, but to store the altered table in a new tibble, e.g., pong_data2, like this: pong_data_mutated &lt;- mutate(pong_data, Participant = Participant + 1) Task 4: Mutating Variables with mutate() You realise another mistake in that all your trial numbers are wrong. The first trial (trial number 1) was a practice so should be excluded. And your experiment actually started on trial 2. Tidy this up by: Creating a new tibble called pong_data_filt and in it store data from pong_data after filtering out all trials with the number 1 (TrialNumber column). Now use the mutate() function to renumber all the remaining trial numbers, in pong_data_filt, starting them at one again instead of two. Store this output in a new tibble called pong_data2. Helpful Hint Step 1: filter(TrialNumber does not equal 1). remember to store this output in a tibble called pong_data_filt Step 2: mutate(TrialNumber = TrialNumber minus 1) exclamation mark, equals 2.2.7 The group_by() Function - to group parts of data altogether The group_by() function groups the rows in a dataset according to a category you specify, e.g. in the animals example above, grouping all cat data together and all dog data together, and all horse data together. Looking at the data within pong_data2, say you wanted to eventually creating means, etc, for the different background color conditions. You would start by grouping trials by BackgroundColor, grouping the data into red background data and blue background data, as such: group_by(pong_data2, BackgroundColor) Or you can add numerous grouping variables depending on how you want to split up the data. Here we group by Hit Or Miss (in HitOrMiss column), and background color (Red or Blue). This gives four grousp - Hit Red, Miss Red, Hit Blue, Miss Blue. group_by(pong_data2, HitOrMiss, BackgroundColor) Note: Nothing actually appears to change in the data, unlike with the other functions, but a big operation has taken place. Look at the output in your console when you run group_by(pong_data2, BackgroundColor). At the top of the output notice that the 2nd line of the output tells us the grouping criteria and how many groups now exist: see the line Groups: BackgroundColor [2]: we grouped by BackgroundColor and there are [2] groups - one for red and one for blue. Task 5: Grouping Data with group_by() Group the data by BlockNumber and by BackgroundColor, in that order, and then enter the number of groups (i.e. a number) you get as a result: Helpful Hint It is the same procedure as this but with different column names: group_by(pong_data2, HitOrMiss, BackgroundColor) The number of groups should be between the sum (i.e. multiplication) of the number of background colors (red and blue) and the number of blocks (12). group_by() is incredibly useful as, once the data is organised into groups, you can then apply other functions (filter, arrange, mutateetc.) to the groups within your data that you are interested in, instead of to the entire dataset. For instance, a common second step after group_by might be to summarise the data 2.2.8 The summarise() Function - to do some calculations on the data The summarise() function lets you calculate some descriptive statistics on your data. For example, say you want to count the number of hits there were for different paddle lengths, or number of hits there were when the background color was red or blue. First we group the data accordingly, storing it in pong_data2_group pong_data2_group &lt;- group_by(pong_data2, BackgroundColor, PaddleLength) And then we summarise it, storing the answer in total_hits pong_data2_hits &lt;- summarise(pong_data2_group, total_hits = sum(HitOrMiss)) And then for fun we can filter just the red, small paddle hits from the summarised data. pong_data2_hits_red_small &lt;- filter(pong_data2_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 50) ## `summarise()` has grouped output by &#39;BackgroundColor&#39;. You can override using the `.groups` argument. ## `summarise()` has grouped output by &#39;BackgroundColor&#39;. You can override using the `.groups` argument. Which would leave us with: Table 2.1: Summarising with group_by() and summarise() BackgroundColor PaddleLength total_hits red 50 516 The name of the column within pong_data2_hits_red_small that has the summarised data is total_hits; this is what you called it when creating pong_data2_hits. You could have called it anything you wanted but always try to use something sensible. Make sure to call your variables something you (and anyone looking at your code) will understand and recognize later (i.e. not variable1, variable2, variable3. etc.), and avoid spaces (use_underscores_never_spaces). summarise() has a range of internal functions that make life really easy, e.g. mean(), median(), n(), sum(), max, min, etc. Some common ones are shown below but see the dplyr cheatsheets for more examples. function example purpose n() N = n() number of values mean() m = mean(X) mean of values in stated column - e.g. column X median() mdn = median(X) median of values in stated column - e.g. column X sum() sm = sum(X) sum of values in stated column - e.g. column X max() mx = max(X) max of values in stated column - e.g. column X min() mn = min(X) min of values in stated column - e.g. column X sd() stdev = sd(X) standard deviation of values in stated column - e.g. column X Task 6: Summarising Data with summarise() Use the lines of code above to calculate the mean number of hits made with the small paddle (50) and the red color background. Enter that value in this box to two decimal places - (e.g. 0.12): A quick point on the ungroup() function The ungroup() undoes the action of the group_by() function. After grouping data together using the group_by() function and then peforming a task on it, e.g. filter(), summarise(), it can be very good practice to ungroup the data before performing another function. Forgetting to ungroup the dataset wont always affect further processing but sometimes it can really mess up other things. Again just a good reminder to always check the data you are getting out of a function a) makes sense and b) is what you expect. Quickfire Questions Which of the Wickham Six would I use to sort columns from smallest to largest: select filter mutate arrange group_by summarise Which of the Wickham Six would I use to calculate the mean of a column: select filter mutate arrange group_by summarise Which of the Wickham Six would I use to remove certain observations - e.g. remove all males: select filter mutate arrange group_by summarise 2.2.9 Two Other Useful Functions The Wickham Six verbs let you to do a lot of things with data however there are thousands of other functions at your disposal. If you want to do something with your data that you are not sure how to do using these functions, do a Google search for an alternative function - chances are someone else has had the same problem and has a help guide. For example, two other functions to note are the bind_rows() function and the count() function. We will breidly show you these here. Binding columns with bind_rows() The bind_rows() function is useful if you want to combine two tibbles together into one larger tibble that have the same column structure - i.e. the have exactly the same columns and you want to combine them by attaching one to the bottom of the other. For example: Say you had on tibble with data for ball speeds 1 and 2 slow_ball&lt;- filter(pong_data2, BallSpeed &lt; 3) And another tibble with data for ball speeds 6 and 7 fast_ball &lt;- filter(pong_data2, BallSpeed &gt;= 6) And you wanted to combine those tibbles together into one big tibble containing these extreme ball speeds. extreme_balls &lt;- bind_rows(slow_ball, fast_ball) We are going to use bind_rows() throughout the labs so do keep it in mind! Quick counts with the count() function Finally, the count() function is a shortcut that can sometimes be used to count up the number of rows you have for groups in your data, without having to use the group_by() and summarise() functions. It is a tally basically. It doesnt sum up values. It just counts how many observations you have. For example, in Task 6 we combined group_by() and summarise() to calculate how many hits there were based on background color and paddle length. Alternatively we could have done: count(pong_data2, BackgroundColor, PaddleLength, HitOrMiss) The results are the same, just that in the count() version we get all the information, including misses, because we are just counting rows. In the summarise() method we only got hits because that was the effect of what we summed. So two different methods give similar answers - coding can be individualised and get the same result! Again, we will use count() at various times in the labs so again it is handy to make a note of. 2.2.10 Last but not least - Pipes (%&gt;%) to make your code efficient By now youll have noticed thattidyverse functions generally take the following grammatical structure (called syntax): function_name(dataset, arg1, arg2,..., argN) where the dataset is the entire tibble of data you are using, and each argument (arg) is some operation on a particular column or variable, or the column name you want to work with. For example: filter(pong_data2, PaddleLength == &quot;50&quot;, BallSpeed &gt; 4) group_by(pong_data2, BallSpeed, Participant) Both of these examples follow the structure of function_name(dataset, arg1, arg2, ....) In the first example, we are filtering (function) the whole pong_data2 dataset by a particular paddle length, then by particular speeds (arguments). In the second, we are grouping by BallSpeed and then by Participant. Note that the order of arguments is specific as it performs argument1 then argument2, etc. Changing the order of arguments may give a different output. So the order you work in is important, and this is called your pipeline. For example, here is a pipeline we used above to find how many hits there were with the small paddle length and the red background. First we group the data accordingly, storing it in pong_data2_group And then we summarise it, storing the answer in total_hits And filter just the red, small paddle hits pong_data2_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) pong_data2_hits &lt;- summarise(pong_data2_group, total_hits = sum(HitOrMiss)) pong_data2_hits_red_small &lt;- filter(pong_data2_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 50) Pipelines allow us to quickly and reproducibly, perform an action that would take much longer manually. However, we can make our code even more efficient, using less code, by stringing our sequence of functions together using pipes, written as %&gt;%. Changing the above code into one using pipes would give us: pong_data_hits_red_small &lt;- pong_data2 %&gt;% group_by(BackgroundColor, PaddleLength) %&gt;% summarise(total_hits = sum(HitOrMiss)) %&gt;% filter(BackgroundColor == &quot;red&quot;, PaddleLength == 50) Both these chunks show exactly the same procedure, but adding pipes can make code easier to read and follow once you understand piping. Code without a pipe would look like function_name(dataset, arg1, arg2,...,argN) but a pipe version would look like dataset %&gt;% function_name(arg1, arg2,...,argN) The premise is that you can pipe (%&gt;%) between functions when the input of a function is the output of the previous function. Alternatively, you can use a pipe to put the data into the first function, as shown directly above. You can think of the pipe (%&gt;%) as saying and then or goes into. E.g. the data goes into this function and then into this function and then into this function. We will expand on this in the lab where you can ask more questions, but try comparing the two chunks of code above and see if you can match them up. One last point on pipes is that they can be written in a single line of code but its much easier to see what the pipe is doing if each function takes its own line. Every time you add a function to the pipeline, remember to add a %&gt;% first and note that when using separate lines for each function, the %&gt;% must appear at the end of the line and not the start of the next line. Compare the two examples below. The first wont work but the second will because the second puts the pipes at the end of the line where they need to be! Example 1: wont work because the pipes (%&gt;%) are in the wrong place. data_arrange &lt;- pong_data2 %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) Example 2: will work because the pipes (%&gt;%) are in the correct place. data_arrange &lt;- pong_data2 %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) Portfolio Point - Pipes are good for the Environment Where piping becomes most useful is when we string a series of functions together, rather than using them as separate steps and having to save the data each time under a new tibble name and getting ourselves all confused. In the non-piped version we have to create a new tibble each time, for example, data, data_filtered, data_arranged, data_grouped, data_summarised just to get to the final one we actually want, which was data_summarised. This creates a lot of tibbles in our environment and can make everything unclear and eventually slow down our computer. The piped version however uses one tibble name, saving space in the environment, and is clear and easy to read. With pipes, we skip unnecessary steps and avoid cluttering our environment. Quickfire Questions What does this line of code say? data %&gt;% filter() %&gt;% group_by() %&gt;% summarise(): take the data and group it and then filter it and then summarise it take the data and filter it and then group it and then summarise it take the data and summarise it and then filter it and then group it take the data and group it and then summarise it and then filter it Job Done - Activity Complete! We have now recapped a number of functions and verbs that you will need as the semester goes on. You will use them in upcoming labs and chapters so be sure to go over these and try them out to make yourself more comfortable with them. Remember to also start looking back at the Grassroots book and remembering some of the work you did there. If you have any questions please post them on the forums. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Happy Wrangling! 2.3 InClass Activity Data Wrangling In the PreClass activity we looked at a series of functions known as the Wickham six one-table to filter, arrange, group_by, select, mutate and summarise. Now we will focus on working with data across two or more tables using functions you will have come across in the Grassroots book. The two main functions we will add to the Wickham six today are pivot_longer() and inner_join(). pivot_longer() allows us to transform a table from wide format to long format (more on this below). inner_join() allows us to combine two tables together based on common columns. Portfolio Point - Still not sure what a function is and how to remember them? A function is a tool that takes an input, performs some action, and gives an output. They are nothing more than that. If you think about it, your toaster is a function: it takes bread as an input; it perfoms the action of heating it up (nicely sometimes; on both sides would be a luxury); and it gives an output, the toast. A good thing about the Wickham six functions is that they are nicely named as verbs to describe what they do - mutate() mutates (adds on a column); arrange() arranges columns, summarise() summarises, etc. In terms of remembering all the functions, the truth is you dont have to know them all or remember them all. However, through practice and repetition, you will quickly learn to remember which ones are which and what package they come from. Sort of like where to find your spoons in your kitchen - you dont look in the fridge, and then the washing machine, and then the drawer. Nope, you learnt, by repetition, to look in the drawer first time. Its the same with functions. Keep in mind that research methods is like a language in that the more you use it and work with it the more it makes sense. A Note on Tidy Data In the style of programming we teach, the most efficient format/layout of data is what is known as Tidy Data, and any data in this format is easily processed through the tidyverse package. You can read more about this type of data layout in this paper: Tidy Data (Wickham, 2014). It is a surprisingly good read. However, the data you work with will not always be formatted in the most efficient way possible. If that happens then our first step is to put it into Tidy Data format. There are two fundamental principles defining Tidy Data: Each variable must have its own column. Each observation must have its own row. Tidy Data (Wickham, 2014) adds the following principle: Each type of observation unit forms a table. And Grolemund and Wickham (2017) restate this third principle as: Each value must have its own cell (i.e. no grouping two variables together, e.g. time/date in one cell). Where a cell is where any specific row and column meet; a single data point in a tibble is a cell for example. The Grolemund and Wickham (2017) book is a very useful read and it is free, but browsing the chapter on Tidy Data will help you visualise how you want to arrange data. Try to keep the principles in mind whilst doing so. Explain this - If it isnt Tidy then what is it? We use Tidy Data because it is really efficient and works well with the tidyverse. However, people used to use data structured in long format or wide format. Long format is where each row is a single observation, typically a single trial in an experiment or a response to a single item on a questionnaire. When you have multiple trials per participant, you will have multiple rows for the same participant. To identify participants, you would need a variable with some kind of participant id, which can be as simple as a distinct integer value for each participant. In addition to the participant identifier, you would have any measurements taken during each observation (e.g., response time) and what experimental condition the observation was taken under. In wide format data, each row corresponds to a single participant, with multiple observations for that participant spread across columns. So for instance, with survey data, you would have a separate column for each survey question. Tidy is a mix of both of these approachs and most functions in the tidyverse assume the tidy format, so typically the first thing you need to do when you get data, particularly wide-format data, is to reshape it through wrangling. Which is why we teach these really important skills. Todays Lab - Analysing the Autism Specturm Quotient (AQ) To continue building your data wrangling skills we will recap on skills from Level 1 by tidying up data from the Autism Spectrum Quotient (AQ) questionnaire. If you have completed the Grassroots book then you may be familiar with the AQ10; a non-diagnostic short form of the AQ with only 10 questions per participant. It is a discrete scale and the higher a participant scores on the AQ10 the more autistic-like traits they are said to display. Anyone scoring 7 or above is recommended for further diagnosis. You can see an example of the AQ10 through this link: AQ10 Example. Remember you can revisit the Grassroots book at any time but we will recap here for you. Today we have 66 participants and your goal in this lab is to find an AQ score for each of them through your data-wrangling skills. We have four data files to work with: responses.csv containing the AQ survey responses to each of the 10 questions for our 66 participants qformats.csv containing information on how a question should be coded - i.e. forward or reverse coded scoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded pinfo.csv containing participant information such as Age, Sex and importantly ID number. Click here to download the files as a zip file. Now unzip the files into a folder you have access to (e.g. your M: drive on campus). We will use zip folders a lot so if this is something you struggle with please ask. Portfolio Point - Open Data is best in csv format csv stands for comma separated values and is a very basic format for storing data in a plain text file. It really just stores numbers and text separated by commas and nothing else. The great thing about being this basic is that it can be read by many different systems and is non-proprietary, i.e., you dont need to purchase commercial software to open it. Now set your working directory to the folder where you saved the .csv files. Do this through the dropdown menus at the top toolbar: Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory and then find your folder with your .csv files. Remember what we previously said about folder structure - use a drive you have access to (e.g. M: drive in the labs, but never call your folders R). Today we will work in an RScript instead of .Rmd but if you want to turn the lab into an R Markdown report or to add elements to your Portfolio then please feel free. Group Discussion Point Now would be a good time to make sure that you are all using RStudio effectively and know what each window does. TRUE or FALSE, the Console is best for practice and the Script Window is for saving: TRUE FALSE TRUE or FALSE, the Environment holds all the data and objects you have loaded in and created: TRUE FALSE TRUE or FALSE, clicking the name of a table in the Environment window will open it in the Script window: TRUE FALSE Explain this - I dont get these answers The answer to all of these are True. The Script window is where you should write code and comments that you are going to save and send to people. The Console is where you should practice stuff - nothing is saved here; it is like a sandbox that just gets wiped away. Any data you load in or create is held in the Environment (or Global Environment) window with the variable name that you gave it. By clicking the name of the table in the Environment window it will open up in the Script window and you can look at it to make sure it is what you expect. This only works for tables but not for other types of data. You will learn the difference as we go along! 2.3.1 Task 1: Open a Script Start a new RScript and save it in the same folder as your .csv files, calling the RScript something informative like Lab2_AQ_DataWrangling.R. Make sure your environment is completely empty so we dont mix up one analysis with the other. You can run the following code line in the console to clear the environment or by clicking the little brush on your environment window. rm(list = ls()) Portfolio point - comments on scripts and running lines Remember that when using a script you can write notes to yourself to remind you what a line of code does. Just put a hashtag at the start of the line and R will ignore this line. This is where you have to be clear on using a Script versus an R Markdown file. In a Script, # means the line is ignored, in Markdown # sets the line as a header!. To run any line on a script, the simplest way is to click anywhere on that line and either press Run on the top of the script window or press CTRL+Enter on the keyboard (or mac equivalent). 2.3.2 Task 2: Bring in Your Library Add a line to your code that brings the tidyverse package into your working environment and run it. Helpful Hint - on Library vs Install Combine the function library() and the package tidyverse and remember that the solutions are at the end of the chapter. On our lab machines in Psychology all the necessary packages will already be on the machines, they just need called into the library. If however you are using your own machine you will have to install the packages first. Do not install packages on the Psychology machines! Why? They are already installed and can cause the package to stop working if a student tries to install the same package on our machines. They are already installed and it is a bit like using apps on your phone. Install is putting the app onto your phone, library is just opening the app. If youve already downloaded the app (package) then you just need to open it (library()) to use it! 2.3.3 Task 3: Load in the Data Now we have to load in the .csv datafiles using the read_csv() function and save them as tibbles in our environment. For example, to load in the data in responses.csv and save it as the tibble responses we would type: responses &lt;- read_csv(&quot;responses.csv&quot;) Add the following lines of code to your script and complete them to load in all four .csv datafiles. Use the above code as an example and name each tibble the same as its original filename (minus the .csv part), again as above, e.g. responses.csv gets saved as responses. Remember to run the lines so that the data is loaded in and stored in your environment. responses &lt;- read_csv() # survey responses qformats &lt;- # question formats scoring &lt;- # scoring info pinfo &lt;- # participant information Portfolio Point - Havent I read_csv before As you work with data and functions you will find there are functions with similar names but that give different results. One of these is the read function for csv. Make sure to always use read_csv() as your function to load in csv files. Nothing else. It is part of the readr package automatically brought in with tidyverse. There is a very similarly named function called read.csv(). DO NOT use this function in these labs. We will always expect you to use read_csv(). Although very similar in name they do not work the same way and create differences in your data. 2.3.4 Task 4: Review Your Data. Group Discussion Point Now that we have the data loaded in it is always best to have a look at the data to get an idea of its layout. We showed you one way before, by clicking on the name in the environment, but you can also use the glimpse() or View() functions in your Console window. Put the name of the data between the brackets to see how it is arranged. Dont add these to your script though - they are just one-offs for testing. As a small group, have a look at the data in responses to see if you think it is Tidy or not and answer the following question: The data in responses is in Tidy Long Wide format Explain This - I dont get why? The reponses tibble is far from being tidy; each row represents multiple observations from the same participant, i.e. each row shows responses to multiple questions - wide format. Remember we want the data in tidy format as described above. Eh, remind whats a tibble? A tibble is simply a dataframe - or a table of data with columns and rows - that is really handy for working with when using the tidyverse package. When we say tibble, you can think of a dataframe with rows and columns of information and numbers stored in them - like responses, it is a tibble. For more info, see here: Tibbles 2.3.5 Task 5: Gathering Data with pivot_longer(). We now have all the data we need loaded in, but in order to make it easier for us to get the AQ score for each participant, we need to change the layout of the responses tibble using the pivot_longer() function. Copy the below code line to your script and run it. rlong &lt;- pivot_longer(responses, cols = Q1:Q10, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;) The first argument given to the pivot_longer() function is the tibble which holds the data we want to wrangle, responses. Remember we could have written this with a pipe as well, e.g. rlong &lt;- responses %&gt;% pivot_longer(...) The second argument is the names of specific columns in the original tibble that we want to gather together, Q1:Q10 meaning all columns between Q1 and Q10. You do not actually need to write cols = but it makes things clearer. Gathering of columns is based on position in the tibble. If the order of columns in the tibble was Q1 then Q10, the above code would only gather those two columns. As it is, in our tibble, the order, is Q1, Q2, Q3,  Q10, and therefore the code gathers all the columns between Q1 and Q10. Colum names are not put in quotes because they exist already in the tibble responses. The third and fourth arguments are the names of the new columns we are creating; the first will store the question numbers, Question. I.e. put the question names (names_to = ...) in a column called Question. the second will store the values/responses, Response. I.e. put the values/responses to the questions (values_to = ...) in a column called Response. These new column names are put in quotes because they do not already exist in the tibble. This is not always the case but is the case for this function. Note that these names could have been anything but by using these names the code makes more sense. Lastly, you do need to write names_to = ... and values_to = ... otherwise the columns wont be created correctly. In case you are wondering, if we wanted to go back the way and ungather the data we just gathered, we would use the pivot_wider() function: e.g. rwide &lt;- rlong %&gt;% pivot_wider(names_from = Question, values_from = Response). But we do not want to do that here so lets not add this to the code. Quickfire Questions Lets see if you understand pivot_longer(). Say I wanted to gather the first three columns of responses (Q1, Q2, Q3), put the question numbers in a column called Jam, the responses in a column called Strawberry, and store everything in a tibble called sandwich. Fill in the box with what you would write: Explain this - I dont get the right answer! sandwich &lt;- pivot_longer(responses, cols = Q1:Q3, names_to = Jam, values_to = Strawberry) pivot_longer() wants the data first, then the columns to gather, then the name of the new column to store the gathered column names in and finally the name of the new column to store the values in. 2.3.6 Task 6: Combining Data. So now our responses data is in tidy format, we are closer to getting an AQ score for each person. However, we still need to add some information to: show if the question is reverse or forward scored - found in qformats show the number of points to give a specific response - found in scoring. This is a typical analysis situation where different information is in different tables and you need to join them altogether. Both these pieces of information are contained in qformats and scoring respectively, but we want to join it to the data in rlong to create one informative tidy table with all the info. We can do this sort of join through the function inner_join(); a function to combine information in two tibbles using a column (or columns) common to both tibbles. Copy the below line into your code and run it. This piece of code combines rows in the tibble rlong with rows in the tibble qformats, based on the common column Question. rlong2 &lt;- inner_join(rlong, qformats, &quot;Question&quot;) Now have a look in rlong2. We have matched each question with its scoring format, forward or reverse. Portfolio Point - Reverse and Forward A lot of questionnaires have some questions that are Forward scored and some questions that are Reverse scored. What does this mean? Imagine a situation where your options in replying to a question are: 1 - extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely disagree. In a forward-scoring question you would get 1 point for extremely agree, 2 for agree, 3 for neutral, etc. In a reverse scoring question you would get 5 for extremely agree, 4 for agree, 3 for neutral, etc. The reasoning behind this shift is that sometimes agreeing or disagreeing might be more favourable depending on how the question is worded. Secondly, sometimes these questions are used just to catch people out - imagine if you had two similar questions where one has the reverse meaning of the other. In this scenario, people should respond opposites. If they respond the same then they might not be paying attention. Now we need to combine the information in our table, rlong2, with the scoring table so we know how many points to attribute each question based on the answer the participant gave, and whether the question was forward or reverse coded. Again, we use the inner_join() function, but this time the common columns found in rlong2 and scoring are QFormat and Response. To combine by two columns you just write them in sequence as shown below. Note: when there is more than one common column between two tibbles you are joining, you should combine by ALL the columns to avoid repeat columns in the new tibble. If you forget to do this, your new tibble will have names such as column_name.x and column_name.y. This will cause confusion so avoid it by combining by all common columns. Copy the below line into your code and run it. This code combine rows in rlong2 and scoring based on the commn columns, QFormat and Response. rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 2.3.7 Task 7: Calculating the AQ Scores. We have now created rscores which has information on how each participant responded to each question and how each question should be coded and scored, all within the one tibble. All we need now is to sum the scores for each participant to get their AQ score. Based on your PreClass learning, copy the below line into your code and complete it to obtain individual aq_scores for each participant. Save your script and run it all again from the start to make sure it works! aq_scores &lt;- rscores %&gt;% group_by() %&gt;% # how will you group individual participants? summarise(AQ = sum()) # which column will you sum to obtain AQ scores? Helpful Hint Each participant could be grouped by their Id. If we summed up the value for each Score we might get a full AQ Score for each particpipant. Portfolio Points - Hang on isnt that a Pipe? Yep, well spotted. Pipes are your friend. Think of them as saying and then or goes into. So in the example above we take rscores and then group it by something and then summarise it into AQ scores based on In most cases, the pipe serves the purpose of putting the input into the function or taking the output of one function and treating it as the input of another function. In the example above the first pipe takes rscores as the input for group_by, and the second pipe takes the output of group_by and puts it as the input to summarise. See how you can almost read it as a chain of actions or steps. Quickfire Questions The whole purpose of this lab was to calculate AQ scores for individual participants. As a small group, try to answer the following questions. Try to do it using code where possible to help you based on your knowledge from the preclass and inclass activity. Remember the cheatsheets as well. Look for the dplyr one! From the options, choose the correct citation for the AQ 10 question questionnaire: Allison, Auyeung, and Baron-Cohen, (2011) Allison, Auyeung, and Baron-Cohen, (2012) Allison and Baron-Cohen, (2012) Auyeung, Allison, and Baron-Cohen, (2012) Complete the sentence, the higher the AQ score the less autistic-like traits displayed has no relation to autistic-like traits the more autistic-like traits displayed Type in the AQ score (just the number) of Participant ID No. 87: Type how many participants had an AQ score of 3 (again just the number): The cut-off for the AQ10 is usually said to be around 6 meaning that anyone with a score of more than 6 should be referred for diagnostic assessment. Type in how many participants we should refer from our sample: Explain This - I dont get these answers From the link above you can see that an appropriate citation for the AQ10 would be (Allison, Auyeung, and Baron-Cohen, (2012)) As mentioned, the higher the score on the AQ10 the more autistic-like traits a participant is said to show. You could do this by code with filter(aq_scores, Id == 87), which would give you a tibble of 1x2 showing the ID number and score. If you just wanted the score you could use pull() which we havent shown you yet but works as follows: filter(aq_scores, Id == 87) %&gt;% pull(AQ). The answer is an AQ score of 2. Same as above but changing the argument of the filter. filter(aq_scores, AQ == 3) %&gt;% count(). The answer is 13. Remember you can do this by counting but the code makes it reproducible every time. filter(aq_scores, AQ &gt; 6) %&gt;% count() or filter(aq_scores, AQ &gt;= 7) %&gt;% count(). The answer is 6. 2.3.8 Task 8: One Last Thing on Pipes You now have a complete code to load in your data, convert it to Tidy, combine the tables and calculate an AQ score for each participant. But, if you look at it, some of your code could be more efficient by using pipes. Go back through your code and rewrite it using pipes %&gt;% so that it is as efficient as possible. Helpful Hint At any point where the first argument of your function is the name of a variable created before that line, there is a good chance you could have used a pipe! Here are all the bits of this code that could be piped together into one chain: rlong &lt;- pivot_longer(responses, cols = Q1:Q10, names_to = Question, values_to = Response) rlong2 &lt;- inner_join(rlong, qformats, Question) rscores &lt;- inner_join(rlong2, scoring, c(QFormat, Response)) aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Job Done - Activity Complete! You have now recapped on a number of handy one-table and two-table verbs. These are great to know as for example, in the above Activity, it actually only took a handful of reproducible steps to get from messy data to tidy data; could you imagine doing this by hand in Excel through cutting and pasting? Not to mention the mistakes you could make! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your data-wrangling skills which you will need in future assignments. If you have any questions, please post them on the forums. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Happy wrangling! Excellent work! You are a DataWrangling expert! Now go try the assignment! 2.4 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. One thing to keep in mind in all assignments is that spelling it really important when learning about being reproducible. Remember that Mycolumn is a different column to mycolumn and when being reproducible only one is correct. Likewise, Mydata and mydata are two different tibbles, and read_csv() and read.csv() are two different functions. Check the instructions and check your work! Lab 2: Formative Data Wrangling Assignment In order to complete this assignment you will need to download the data .csv files, as well as the assignment .Rmd file, which you need to edit, titled GUID_Level2_Semester1_Lab2.Rmd. These can be downloaded within a zip file from the below link. Once downloaded and unzipped, you should create a new folder that you will use as your working directory; put the data files and the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here. Now open the assignment .Rmd file within RStudio. You will see there is a code chunk for each task. As you did in previous assignments, follow the instructions on what to edit in each code chunk. This will often be entering code based on what we have covered up until this point, either entering code or a value. In this chapter we recapped on data-wrangling using the Wickham 6 verbs, looked at additional functions such as pivot_longer() and inner_join(), and at piping chains of code for efficiency using %&gt;%. You will need these skills to complete the following assignment so please make sure you have carried out the PreClass and InClass activities before attempting this formative assignment. Remember to follow the instructions and if you get stuck at any point to post questions on the available forums. Also, two useful online resources are: Hadley Wickhams R for Data Science book @ http://r4ds.had.co.nz RStudios dplyr cheatsheet @ Rstudio.com As well as the PsyTeachR Grassroots book and remembering some of the work you did there. 2.4.1 Todays Topic - The Ageing Brain A key topic in current psychologial research, and one which forms a main focus for some of the research in our School, is that of human ageing. In this research we use brain imaging techniques to understand how changes in brain function and structure relate to changes in perception and behaviour. A typical ageing experiment will compare a measure (or a number of measures) such as performance on a cognitive or perceptual task between younger and older adults (i.e. a between-subjects design experiment). However, in order to make sure we are studying healhty ageing, we first have to screen our older participants for symptoms of age-related dementia (Alzheimers Disease), where cognitive function can be significantly impaired. We do this using a range of cognitive tests. Some studies will also test participants sensory acuity (ability to perceive something), as a function of age (particularly eyesight and hearing). The data you have downloaded for this lab is example screening data taken from research investigating how the ageing brain processes different types of sounds. The tests used in this study are detailed below. Please note that the links are there to provide you with further information and examples of the tests once you have completed the assignment if you so wish; you do not have to read them to complete the assignment. Montreal Cognitive Assessment (MoCA) : a test specifically devised as a stand-alone screening tool for mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, and abstraction skills. Example here Working Memory Digit Span Test (D-SPAN): measures the capacity of participants short-term (working) memory. D2 Test of Attention: measures participants selective and sustained concentration and visual scanning speed. Better Hearing Institute Quick Hearing Check: a self-report questionnaire which measures participants subjective experience of their own hearing abilities. Main Webpage for Group here The Data Files You have just downloaded the three .csv files containing all the data you need. Below is a list of the .csv file names and a description of the variables each contains: p_screen.csv contains particpants demographic information including: ID Participant Id number - for confidentiality (no names or other identifying info) AGE in years SEX M for male, F for female HANDEDNESS L for left-handed, R for right-handed EDUCATION in years MUSICAL whether they have any musical abilties/experience (YES or NO) FLANG speak any foreign languages (YES or NO) MOCA Montreal Cognitive Assessment score D-SPAN Working Memory Digit Span test score D2 D2 Test of Attention score QHC_responses.csv contains participants responses to each question on the Better Hearing Institute Quick Hearing Check (QHC) questionnaire. Column 1 represents participants ID (matching up to that in p_screen.csv). Each column thereafter represents the 15 questions from the questionnaire. Each row represents a participant and their response to each question. QHC_scoring.csv contains the scoring key for each question of the QHC, with the columns: RESPONSE the types of responses participants could give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE) SCORE the points awarded for each response type (from 0 to 4). A score for each participant can be calculated by converting their categorical responses to values and summing the values. Before starting lets check: The .csv files are saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Semester1_Lab2.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 2.4.2 Load in the data You will see a code chunk called libraries, similar to the one below, at the top of your .Rmd assignment file. It is all set-up to load in the data for you and to call tidyverse to the library(). Run this code chunk now to bring in the data and tidyverse. You can do this in the console, in a script, or even through the code chunk by clicking the small green play symbol in the top right of the code chunk. library(&quot;tidyverse&quot;) screening &lt;- read_csv(&quot;p_screen.csv&quot;) responses &lt;- read_csv(&quot;QHC_responses.csv&quot;) scoring &lt;- read_csv(&quot;QHC_scoring.csv&quot;) View the data It is always a good idea to familiarise yourself with the layout of the data that you have just loaded in. You can do this through using glimpse() or View() in the Console window, but you must never put these functions in your assignment file. The Tasks: Now that we have the data loaded, tidyverse attached, and have viewed our data, you should now try to complete the following 9 tasks. You may want to practice them first to get the correct code and format, and to make sure they work. You can do this in the console or a script, but remember, once you have the correct code, edit the necessary parts of the assignment .Rmd file to produce a reproducible Rmd file. This is what you will do from now on for all other assessment files so practicing this now will really help. In short, go through the tasks and change only the NULL with what the question asks for and then make sure that the file knits at the end so that you have a fully reproducible code. 2.4.3 Task 1 - Oldest Participant Replace the NULL in the T1 code chunk with the Participant ID of the oldest participant. Store this single value in oldest_participant (e.g. oldest_participant &lt;- 999. hint: look at your data, who is oldest? oldest_participant &lt;- NULL 2.4.4 Task 2 - Arranging D-SPAN Replace the NULL in the T2 code chunk with code that arranges participants D-SPAN performance from highest to lowest using the appropriate one-table dplyr (i.e., Wickham) verb. Store the output in cogtest_sort. (e.g. cogtest_sort &lt;- verb(data, argument)) hint: arrange your screening data cogtest_sort &lt;- NULL 2.4.5 Task 3 - Foreign Language Speakers Replace the NULL in each of the two lines of code chunk T3, so that descriptives has a column called n that shows the number of participants that speak a foreign language and number of participants that do not speak a foreign language, and another column called median_age that shows the median age for those two groups. If you have done this correctly, descriptives should have 3 columns and 2 rows of data, not including the header row. hint: First need to group_by() foreign language hint: Second need to summarise(). You will need the n() function. Pay attention to specific column names given. screen_groups &lt;- NULL descriptives &lt;- NULL 2.4.6 Task 4 - Creating Percentage MOCA scores Replace the NULL in the T4 code chunk with code using one of the dplyr verbs to add a new column called MOCA_Perc to the dataframe screening In this new column should be the MOCA scores converted to percentages. The maximum achievable score on MOCA is 30 and percentages are calculated as (participant score / max score) * 100. Store this output in screening. hint: mutate() something using MOCA and the percentage formula screening &lt;- NULL 2.4.7 Task 5 - Remove the MOCA column Now that we have our MoCA score expressed as a percentage MOCA_Perc we no longer need the raw scores held in MOCA. Replace the NULL in the T5 code chunk using a one-table dplyr verb to keep all the columns of screening, with the same order, but without the MOCA column. Store this output in screening. hint: select your columns screening &lt;- NULL Halfway There! The remaining tasks focus on merging two tables. You suspect that the older adults with musical experience might report more finely-tuned hearing abilities than those without musical experience. You therefore decide to check whether this trend exists in your data. You measured participants self reported hearing abilties using the Better Hearing Institute Quick Hearing Check Questionnaire. In this questionnaire, participants rated the extent to which they agree or disagree with a list of statements (e.g. I have a problem hearing over the telephone) using a 5 point Likert scale (Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree). Each participants response to each question is contained in the responses dataframe in your environment. Each response type is worth a certain number of points (e.g. Strongly Disagree = 0, Strongly Agree = 5) and the scoring key is contained in the scoring dataframe. A score for each participant is calculated by totalling up the number of points across all the questions to derive an overall score. The lower the overall score, the better the participants self-reported hearing ability. In order to score the questionnaire we first need to perform a couple of steps. 2.4.8 Task 6 - Gather the Responses together Replace the NULL in the T6 code chunk using code to gather the responses to all the questions of the QHC from wide format to tidy/long format. Put the names in Question and the values in RESPONSE. Store this output in responses_long. hint: pivot_lnger() hint: names to Question hint: values to RESPONSE responses_long &lt;- NULL 2.4.9 Task 7 - Joining the data Now we need to join the number of points for each response in scoring to the participants responses in responses_long. Replace the NULL in the T7 code chunk using inner_join() to combine responses_long and scoring into a new variable called responses_points. hint: join them by the column common to both scoring and responses_long responses_points &lt;- NULL 2.4.10 Task 8 - Working the Pipes Below we have given you five lines of code that takes the data in its current long format and then creates a QHC score for each participant (group_by()...summarise()). It then joins the screening information (inner_join()) before calculating a mean QHC score for the two groups of participants - those that play musical intruments and those that dont. This final step is stored as a tibble called musical_means. participant_groups &lt;- group_by(responses_points, ID) participant_scores &lt;- summarise(participant_groups, Total_QHC = sum(SCORE)) participant_screening &lt;- inner_join(participant_scores, screening, &quot;ID&quot;) screening_groups_new &lt;- group_by(participant_screening, MUSICAL) musical_means &lt;- summarise(screening_groups_new, mean_score = mean(Total_QHC)) Use the above five lines of code to replace the NULL in the T8 code chunk with a functioning code pipeline using pipes. Put each function on a new line one under the other. This pipeline should result in the mean QHC values of musical and non-musical people being stored in the tibble musical_means. This final tibble will consist of two rows by two columns (i.e. four cells in total). hint: in pipes, the output of the previous function is the input of the subsequent function. hint: function1() %&gt;% function2() musical_means &lt;- NULL 2.4.11 Task 9 - Difference in Musical Means Finally, replace the NULL in the T9 code chunk with the value of how much higher the QHC score of people who play music is compared to people who dont play music. This should be a single numeric value, to two decimal places, e.g. 2.93 hint: look in musical means and enter the difference between the two means. QHC_diff &lt;- NULL Job Done - Activity Complete! Well done, you are finished! Now you should go check your answers against the solutions at the end of this chapter. You are looking to check that the answers you have submitted are exactly the same as the ones in the solution - for example, remember that Mycolumn is different to mycolumn and only one is correct. If you have any questions, please post them on the forums. 2.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 2.5.1 PreClass Activities 2.5.1.1 PreClass Task 1 Using select() to include only stated columns: select(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss) Using select() to exclude certain columns: select(pong_data, -JudgedSpeed, -BallSpeed, -BlockNumber) Using select() to change the order of columns: select(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber) Return to Task 2.5.1.2 PreClass Task 2 arrange(pong_data, desc(HitOrMiss), desc(JudgedSpeed)) Return to Task 2.5.1.3 PreClass Task 3 filter(pong_data, JudgedSpeed == 1, BallSpeed %in% c(&quot;2&quot;, &quot;4&quot;, &quot;5&quot;, &quot;7&quot;), HitOrMiss == 0) Return to Task 2.5.1.4 PreClass Task 4 first step is created with filter() the second step is created with mutate() pong_data_filt &lt;- filter(pong_data, TrialNumber &gt;= 2) pong_data2 &lt;- mutate(pong_data_filt, TrialNumber = TrialNumber - 1) Return to Task 2.5.1.5 PreClass Task 5 group_by(pong_data2, BlockNumber, BackgroundColor) Return to Task 2.5.1.6 PreClass Task 6 pong_data2_group &lt;- group_by(pong_data2, BackgroundColor, PaddleLength) pong_data2_hits &lt;- summarise(pong_data2_group, mean_hits = mean(HitOrMiss)) ## `summarise()` has grouped output by &#39;BackgroundColor&#39;. You can override using the `.groups` argument. You should find that the number of hits made with the small paddle (50) and the red color background is 0.450655 Return to Task 2.5.2 InClass Actitivies 2.5.2.1 InClass Task 2 library(tidyverse) or library(&quot;tidyverse&quot;) Note, there is no difference between library(tidyverse) and library(\"tidyverse\") both will work. Return to Task 2.5.2.2 InClass Task 3 responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) scoring &lt;- read_csv(&quot;scoring.csv&quot;) pinfo &lt;- read_csv(&quot;pinfo.csv&quot;) Note that there is a difference between responses &lt;- read_csv(\"responses.csv\") and responses &lt;- read_csv(responses.csv). You will need quotes around the .csv filename as shown in the code chunk above (e.g. responses &lt;- read_csv(\"responses.csv\")), or the code wont work. Return to Task 2.5.2.3 InClass Task 7 aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% # group by the ID number in column Id summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores. Return to Task 2.5.2.4 InClass Task 8 aq_scores2 &lt;- responses %&gt;% pivot_longer(cols = Q1:Q10, names_to = &quot;Question&quot;, values_to = &quot;Response&quot;), inner_join(qformats, &quot;Question&quot;) %&gt;% inner_join(scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Return to Task 2.5.3 Test Yourself Activities 2.5.3.1 Assignment Task 1 - Oldest Participant Whether you coded this answer or just read from the data, the Participant with ID Number 3 is the oldest. oldest_participant &lt;- 3 This could also be answered with code. We havent quite shown you how yet but it would look like this: oldest_participant_code &lt;- arrange(screening, desc(AGE)) %&gt;% slice(1) %&gt;% pull(ID) Return to Task 2.5.3.2 Assignment Task 2 - Arranging D-SPAN arrange() is the main function here You also needed to use desc() to sort from high to low cogtest_sort &lt;- arrange(screening, desc(DSPAN)) Return to Task 2.5.3.3 Assignment Task 3 - Foreign Language Speakers First group the screening data by FLANG using group_by() Next, summarise, paying attention to use the variable names as instructed n() is a function that we use within summarise() to count how many observations we have. It works like count() but you cant use count() within summarise() median() is the function that we use within summarise() to calculate the median. Much like we do with sum() or mean() or sd(), etc. screen_groups &lt;- group_by(screening, FLANG) descriptives &lt;- summarise(screen_groups, n = n(), median_age = median(AGE)) Return to Task 2.5.3.4 Assignment Task 4 - Creating Percentage MOCA scores mutate() is the function to add a new column to data Here we are mutating/adding on a column called MOCA_Perc which shows a participants MOCA score divided by 30 and multiplied by 100. screening &lt;- mutate(screening, MOCA_Perc = (MOCA / 30) * 100) Return to Task 2.5.3.5 Assignment Task 5 - Remove the MOCA column select() is the key function to keep and remove certain columns. Two options here; both would give the same dataframe. The first option shows how to deselect a column and keep everything else. The second option shows how to select all the columns you want. Remember that order is very important and you should select the columns in the order you want. Option 1: screening &lt;- select(screening, -MOCA) Option 2: screening &lt;- select(screening, ID, AGE, SEX, HANDEDNESS, EDUCATION, MUSICAL, FLANG, DSPAN, D2, MOCA_Perc) Return to Task 2.5.3.6 Assignment Task 6 - Gather the Responses together pivot_longer() is the function to use here. People take a while to understand this function but spend some time looking at the example below and it will start to make some sense. The first argument is the data. In this case responses. The second argument is the name of the columns you want to gather. Here we are gathering all columns between the Q1 column and the Q15 column. Remember that the colon (:) says from  to  You do not actually need to write cols = but it makes things clearer. Gathering of columns is based on position in the tibble. If the order of columns in the tibble was Q1 then Q15, the above code would only gather those two columns. As it is, in our tibble, the order, is Q1, Q2, Q3,  Q15, and therefore the code gathers all the columns between Q1 and Q15. Colum names are not put in quotes because they exist already in the tibble responses. The third and fourth arguments are the names of the new columns we are creating; the first will store the question numbers, Question. I.e. put the question names (names_to = ...) in a column called Question. the second will store the values/responses, Response. I.e. put the values/responses to the questions (values_to = ...) in a column called Response. These new column names are put in quotes because they do not already exist in the tibble. This is not always the case but is the case for this function. Note that these names could have been anything but by using these names the code makes more sense. Lastly, you do need to write names_to = ... and values_to = ... otherwise the columns wont be created correctly. responses_long &lt;- pivot_longer(responses, cols = Q1:Q15, names_to = &quot;Question&quot;, values_to = &quot;RESPONSE&quot;) Return to Task 2.5.3.7 Assignment Task 7 - Joining the data inner_join() will combine all common information in two sets of data by a common column or columns. Here we are joining the data in responses_long with the data in scoring by the common column RESPONSE. Keep in mind that inner_join() keeps only the rows that have data in both datasets. It will remove rows that only have data in one dataset. When joining two datasets, join by ALL common columns when there is more than one column in common. responses_points &lt;- inner_join(responses_long, scoring, &quot;RESPONSE&quot;) Return to Task 2.5.3.8 Assignment Task 8 - Working the Pipes This is the code we started with. participant_groups &lt;- group_by(responses_points, ID) participant_scores &lt;- summarise(participant_groups, Total_QHC = sum(SCORE)) participant_screening &lt;- inner_join(participant_scores, screening, &quot;ID&quot;) screening_groups_new &lt;- group_by(participant_screening, MUSICAL) musical_means &lt;- summarise(screening_groups_new, mean_score = mean(Total_QHC)) Below is how to transcribe the above series of functions into a pipeline. Remember, when using pipes, the output of the previous function is the input of the subsequent function musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) Return to Task 2.5.3.9 Assignment Task 9 - Difference in Musical Means People who play music have a QHC score that is 1.53 units higher than people who dont play music. You can do this by looking in musical_means, reading the values, and doing some quick maths. A second option is through code. Code is always better as it can reduce error and is reproducible! # Option 1 QHC_diff &lt;- 1.53 # Option 2 # You will soon learn the functions to do this by code but here is how you could do it. QHC_diff_code &lt;- pivot_wider(musical_means, names_from = &quot;MUSICAL&quot;, values_from = &quot;mean_score&quot;) %&gt;% mutate(diff = YES - NO) %&gt;% pull(diff) %&gt;% round(2) Return to Task Chapter Complete! 2.6 Additional Material Below is some additional material that might help your wrangling. More on read_csv() In the preclass activity we used the following code to load in our data: pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) This is a totally acceptable approach and it is the one we will use 99% of the time. Now one thing to note that the read_csv() function by default always loads any number as double, meaning that it can take a decimal. As shown here by the at the start of each variable. The one column that is not a double is the BackgroundColor column, which is of course characters . Note: we can use glimpse() from dplyr to check our data types. glimpse(pong_data) ## Rows: 4,608 ## Columns: 8 ## $ Participant &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ ## $ JudgedSpeed &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, ~ ## $ PaddleLength &lt;dbl&gt; 50, 250, 50, 250, 250, 50, 250, 50, 250, 50, 50, 250, ~ ## $ BallSpeed &lt;dbl&gt; 5, 3, 4, 3, 7, 5, 6, 2, 4, 4, 7, 7, 3, 6, 5, 7, 2, 5, ~ ## $ TrialNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~ ## $ BackgroundColor &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;r~ ## $ HitOrMiss &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, ~ ## $ BlockNumber &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ However you might not always want this default and might want to stipulate the data-type to load the data in as. Particularly if you already know the data-type you should have. Lets look at this first and then talk about it. pong_data3 &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;, col_types = &quot;iiiiicii&quot;) Which if we look at again, we now see that the numerical data are integers instead of double . glimpse(pong_data3) ## Rows: 4,608 ## Columns: 8 ## $ Participant &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ ## $ JudgedSpeed &lt;int&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, ~ ## $ PaddleLength &lt;int&gt; 50, 250, 50, 250, 250, 50, 250, 50, 250, 50, 50, 250, ~ ## $ BallSpeed &lt;int&gt; 5, 3, 4, 3, 7, 5, 6, 2, 4, 4, 7, 7, 3, 6, 5, 7, 2, 5, ~ ## $ TrialNumber &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,~ ## $ BackgroundColor &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;r~ ## $ HitOrMiss &lt;int&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, ~ ## $ BlockNumber &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ So what is the difference? Note that in the new code we have specified the col_types argument as \"iiiiicii\". The col_types argument allows you to control the data type for each variable. If you dont specify this argument, the default is for read_csv() to guess, and when it sees numbers in a column, it will default to treating it as type double. What does the \"iiiiicii\" string do? Well, we know that there are 8 different columns in the csv file, and we have 8 characters in the string \"iiiiicii\"; each letter in this string tells read_csv() the data type for each of these columns. The string has five is followed by one c followed by two more is, which tells read_csv() to treat the first five columns as type integer (i), the sixth column as type character (c), and the last two columns as type integer. (If we wanted a column to be read in as a double, we would use \"d\".) Whilst this is a very useful approach if you are already familiar with the type and structure of the data you are working with, it can cause issues if you dont know that. For instance, you need to know exactly how many columns there are, what order, and what type they are in. So it can get tricky. For this series of lab activities, we will just stick to using the basic read_csv() defaults and not state column types. More on Code Layout One issue we see a lot is people not being able to debug their code quickly (i.e. find issues) because of the way the code is laid out. Pipes (%&gt;%) helps with that, but so does taking new lines for different parts of your code. After a comma (,) or a pipe (%&gt;%), you can take a new line to continue your code to make it easier to read and follow. For example, both of the following will work, but the second is easiest to read. musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) And you could even expand this second option further to make it clearer on the group_by() and inner_join() what are the different inputs: musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) Remember, Tidy Code and Tidy Data Make Wrangling Fun! OK, that is not catchy, but true! More on gathering data - pivot_longer() and gather() With time, some functions change a bit, others change entirely, and others get forgotten about. gather() is one of these functions. gather() was one of these functions that many people just couldnt ever get right in their head and as such pivot_longer() was created. The first version of this book was written using the gather() function and we have tried to update the book to replace it with the pivot_longer() but we will make mistakes and no doubt gather() will appear somewhere, or you may get some old code and see the gather() function and wonder what it does. As such, we have put below a version of the code from the inclass using the gather() function instead of the pivot_longer() function, just in case it helps. gather() allows us to transform a table from wide format to long format. It has now been replaced by pivot_longer() spread() allows us to transform a table from long format to wide format. It has now been replaced by pivot_wider() An example with gather() In the example in class, Task 5, we have all the data we need loaded in, but in order to make it easier for us to get the AQ score for each participant, we need to change the layout of the responses tibble. In class we use the pivot_longer() function but you could have done it with the gather() function as follows rlong &lt;- gather(responses, Question, Response, Q1:Q10) The first argument given to the gather() function is the dataset which holds the data we want to wrangle, responses. The second and third arguments are the names we want to give the columns we are creating; the first will store the question numbers, Question the second will store the responses, Response. Note that these names could have been anything but by using these names the code makes more sense. Finally, the fourth argument is the names of specific columns in the original tibble that we want to gather together - columns Q1 to Q10. The colon (:) says all columns from this to that - all columns from Q1 to Q10. Notes: Gathering of columns is based on position in the tibble. If the order of columns in the tibble was Q1 then Q10, the above code would only gather those two columns. As it is, in our tibble, the order, is Q1, Q2, Q3,  Q10, and therefore the code gathers all the columns between Q1 and Q10. If you had just wanted columns Q1 and Q10, then you could do c(Q1, Q10). pivot_longer() and gather() do not function exactly the same. They tend to sort data differently. Make sure you are using the function you need to do. You should always always always check the output of your functions to make sure they are doing what you expect. In case you are wondering, if we wanted to go back the way and ungather the data we just gathered, we would use the spread() function: e.g. rwide &lt;- spread(rlong, Questions, Response). But we do not want to do that here so lets not add this to the code. Quickfire Questions Lets see if you understand gather(). Say I wanted to gather the first three columns from a tibble called responses where the columns are called Q1, Q2, Q3 (in that order), put the question numbers in a column called Jam, the responses in a column called Strawberry, and store everything in a tibble called sandwich. Fill in the box with what you would write: Explain this - I dont get the right answer! sandwich &lt;- gather(responses, Jam, Strawberry, Q1:Q3) gather wants the data first, then the name of the new column to store the gathered column names, then the name of the new column to store the data, and then finally which columns to gather. As mentioned above we hope to replace every instance of gather() and spread() in this book with pivot_longer() and pivot_wider(), but if we make mistakes the above might help. More on binding and joining In this lab we looked at two methods of combining datasets. We used bind_rows() and we used inner_join(). They do quite different tasks but we thought a quick summary might help. Lets say we have the following two tables that show Perceived Emotion Accuracy in five different cats: Table 2.2: Cat data - not very good ID Animal Perceived_Emotion_Accuracy 1 Cats 11 2 Cats 8 3 Cats 8 4 Cats 10 5 Cats 9 And in five different dogs: Table 2.3: Dog data - smart! ID Animal Perceived_Emotion_Accuracy 1 Dogs 82 2 Dogs 78 3 Dogs 83 4 Dogs 82 5 Dogs 84 Because these two tables have no overlapping information about participants but have a similar tibble structure (i.e. number and names of columns) then the way we can combine the tibbles is by putting one tibble below the other using bind_rows() all_animals &lt;- bind_rows(cats, dogs) Which would give us: Table 2.4: By using bind_rows() ID Animal Perceived_Emotion_Accuracy 1 Cats 11 2 Cats 8 3 Cats 8 4 Cats 10 5 Cats 9 1 Dogs 82 2 Dogs 78 3 Dogs 83 4 Dogs 82 5 Dogs 84 And say now we want to tack on the age information from each participant. Because all_animals and ages have overlapping information that we can use to link the information, we can put the new information at the side of the tibble as a new column using inner_join(). all_animals_ages &lt;- inner_join(all_animals, ages, c(&quot;ID&quot;, &quot;Animal&quot;)) Which would give us: Table 2.5: By using inner_join() ID Animal Perceived_Emotion_Accuracy Ages 1 Cats 11 5 2 Cats 8 4 3 Cats 8 3 4 Cats 10 5 5 Cats 9 5 1 Dogs 82 5 2 Dogs 78 5 3 Dogs 83 3 4 Dogs 82 6 5 Dogs 84 4 There is actualy bind_cols() as well as a host of other join functions, e.g. full_join(), anti_join(). You will pick up more of these functions as we continue but we hope this helps clarify these two functions a little. A quick table on some useful joins()` function Example Final tibble has full_join() full_join(x,y, by = ID) information for all observations found in both original tibbles inner_join() inner_join(x,y, by = ID) information for only observations found in both original tibbles anti_join() anti_join(x,y, by = ID) information for observations found in tibble x but not tibble y anti_join(y,x, by = ID) information for observations found in tibble y but not tibble x End of Additional Material! "],["visualisation-through-ggplot2.html", "Lab 3 Visualisation Through ggplot2 3.1 Overview 3.2 PreClass Activity 3.3 InClass Activity 3.4 Assignment 3.5 Solutions to Questions 3.6 Additional Material", " Lab 3 Visualisation Through ggplot2 3.1 Overview In the last lab we encouraged you to always be looking at your data, making sure you are understanding your data, paying attention to how it is made up in regards to data types, and what the different functions do to your data. Remember that it is one thing to get a function to run, but you need to check it is doing what you think it is doing. It is just a function. You are the human! A second way of looking at your data, and what we are going to focus on in this chapter, is through visualisation - figures and plots - to help understand patterns and effects in your data. For example, when we asked you about the relationship between age and the Ponzo illusion in Chapter 1. Visualisation is very important for understanding your data, for example in regards to seeing differences between groups, but also for seeing where things dont quite match up with what you think is happening. A great example of this is Anscombes Quartet, which you can read up about at a later date if you like - see here - four datasets given exact same means but with very different underlying structures when visualised. The key point is that it is always good to visualise your data and visualisation should be a common step in your practical skill set. In the PsyTeachR Grassroots book we introduced data visualisation using ggplot2, the main visualisation package of tidyverse. You should look back at that when working through this chapter, and you can find additional info here at the main page for the package: ggplot2. Today we will revisit plotting data and expand your skills in order to make effective and informative figures. This will become really beneficial to you as your progress as visualisation of any data is a skill that applies to multiple careers, not just Psychology. In this lab you will: Recap on visualisation Expand your skills to produce new figures Learn about Mental Rotation 3.2 PreClass Activity Testing Mental Rotation Ability The data we will use today comes from a recent replication of a classic experiment merging the fields of Perception and Cognition. Shepard and Metzler (1971) demonstrated that when participants are shown two similar three-dimensional shapes, one just a rotated version of the other (see the figure below - top panel), and asked participants whether they were the same shape or not, the reaction time and error rates of responses were a function of rotation; i.e. the larger the difference in rotation between the two shapes, the longer it took participants to say same or different, and the more errors they made. Figure 3.1: The Mental Rotation Task as shown in Ganis and Kievit (2016) Figure 1 The image shown in Figure 3.1 actually comes from a recent replication by Ganis and Kievit (2016). In the top panel the two shapes are the same but the shape on the right is rotated vertically at 150 degrees from the original (the left shape) and so participants should respond same. In the bottom panel however the two shapes are different; the one on the right is again rotated at 150 degrees but in this trial it takes longer for participants to realise that they are different shapes. You can read more about Ganis and Kievit (2016) in your own time but the basic methods are that they ran 54 participants on a series of these images using 4 angles of rotation (0, 50, 100, 150 degrees) and asked people to respond same or different on each trial. The data can be downloaded from here. You should use this data to follow along below and try to answer the questions. Visualising Data Download the data folder, unzip it, and save it to a folder you have access to (e.g. your M: drive if using the lab machines). Set your working directory to that folder Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Open a new Rscript and save it within the folder that contains the data, giving the script a sensible name, e.g. Lab3_preclass_visualisations.R. (If you prefer to work in R Markdown like the assignments will be in then that is totally fine as well. Just remember you will need to make your own code chunks if using R Markdown, as shown in Chatper 1) Copy the three code lines below into your script and run them to bring tidyverse into the library and to read in the two datafiles. library(&quot;tidyverse&quot;) menrot &lt;- read_csv(&quot;MentalRotationBehavioralData.csv&quot;) demog &lt;- read_csv(&quot;demographics.csv&quot;) Note, there is no difference between library(tidyverse) and library(\"tidyverse\") both will work. However, there is a difference between demog &lt;- read_csv(\"demographics.csv\") and demog &lt;- read_csv(demographics.csv). You will need quotes around the .csv filename as shown in the code chunk above (e.g. demog &lt;- read_csv(\"demographics.csv\")), or the code wont work. Portfolio Point - Why load tidyverse and not just ggplot2? This is a really great question as we always seem to be saying to use dplyr or readr or ggplot, but we never actually call them in. Remember however that tidyverse is actually a collection of packages, the most common packages in fact, and we use it to bring in these common packages (including ggplot2) because you will probably need the other packages along with it for the codes to run smoothly. We will try to tell you when you need to call other packages alongside tidyverse but do keep in mind that most of your codes will at least start with the tidyverse package. Small point, if looking for help on ggplot, the package is actually called ggplot2. This is the newer version of the package, so search ggplot2 if you need help. Lets start by having a look at the data we have brought in. You can do this whichever way you choose; we mentioned three ways in the previous labs - check your notes. First, demog - short for demographics. It has three columns: Participant - the ID of the participant Age - the age of the participant Sex - the sex of the participant Secondly, menrot - short for mental rotation. It has 8 columns: Participant - the ID of the participant; matches with ID in demog Trial - the trial number in the experiment for each participant Condition - the name of the image shown; R indicates the rotated image was different Time - the reaction time to respond on each trial in milliseconds DesiredResponse - what participants should have responded on each trial; Different or Same ActualResponse - what participants did respond on each trial; Different or Same Angle - the angle that the shape on the right was rotated compared to the shape on the left (0, 50, 100, 150) CorrectResponse - whether the participant was correct or incorrect on a given trial Portfolio Point - A nice procedure Ganis and Kievit (2016) is a very short paper that is really to introduce the stimuli set rather than give an extensive background on the topic of mental rotation - we call this a methods paper. That said, the writing of the paper is very clear and the procedure is well detailed as to how they ran the actual experiment. When writing a procedure, remember to give as much information as needed to allow someone to exactly replicate your study. Have a read at this procedure when you have time and think about what information is there, but also what information is not there, to help you develop your writing and your reports. For example, which fingers did the participants use to respond and why would that be important? Creating Some Plots We now have our data and we want to create some plots to visualise it. We will show you the code to create four types of plots and then get you to practice some yourself, but you will remember some of this from the PsyTeachR Grassroots book. As we go through the plots, you should edit/change the code we give you and see what differences you can control and what changes you can create in the plots. Editing and altering code that works to see what happens when you change something is a great way of working. Be sure to ask any questions you have on the forums or with staff. Portfolio Point - A note on how ggplot works The two main things to know about working with ggplot are that: the usual format is: ggplot(data, aes(x = x_axis, y = y_axis)) + geom_type_of_plot() it works on a concept of layers On point a: The first thing you enter is your dataframe/tibble; your data. Then within the aes() you say what is my x_axis and y_axis, using the column names from within your tibble. aes stands for aesthetics and maps data into visual features. Finally you tell the code what type of plot you want. On point b: Layers are a common way for graphics to work. Think about it as ggplot() fucntion creating your first layer and then every function after that is adding more layers on top to create the figure you want. The first layer is always your data and the axis/axes, i.e. `ggplot(.). The second layer, added by using the plus symbol +, is the type of plot. We will look at adding more layers as we progress. ggplot() is an incredibly powerful package that is used by a whole range of industries, including newspapers and mainstream media outlets, as it can make quite sophisticated images. One of the beauties of skills in research methods and data analysis is just how transferable they are across many fields. 3.2.1 Scatterplots - geom_point() Scatterplots are a great way of visualising continuous data - data that can take any value on the scale it is measured. For example, in the current dataset, you can use scatterplots to explore the potential relationship between two continuous variables such as Age and Reaction Time: Do both variables increase/decrease at the same rate (i.e. a positive relationship)? Does one variable increase and the other decrease (i.e. a negative relationship)? Or maybe there is no overall relationship? In our data, say we want to test if the overall average time to respond in the mental rotation task is related to the age of the participant. We could show this relationsthip in a scatterplotusing the code below, which: Wrangles the data to create an average response time for each participant, Mean_Time, and then joins this information to the demographic data, by Participant. All this is stored in the tibble menrot_time_age. It then plots a scatterplot (geom_point()) where age is plotted on the x axis, and Mean_Time is on the y axis Finally, it uses an additional aes call to color by Sex which will color each point based on whether it was a male or female participant responding. This is the default coloring of this call when there are two options. Later we will look at controlling this and using more colors ourselves. menrot_time_age &lt;- group_by(menrot, Participant) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_time_age, aes(x = Age, y = Mean_Time, color = Sex)) + geom_point() Figure 3.2: A scatterplot of Mean Time as a function of Age Quickfire Questions Looking at the scatterplot in Figure 3.2, what can you say about the relationship between age and overall response time? as age increases, overall response time increases as age increases, overall response time decreases there is no overall relationship Looking at the scatterplot, what can you say about difference between male and female participants? males show more of an increase in overall response time with age than females females show more of an increase in overall response time with age than males there is no real difference between males and females in terms of overall response time and age Explain This - I dont get these answers If you look at the figure, does it appear that as age increases (x axis) so does overall resposne time (y axis)? Or as age decreases so does overall response time? Or maybe even as age increases, overall response time decreases? Etc etc. Well, actually, looking at the figure there appears to be no relationship between the two variables at all and it is not the case that as one either increases or decreases so does the other. The relationship appears flat. When comparing sex, based on the color of the dots, again there appears to be no major differences here as the relationship looks flat for both sex. Later in the book we will look at correlational analysis - a method of quantifying the relationship between two variables. Note: It will often be the case that to visualise data you first have to wrangle it into a format. When we do this we will be using the functions we saw in Chapter 2, so make sure you have been through those tasks and have understood what the wrangle verbs are doing and how pipes work. Keep in mind that most functions use the format, function(data, argument) 3.2.2 Histograms - geom_histogram() Histograms are a great way of showing the overall distribution of your data. Does your data look normally distributed? Or is your data skewed - positive skew or negative skew? Is it peaky? Is it flat? These are terms that will become familiar to you as you learn more about statistics, so try to think about these terms and concepts when visualising, and looking at, your data. Looking at our data, say we wanted to test if the overall distribution of mean response times for correct trials was normally distributed. We could visualise this question through the following code, which: Wrangles the data to create an average response time for each participant, Mean_Time, and then filters this information for correct trials only. This is then stored in the tibble menrot_hist_correct Plots a histogram (geom_histogram()) where Mean_Time is plotted on the x axis, and the count of each value in Mean_Time is plotted on the y axis. The code creates the y axis automatically and we dont have to state it: menrot_hist_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) ggplot(data = menrot_hist_correct, aes(x = Mean_Time)) + geom_histogram() Figure 3.3: A histogram of distribution of Mean Time counts Quickfire Questions Looking at the histogram in Figure 3.3, what can you say about the overall shape of the distribution? the data looks reasonably normally distributed the data looks positively skewed the data looks negatively skewed Looking at the histogram, what is the most common average overall response time for correct trials? approximately 2000 milliseconds approximately 2500 milliseconds approximately 3000 milliseconds Explain This - I dont get these answers Keep in mind that real data will never give that beautiful textbook shape that you see in classic diagrams when looking for normally distributed data or skewed data. Your decisions regarding your distributions will often requre a degree of judgement. From your lectures you will remember that positive skewed data means that most of the data is shifted to the left (low numbers) with a tail stretching to the right (high numbers). Negative skew is where most of the data is shifted to the right (high numbers) with a tail stretching to the left (low numbers). Normally distributed data has most of the data in the middle with even tails on either side. Although not perfect, the data shown in our histogram is a reasonable representation of normally distributed data in the real world; particularly for a small sample of participants. As the y axis is the count of the values on the x axis, the most common overall response time can be found by reading the highest column of the data. For this distribution, this looks to be around 2500 milliseconds or 2.5 seconds. 3.2.3 Boxplots - geom_boxplot() Boxplots are a great means for visualising the spread of your data and for highlighting outliers in your data. When looking at boxplots, you should consider: whether the median (thick horizontal black line) is in the middle of its box or is higher or lower than the middle of the box? whether the box is evenly disributed round the median or not? are the box whiskers (vertical tails at top and bottom of box) a similar length on both sides of the box? are there any outliers - usually highlighted as a star or a dot beyond the whiskers? Using todays data, lets look at and compare the distributions of mean reaction times for correct and incorrect responses. This can be done using the below code, which: Repeats the first two wrangle steps as when we created a scatterplot, but additionally groups by CorrectResponse, and stores the data in the tibble menrot_box_correct Plots a boxplot (geom_boxplot()) of the overall average response times on the y-axis, Mean_Time, based on the condition, CorrectResponse, on the x-axis Uses an additional aes call to fill the colour of the boxplots, of the two categories, based on whether CorrectResponse was correct or incorrect. Again these are default and we will look at editing this later. Turns off the legend using the guides() call as it isnt needed because the x-axis tells you which group is which. More on that later though. Run the code as is. Now run the code with fill = TRUE instead. Whats the difference? Notice that fill is the name of the call in the ggpot(...) function. They are linked. menrot_box_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = CorrectResponse)) + geom_boxplot() + guides(fill = FALSE) ## `summarise()` has grouped output by &#39;Participant&#39;. You can override using the `.groups` argument. ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = ## &quot;none&quot;)` instead. Figure 3.4: A boxplot of the spreads of Mean Time for Correct and Incorrect Responses Quickfire Questions Looking at the boxplots, how many outliers were there? 1 2 3 0 Looking at the boxplots in Figure 3.4, which condition has the longer median overall average response time to the mental rotation task? Median response time was longer for the Correct responses Median response time was longer for the Incorrect responses Both medians are the same approximately Explain This - I dont get these answers There are a number of ways of determining outliers which will be discussed in more detail when you learn about statistics. Two methods are through standard deviations (usually 2.5 or 3 SD are used as cut-offs) or through boxplots, where an outlier is determined as \\(1.5*IQR\\) (inter-quartile range) above or below the top and bottom of the box. Outliers are shown as dots above or below the whiskers of the boxplot. As you can see in the figure there are no outliers to see in this data. The median is one of the 5 values required to make a boxplot and is shown as the horizontal thick black line within the box itself. Looking at the two conditions and comparing the position of the median on the y axis (response time) we can see that the median response time for incorrect trials was higher than correct trials. This would suggest that people take longer to make up their mind and to give a decision on the trials that they get wrong. Makes sense if you think about it; uncertainty takes longer and leads to more errors. 3.2.4 Barplots - geom_bar() or geom_col() Barplots typically show specific values of a condition. Sometimes this will be really simple like the count of a variable the mean, e.g. how many people replied yes. Others will show something a bit more complex such as the average spread of values via error bars, e.g. standard error. When looking at barplots, the main considerations are whether or not there appears to be a difference between the conditions you are interested in or are all conditions about the same? It is worth knowing that barplots are now used less frequently than they were as they actually do not show a lot of information, as discussed in this blog, One simple step to improving statistical inference. However, you will still see them being used so it is great to be able to create and interpret them. Using geom_bar() Using our data, lets say we are interested in whether there is a difference in the average percentage of correct and incorrect responses across male and female participants. We could visualise this using the following code, which: Wrangles the data through a series of steps to establish the overall percent average for correct and incorrect responses for both sex, stored in menrot_resp_sex. Plots a barplot (geom_bar()) with the condition Sex on the x axis, the Avg_Percent on the y axis, created through the wrangle, and fill the bars based on CorrectResponse Finally, within the geom_bar it says to treat the data as final values and not to average them, stat = \"identity\", and makes both columns visible by moving them apart position = position_dodge(.9)) - without this last step the bars would overlap and you wouldnt see everything. Try changing the .9 Note: Each participant did 96 trials in this study. total_n_trials &lt;- 96 menrot_resp_sex &lt;- count(menrot, Participant, CorrectResponse) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% mutate(PercentPerParticipant = (n/total_n_trials)*100) %&gt;% group_by(Sex, CorrectResponse) %&gt;% summarise(Avg_Percent = mean(PercentPerParticipant)) ggplot(data = menrot_resp_sex, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge(.9)) ## `summarise()` has grouped output by &#39;Sex&#39;. You can override using the `.groups` argument. Figure 3.5: A barplot of the average percent Correct and Incorrect responses for Female and Male participants - using geom_bar() Using geom_col() geom_col() - short for column - is an alternative to geom_bar() that does not require the part of the code where you say to not do anything to the data, i.e. stat=\"identity\". This is shown below. Notice the difference in codes but that they produce the same figure! ggplot(data = menrot_resp_sex, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_col(position = position_dodge(.9)) Figure 3.6: A barplot of the average percent Correct and Incorrect responses for Female and Male participants - using geom_col() Quickfire Questions Looking at the barplot and data, on average, which sex had the most correct responses? female male both the same can't tell Looking at the barplot and data, on average, which sex had the most incorrect responses? female male both the same can't tell Looking at the code, what happens if you decrease the position.dodge() value? the bars get further apart the bars start to overlap nothing changes in the figure Looking at the code, what happens if you change the aes call of fill to color? the bars will stay a different color the bars become grey and the outlines become different colors nothing changes in the figure Explain This - I dont get these answers Remember that in barplots plotting the mean, the top of the column is the average value of that condition. This is actually why people do not like barplots; though commonly used, they really only show you one value for your data, the average, and they disregard all other information unless some indication of spread is given. With that in mind, comparing the two Correct columns we can see that females had on average more correct responses than males. Doing the same for the Incorrect columns we can see that males had more incorrect responses than females. This actually makes sense as the response option in the experiment was either correct or incorrect, so when you add all the correct and incorrect percentage responses for one sex together you should get 100%. If females gave more correct reponses then they must have given less incorrect responses. The last two questions are about playing with the code. Remember we said that plots work through a concept of layers. If you set position.dodge() to 0, you will find that one of the columns disappears because they completely overlap now. So we need to set position.dodge() to a reasonable value to have the columns separate. Why not set it at 1? In barplots you often find that the different levels (or categories) of the the same variable are touching. Note however that the value of the dodge, in this case 1, is relative to the size of the x axis - if the scale of your x-axis ran from 0 to 100 then a dodge of 1 will have very little effect. Sometimes you need a little trial and error. Always look at the output of your code. The final point shows that you can add a lot more calls than just x and y axis to change the presentation of your figures. fill changes the color of the columns, color changes the outline color of the columns. We will see more of these as we progress and we will look at the difference between putting them inside the aes() and outside of it. Have a play about with these on other figures and see what happens. It is worth pointing out here though, above where we turned the legend off using guides(fill = FALSE), that works because we used the fill =  call to change colours. If we had used the color =  call to change colours we need to use guides(colors = FALSE) to turn off the legend. See how they are linked? The guide matches what you called. 3.2.5 Themes, Labels, Guides, and facet_wraps() Before we finish, we want to mention a couple of other layers you can add to your ggplot calls to make your figures look more professional. We will show you the code here but we want you to run them and teach yourself how they work by changing the code, removing parts within ggplot, and by adding them to the other figures we have shown above. Ask any questions you have about these functions on the forums or check them with a member of staff. themes - changing the overall presentation of your figure. Try running the below code and comparing the figure to the barplot above. Remember, ?theme_bw() will give some information or look at the cheatsheets for different themes such as theme_light(), theme_classic(), theme_gray() and theme_dark(). theme_gray() is actually the default and is equivalent to not stating any theme function. _ theme_classic() is very close to a basic APA figure presentation. labs - putting appropriate labels on your figures so readers understand what is being displayed. Try changing the text within the quotes. facet_wraps - splitting data into separate figures for clarity. This will only work when one of your conditions is categorical but it can be a really effective means of displaying information. guides - remove it and see what happens. Do you understand why we use fill in this situation but perhaps not others? Try running and editing this code. total_n_trials &lt;- 96 menrot_better_plot &lt;- count(menrot, Participant, CorrectResponse) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% mutate(PercentPerParticipant = n/total_n_trials) %&gt;% group_by(Sex, CorrectResponse) %&gt;% summarise(Avg_Percent = mean(PercentPerParticipant)) ggplot(data = menrot_better_plot, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_col(position = position_dodge(.9)) + labs(x = &quot;Sex of Participant&quot;, y = &quot;Percent Average (%)&quot;) + guides(fill = FALSE) + facet_wrap(~CorrectResponse) + theme_bw() A figure for all occasions As you progress through Psychology you will come across a variety of different figures and plots, each looking slightly different and giving different information. When looking at these figures, and indeed when choosing one for your own analyses, you have to think about which figure is the most appropriate for your data. For example, scatterplots are great when both variables are continuous; boxplots and histograms are great for viewing spreads of data; barplots are commonly used where one variable is categorical - but as above note that barplots can be misleading and lots of new approaches to display categorical information are being created. Always keep asking yourself, does my plot display my data correctly. Also, do I have the right number of dots/conditions/groups in my figure? Too many or too few would suggest something is not quite right. Look at your data! Job Done - Activity Complete! We will look more figures and controlling different aspects of them in the coming chapters. We will also look at interpreting the figures. The skill of interpretation is of course more important than what color to show them in. But changing colors is fun and a good way to learn how to work with the codes! If you have any questions please post them on the available forums or check with a member of staff. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 3.3 InClass Activity Mental Rotation and Visualisations Visualisation is a key part of data analysis in exploring data, checking assumptions, and displaying results. Also, it is a really important skill for when you want to share your findings with others. As you develop your skills in Psychology you will start to analyse your own datasets, and learning how best to plot in different ways will benefit you in the long run. Keep in mind that this is a skill that you can carry into your future career even if that is outside of science. For visualisaion we use ggplot2 and below we have listed some great online resources that you might want to consult if you want a fuller understanding. R Graphics Cookbook ggplot2 book ggplot2 cheatsheet ggplot2 Reference Guide But dont forget about what you have already covered in the PsyTeachR Grassroots book. It is highly recommended that you consult the ggplot2 cheatsheet when you work with the package to get familiar with the package. 3.3.1 Mental Rotation: Angle and Reaction Time We will use the same dataset for this class as we did in the preclass activity. To recap, Ganis and Kievit (2016) created a modern stimuli set to test the classic experiment on mental rotation first proposed by Shepard and Metzler (1971). The idea is that the more rotated a test image is from an original image, the longer it will take for participants to determine if they are the same image or if they are completely different images. Results show that this is the case and suggest that, when doing mental rotation tasks, people do a form of internal linear interpolation, or internal mental rotation, meaning that they rotate the image in their mind back to the original angle and then compare the two images. This would be opposed to a more rapid contrast and compare approach not involving any rotation. As a result, the more rotation an image needs, the longer the task takes. Ganis and Kievit ran 54 participants on a series of these rotated images using 4 angles of rotation (0, 50, 100, 150 degrees rotated compared to the original) and asked people to respond same or different on each trial. Today, to further your understanding of this experiment as well as develop your skills in visualisation and interpretation, we will look at mean reaction time for correct trials, as a function of the angle of rotation and sex. The data can be downloaded from here. Portfolio Point - as a function means? You will come across this phrase as a function of quite a bit when dealing with visualisations. It means something like compared by or across. So you could say we are going to look at Mean Reaction Time across the four different Angles of Rotation which would be written as Mean Reaction Time as a function of Angle of Rotation. Usually it would be plotted as y axis as a function of x axis. It is a similar idea to the functions we use in our codes in that you want to see what happens when you put y in the function x. It is good to become familiar with the terms and language that is used in reports so that a) you understand what you are reading and b) you can use the same language in your writing to give a professional feel. Remember to be developing your own notes as you go! 3.3.2 Task 1: Loading and Viewing the Data Download the data, unzip it, and save it to a folder you have access to - e.g. somewhere on your M: drive Set your working directory to the folder with your data in it. Do this through the menus at the top of RStudio, not through code. Start a new script, save it to the folder with your data in it. Again R script or R Markdown will both be fine. Your choice. In your script, load in the tidyverse library. Load in both datasets exactly as we did in the preclass, storing the experimental data in menrot and the demographic data in demog. Helpful Hint library() menrot &lt;- read_csv() demog &lt;- read_csv() Remember to always be looking at your data as good practice and to make sure it is as you expect it to be. Use either View() or glimpse() but do this in the console window of RStudio, never in your script or .Rmd file. Other useful functions that you can use to check your data structure are: str() - This shows what type your data is. Look for words like table, dataframe, character, integer, double. head(), tail(), and names() - These show the top six and bottom six rows with column names, or just the column names with names(). dim() - This shows the dimensions of the data. Refer to the preclass for a list of what all the columns refer to if you are not sure. And keep in mind that to be reproducible means being careful with the spelling and punctuation of the names of functions, tibbles, columns, conditions, etc, at all times - e.g. Juggler is not the same as juggler. Quickfire Questions Take a couple of minutes to try the above functions and to answer the following questions. From the options, what type of data is the variable Angle, found in the dataframe menrot? character integer double/numerical Type in the box the name of the dataframe that contains information regarding the sex of the participants: From the options, which of these is a column within menrot? Correctresponse CorretcResponse CorrectResponse correctReponse From the options, according to the dim() call, how many rows are there in demog? 3 8 54 5184 Explain This - I dont get these answers This is about making sure you are loading in your data correctly, using the instructed names - so that you are being reproducible - and that you understand the data you are looking at. If you completed Task 1 successfully, loading in the data to the correct dataframes, then the following answers should work in the above questions. calling str(menrot) and looking through the information that comes out you should see that the data in the column Angle are loaded in as double/numerical. Technically they are integers (whole numbers with no decimal places) but the default load in would make them numerical. demog should have been where you loaded information regarding the demographics including sex of participant. names(menrot) will give you column names. This question is about making sure of the correct spelling: CorrectResponse. All the other spellings would not work in this data as spelling of column names is specific! dim(demog) shows you the number of rows (54) by the number of columns (3). 3.3.3 Task 2: Recreating the Figure Lets start by making a representation of the top part of Figure 2 in Ganis and Kievit (2016) - Mean Reaction Time as a function of Angle of Rotation. Copy the lines of code below into your script. Replace the NULLs in order to recreate the figure below similar to that of Ganis and Kievit (2016) Figure 2 (top). Note that this figure shows information for only correct responses just as in Ganis and Kievit (2016). Remember ggplot is a case of layers. The first layer says where is my data and what do I want on each axis. Every subsequent layer says how I want the data displayed - points (geom_point()) with a connecting line (geom_line()). After running through all these tasks, come back to this one and see if you can figure out what coord_cartesian() does by changing the numbers in the ylim = () call. There is a comment on this in the solutions. menrot_angle &lt;- filter(menrot, CorrectResponse == NULL) %&gt;% inner_join(demog, NULL) %&gt;% group_by(NULL) %&gt;% summarise(mean_Resp = mean(NULL)) ggplot(data = NULL, aes(x = NULL, y = NULL)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Helpful Hint The first four lines, to create menrot_angle, are all functions from the Wickham Six verbs so refer back to your portfolio to see how they work. Which answer in CorrectResponse would allow you to keep just the correct answers? What common variable will allow you to join this information to demog? You only really care about the four levels of rotation, so what variable/column will you group_by? Which variable/column do you want the mean of for a mean response time? For the ggplot line, think of the format, data, then the x axis name, then the y axis name. See your portfolio for examples. Figure 3.7: Basic Scatterplot of Response Time by Angle of Rotation Group Discussion Point Great, you have replicated the figure! However, do you know what it means? Spend a couple of minutes discussing with your partner or group - what does the figure tell you about mean reaction time and angle of rotation, and how does it fit with the overall theory we introduced above? Answering this question may help: From the options, the figure would suggest that as angle of rotation increases: mean reaction time decreases mean reaction time stays the same mean reaction time increases Portfolio Point - Reaction Time as a function of Rotation As you can see from the figure, consistent with Shepard and Metzler (1971), the participants from Ganis and Kievit (2016) showed an increase in reaction time as the angle of rotation increased. Therefore, Ganis and Kievit (2016) have replicated the findings of Shepard and Metzler (1971). A quick note though is that, yes, mean reaction time does increase with angle of rotation but it is not a consistent increase. You will see that the difference between mean reaction times for 150 and 100 degrees is smaller than between 0 and 50. Reaction times start to plateau after a certain angle of rotation. 3.3.4 Task 3: Examining Additional Variable Effects In the preclass, we looked at sex of participant a lot and that was quite interesting. It is not covered in Ganis and Kievit (2016) so let us take a look at it for them. To your pipeline of Task 2, add the variable Sex to the group_by() function to group the data by Angle and Sex. Running the code again creates the below figure. Helpful Hint group_by(Angle, Sex) Remember, to add more grouping variables just separate them by a comma. Everything else in our code stays the same. ## `summarise()` has grouped output by &#39;Angle&#39;. You can override using the `.groups` argument. Figure 3.8: Separating points by Sex Hmmm, that figure does not look that informative. It looks similar to the one we created but the dots have doubled - we now have 8 instead of 4 - but we do not know which is male or female, and the connecting line is confusing. We need to add a little more to the code to tell it how to separate the data based on sex. 3.3.5 Task 4: Grouping the Figure Data In the preclass we used fill and color inside the aes() to change basic information about the figure. There is also one called group. Add a group call inside your aes(...) to separate the data by Sex. Run the code again and see what your figure looks like Helpful Hint ggplot(.., aes(x = , y = , group = ???)) You should now at least see different lines for the two sex, but we still cant tell which sex is which line, can we? It just looks like two black parallel lines, one slightly higher than the other. What would be ideal would be changing the color of the points based on whether they are from male or female participants! Fortunately, the geoms can also take information as well. 3.3.6 Task 5: Identifying Groups Using aes() Add an aes() call inside your geom_point() function to color the dots by Sex. Helpful Hint geom_point(aes(??? = ???) You should now have something like this: ## `summarise()` has grouped output by &#39;Angle&#39;. You can override using the `.groups` argument. Figure 3.9: Separate lines for each Sex Great! We can now see that the female line is on top and the male line is on the bottom. But before we start interpreting this figure lets finish tidying it up with a few more tasks. For example the dots for each data point are perhaps a little small for our old eyes to see, so we could increase them in size. Also, color is great if you can print in color but we could also change the shape of the dots to help people distinguish between the Sex for when displaying in color isnt an option. To do this we will use the additional calls of shape and size within our geom_point(). 3.3.7 Task 6: Changing the Shape and Size of Data Points We want each Sex to have different shaped points, so add a call to shape within the aes() call of our geom_point() function, just like you did for color. However, we want each Sex to have the same size of point, so add a call to size within the geom_point() function, but not inside the aes() call. Set an appropriate number for the size instead of naming a variable. Maybe size = 3? Helpful Hint geom_point(aes(color = Sex, Shape = ???), size = ???) This should give you a figure like this: ## `summarise()` has grouped output by &#39;Angle&#39;. You can override using the `.groups` argument. Figure 3.10: Changing Shape and Size of Data Points A quick point - in, out, whats the aes about? Hopefully you are beginning to spot the difference between setting a call within aes() (which stands for aesthetics) and setting outside the aes(). Outside aes() means that all observations take the one value or color or type. Inside means that each observation within a condition takes the same value or color or type, but different conditions have different values/color/type Lets look at what we have done above to help us compare. size is called outside of the aes() and we assign it a specific value. As you can see from the Task 6 figure, each condition now has the same size of points. We could set this size to what we want but keep it smallish: 3 or 5 are ok; 50 would be artistic but not that informative. In contrast, we called shape inside the aes() and we set it based on a variable, Sex. Doing it this way ensures that each level of the Sex variable, male or female, get a different shape. Had we instead set shape outside the aes(), something like geom_point(shape = 3, size = 3) then all conditions would have the same shape and the same size. Different numbers relate to different shapes and different sizes. For example compare shape = 3 to shape = 13 Likewise, had we set the size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) then both male and female would have different shapes AND different sizes. You can play around with this in your code to see how things work. And of course there are other arguments you could use. For example if you wanted all points to have the same color, say red for example, then you could do geom_point(color = \"red\"). Remember to put the quotes around the color. So hopefully this is starting to make sense and you can think about implementing it in your own figures. Note that arguments are separated by a comma. e.g. geom_point(color = \"red\", size = 3, shape = 2) 3.3.8 Task 7: Adding Labels and Changing the Background This figure is looking really nice now. Lets finish it off by making it look a little more professional with appropriate labels and by editing the background. We introduced these to you in the preclass so hopefully you had a play with them to see how they work. To change a label, we use the labs() function and it works like labs(x = \"Name\", y = \"Name\", title = \"Name\"). Add this function to your code so that the y axis indicates Mean Reaction Time (ms) and the x axis indicates Angle of Rotation (degrees). We havent set a title here but you can if you want some practice. Titles on Psychology figures are not common. That said, a recent change to the APA style guide might change that! Set the figure as theme_bw() - this looks nice but there are other options you might want to try which you can explore through ?theme or the cheatsheet. Helpful Hint labs(x = , y = ) + theme_bw() The key thing is to remember to + the layer into the ggplot chain. And dont get this confused with pipes (%&gt;%). Note: You add (+) layers to figures, and you pipe (%&gt;%) between functions. 3.3.9 Task 8: Separating a Variable and Removing Legends Finally in the preclass we showed you two other functions that you could use to tidy up figures: facet_wrap() and guides(). facet_wrap() is really effective for splitting up figures into panels based on a variable; it works like facet_wrap(~variable) where ~ can be read as by. So split up the figure by variable, for example Sex. And if you had two variables to split the figure by, then: facet_wrap(~variable1 + variable2). guides() is handy for turning on and off legends which might be taking up space. For instance, if you use a facet_wrap() to split the panels into Female and Male, do you really need the legend on the right saying Female and Male? You will normally have a guide for all the color, shape, etc, calls you have in your pipeline within your aes() calls. It works like guide(call = FALSE). Add a facet_wrap() to have separate panels in your figure based on Sex. Turn off all guides so that you do not have a legend in your figure. Helpful Hint facet_wrap(~variable) guides(group = FALSE, ???? = False, .) Group Discussion Point If you have followed all the tasks correctly then you should have the following figure: ## `summarise()` has grouped output by &#39;Sex&#39;. You can override using the `.groups` argument. ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = ## &quot;none&quot;)` instead. Figure 3.11: The finished figure! Take a few minutes with your partner or group to look at the figure and try to intepret it in terms of reaction time as a function of rotation and sex. Try answering the following questions to help your discussion: In both sexes, mean reaction time decreases with increases with is unaffected by angle of rotation. Angle of Rotation influences female participants male participants more than female participants male participants Portfolio Point - Interpreting the results By looking at the figure, as angle of rotation increases (moving to the right of the x axis), the mean reaction time increases (getting higher on the y axis), indicating that participants take longer to respond the further the target image is rotated from the original. Also, as the male mean reaction times are quicker overall than the female mean reaction times, and the differences in reaction times between 0 degrees and 150 degrees is smaller in males, then you could perhaps say that males are affected less than females, or males perform the task quicker. Keep in mind that we are only looking at correct responses here. As such, this figure would suggest the difference is not just male participants just responding quicker overall. Instead it may suggest that males are responding correctly quicker overall but you would need to take into consideration the difference in overall correct reponses between females and males which we saw in the preclass. Differences in mental roration tasks have received much attention over the years and you should refer to the reference sections of the two main papers of this activity should you wish to follow the topic further and wish to add this to your Portfolio. 3.3.10 Additional Considerations (briefly!) Many of the options we have seen in terms of geom_point() could have been applied to geom_line() to make alterations to the line. Try playing with these options. For example, the code line below would result in both sexes having a red line of equal size but the style of line being different. Give it a shot! geom_line(aes(linetype = Sex), size = .5, color = \"red\") Finally, if you look closely at the figure, you will see that the line between points actually goes in front of the points. It looks a bit messy. How could you make it tidier by having the line run behind the data points? Not sure? Remember that figures are constructed based on a series of layers. We draw one layer and then the next, so changing Task 8 to would draw the lines first and then the points on top. Give it a go! ggplot(data = menrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_line() + geom_point(aes(color = Sex, shape = Sex), size = 3) + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + facet_wrap(~Sex) + guides(group = FALSE, color = FALSE, shape = FALSE) + theme_bw() Job Done - Activity Complete! In this chapter we have looked at working with layers and a variety of calls to shape, color, fills, etc, to create professional looking figures. Understanding figures through ggplot can seem like trial and error until you have a lot of experience. That is fine but just remember to keep adding notes to your Portfolio to help out the future you. And the beauty of it all is that once you have a figure you really like, you run the code and you get exactly the same figure again! Amazing or what??? You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the available forums. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 3.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 3.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 3.5.1 InClass Activities 3.5.1.1 InClass Task 1 library(&quot;tidyverse&quot;) menrot &lt;- read_csv(&quot;MentalRotationBehavioralData.csv&quot;) demog &lt;- read_csv(&quot;demographics.csv&quot;) Return to Task 3.5.1.2 InClass Task 2 menrot_angle &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_angle, aes(x = Angle, y = mean_Resp)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) coord_cartesian() is a function that can be used to show certain parts of a figure, controlling the visible X and Y axes. expand = TRUE adds a smaller buffer to the numbers you set. If you want to remove this buffer then set expand = FALSE. Return to Task 3.5.1.3 InClass Task 3 menrot_angle_sex &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.4 InClass Task 4 menrot_grouped &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_grouped, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.5 InClass Task 5 menrot_grouped_color &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_grouped_color, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex)) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.6 InClass Task 6 menrot_shape_size &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_shape_size, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.7 InClass Task 7 menrot_lab_theme &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_lab_theme, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + theme_bw() Return to Task 3.5.1.8 InClass Task 8 manrot_facet_guide &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ## `summarise()` has grouped output by &#39;Angle&#39;. You can override using the `.groups` argument. ggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + facet_wrap(~Sex) + guides(group = FALSE, color = FALSE, shape = FALSE) + theme_bw() ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = ## &quot;none&quot;)` instead. Figure 3.12: Task 8 Remembering it is a layer system, we could use the below code to have the lines behind the points. manrot_facet_guide &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ## `summarise()` has grouped output by &#39;Angle&#39;. You can override using the `.groups` argument. ggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_line() + geom_point(aes(color = Sex, shape = Sex), size = 3) + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + facet_wrap(~Sex) + guides(group = FALSE, color = FALSE, shape = FALSE) + theme_bw() ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = ## &quot;none&quot;)` instead. Figure 3.13: Task 8 Alternative Return to Task Chapter Complete! 3.6 Additional Material Below is some additional material that might help you understand figures a bit more and how to present them in reports More on aes() - when to and when not to! The inclass activity has a short note about using the aes() call but we see people having issues with this so we thought a quick demonstration might help. Here is the code from the activity menrot_angle_sex &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle, Sex) %&gt;% summarise(mean_Resp = mean(Time)) ## `summarise()` has grouped output by &#39;Angle&#39;. You can override using the `.groups` argument. ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + theme_gray() Figure 3.14: Changing Shape and Size of Data Points Specifically, we are going to focus on the geom_point() line. Inside the aes() we stated color = Sex, shape = Sex and outside the aes() we stated, size = 3. Earlier we said, outside aes() means that all observations take the one value or color or type. Inside means that each observation within a condition takes the same value or color or type, but different conditions have different values/color/type. So based on that understanding, in the above plot, all shapes have the same size (i.e. 3), but there is a different shape and color of shape for each sex. Now lets demonstrate some alternatives. All points have the same shape, color and size - nothing is in an aes(). here we need to state the color, shape and size we want all observations to have. We have chosen red for color (in quotes) shape style 3 (+) and size 6. We are not using a variable to split observations into groups. we have changed the size to make the visualisation clearer ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(color = &quot;red&quot;, shape = 3, size = 6) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + theme_bw() Figure 3.15: All points have the same shape, color and size All points have the same shape and size but color is determined by Sex (so goes inside the aes()). here we need to state the shape and size we want all dots to have. But this time we are giving the different levels within Sex (i.e male and female) different colors - by putting that inside the aes() ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex), shape = 3, size = 6) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + theme_bw() Figure 3.16: All points have the same shape and size but color is determined by Sex We showed the code for setting color and shape by Sex previously. Instead now we will set the color, shape and size by the variable Sex. here as all options are in the aes() we will have different colors, sizes, and shapes between males and females, but all males will have the same color, size and shape, and all females will have the same color size and shape. ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex, size = Sex)) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + theme_bw() ## Warning: Using size for a discrete variable is not advised. Figure 3.17: All points have the same shape and size but color is determined by Sex You actually get a warning when doing this option. Why? Because if you had numerous options then you would have too many different shapes being created and it would cause issue with the code and may even crash it. Pay attention to warnings. And the reason you have to decide what is the best approach for displaying your data is because if all observations within a condition are the same, then showing them as all different colors and shapes makes very little sense. You always need to think about what you are trying to convey. Look at these two figures and think about which one is easier to understand that all observations are from the same condition. The one on the left! The one on the right suggests there is something different about all the observations. Figure 3.18: The plot on the left suggests all observations are of the same condition. The figure on the right suggests a difference between all observations. Always think about what your information you convey in your figures! Hopefully that is beginning to become clearer. Insider the aes() means that all observations within a variable/condition are shown the same, but are different from observations from a different variable/condition. Outside the aes() means that all observations are shown as the same regardless of condition. Combining Plots into one Figure Space within a report is a commodity. Figures can be incredibly useful in getting information across in a very efficient manner, but when you have a strict word count, having multiple figures can really chew into the limit, given that each figure needs a legend and each legend counts. One way to get around this is to merge figures together into one big figure that perhaps convey similar or related information. We are going to show you how to do that using a package called patchwork. DO NOT install packages in the Boyd Orr labs; most are already there and just need called in through library(). If it is not there, speak to a member of the team. patchwork is unlikely to be on our lab machines so please only try this on your own machine. If you havent previously installed the patchwork package on your own machine before, you will have to install it first, e.g. install.packages(patchwork). Thinking back to the preclass, plots like boxplots and histograms, when combined, can be incredibly useful in understanding the overall shape of your data and whether or not it fits the assumptions of inferential tests, something we will come on to later. If we were to create two separate plots, we might get something like this: menrot_hist_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_hist_correct, aes(x = Mean_Time, fill = Sex)) + geom_histogram() + theme_bw() Figure 3.19: A histogram of distribution of Mean Time counts by Sex And: menrot_box_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) ## `summarise()` has grouped output by &#39;Participant&#39;. You can override using the `.groups` argument. ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = Sex)) + geom_boxplot() + theme_bw() Figure 3.20: A boxplot of the spreads of Mean Time for Correct Responses by Sex Now given that they both divide the data by sex, you can start to see how the figure legend for each plot becomes a bit repetitive, and how combining them into one figure would potentially make things easier. There are a number of packages to do this, but patchwork is very straightforward and flexible. Lets now call in patchwork library(patchwork) The first thing you need to do when using patchwork is save your plots in an object (just like you would with the output of any function). Using the code above, this might look like below for the boxplot: p_box &lt;- ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = Sex)) + geom_boxplot() + labs(title = &quot;A&quot;) + theme_bw() And below for the histogram: p_hist &lt;- ggplot(data = menrot_hist_correct, aes(x = Mean_Time, fill = Sex)) + geom_histogram() + labs(title = &quot;B&quot;) + theme_bw() Note: The reason for the inclusion of a title on each plot will become clear in a second. Note: When you run these codes no plots will be generated as you are saving them as objects - the boxplot in p_box and the histogram in p_hist. It is important to realise this distinction as if someone asks you to make produce a code where the figure is generated when your code knits, and you have saved your plot as an object, then your figure might not show. If you have save a plot as an object, you can generate the figure by just calling the name of the object. If you look at the ggplot cheatsheet you will see this approach a lot. Here is how we would call the boxplot stored in p_box. p_box Figure 3.21: A boxplot of the spreads of Mean Time for Correct Responses by Sex p_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.22: A histogram of distribution of Mean Time counts by Sex Note: You will see a warning from the histogram plot about selecting a binwidth. We havent really looked at this yet but will do in due course. If you wanted to fix that warning then changing the histrogram code to something like + geom_histogram(binwidth = 100) might work. The value that you enter is relative to the scale of your data. A binwidth of 1 would create a bin every increase of 1 ms. A binwidth of 100 creates a bin every 100 ms. So far so nothing exciting. Looks just like what we have seen in the lab. Didnt we say we wanted both the plots in a single figure, right? Well to do that in patchwork, we simply add the plots together using a plus sign (+), as such: p_box + p_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.23: A boxplot (A - left) of the spreads of Mean Time for Correct Responses, and histogram (B - right) of distribution of Mean Time counts, both separated by Sex (female - red, male - cyan) Note: We can use the titles\" we added to the plots in the original code above to tell readers which plot, within the combined figure, we are referring to, A or B, left or right, as shown in the figure legend beneath the figure. This might seem a bit pedantic, but you have no control over how somebody views your published figure, and as such clarity is paramount! Awesome, that it? No! We can also change the configuration of the plots in the combined figure. Say we wanted the plots on top of each other - portrait rather than landscape - well in that instance we divide the plots using the divide sign (/), as such: p_box / p_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.24: A boxplot (A - top) of the spreads of Mean Time for Correct Responses, and histogram (B - bottom) of distribution of Mean Time counts, both separated by Sex (female - red, male - cyan) And now we refer to top and bottom, rather than left and right. In fact, patchwork is really flexible and can work with multiple plots and arrangements. Hypothetically, say you had three plots and wanted two on top of one, then you would use the approach of combining + and / as such: (plot1 + plot2)/plot3 Remember the trick to using patchwork is to save your plots as objects first (p1 &lt;- ggplot(....)) and the rest is easy. But be sure to always know if your figure is to be shown when knitted or not; more often than not, seeing the figure is more important than seeing the code. Happy Visualising! End of Additional Material! "],["revisiting-probability-distributions.html", "Lab 4 Revisiting Probability Distributions 4.1 Overview 4.2 PreClass Activity 1 4.3 PreClass Activity 2 4.4 InClass Activity 1 4.5 InClass Activity 2 (Additional) 4.6 InClass Activity 3 (Additional) 4.7 Test Yourself 4.8 Solutions to Questions", " Lab 4 Revisiting Probability Distributions 4.1 Overview Probability is to some degree the cornerstone of any Psychological theory that is based on quantitative analysis. We establish an outcome (e.g. a difference between two events), then establish the probability of that outcome against some model or standard. Probability is important for quantifying the uncertainty in our conclusions. You will have already heard about probability in lectures/journal articles/etc but we will try to help you gain a deeper understanding of probability through the course of the next few chapters and in how we use it to make an inference about a population. We will start by looking at some of the general ideas behind probability. We wont be using a lot of Psychology data or concepts here as it can be easier to understand probability first in everyday concrete examples. That said, whilst reading the examples, and trying them out, think about how they might relate to Psychology examples and be sure to ask questions. This preclass is a bit of a read so take your time and try to understand it fully. Much of it will be familiar though from the PsyTeachR Grassroots book as it recaps some of the ideas. Also, there are no cheatsheets for this chapter as we will not be using a specific package. However you can make full use of the R help function (e.g. ?sample) when you are not clear on what a function does. Also, remember what we have said previously; do not be shy to do what we do and run a Google Search for finding out more about some of the stats concepts covered here. There are loads of videos and help pages out there with clear examples to explain difficult concepts. In this chapter you will: Revise probability concepts that we discussed in the PsyTeachR Grassroots book Calculate probabilities Create probability distributions Make estimations from probability distributions. Portfolio Point - Samples, Populations and Inference The population is the whole group that you want to know something about - everyone or everything in that group. The sample is the part of the population that you are testing. The sample is always smaller than the population as it is unlikely that you would ever be able to test everyone in a population, but the sample should be representative of the population based on random sampling. This means that even though you are not using the whole population, the sample you are using represents the whole population because you randomly sampled people into it. If this is true, that the sample is representative of the population, then testing on the sample allows you to make some inference about the population; you infer a characteristic of the population from testing on the sample. 4.1.1 Discrete or Continuous Datasets This is a short tutorial with just a couple of quick recap questions on how the level of measurement can alter the way you tackle probability - i.e. whether the data is discrete or continuous. Quickfire Questions Discrete data can only take specific/certain/exact values (e.g. groups, integers). For example, the number of participants in an experiment would be discrete - we cant have half a participant! Discrete variables can also be further broken down into nominal/categorical and ordinal variables. Fill in the blanks in the below sentences using the words: ordinal, nominal/categorical. Nominal Ordinal data is based on a set of categories that have no natural ordering (e.g. left or right handed). For example, you could separate participants according to left or right handedness or by course of study (e.g., psychology, biology, history, etc.). Nominal Ordinal data is a set of categories that have a natural ordering; you know which is the top/best and which is the worst/lowest, but the difference between categories may not be constant. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Continuous data on the other hand can take any value in the scale being measured. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), also reaction time or the distance you travel to university every day. Fill in the blanks in the below sentences using the two remaining levels of measurement not offered above Continuous data can be broken into or data - more of this another time. When you read journal articles or when you are working with data in the lab, it is really good practice to take a minute or two to figure out the type of variables you are reading about and/or working with. Explain This - I dont get these answers The four level of measurements are nominal (also called categorical), ordinal, interval and ratio. Discrete data only uses categories or whole numbers and is therefore either nominal or ordinal data. Continuous data can take any value, e.g. 9.00 or 9.999999999, and so is either interval or ratio data. You should now have a good understanding of discrete vs continuous data. This will help you complete the activities of this chapter and in your understanding of probability and inference. 4.2 PreClass Activity 1 4.2.1 General Probability Calculations Today we will begin by recapping the concepts of probability calculations from lectures and the PsyTeachR Grassroots book, looking at discrete distributions - where values are categories (e.g. house, face, car) or whole numbers (e.g. 1, 2 3 and not 1.1, 1.2 etc). At the end of this section, there are three Quickfire Questions to test your understanding. Use the forums to start a discussion on any questions you are unsure or ask a member of staff. When we talk about probability we mean we are interested in the likelihood of an event occurring. The probability of any discrete event occuring can be formulated as: \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] The probability of an event is represented by a number between 0 and 1, and the letter p. For example, the probability of flipping a coin and it landing on tails, most people would say, is estimated at p = .5, i.e. the likelihood of getting tails is \\(p = \\frac {1}{2}\\) as there is one desired outcome (tails) and two possibilities (heads or tails). For example: 1. The probability of drawing the ten of clubs from a standard pack of cards would be 1 in 52: \\(p = \\frac {1}{52} \\ = .019\\). One outcome (ten of clubs) with 52 possible outcomes (all the cards) 2. Likewise, the probability of drawing either a ten of clubs or a seven of diamonds as the the first card that you draw from a full deck would be 2 in 52: \\(p = \\frac {2}{52} \\ = .038\\). In this case you are adding to the chance of an event occurring by giving two possible outcomes so it becomes more likely to happen than when you only had one outcome. 3. Now say you have two standard packs of cards mixed together. The probability of drawing the 10 of clubs from this mixed pack would be 2 in 104: \\(p = \\frac{2}{104}= .019\\). Two possible outcomes but more alternatives than above, 104 this time, meaning it is less probable than Example 2 but the same probability as Example 1. The key thing to remember is that probability is a ratio between the number of ways a specified outcome can happen and the number of all possible outcomes. 4. Lets instead say you have two separate packs of cards. The probability of drawing the 10 of clubs from both packs would be: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). The probability has gone down again because you have created an event that is even more unlikely to happen. This is called the joint probability of events. * To find the joint probability of two separate events occuring you multiply together the probabilities of the two individual separate events (often stated as independent, mutually exclusive events). 5. What about the probability of drawing the 10 of clubs from a pack of 52, putting it back (which we call replacement), and subsequently drawing the 7 of diamonds? Again, this would be represented by multiplying together the probability of each of these events happening: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). * The second event (drawing the 7 of diamonds) has the same probability as the first event (drawing the 10 of clubs) because we put the original card back in the pack, keeping the number of all possible outcomes at 52. This is **replacement**. 6. Finally, say you draw the 10 of clubs from a pack of 52 but this time dont replace it. What is the probability that you will draw the 7 of diamonds in your next draw (again without replacing it) and the 3 of hearts in a third draw? This time the number of cards in the pack is fewer for the second (51 cards) and third draws (50 cards) so you take that into account in your multiplication: \\(p = \\frac{1}{52} \\times \\frac{1}{51}\\times \\frac{1}{50}= .000008\\). Portfolio Point - Presenting probabilities So the probability of an event is the number of all the possible ways an event could happen, divided by the number of all the possible outcomes. When you combine probabilities of two separate events you multiple them together to obtain the joint probability. You may have noticed that we tend to write p = .008, for example, as opposed to p = 0.008 (with a 0 before the decimal place). Why is that? Convention really. As probability can never go above 1, then the 0 before the decimal place is pointless. Meaning that most people will write p = .008 instead of p = 0.008, indicating that the max value is 1. We allow either version in the answers to this chapter as we are still learning but try to get in the habit of writing probability without the 0 before the decimal place. Quickfire Questions What is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely your name? Enter your answer to 3 decimal places: What is the probability of randomly drawing your name out of a hat of 12 names, putting it back, and drawing your name again? Enter your answer to 3 decimal places: Tricky: In a stimuli set of 120 faces, where 10 are inverted and 110 are the right way up, what is the probability of randomly removing one inverted face on your first trial, not replacing it, and then removing another inverted face on the second trial? Enter your answer to three decimal places: Helpful Hint Out of 12 possible outcomes you are looking for one possible event. There are two separate scenarios here. In both scenarios there are 12 possible outcomes in which you are looking for one possible event. Since there are two separate scenarios, does this make it more or less likely that you will draw your name twice? In the first trial here are 120 possible outcomes (faces) in which you are looking for 10 possible events (inverted faces). In the second trial you have removed the first inverted face from the stimuli set so there are now only 119 trials in total and 9 inverted faces. Remember you need to multiply the probabilities of the first trial and second trial results together! Explain This - I dont get these answers p = .083. One outcome (your name) out of 12 possibilities, i.e. \\(p = \\frac{1}{12}\\) p = .007. Because you replace the name on both draws it is \\(p = \\frac{1}{12}\\) in each draw. So \\(p = \\frac{1}{12} * \\frac{1}{12}\\) and then rounded to three decimal places p = .006. In the first trial you have 10 out of 120, but as you remove one inverted face the second trial is 9 out of 119. So the formula is \\(p = \\frac{10}{120} * \\frac{9}{119}\\) 4.2.2 Creating a Simple Probability Distribution We will now recap plotting probability distributions by looking at a simulated coin toss. You may remember some of this from the the PsyTeachR Grassroots book but dont worry if not as we are going to work through it again. Work or read through this example and then apply the logic to the quickfire questions at the end of the section. Scenario: Imagine we want to know the probability of X number of heads in 10 coin flips - for example, what is the probability of flipping a coing 10 times and it coming up heads two times. To simulate 10 coin flips we will use the sample() function where we randomly sample (with replacement) from all possible events: i.e. either heads or tails. Lets begin: Open a new script and copy in the code lines below. The first line of code loads in the library as normal. The second line of code provides the instruction to sample our options Heads or Tails, ten times, with replacement set to TRUE. Note: Because our event labels are strings (text), we enter them into the function as a vector; i.e. in quotes Note: Below the lines of code, you will see the output that we got when we ran our code. Dont worry if your sequence of heads and tails is different from this output; this is to be expected as we are generating a random sample. library(&quot;tidyverse&quot;) sample(c(&quot;HEADS&quot;, &quot;TAILS&quot;), 10, TRUE) ## [1] &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; &quot;TAILS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;HEADS&quot; ## [10] &quot;TAILS&quot; Note: If you want to get the same output as we did, add this line of code to your script prior to loading in the library. This is the set.seed() function which you can put a number in so that each time you run a randomisation you get the same number. set.seed(1409) Portfolio Point - Sampling and Replacement Sampling is simply choosing or selecting something - here we are randomly choosing one of the possible options; heads or tails. Other examples of sampling include randomly selecting participants, randomly choosing which stimuli to present on a given trial, or randomly assigning participants to a condition e.g.drug or placeboetc. Replacement is putting the sampled option back into the pot of possible options. For example, on the first turn you randomly sample HEADS from the options of HEADS and TAILS with replacement, meaning that on the next turn you have the same two options again; HEADS or TAILS. Sampling without replacement means that you remove the option from subsequent turns. So say on the first turn you randomly sample HEADS from the options HEADS and TAILS but without replacement. Now on the second turn you only have the option of TAILS to randomly sample from. On the third turn without replacement you would have no options. So replacement means putting the option back for the next turn so that on each turn you have all possible outcome options. Why would you or why wouldnt you want to use sampling with replacement in our coin toss scenario? If you arent sure then set replacement as FALSE (change the last argument from TRUE to FALSE) and run the code again. The code will stop working after 2 coin flips. We want to sample with replacement here because we want both options available at each sampling - and if we didnt then we would run out of options very quickly since were doing 10 flips. So far our code returns the outcomes from the 10 flips; either heads or tails. If we want to count how many heads we have we can simply sum up the heads. However, heads isnt a number, so to make life easier we can re-label our events (i.e. our flips) as 0 for tails and 1 for heads. Now if we run the code again we can pipe the sample into a sum() function to total up all the 1s (heads) from the 10 flips. Run this line of code a number of times, what do you notice about the output? Note: As our event labels are now numeric, we dont need the vector. Note: 0:1 means all numbers from 0 to 1 in increments of 1. So basically, 0 and 1. sample(0:1, 10, TRUE) %&gt;% sum() ## [1] 5 The ouptut of this line changes every time we run the code as we are randomly sampling 10 coin flips each time. And to be clear, if you get an answer of 6 for example, this means 6 heads, and in turn, 4 tails. By running this code over and over again we are basically demonstrating how a sampling distribution is created. Portfolio Point - Whats a sampling distribution? A sampling distribution shows you the probability of drawing a sample with certain characteristics from the population; e.g. the probability of 5 heads in 10 flips, or the probability of 4 heads in 10 flips, or the probability of X heads in 10 flips of the coin. Now in order to create a full and accurate sampling distribution for our scenario we need to replicate these 10 flips a large number of times - i.e. replications. The more replications we do the more reliable the estimates. Lets do 10000 replications of our 10 coin flips. This means we flip the coin 10 times, count how many heads, save that number, and then repeat it 10000 times. We could do it the slow way we demonstrated above, just running the same line over and over and over again and noting the outcome each time. Or we could use the replicate() function. Copy the line of code below into your script and run it. Here we are doing exactly as we said and saving the 10000 outputs (counts of heads) in the dataframe called heads10k (k is shorthand for thousand). heads10k &lt;- replicate(10000, sample(0:1, 10, TRUE) %&gt;% sum()) So to reiterate, the sum of heads (i.e. the number of times we got heads) from each of these 10000 replications are now stored as a vector in heads10k. If you have a look at heads10k, as shown in the box below, it is a series of 10000 numbers between 0 and 10 each indicating the number of heads, or more specifically 1s, that you got in a set of 10 flips. ## int [1:10000] 7 4 5 6 2 6 2 6 8 5 ... Now in order to complete our distribution we need to: Convert the vector (list of numbers for the heads counts) into a data frame (a tibble) so we can work on it. The numbers will be stored in a column called heads. Then group the results by the number of possible heads; i.e. group all the times we got 5 heads together, all the times we got 4 heads together, etc. Finally, we work out the probability of a heads result, (e.g. probability of 5 heads), by totaling the number of observations for each possible result (e.g. 5 heads) and submitting it to our probability formula above (number of outcomes of event divided by all possible outcomes) so the number of times we got a specific number of heads (e.g. 5 heads) divided by the total number of outcomes (i.e. the number of replications - 10000). We can carry out these steps using the following code: Copy the below code into your script and run it. data10k &lt;- tibble(heads = heads10k) %&gt;% # creating a tibble/dataframe group_by(heads) %&gt;% # group by number of possibilities summarise(n = n(), p=n/10000) # count occurances of possibility, # &amp; calculate probability (p) of # each We now have a discrete probability distribution of the number of heads in 10 coin flips. Use the View() function to have look at your data10k variable. You should now see for each heads outcome, the total number of occurrences in 10000 replications (n) plus the probability of that outcome (p). Table 4.1: A sampling distribution for the number of heads in 10 flips of a coin. p = probability of obtaining that number of heads in 10000 replications of 10 flips of a coin heads n p 0 7 0.0007 1 82 0.0082 2 465 0.0465 3 1186 0.1186 4 2070 0.2070 5 2401 0.2401 6 2113 0.2113 7 1166 0.1166 8 422 0.0422 9 79 0.0079 10 9 0.0009 It will be useful to visualize the above distribution: ggplot(data10k, aes(heads,p)) + geom_col(fill = &quot;skyblue&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 10 flips (p)&quot;) + theme_bw() + scale_x_discrete(limits=0:10) ## Warning: Continuous limits supplied to discrete scale. ## Did you mean `limits = factor(...)` or `scale_*_continuous()`? Figure 4.1: Probability Distribution of Number of Heads in 10 Flips So in our analysis, the probability of getting 5 heads in 10 flips is 0.2401. But remember, do not be surprised if you get a slightly different value. Ten thousand replications is a lot but not a huge amount compared to infinity. If you run the analysis with more replications your numbers would become more stable, e.g. 100K. Note that as the possible number of heads in 10 flips are all related to one another, then summing up all the probabilities of the different number of heads will give you a total of 1. This is different to what we looked at earlier in cards where the events were unrelated to each other. As such, you can use this informaiton to start asking questions such as what would be the probability of obtaining 2 or less Heads in 10 flips? Well, if the probability of getting no heads (in 10 flips) in this distribution is 0.0007, and the probability of getting 1 head is 0.0082, and the probability of getting 2 heads is 0.0465, then the probability of 2 or less Heads in this distribution is simply the sum of these values: 0.0554. Pretty unlikely then! Quickfire Questions Look at the probability values corresponding to the number of coin flips you created in the data10k sample distribution (use View() to see this): Choose from the following options, if you wanted to calculate the probability of getting 4, 5 or 6 heads in 10 coin flips you would: multiply individual probabilities together sum individual probabilities together Choose from the following options, if you wanted to calculate the probability of getting 6 or more heads in 10 coin flips you would: multiply individual probabilities together sum individual probabilities together Choose from the following options, the distribution we have created is: continuous discrete Explain This - I dont understand the answers! If you think about it, we cant get 5.5 heads or 2.3 heads, we can only get whole numbers, 2 heads or 5 heads. This means that the data and the distribution is discrete. (Dont be confused by one of the functions saying continuous) To find the probability of getting say 4, 5, or 6 heads in 10 coin flips, you are combining related scenarios together, therefore you need to find the individual probabilities of getting 4, 5 or 6 heads in 10 coin flips, then sum the probabilities together to get the appropriate probability of obtaining 4, 5 or 6 heads. It is the same with 6 or more heads, just sum the probabilities of 6, 7, 8, 9 and 10 heads to get the probability of 6 or more heads. Not sure if you should be summing or multiplying probabilities? A good way to remember, from both the coin flip examples and from the pack of cards examples earlier, is that if the scenarios are related you are summing probabilities, if scenarios are separate you are multiplying probabilities. Related scenarios are usually asking you about the probability of either / or scenarios occuring, whereas separate scenarios usually ask about the probability of one scenario and another scenario both occuring. Your sample distribution data10k has already completed the first part of this calculation for you (finding individual probabilities of n heads in 10 coin flips), so all you need to to is sum the required probabilities together! 4.2.3 The Binomial Distribution - Creating a Discrete Distribution Great, so we are now learning how probabilities and distributions work. However, if we had wanted to calculate the probability of 8 heads from 10 coin flips we dont have to go through this entire procedure each time. Instead, because we have a dichotomous outcome, heads or tails, we can establish probabilities using the binomial distribution - effectively what you just created. You can look up the R help page on the binomial distribution (type ?dbinom directly into the console) to understand how to use it but we will walk through some essentials here. Well use 3 functions to work with the binomial distribution and to ask some of the questions we have asked above: dbinom() - the density function. This function gives you the probability of x successes (e.g. heads) given the size (e.g. number of trials) and probability of success prob on a single trial (here its 0.5, because we assume were flipping a fair coin - Heads or Tails) pbinom() - the distribution function. This function gives you the probability of getting a number of successes below a certain cut-off point given the size and the prob. This would be for questions such as the probability of 5 heads or less for example. It sums the probability of 0, 1, 2, 3, 4 and 5 heads. qbinom() - the quantile function. This function is the inverse ofpbinom in that it gives you the x axis value below (and including the value) which the summation of probabilities is greater than or equal to a given probability p, given the size and prob. In other words, how many heads would you need to have a probability of p = 0.0554 Lets look at each of these functions in turn a little. The thing to keep in mind about probability is that every event has a likelihood of occuring on their distribution. We are trying to look at how those numbers come about and what they mean for us. 4.2.4 dbinom() - The Density Function Using the dbinom() function we can create probabilities for any possible outcomes where there are two possibilities of outcome on each trial - e.g. heads or tails, cats or dogs, black or red. We are going to stick with the coin flip idea. Here we are showing the code for obtaining 3 heads in 10 flips: dbinom(3, 10, 0.5) or all possible outcomes of heads (0:10) in 10 flips: dbinom(0:10, 10, 0.5) And if we plot the probability of all possible outcomes in 10 flips it would look like this: Figure 4.2: Probability Distribution of Number of Heads in 10 Flips The dbinom (density binom) function takes the format of dbinom(x, size, prob), where the arguments we give are: x the number of heads we want to know the probability of. Either a single one, 3 or a series 0:10. size the number of trials (flips) we are doing; in this case, 10 flips. prob the probability of heads on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5 Now say if we wanted to know the probability of 6 heads out of 10 flips. We would only have to change the first argument to the code we used above for 3 heads, as such: dbinom(6, 10, 0.5) ## [1] 0.2050781 So the probability of 6 heads, using dbinom() is p = 0.2050781. If you compare this value to the data10k value for 6 you will see they are similar but not quite the same. Tis is because dbinom() uses a lot more replications than the 10000 we used in our simulation. In terms of visualising what we have just calculated, p = 0.2050781 is the height of the green bar in the plot below. Figure 4.3: Probability Distribution of Number of Heads in 10 Flips with the probability of 6 out of 10 Heads highlighted in green Quickfire Questions To three decimal places, what is the probability of 2 heads out of 10 flips? Explain This - I cant get the right answer You want to know the probability of 2 heads in 10 flips. X is therefore 2; Size is therefore 10; and the probability of outcomes on each trial stays the same at .5. As such the would be dbinom(2, 10, 0.5) = .04394531 or rounded = .044 4.2.5 pbinom() - The Cumulative Probability Function What if we wanted to know the probability of up to and including 3 heads out of 10 flips? We have asked similar questions above. We can either use dbinom for each outcome up to 3 heads and sum the results: dbinom(0:3, 10, 0.5) %&gt;% sum() ## [1] 0.171875 Or we can use the pbinom() function; known as the cumulative probability distribution function or the cumulative density function. The first argument we give is the cut-off value up to and including the value which we want to know the probability of (here its up to 3 heads). Then, as before, we tell it how many flips we want to do and the probability of heads on a single trial. Copy this line into your script and run it: pbinom(3, 10, 0.5, lower.tail = TRUE) ## [1] 0.171875 So the probability of up to and including 3 heads out of 10 flips is 0.172. For visualization, what we have done is calculated the cumulative probability of the lower tail of the distribution (lower.tail = TRUE; shown in green below) up to our cut-off of 3: Figure 4.4: Probability Distribution of Number of Heads in 10 Flips with the probability of 0 to 3 Heads highlighted in green - lower.tail = TRUE So the pbinom function gives us the cumulative probability of outcomes up to and including the cut-off. But what if we wanted to know the probability of outcomes including and above a certain value? Say we want to know the probability of 7 heads or more out of 10 coin flips. The code would be this: pbinom(6, 10, 0.5, lower.tail = FALSE) ## [1] 0.171875 Lets explain this code a little. First, we switch the lower.tail call from TRUE to FALSE to tell pbinom() we dont want the lower tail of the distribution this time (to the left of and including the cut-off), we want the upper tail, to the right of the cut-off. This results in the cumulative probability for the upper tail of the distribution down to our cut-off value (shown in green below). Next we have to specify a cut-off but instead of stating 7 as you might expect, even though we want 7 and above, we specify our cut-off as 6 heads. But why? We set the cut-off as 6 because when working with the discrete distribution, only lower.tail = TRUE includes the cut-off (6 and below) whereas lower.tail = FALSE would be everything above the cut-off but not including the cut-off (7 and above). So in short, if we want the upper tail when using the discrete distribution we set our cut-off value (x) as one lower than the number we were interested in. We wanted to know about 7 heads so we set our cut-off as 6. Figure 4.5: Probability Distribution of Number of Heads in 10 Flips with the probability of 7 or more Heads highlighted in green - lower.tail = FALSE Portfolio Point - Am all in a Tail Spin! Lower TRUE or FALSE The most confusing part for people we find is the concept of lower.tail. If you look at a distribution, say the binomial, you find a lot of the high bars are in the middle of the distribution and the smaller bars are at the far left and right of the distribution. Well the far left and right of the distribution is called the tail of the distribution - they tend to be an extremity of the distribution that taper off like a..well like a tail. A lot of the time we will talk of left and right tails but the pbinom() function only ever considers data in relation to the left side of the distribution - this is what it calls the lower.tail. Lets consider lower.tail = TRUE. This is the default, so if you dont state lower.tail =  then this is what is considered to be what you want. lower.tail = TRUE means all the values to the left of your value including the value you state. So on the binomial distribution if you say give me the probability of 5 heads or less, then you would set lower.tail = TRUE and you would be counting and summing the probability of 0, 1, 2, 3, 4 and 5 heads. You can check this with dbinom(0:5, 10, .5) %&gt;% sum(). However, if you say give me the probability of 7 or more heads, then you need to do lower.tail = FALSE, to consider the right-hand side tail, but also, you need to set the code as pbinom(6, 10, .5, lower.tail = FALSE). Why 6 and not 7? Because the pbinom() function, when lower.tail = FALSE, starts at the value plus one to the value you state; it always considers the value you state as being part of the lower.tail so if you say 6, it includes 6 in the lower.tail and then gives you 7, 8, 9 and 10 as the upper tail. If you said 7 with lower.tail = FALSE, then it would only give you 8, 9 and 10. This is tricky but worth keeping in mind when you are using the pbinom() function. And remember, you can always check it by using dbinom(7:10, 10, .5) %&gt;% sum() and seeing whether it matches pbinom(6, 10, 0.5, lower.tail=FALSE) or pbinom(7, 10, 0.5, lower.tail=FALSE) Quickfire Questions Using the format shown above for the pbinom() function, enter the code that would determine the probability of up to and including 5 heads out of 20 flips, assuming a probability of 0.5: To two decimal places, what is the probability of obtaining more than but not including 50 heads in 100 flips? Helpful Hint You are looking to calcuate the probability of 5 or less heads (x) out of 20 flips (size), with the probability of heads in one trial (prob) remaining the same. Do you need the lower.tail call here if you are calculating the cumulative probability of the lower tail of the distribution? You are looking to calculate the probability of 51 or more heads (x), out of 100 flips (size), with the probability of heads in one trial (prob) remaining the same (0.5). Do you need the lower.tail call here if you are calculating the cumulative probability of the upper tail of the distribution? Remember, because you are not looking at the lower.tail, the value of heads that you enter in pbinom() will not be included in the final calculation, e.g. entering pbinom(3, 100, lower.tail = FALSE) will give you the probability for 4 and above heads. If you were instead looking at the lower.tail, entering pbinom(3, 100, lower.tail = TRUE) would give you the probability of 3 and below heads. Explain This - I cant get these answers The code for the first one would be: pbinom(5, 20, 0.5) or pbinom(5, 20, 0.5, lower.tail = TRUE) The code for the second one would be: pbinom(50, 100, 0.5, lower.tail = FALSE), giving an answer of .46. Remember you can confirm this with: dbinom(51:100, 100, 0.5) %&gt;% sum() 4.2.6 qbinom() - The Quantile Function The qbinom() function is the inverse of the pbinom() function. Whereas with pbinom() you supply an outcome value x and get a tail probability, with qbinom() you supply a tail probability and get the outcome value that (approximately) cuts off that tail probability. Think how you would rephrase the questions above in the pbinom() to ask a qbinom() question. Worth noting though that qbinom() is approximate with a discrete distribution because of the jumps in probability between the discrete outcomes (i.e. you can have probability of 2 heads or 3 heads but not 2.5 heads). Lets see how these two functions are inverses of one another. Below is code for the probability of 49 heads or less in 100 coin flips p1 &lt;- pbinom(49, 100, .5, lower.tail = TRUE) p1 ## [1] 0.4602054 This tells us that the probability is p = 0.4602054. If we now put that probability (stored in p1) into qbinom we get back to where we started. E.g. qbinom(p1, 100, .5, lower.tail = TRUE) ## [1] 49 This can be stated as the number of heads required to obtain a probability of 0.4602054 is 0.4602054. So the qbinom() function is useful if you want to know the minimum number of successes (heads) that would be needed to achieve a particular probability. This time the cut-off we specify is not the number of heads we want but the probability we want. For example say we want to know the minimum number of heads out of 10 flips that would result in a 5% heads success rate (a probability of .05), we would use the following code. qbinom(.05, 10, 0.5, lower.tail = TRUE) ## [1] 2 Because we are dealing with the lower tail, this is telling us for a lower tail probability of .05, we would expect at most 2 heads in 10 flips. However, it is important to note that for a discrete distribution, this value is not exact. We can see this by pbinom(2, 10, .5, lower.tail = TRUE) ## [1] 0.0546875 which is not exactly p = .05 but very close. This is because the data we are working with is a discrete distribution and so qbinom() gives us the closest category to the boundary. Portfolio Point - I dont understand qbinom arguments We found a lot of students asking about qbinom() and how it works when you are inputting two different probabilities as the arguments for qbinom(). Let us try and make things clearer. First, it is useful to remember that it is the inverse of pbinom(): pbinom() gives you a tail probability associated with x, and qbinom() gives you the closest x that cuts off the specified tail probability. If you understand pbinom() then try reversing the question and see if that helps you understand qbinom(). The qbinom() function is set up as qbinom(p, size, prob). First of all, you have used prob in the previous two functions, dbinom() and pbinom(), and it represents the probability of success on a single trial (here it is the probability of heads in one coin flip, prob = .5). Now, prob represents the probability of success in one trial, whereas p represents the tail probability you want to know about. The function gives you the value of x that would yield that probability you asked for. So you give qbinom() a tail probability of p = .05, in 10 flips, when the probability of a success on any one flip is prob = .5. And it tells you the answer is 2, meaning that getting up to 2 flips on 10 trials has a probability of roughly .05. qbinom() also uses the lower.tail argument and it works in a similar fashion to pbinom(). Quickfire Questions Type in the box, the maximum number of heads associated with a tail probability of 10% (.1) in 17 flips: Explain This - I cant get the answer The answer would be 6 because the code would be: qbinom(0.1, 17, 0.5, lower.tail = TRUE) Remember that you want an overall probability of 10% (p = .1), you have 17 flips in each go (size = 17), and the probability of heads on any one flip is .5 (prob = .5). And you want the maximum number for the lower tail, so the lower.tail is TRUE. Job Done - Activity Complete! Excellent! We have now recapped some general probability concepts as well as going over the binomial distribution again. We will focus on the normal distribution in the lab but it would help to read the brief section on as part of the PreClass activities. Always keep in mind that what you are trying to get an understanding of is that every value in a distribution has a probability of existing in that distribution. That probability may be very large, meaning that, for bell-shaped distributions we have looked at, the value is from the middle of the distribution, or that probability might be rather low, meaning it is from the tail, but ultimately every value of a distribution has a probability. If you have any questions please post them on the available forums or run them by a member of staff. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Great work! 4.3 PreClass Activity 2 4.3.1 Continuous Data - Brief Recap on The Normal Distribution This a very short recap on the normal distribution which will help you work through the rest of this chapter if you read it before attempting the rest. In the first PreClass activity, we have seen how we can use a distribution to estimate probabilities and determine cut-off values (these will play an important part in hypothesis testing in later chapters!), but we have looked only at the discrete binomial distribution. Many of the variables we will encounter will be continuous and tend to show a normal distribution (e.g. height, weight, IQ). Lets say were interested in the height of the population of Level 2 psychology students, which we estimate to be between 146cm and 194cm. If we plotted this as a continuous, normal distribution, it will look like: Figure 4.6: The Normal Distribution of height in Level 2 Psychology students (black line). Green line represents the mean. Blue line represent 1 Standard Deviation from the mean. Yellow line represents 2 Standard Deviation from the mean. Red line represents 3 Standard Deviation from the mean. The figure shows the hypothetical probability density of heights ranging from 146cm to 194cm in the population of level 2 psychology students (black curve). This data is normally distributed and has the following properties: 4.3.2 Properties of the Normal distribution: 1. The distribution is defined by its mean and standard deviation: The mean (\\(\\mu\\)) describes the center, and therefore peak density, of the distribution. This is where the largest number of the people in the population will be in terms of height. The standard deviation (\\(\\sigma\\)) describes how much variation there is from the mean of the distribution - on the figure, the standard deviation is the distance from the mean to the inflection point of the curve (the part where the curve changes from a upside-down bowl shape to a right-side-up bowl shape). 2. Distribution is symmetrical around the mean: The mean lies in the middle of the distribution and divides the area under the curve into two equal sections - so we get the typical bell-shaped curve. 3. Total area under the curve is equal to 1: If we were to add up the probabilities (densities) for every possible height, we would end up with a value of 1. 4. The mean, median and mode are all equal: A good way to check if a given dataset is normally distributed is to calculate each measure of central tendency and see if they are approximately the same (normal distribution) or not (skewed distribution). 5. The curve approaches, but never touches, the x axis: You will never have a probability of 0 for a given x axis value. 6. The normal distribution follows the Empirical Rule: The Empirical Rule states that 99.7% of the data within the normal distribution falls within three standard deviations (\\(\\pm3\\sigma\\)) from the mean, 95% falls within two standard deviations (\\(\\pm2\\sigma\\)), and 68% falls within one standard deviation (\\(\\pm\\sigma\\)). Job Done - Activity Complete! We will look at the normal distribution more in the InClass activity but if you have any questions please post them on forums or ask a member of staff. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 4.4 InClass Activity 1 We are going to continue our study of distributions and probability to help gain an understanding of how we can end up with an inference about a population from a sample. In the preclass activity, we focused a lot on basic probability and binomial distributions. In this activity, we will focus more on the normal distribution; one of the key distributions in Psychology. The assignment for this chapter is formative and should not be submitted. This of course does not mean that you should not do it. As will all the formative labs, completing them and understanding them, will benefit you when it comes to completing the summative activities. It will be really beneficial to you when it comes to doing the assignment to have run through both the preclass and inclass activities. You may also want to refer to the Lecture series for help. If you are unclear at any stage please do ask; probability is challenging to grasp at first. Portfolio Point - Always be adding to your knowledge in your own words Remember to add useful information to you Portfoilio! One of the main reasons we do this is because there is a wealth of research that says the more interactive you are with your learning, the deeper your understanding of the topic will be. This relates to the Craik and Lockhart (1972) levels of processing model you will read about in lectures. Always make notes about what you are learning to really get an understanding of the concepts. And, most importantly, in your own words! Reference: Craik, F. I. M., &amp; Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal behavior, 11, 671-684. 4.4.1 Continuous Data and the Normal Distribution Continuous data can take any precise and specific value on a scale, e.g. 1.1, 1.2, 1.11, 1.111, 1.11111,  Many of the variables we will encounter in Psychology will: be continuous as opposed to discrete. tend to show a normal distribution. look similar to below - the bell-shaped curve - when plotted. But can you name any? Take a couple of minutes as a group to think of variables that we might encounter that are normally distributed. We will mention some as we move through the chapters! Figure 4.6: The Normal Distribution of height in Level 2 Psychology students (black line). Green line represents the mean. Blue line represent 1 Standard Deviation from the mean. Yellow line represents 2 Standard Deviation from the mean. Red line represents 3 Standard Deviation from the mean. 4.4.2 Estimating from the Normal Distribution Unlike coin flips, the outcome in the normal distribution is not just 50/50 and as such we wont ask you to create a normal distribution as it is more complicated than the binomial distribution you estimated in the preclass. Instead, just as with the binomial distribution (and other distributions) there are functions that allow us to estimate the normal distribution and to ask questions about the distribution. These are: dnorm() - the Density function for the normal disribution pnorm() - the Cumulative Probability function for the normal disribution qnorm() - the Quantile function for the normal disribution You might be thinking those look familiar. They do in fact work in a similar way to their binomial counterparts. If you are unsure about how a function works remember you can call the help on it by typing in the console, for example, ?dnorm or ?dnorm(). The brackets arent essential for the help. Quickfire Questions Type in the box the binomial counterpart to dnorm()? Type in the box the binomial counterpart to pnorm()? Type in the box the binomial counterpart to qnorm()? Explain This - I dont get the answers The counterpart functions all start with the same letter, d, p, q, it is just the distribution name that changes, binom, norm, t - though we havent quite come across the t-distribution yet. dbinom() is the binomial equivalent to dnorm() pbinom() is the binomial equivalent to pnorm() qbinom() is the binomial equivalent to qnorm() There is also rnorm() and rbinom() but we will look at them another time. 4.4.3 dnorm() - The Density Function for the Normal Distribution Using dnorm(), like we did with dbinom, we can plot a normal distribution. This time however we need: x, a vector of quantiles (in other words, a series of values for the x axis - think of this as the max and min of the distribution we want to plot) the mean of our data and standard deviation sd of our data. We will use IQ as an example. There is actually some disagreement of whether or not IQ is continuous data and to some degree it will depend on the measurement you use. IQ however is definitely normally distributed and we will assume it is continuous for the purposes of this demonstration. Many Psychologists are interested in studying IQ, perhaps in terms of heritability, or interested in controlling for IQ in their own studies to rule out any effect (e.g. clinical and autism studies). 4.4.3.1 Task 1: Standard Deviations and IQ Score Distribution Copy the below code into a new script and run it. Remember that you will need to call tidyverse to your library first. e.g. library(\"tidyverse\"). This code creates the below plot showing a normal distribution of IQ scores (M = 100, SD = 15) ranging from 40 to 160. These are values considered typical for the general population. First we set up our range of IQ values from 40 to 160 Then we plot the distribution of IQ_data, where we have M = 100 and SD = 15 IQ_data &lt;- tibble(IQ_range = c(40, 160)) ggplot(IQ_data, aes(IQ_range)) + stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) + labs(x = &quot;IQ Score&quot;, y = &quot;probability&quot;) + theme_classic() Figure 4.7: Distribution of IQ scores with mean = 100, sd = 15 Which part of the code do you need to change to alter the SD of your plot? mean = 100 sd = 15 (40, 160) Now copy and edit the above code to plot a distribution with mean = 100 and sd = 10, and visually compare the two plots. Group Discussion Point What does changing the standard deviation (sd) do to the shape of the distribution? Spend a few minutes changing the code to various values and running it, and discussing with your group to answer the following questions: What happens to the shape of the distribution if you change the sd from 10 to 20? the distribution gets narrower the distribution gets wider What happens to the shape of the distribution if you change the sd from 10 to 5? the distribution gets narrower the distribution gets wider What does a small or large standard deviation in your sample tell you about the data you have collected? Explain This - I dont get Standard Deviations! Changing the SD from 10 to 20 means a larger standard deviation so you will have a wider distribution. Changing the SD from 10 to 5 means a smaller standard deviation so you will have a narrower distribution. Smaller SD results in a narrower distribution meaning that the data is less spread out; larger SD results in a wider distribution meaning the data is more spread out. A note on the Standard Deviation: You will know from your lectures that you can estimate data in two ways: point-estimates and spread estimates. The mean is a point-estimate and condenses all your data down into one data point - it tells you the average value of all your data but tells you nothing about how spread out the data is. The standard deviation however is a spread estimate and gives you an estimate of how spread out your data is from the mean - it is a measure of the standard deviation from the mean. So imagine we are looking at IQ scores and you test 100 people and get a mean of 100 and an SD of 5. This means that the vast majority of your sample will have an IQ around 100 - probably most will fall within 1 SD of the mean, meaning that most of your participants will have an IQ of between 95 and 105. Now if you test again and find a mean of 100 and an SD of 20, this means your data is much more spread out. If you take the 1 SD approach again then most of your participants will have an IQ of between 80 and 120. So one sample has a very tight range of IQs and the other sample has a very wide range of IQs. All in, from the point-estimate and spread estimate of your data you can tell the shape of your sample distribution. So far so good! But in the above example we told dnorm() the values at the limit of our range and it did the rest; we said give us a range of 40 to 160 IQ scores. However, we could plot it another way by telling dnorm() the sequence, or range, of values we want and how much precision we want between them. 4.4.3.2 Task 2: Changing Range and Step Size of The Normal Distribution Copy the code below in to your script and run it. Here we plot the standard Normal Distribution from -4 to 4 in steps of 0.01. We have also stated a mean of 0 and an sd of 1. ND_data &lt;- tibble(ND_range = seq(-4, 4, 0.01)) ggplot(ND_data, aes(ND_range)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + labs(x = &quot;SD units&quot;, y = &quot;probability&quot;, title = &quot;The Normal Distribution&quot;) + theme_classic() Figure 4.8: The Normal Distribution with Mean = 0 and SD = 1 Quickfire Questions Fill in the box to show what you would type to create a tibble containing a column called ND_range that has values ranging from -10 to 10 in steps of .05: ND_data &lt;- Now that you know what to change, try plotting a normal distribution with the following attributes: range of -10 to 10 in steps of 0.05, a mean of 0, and a standard deviation of 1. Compare your new plot the the original one we created. What change is there in the distribution? Distribution widens No change in distribution Distribution narrows Explain This - I dont understand the answer! To change the distribution you would write: ND_data &lt;- tibble(ND_range = seq(-10, 10, 0.05)) However, when comparing the plots, whilst the plot itself may look thinner, the distribution has not changed. The change in appearance is due to the range of sd values which have been extended from -4 and 4 to -10 and 10. The density of values within those values has not changed however and you will see, more clearly in the second plot, that values beyond -3 and 3 are very unlikely. Remember, every value has a probability on a distribution and we have been able to use the dnorm() function to get a visual representation of how the probability of values change in the normal distribution. Some values are very probable. Some values are very unpprobable. This is a key concept when it comes to thinking about significant differences later in this series. However, as you know, there is one important difference between continuous and discrete probability distributions - the number of possible outcomes. With discrete probability distributions there are usually a finite number of outcomes over which you take a probability. For instance, with 5 coin flips, there are 5 possible outcomes for the number of heads: 0, 1, 2, 3, 4, 5. And because the binomial distribution has exact and finite outcomes, we can use dbinom() to get the exact probability for each outcome. In contrast, with a truly continuous variable, the number of possible outcomes are infinite, because you not only have 0.01 but also .0000001 and .00000000001 to arbitrary levels of precision. So rather than asking for the probability of a single value, we ask the probability for a range of values, which is equal to the area under the curve (the black line in the plots above) for that range of values. As such, we will leave dnorm() for now and move onto looking at establishing the probability of a range of values using the Cumulative Probability function 4.4.4 pnorm() - The Cumulative Probability function Just as dnorm() works like dbinom(), pnorm() works just like pbinom(). So, pnorm(), given the mean and sd of our data, returns the cumulative density function (cumulative probability) that a given probability (p) lies at a specified cut-off point and below, unless of course lower.tail = FALSE is specified in which case it is from the cut-off point and above. OK, in English that people can understand, that means the pnorm() function tells you the probability of obtaining a given value or lower if you set lower.tail = TRUE. Contrastingly, pnorm() function tells you the probability of obtaining a given value or higher if you set lower.tail = FALSE. We will use height to give a concrete example. Say that we test a sample of students (M = 170cm, SD = 7) and we we want to calculate the probability that a given student is 150cm or shorter we would do the following: Remember, lower.tail = TRUE means lower than and including the value of X TRUE is the default so we dont actually need to declare it pnorm(150, 170, 7, lower.tail = TRUE) This tells us that finding the probability of someone 150cm or shorter in our class is about p = 0.0021374. Stated differently, we would expect the proportion of students to be 150cm or shorter to be 0.21% (You can convert probability to proportion by multiplying the probability by 100). This is a very small probability and suggests that it is pretty unlikely to find someone shorter than 150cm in our class. This is mainly because of how small the standard deviation of our distribution is. Think back to what we said earlier about narrow standard deviations round the mean! Another example might be, what would be the probability of a given student being 195 cm or taller? To do that, you would set the following code: pnorm(195, 170, 7, lower.tail = FALSE) This tells us that finding the probability of someone 150cm or shorter in our class is about p = 0.9998225. Or 99.98%. So again, really unlikely. Did you notice something different about the cut-off for this example and from when using the dbinom() function and looking above a cut-off? Why might that be? We will discuss in a second but first a quick task. 4.4.4.1 Task 3: Calculating Cumulative Probability of Height Edit the pnorm() code above to calculate the probability that a given student is 190cm or taller. To three decimal places, as in Task 3, what is the probability of a student being 190cm or taller in this class? Explain This - I dont understand the answer or the tail! The answer is .002. See the solution code at the end of the chapter. The key thing is that there is a difference in where you need to specify the cut-off point in the pbinom() (discussed in the preclass activity) and pnorm() functions for values above x, i.e. when lower.tail = FALSE. If you had discrete data, say the number of coin flips that result in heads, and wanted to calculate the probability above x, you would apply pbinom() and have to specify your cut-off point as x-1 to include x in your calculation. For example, to calculate the probability of 4 or more heads occuring in 10 coin flips, you would specify pbinom(3, 10, 0.5, lower.tail = FALSE) as lower.tail includes the value you state. For continuous data, however, such as height, you would be applying pnorm() and therefore can specify your cut-off point simply as x. In the above example, for the cut-off point of 190, a mean of 170 and standard deviation of 7, you can write pnorm(190, 170, 7, lower.tail = FALSE). The way to think about this is that setting x as 189 on a continuous scale, when you only want all values greater than 190, would also include all the possible values between 189 and 190. Setting x at 190 starts it at 190.0000000001. This is a tricky difference between pbinom() and pnorm() to recall easily, so best include this explanation point in your portfolio to help you carry out the correct analyses in the future! 4.4.4.2 Task 4: Using Figures to Calculate Cumulative Probability Have a look at the distribution below: Figure 4.9: The Normal Distribution of Height with the probability of people of 185cm highlighted in purple, with a mean = 170cm and SD = 7 Using the information in the figure, and the mean and SD as above, calculate the probability associated with the shaded area. Helpful Hint You already have your mean and standard deviations to input in pnorm(), look at the shaded area to obtain your cut-off point. What should the lower.tail call be set to according to the shaded area? Quickfire Question To three decimal places, what is the cumulative probability of the shaded area in Task 4? Explain This - I dont get this answer The answer should be p = .016. See the solution code at the end of the chapter for the correct code. Remember, lower.tail is set to FALSE as you want the area to the right. So pnorm() is great for telling us the probability of obtaining a specific value or greater on a distribution, given the mean and standard deviation of the distribution. The significance of this will come clearer in the coming chapters but this is a key point to have in mind as we progress through our understanding of analyses. We will leave it there for now and look at the last function of the normal distribution. 4.4.5 qnorm() - The Quantile Function Using qnorm() we can do the inverse of pnorm(), and instead of finding out the cumulative probability from a given set of values (or to a cut-off value), we can find a cut-off value given a desired probability. For example, we can use the qnorm() function to ask what is the maximum IQ a person would have if they were in the bottom 10% of the above IQ distribution (M = 100 &amp; SD = 15)? Note: We first need to convert 10% to a probability by dividing by 100 10% = 10 / 100 = 0.1. qnorm(0.1, 100, 15) So anyone with an IQ of 80.8 or lower would be in the bottom 10% of the distribution. Or to rephrase that, a person in the bottom 10% of the distribution would have a max IQ value of 80.8. To recap, we have calculated the inverse cumulative density function (or inverse of the cumulative probability) of the lower tail of the distribution, with a cut-off of a probability of 0.1 (10%), illustrated in purple below: Figure 4.10: The Normal Distribution of Height with the bottom 10% of heights highlighted in purple Again, in English that people can understand, that means the qnorm() function tells you the maximum value a person can have to maintain a given probability if you set lower.tail = TRUE. Contrastingly, pnorm() function tells you the the minimum value a person can have to maintain a given probability if you set lower.tail = FALSE. 4.4.5.1 Task 5: Using pnorm() and qnorm() to find probability and cut-off values Calculate the lowest IQ score a student must have to be in the top 5% of the above distribution. More challenging: Using the appropriate normal distribution function, calculate the probability that a given student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15. Helpful Hint Part 1: Remember to include the lower.tail call if required! If you are unsure, visualise what you are trying to find (i.e. the lowest IQ score you can have to be in top 5%) by sketching it out on a normal distribution curve. It may help to reverse the question to sound more like the previous example. Part 2: For the second part, each function, not necessarily qnorm(), gives one value, so you are looking to do a separate calculation for each IQ. Then you have to combine these two values, but are you summing or subtracting them? Is it more or less likely for students to have an IQ that falls between this range than above or below a cut-off? Again try sketching out what you are trying to achieve. Quickfire Questions To one decimal place, enter your answer for Task 5 part 1: What is the lowest IQ score a student must have to be in the top 5% of the distribution? To two decimal places, enter your answer for Task 5 part 2: What is the probability that a student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15? Explain This - I dont get this answer The question can be rephrased as what value would give you 95% of the distribution - and the answer would be 124.7. See the solution code for Task 5 Question 1 at the end of the chapter. You could use pnorm() to establish the probability of an IQ of 110. And you could use pnorm() again to establish the probability of an IQ of 105. The answer is the difference between these two probabilities and should be p = .12. See the solution code for Task 5 Question 2 at the end of the chapter. Job Done - Activity Complete! You have now recapped probability and the binomial and normal distributions. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the available forums for discussion. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Excellent work! You are Probably an expert! Now go try the assignment! 4.5 InClass Activity 2 (Additional) In the preclass activity, we focused a lot on basic probability and binomial distributions. If you have followed it, and understood it correctly, you should be able to have a go at the following scenario to help deepen your understanding of the binomial distribution. Have a go and see how you get on. Solutions are at the end of the chapter. 4.5.1 Return of the Binomial {Ch4InClassQueBinomial} You design a psychology experiment where you are interested in how quickly people spot faces. Using a standard task you show images of faces, houses, and noise patterns, and ask participants to respond to each image by saying face or not face. You set the experiment so that, across the whole experiment, the number of images per stimuli type are evenly split (1/3 per type) but they are not necessarily evenly split in any one block. Each block contains 60 trials. To test your experiment you run 1 block of your experiment and get concerned that you only saw 10 face images. As this is quite a low number out of a total of 60 you think something might be wrong. You decide to create a probability distribution for a given block of your experiment to test the likelihood of seeing a face (coded as 1) versus not seeing a face (coded as 0). You start off with the code below but it is incomplete. blocks_5k &lt;- replicate(n = NULL, sample(0:1, size = NULL, replace = NULL, c(2/3,1/3)) %&gt;% sum()) Copy the code to a script and replace the NULLs with the correct value and statement to establish how many different numbers of faces you might see in 1 block of 60 trials, over 5000 replications. Answering the following questions to help you out: Quickfire Questions The n NULL relates to: Number of Monte Carlo replications Number of blocks in the experiment Number of trials in the experiment Number of faces in the experiment The size NULL relates to: Number of faces in a block Number of houses in a block Number of trials in a block Number of noise patterns in a block replace should be set to: TRUE FALSE Explain This - I dont understand these answers replicate is how many times you want to run the sample, n = 5000. Monte Carlo replications are their official name. The number of trials is the size, 60. And in order for the code to work, and to not run out of options, you need to set replace as TRUE. You saw a very similar code in the preclass so be sure to have had a look there. The solution code is shown at the end of the chapter Portfolio Point - Why the odd weight of probability? Note that the values in the prob argument (i.e. c(2/3, 1/3) ) set probabilities for the two outcomes specified in the first argument (0, 1). So, we are setting a 2/3 probability of 0 (no face), and a 1/3 probability of 1 (face). Why? Remember that in this instance you have three different stimuli (face, house and noise) and therefore we have a greater chance of not seeing a face in the experiment (i.e. you see a house or noise) than seeing a face in the experiment. You need to remember when using sample (?sample) to set the probability appropriately or you will end up with an incorrect calculation. If you completed the above task correctly, your blocks_5k code has created 5000 values where each value represents the number of faces you saw in each individual replication (i.e. block) of 60 trials - like the number of heads you saw in 10 flips. If your code has worked, then running the chunk below should produce a probability distribution of your data. Run it and see. If not, something has gone wrong in your code above; debug or compare you code to the solution at the end of the chapter. data5k &lt;- tibble(faces_per_block = blocks_5k) %&gt;% count(faces_per_block) %&gt;% mutate(p = n / 5000) ggplot(data5k, aes(faces_per_block, p)) + geom_col() + theme_light() If you view data5k, you will now see three columns: faces_per_block tells you all the number of face trials seen in a block of 60 trials. Note that it does not run from 0 to 60. n tells you the number of blocks in which the corresponding number of face trials appeared, e.g. in 228 blocks there were 15 face trials. (Remember this is sampled data so your values may be different!). p tells you the probability of the corresponding number of faces_per_block. From your data5k tibble, what was the probability of seeing only 10 faces in a block? Ours was 0.002 but remember that we are using a limited number of replications here so our numbers will vary from yours. To find this value you can use View() or glimpse() on data5k. Group Discussion Point Discuss in your groups the following questions: If the probability of seeing only 10 faces was approximately p = 0.002, what does that say about whether our experiment is coded correctly or not? We only have data for 28 potential different number of face trials within a block. Why does this value not stretch from 0 to 60, or at least have more potential values than 28? Portfolio Point - What does it say about your experimental design? On the first point, it says that the probability of having a block with only 10 faces is pretty unlikely - it has only a small probability. However, it is still possible. Does that say there is an error in your code? Not necessarily as it is possible you just happened to get a block with only 10 faces. Despite it being a low probability it is not impossible. What it might say, however, is that if this is something that concerns you, this potentially low number of faces in a block, then you need to think about the overall design of your experiment and consider having an equal weighting of stimuli in each block. On the second point, you need to think about the width of the distribution. The probability of a face is 1/3 or p = .33. If there are 60 trials per block then that works out, on average, at approx 20 faces per block . The further you move away from that average, 20, the less probable it is to see that number of faces. E.g. 10 faces had a very low probability as will 30 faces. 40 faces will have almost no probability whatsoever. In only 5000 trials not all values will be represented. Quickfire Questions Using the appropriate binom() function, to three decimal places, type in the actual probability of exactly 10 faces in 60 trials: Using the appropriate binom() function, to three decimal places, type in the actual probability of seeing more than, but not including, 25 faces in a block of 60 trials? Using the appropriate binom() function, enter the number of faces needed in a block of 60 trials that would cut off a lower tail probability of .05. Helpful Hint You are looking for the probability of one specific value of faces appearing, i.e. 10 faces in a total of 60 trials. You are not looking for the cumulative probability or a range of values. What does this tell you about the apporpriate binom() function you require? Also, in the sample function, we created a vector of probability dependent on the appearance of both faces and other stimuli. However, for the binom() function, you need to include only the probability of faces appearing - \\(\\frac{1}{3}\\). You are looking for the cumulative probability for seeing more than but not including 25 faces in a block of 60 trials. What does this tell you about the apporpriate binom() function you require? Remember to set an appropriate call for lower.tail! The probability level has been provided for you (5% 0r .05). With this you are looking to find the cutoff value for this tail. What does this tell you about the appropriate binom() function you require? If you are looking for the probability of values at or below this cutoff, is lower.tail set to TRUE or FALSE? Explain This - I dont get these answers The answer would be .002 as you require the density, dbinom(), of 10 faces out of a possible 60 given a probability of 1/3. The appropriate code is as follows: Code: dbinom(10, 60, 1/3) The answer would be .068 as you require the cumulative probability, pbinom(), of more than 25 faces out of 60 given a probability of 1/3. As it is more than 25, setting 25 with lower.tail set as FALSE will give you 26 and above (remember this is a binomial distribution, so where you set your cut-off is very important). The appropriate code is as follows: Code: pbinom(25, 60, 1/3, lower.tail = FALSE) The answer would be 14 using the qbinom() function based on wanting to maintain 5% probability of faces in 60 trials. The appropriate code is as follows: Code: qbinom(.05, 60, 1/3) Job Done - Activity Complete! Well done on completing this activty! Once you have completed the main body inclass assignment on the Normal Distribution, you should be ready to complete the assignment for this lab. Good luck and remember to refer back to the codes and questions in this lab and the preclass if you get stuck!. 4.6 InClass Activity 3 (Additional) One thing we looked at in the lecture and in the preclass activity were some rather basic ideas about probability using cards. Here is a couple of very quick questions using dice (or die if you prefer) to make sure you understand this. Have a go and see how you get on. If you dont get the right answers, be sure to post any questions on the available forums for discussions. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 4.6.1 Back to Basics Show your answer either as a fraction (e.g. 1/52) or to 3 decimal places (e.g. 0.019). What is the probability of rolling a 4 on a single roll of a six-sided dice? What is the probability of rolling a 4, 5 or 6 on a single roll? What is the probability of rolling a 4 and then a 6 straight after? Explain this - I dont get these answers Remember: Probability is the number of times or ways an event can occur divided by the total number of all possible outcomes. When you have two separate events, the probability of them both happening is the joint probability which is calculated by mutliplying all the individual probabilities together. .167 as you have 1 outcome over 6 possibilities - \\(\\frac{1}{6}\\) .5 as you have 3 outcomes over 6 possibilities - \\(\\frac{3}{6}\\) .028 as you have 1 out of 6 on both occasions so the joint probability is used - \\(\\frac{1}{6} * \\frac{1}{6}\\). In this instance we do not have to worry about replacement as a dice always has 6 values. Unless of course you were to rub off a value after each roll of the dice but then that would be pretty unusual and you would go through dice pretty quickly, making them rather redundant little square cubes. Probably best we just assumed that didnt happen, we never talk about it again, and we assume replacement in this instance. Job Done - Activity Complete! Well done on completing this activity! Once you have completed the main body inclass assignment on the Normal Distribution, you should be ready to complete the assignment for this lab. Good luck and remember to refer back to the codes and questions in this lab and the preclass if you get stuck!. 4.7 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. Lab 4: Formative Probability Assignment! In order to complete this assignment, you first have to download the assignment .Rmd file which you need to edit - titled GUID_Level2_Semester1Lab4.Rmd. This can be downloaded within a zip file from the link below. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here. Now open the assignment .Rmd file within RStudio. You will see there is a code chunk for each of the 10 tasks. Much as you did in the previous assignments, follow the instructions on what to edit in each code chunk. This will often be entering code or a single value and will be based on the skills learnt in the current lab as well as all previous labs. 4.7.1 Todays Topic - Probability In the lab you recapped and expanded on your understanding of probability, including a number of binom and norm functions as well as some more basic ideas on probability. You will need these skills to complete the following assignment so please make sure you have carried out the PreClass and InClass activities before attempting this formative assignment. Remember to follow the instructions and if you get stuck at any point to questions on the forums. Before starting lets check The .Rmd file is saved into a folder on your computer that you have access to and you have manually set this folder as your working directory. For assessments we ask that you save it with the format GUID_Level2_Semester1_Lab4.Rmd where GUID is replaced with your GUID. Though this is a formative assessment it may be good practice to do the same here. Note: You should complete the code chunks below by replacing the NULL (except the library chunk where the appropriate code should just be entered). Not all answers require coding. On those that do not require code, you can enter the answer as either code, mathematical notation, or an actual single value. Pay attention to the number of decimal places required. 4.7.2 Load in the Library There is a good chance that you will need the tidyverse library at some point in this exercise so load it in the code chunk below: # hint: something to do with library() and tidyverse Basic probability and the binomial distribution questions Background Information: You are conducting an auditory discrimination experiment in which participants have to listen to a series of sounds and determine whether the sound was human or not. On each trial participants hear one brief sound (100 ms) and must report whether the sound was from a human (coded as 1) or not (coded as 0). The sounds were either: a person, an animal, a vehicle, or a tone, with each type of sound equally likely to appear. 4.7.3 Assignment Task 1 On a single trial, what is the probability that the sound will not be a person? Replace the NULL in the t1 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to two decimal places. t1 &lt;- NULL 4.7.4 Assignment Task 2 Over a sequence of 4 trials, with one trial for each sound, and sampled with replacement, what is the probability of the following sequence of sounds: animal, animal, vehicle, tone? Replace the NULL in the t2 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to three decimal places. t2 &lt;- NULL 4.7.5 Assignment Task 3 Over a sequence of four trials, with one trial for each sound, without replacment, what is the probability of the following sequence of sounds: person, tone, animal, person? Replace the NULL in the t3 code chunk with either mathematical notation or a single value. t3 &lt;- NULL 4.7.6 Assignment Task 4 Replace the NULL below with code, using the appropriate binomial distribution function, to determine the probability of hearing exactly 17 tone trials in a sequence of 100 trials. Assume the probability of a tone on any single trial is 1 in 4. Store the output in t4. t4 &lt;- NULL 4.7.7 Assignment Task 5 Replace the NULL below with code using the appropriate binomial distribution function to determine what is the probability of hearing 30 vehicle trials or more in a sequence of 100 trials. Assume the probability of a vehicle trial on any one trial is 1 in 4. Store the output in t5. Hint: do we want the upper or lower tails of the distribution? t5 &lt;- NULL 4.7.8 Assignment Task 6 If a block of our experiment contained 100 trials, enter a line of code to run 10000 replications of one block, summing how many living sounds were heard in each replication. Code 1 for living sounds (person/animal) and 0 for non living sounds (vehicle/tone) and assume the probability of a living sound on any given trial is \\(p = .5\\). t6 &lt;- NULL Normal Distribution Questions Previously, in Lab 2, we looked at an ageing research project investigating differences in visual processing speed between younger (M = 22 years) and older adults (M = 71 years). One check in this experiment, prior to further analysis, is to make sure that older participants do not show signs of mild cognitive impairment (early symptoms of Alzheimers disease). To do this, we carry out a battery of cognitive tests to screen for such symptoms. One of the tests is the D2 test of attention which is a target cancellation task (i.e. participants cross out all letter ds with two dashes from a line of letters - see Lab 2 for an example). It is designed to test peoples selective and sustained attention and visual scanning speed. The results of the test give a single score of Concentration Performance for each participant. The key piece of information for this analysis is that the distributions of D2 test scores are typically normally distributed (M = 100, SD = 10). 4.7.9 Assignment Task 7 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of 90 or lower? Store the output in t7 t7 &lt;- NULL 4.7.10 Assignment Task 8 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of 120 or more? Store the output in t8 t8 &lt;- NULL 4.7.11 Assignment Task 9 Replace the NULL below with code using the appropriate function(s) to determine what is the difference in scores that cut off the top 5% and bottom 5% of the distribution? Store the output in t9. t9 &lt;- NULL 4.7.12 Assignment Task 10 Finally, if a participant says to you that they are worried because they have heard that their Concentration Performance was in the bottom 2% of scores on the distribution, what is the maximum D2 score that they can have? Replace the NULL below with a single value to two decimal places. Do not enter code. Store this in t10 t10 &lt;- NULL Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solutions at the end of the chapter. You are looking to check that questions looking for a single value have the same answer as you, and that questions asking for code have the same code as you or give the same answer where alternative variations in the code are allowed (e.g. including lower.tail = TRUE or not including it as it is default). Remember that a single value is not the same as a coded answer and that spelling matters, replicate() is not the same as replicat(). Where there are alternative answers, it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions, please post them on the forums! Lastly, keep in mind the main point about probability, that we are interested in determining the probability of a given value on a distribution! That is what it is all about and is what we will look at more of in Chapter 5. 4.8 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 4.8.1 InClass Activities 4.8.1.1 InClass Task 1 First we set up our range of IQ values from 40 to 160 Then we plot the distribution of IQ_data, where we have M = 100 and SD = 10 library(tidyverse) IQ_data &lt;- tibble(IQ_range = c(40, 160)) ggplot(IQ_data, aes(IQ_range)) + stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) + labs(x = &quot;IQ Score&quot;, y = &quot;probability&quot;) + theme_classic() Figure 4.11: Distribution of IQ scores with mean = 100, sd = 10 Return to Task 4.8.1.2 InClass Task 2 ND_data &lt;- tibble(ND_range = seq(-10,10,0.05)) ggplot(ND_data, aes(ND_range)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + labs(x = &quot;SD units&quot;, y = &quot;probability&quot;, title = &quot;The Normal Distribution&quot;) + theme_classic() Figure 4.12: Normal Distribution shown on a scale of -10 to 10, with a mean = 0, sd = 1 Return to Task 4.8.1.3 InClass Task 3 Key thing is to set lower.tail to FALSE to calculate the above area. When using pnorm() you state the actual cut-off even if using lower.tail = FALSE. If using pbinom() you state the cut-off minus one if using lower.tail = FALSE pnorm(190, 170, 7, lower.tail = FALSE) ## [1] 0.002137367 Return to Task 4.8.1.4 InClass Task 4 The highlighted area is 185cm and above. Key thing is to set lower.tail to FALSE to calculate the area from the cut-off and above. pnorm(185, 170, 7, lower.tail = FALSE) ## [1] 0.01606229 Return to Task 4.8.1.5 InClass Task 5 Question 1 - the lowest IQ score a student must have to be in the top 5% of the above distribution. qnorm(0.95, 100, 15, lower.tail = TRUE) ## [1] 124.6728 Question 2 - calculate the probability that a given student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15. pnorm(105, 100, 15, lower.tail = FALSE) - pnorm(110, 100, 15, lower.tail = FALSE) ## [1] 0.1169488 Return to Task 4.8.2 Return of the Binomial 4.8.2.1 Task 1 blocks_5k &lt;- replicate(n = 5000, sample(0:1, size = 60, replace = TRUE, c(2/3,1/3)) %&gt;% sum()) Return to Task 4.8.3 Test Yourself Activities 4.8.3.1 Load in the Library library(tidyverse) Return to Task 4.8.3.2 Assignment Task 1 t1 &lt;- 3/4 t1 &lt;- .75 The probability that the sound will not be a person is 0.75 Return to Task 4.8.3.3 Assignment Task 2 t2 &lt;- (1/4) * (1/4) * (1/4) * (1/4) t2 &lt;- .004 The probability of that sequence of sounds is 0.004 Return to Task 4.8.3.4 Assignment Task 3 t3 &lt;- (1/4) * (1/3) * (1/2) * (0/1) t3 &lt;- 0 The probability of that sequence of sounds is 0 The reason is that there is no replacement and as such the repeat of the person trial cant happen. Return to Task 4.8.3.5 Assignment Task 4 t4 &lt;- dbinom(17, 100, 1/4) Assuming a probability of a tone on a given trial is 1 in 4, the probability of hearing 17 tone trials in a sequence of 100 trials is 0.0165156 Return to Task 4.8.3.6 Assignment Task 5 This could have been answered using either pbinom() or dbinom(). The trick is to remember where to set the cut-off depending on the function used. t5 &lt;- pbinom(29, 100, 1/4, lower.tail = FALSE) t5 &lt;- dbinom(30:100, 100, 1/4) %&gt;% sum() In this scenario, the probability of hearing 30 vehicle trials or more in a sequence of 100 trials is 0.149541 Return to Task 4.8.3.7 Assignment Task 6 The appropriate code would be: t6 &lt;- replicate(10000, sample(0:1, 100, TRUE, c(.5,.5)) %&gt;% sum()) If you were to look at your output you would see something like the following. Remember your numbers will vary from ours due to random sampling. Here we are only showing the first 10 values of 10000 ## int [1:10000] 44 54 47 52 54 45 54 48 48 55 ... Return to Task 4.8.3.8 Assignment Task 7 t7 &lt;- pnorm(90, 100, 10, lower.tail = TRUE) The probability of a given participant having a D2 score of 90 or lower is 0.1586553 Return to Task 4.8.3.9 Assignment Task 8 t8 &lt;- pnorm(120, 100, 10, lower.tail = FALSE) The probability that a given participant will have a D2 score of 120 or more is 0.0227501 Return to Task 4.8.3.10 Assignment Task 9 t9 &lt;- qnorm(.95, 100, 10) - qnorm(.05, 100, 10) The difference in scores that cut off the top and bottom 5% of the distribution is 32.8970725 Return to Task 4.8.3.11 Assignment Task 10 t10 &lt;- 79.46 The maximum D2 score that they can have in this situation is 79.46 Return to Task Chapter Complete! "],["permutation-tests-a-skill-set.html", "Lab 5 Permutation Tests - A Skill Set 5.1 Overview 5.2 PreClass Activity 5.3 InClass Activity 5.4 Assignment 5.5 Solutions to Questions 5.6 Additional Material", " Lab 5 Permutation Tests - A Skill Set 5.1 Overview To many, a lot of statistics must seem a bit like blind faith as it deals with estimating quantities, values and differences we havent observed (or cant observe), e.g. the mean of a whole population. As such, we have to know if we can trust our statistical analyses and methods for making estimations and inferences because we rarely get a chance to compare the estimated values (e.g. from a sample) to the true values (e.g. from the population) to see if they match up. One way to test a procedure, and in turn learn about statistics, is through data simulation. In simulations, we create a population and then draw samples and run tests on the data, i.e. on this known population. By running lots of simulations we can test our procedures and make sure they are acting as we expect them to. This approach is known as a Monte Carlo simulation, named after the city famous for the many games of chance that are played there. Portfolio Point - Monte Carlo or Bust You can go read up on the Monte Carlo approach if you like. It can, however, get quite indepth, as having a brief glance at the wikipedia entry on it highlights. The main thing to keep in mind is that the method involves creating a population and continually taking samples from that population in order to make an inference. This is what we will show you in this chapter. Data simulation and creating your own datasets, to see how tests work, is a great way to understand statistics. Something else to keep in mind, when doing this chapter, is how easy it really is to find a significant result if even randomly created data can give a significant result. This may help dispell any notion that there is something inherently important about a significant result, in itself. In this chapter, you will perform your first hypothesis test using a procedure known as a permutation test. We will help you learn how to do this through building and running data simulation procedures. So overall the aims of this chapter are to: Learn how to write code to simulate data Learn about hypothesis testing for significant differences through a permutation test Portfolio Point - Simulating Data is not the same as Faking Data Now you may be thinking simulating data sounds very much like making fake data which you know is one of the Questionable Research Practices (QRP) that has lead to issues in the past. Lets be clear, faking data for the purpose of finding an enviable result and publishing it (or even just telling people about it as though it is a real result) is very wrong. Never ever do this! It has unfortunately happened in the past and likely still happens. You can read more about the consequences and prevalences of faking data in John, Loewenstein &amp; Prelec (2012) or in Chapter 5 of Chris Chambers excellent book, The seven deadly sins of psychology: a manifesto for reforming the culture of scientific practice, under the sin of Corruptability. There are three copies of this book in the University of Glasgow library (and Phil says you can borrow his if you promise to give it on to someone else after you have finished it!) Simulating data however is not a QRP as the intent is to test your analytical methods and to test statistical theory. You simulate data to see how certain statistical tests wourk under different conditions (e.g. big or small difference between conditions). Alternatively, you simulate data to test your analysis code is working whilst you collect data - running an experiment takes time and so as you are gathering data, you can be using simulated data in the same shape as your eventual data to test your analysis code. Then when you have your actual data you just swap the simulated data for the real data. This is a great thing to do as it allows you to check ahead of evening running the experiment that your analysis will work and that you have not forgotten to collect some important piece of information - think of Registered Reports as discussed in Munafo et al., (2019). Lastly, maybe you want to simulate data to practice hand calculations to learn about tests. Even learning about a mean requires data so you can simulate data numerous times for yourself to practice hand calculating a mean. As you can see there is a lot of important and useful reasons to learn about simulating data. And that it is very different from faking data. Simulating data is a very useful tool in a researchers toolbox. Faking data is very bad practice, often illegal, and you should never do it or feel pressured by anyone else to fake data. If you are ever in this situation please feel free to reach out to the authors of this book. In order to complete this chapter you will require the following skills, which we will teach you today: Skill 1: Generating random numbers with base::rnorm() Skill 2: Permuting values with base::sample() Skill 3: Creating a tibble (a type of data table) using tibble::tibble() Skill 4: Computing and extracting a difference in group means using dplyr::pull() and purrr::pluck() Skill 5: Creating your own custom functions using base::function() Skill 6: Repeating operations using base::replicate() 5.2 PreClass Activity We will now take each skill in turn. Be sure to try them all out. It looks a lot of reading but it is mainly just showing you the output of the functions so you can see you are using them correctly. The key thing is to try them yourselves and dont be scared to change things to see what might happen if you do it slightly differently. We will also ask a couple of questions along the way to make sure you are understanding the skills. The trick with this chapter is to be always thinking about the overarching aim of: 1) learning about simulating data, and 2) learning about hypothesis testing. This PreClass activity is setting up the skills to simulate data. We will get to the actual use of it in the InClass activity. 5.2.1 Skill 1: Generating Random Numbers The first step in any simulated dataset is getting some numbers. We can use the base::rnorm() function to generates values from the normal distribution which we saw in Chapter 4. It is related to the dnorm(), pnorm() and qnorm() functions we have already seen. Where as those functions ask questions of the normal distribution, the rnorm() generates values based on the distribution and takes the following arguments: n: the number of observations to generate mean: the mean of the distribution (default 0) sd : the standard deviation of the distribution (default 1) For example, to generate 10 or even 50 random numbers from a standard normal distribution (M = 0, SD = 1), you would use rnorm(10, 0, 1) or rnorm(50, 0, 1) respectively. Type rnorm(50, 0, 1) into your console and see what happens. Use the below example for rnorm(10, 0, 1) to help you. Try increasing n to 1000. rnorm(10, 0, 1) ## [1] 0.4718362 -1.2382692 -1.2042214 -0.2418053 -0.6838115 0.7842449 ## [7] -0.9419668 1.3458579 0.9236536 1.9262364 Quickfire Question If you enter rnorm(50, 0, 1) again you will get different numbers. Why? I have made a mistake The numbers are random R has made a mistake Phil has made a mistake So far so good. One thing worth pointing out is that the default arguments to rnorm() for mean and sd are mean = 0 and sd = 1. What that means in practice is that you dont have to write rnorm(10, 0, 1) all the time if you just want to use those default mean and sd values. Instead you could just write rnorm(10) and it would work just as well giving you ten values from a normal distribution of mean = 0 and sd = 1. But lets say you want to change the mean or sd, then you would need to pass (and change) the mean and sd arguments to the function as shown below. rnorm(n = 10, mean = 1, sd = .5) Try changing the mean and sd values a couple of times and see what happens. You get different numbers again that will be around the mean you set! Set a mean of 10, then a mean of 100, to test this. Great. We can now generate numbers. But finally, for this Skill, say you wanted to create a vector with two sets of 50 random numbers from two separate samples: one set of 50 with a mean of 75 (sd = 1) and the other with a mean of 90 (sd = 1) - almost like creating two groups. In this case you would concatenate (i.e. link) numbers created from two separate rnorm() calls together into a single vector using the c() function from base R. For example, you would use: random_numbers &lt;- c(rnorm(50, 75), rnorm(50, 90)) Quickfire Questions In the above example code, what is the standard deviation of the two samples you have created? 50 75 90 1 Explain This - I dont get this answer! What is the default sd of the function? Both populations would have an sd of 1, because that is the default. Remember if there is a default value to an argument then you dont have to state it in the code. Remember you can also easily change the sd value of each rnorm() by just adding sd =  to each function. They can have the same sd or different sds. Try it out! One useful thing though, as mistakes happen, it is always good to check that your new vector of numbers has the right number of data points in it - i.e. the total of the two samples; a sanity check if you will. The new vector random_numbers should have 100 elements. You could verify this using the length() function: length(random_numbers) Which tells us that we have generated 100 numbers just as expected! Awesome. Check us out - already mastered generating random numbers! Skill 1 out of 6 Complete! 5.2.2 Skill 2: Permuting Values Another skill that is useful to be able to do is to generate permutations of values. Say for example you have already the values collected, with certain numbers atttributed to group A and certain numbers attributed to group B. Permutating those values allows you to change who was in what group and allows you to ask if there was some important difference between the groups based on their original values. Portfolio Point - What are Permutations? A permutation is just a different ordering of the same values. For example, the numbers 1, 2, 3 can be permuted into the following 6 sequences: 1, 2, 3 1, 3, 2 2, 1, 3 2, 3, 1 3, 1, 2 3, 2, 1 The more values you have, the more permutations of the order you have. The number of permutations can be calculated by, for example, 3 x 2 x 1 = 6, where 3 is the number of values you have. Or through code: factorial(3) = 6. This assumes that each value is used once in the sequence and that each value never changes, i.e. 1234 cannot suddenly become 1235. We can create random permutations of a vector using the sample() function. Lets use one of Rs built in vectors: letters. Type letters into the console, as below, and press RETURN/ENTER. You will see it contains all the lowercase letters of the English alphabet. Now, I bet you are wondering what LETTERS does, right? letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; ## [20] &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; We can use the base::sample() function with letters to put the letters into a random order: Run the below line. Run it again. And again. What do you notice? And why is our output different from yours? (The answer is below) sample(letters) ## [1] &quot;c&quot; &quot;u&quot; &quot;d&quot; &quot;f&quot; &quot;i&quot; &quot;e&quot; &quot;a&quot; &quot;w&quot; &quot;p&quot; &quot;x&quot; &quot;k&quot; &quot;t&quot; &quot;q&quot; &quot;h&quot; &quot;s&quot; &quot;y&quot; &quot;v&quot; &quot;z&quot; &quot;l&quot; ## [20] &quot;o&quot; &quot;j&quot; &quot;n&quot; &quot;r&quot; &quot;m&quot; &quot;g&quot; &quot;b&quot; Quickfire Questions If month.name contains the names of the twelve months of the year, how many possible permutations are there of sample(month.name)? 1 12 144 479001600 Portfolio Point - Different samples with sample() Each time you run sample(letters) it will give you another random permutation of the sequence. That is what sample() does - creates a random permutation of the values you give it. Try repeating this command many times in the console. Because there are so many possible sequences, it is very unlikely that you will ever see the same sequence twice! An interesting thing about sample() is that sample(c(1,2,3,4)) is the same as sample(4). And to recap, there would be 24 different permutations based on factorial(4), meaning that each time you type sample(4) you are getting one of those 24 different orders. So what would factorial(12) be? Top Tip: Remember that you can scroll up through your command history in the console using the up arrow on your keyboard; this way, you dont ever have to retype a command youve already entered. And thinking about Skill 1 (generate) and Skill 2 (permute) we could always combine these steps with code such as: random_numbers &lt;- c(rnorm(5, 75),rnorm(5, 90)) random_numbers_perm &lt;- sample(random_numbers) The first line of code creates the random_numbers - two sets of five. random_numbers ## [1] 74.35868 74.17160 75.91439 76.84620 73.71939 89.19206 91.11152 89.41236 ## [9] 90.28996 90.47787 And the second line permutes those numbers into a different order random_numbers_perm ## [1] 74.17160 91.11152 90.28996 89.41236 75.91439 89.19206 76.84620 73.71939 ## [9] 90.47787 74.35868 Brilliant! We can now permutate values as well as generate them. The importance of this in hypothesis testing will become clearer as the chapter goes on, but as a teaser, remember that probability is the likelihood of obtaining a specific value in consideration of all possible outcomes. Skill 2 out of 6 Complete! 5.2.3 Skill 3: Creating Tibbles Working with tables/tibbles is important because most of the data we want to analyze comes in a table, i.e. tabular form. There are different ways to get tabular data into R for analysis. One common way is to load existing data in from a data file (for example, using readr::read_csv() which you have seen before). But other times, you might want to just type in data directly. You can do this using the tibble::tibble() function - you have seen this function a couple of times already in this book. Being able to create a tibble of data is a very useful skill when you will want to create some data on the fly just to try certain codes or functions. 5.2.3.1 Entering Data into a Tibble The tibble() function takes named arguments - this means that the name you give each argument within the tibble function, e.g. Y = rnorm(10) will be the name of the column that appears in the table, i.e. Y. Its best to see how it works through an example. The below command creates a new tibble with one column named Y, and the values in that column are the result of a call to rnorm(10): 10 randomly sampled values from a standard normal distribution (mean = 0, sd = 1), as we learnt in Skill 1. tibble(Y = rnorm(10)) Y 0.6275753 -1.3282050 0.9986710 -1.0552458 1.0973749 1.2884957 -1.0587038 0.8326353 -0.1169799 0.7732466 If, however, we wanted to sample from two different populations for Y, we could combine two calls to rnorm() within the c() function. Again this was in Skill 1, here we are now just storing it in a tibble. See below: tibble(Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20))) Y -9.584343 -11.526960 -9.602943 -9.259919 -10.418807 20.532379 19.869586 19.902063 19.244200 20.431410 Now we have sampled a total of 10 observations - the first 5 come from a group with a mean of -10, and the second 5 come from a group with a mean of 20. Try changing the values in the above example to get an idea of how this works. Maybe even add a third group! Great. We have combined Skills 1 and 3 to create a tibble of data that we can start to work with. But, of course, it would be good to know which group/condition each data point refers to and so we should add some group names. We can do this using the base::rep() function. 5.2.3.2 Repeating Values to Save Typing But before finalising our tibble, and adding in the groups, it will help to learn a little about rep(). This function is most useful for automatically repeating conditions/values/data in order to save typing. For instance, if we wanted 20 letter As in a row, we would type: rep(&quot;A&quot;, times = 20) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [20] &quot;A&quot; The first argument to rep() is the vector containing the information you want repeated, (i.e. the letter A), and the second argument, times, is the number of times to repeat it; in this case 20. If you wanted to add more information, e.g. if the first argument has more than one element, say groups A and B, it will repeat the entire vector that number of times; A B, A B, A B,  . Note that we enclose A and B in the c() function, and with quotes, so that it is seen as a single argument or list that we want repeating. The quotes around A and B (i.e. A) are saying that these are character data. rep(c(&quot;A&quot;, &quot;B&quot;), times = 20) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [20] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; ## [39] &quot;A&quot; &quot;B&quot; Ok that works if we want a series of A B A B etc. But sometimes we want a specific number of As followed by a specific number of Bs; A A A B B B - meaning something like everyone in Grp A and then everyone in Grp B. We can do this as well! If the times argument has the same number of elements as the vector given for the first argument, it will repeat each element of the first vector as many times as given by the corresponding element in the times vector. In other words, for example, times = c(2, 4) for vector c(\"A\", \"B\") will give you 2 As followed by 4 Bs. rep(c(&quot;A&quot;, &quot;B&quot;), times = c(2, 4)) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; Quickfire Questions The best way to learn about this function is to play around with it in the console and see what happens. Answering these questions will help. From the dropdown menus, the correct output of the following function would be: rep(c(\"A\", \"B\", \"C\"), times = (2, 3, 1)) - A A A B B C A A B B B C A A B B C C A B C A B C rep(1:5, times = 5:1) - 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 5 5 5 5 5 4 4 4 4 3 3 3 2 2 1 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5.2.3.3 Bringing it Together in a Tibble Great. Lets get back to creating our tibble of data. Now we know rep(), we can complete our tibble of simulated data by combining what weve learned about generating random numbers (Skill 1) and repeating values (Skill 3). We want our table to look like this: group Y A -11.435732 A -9.123214 A -10.581591 A -9.892297 A -9.745320 B 19.710862 B 19.769016 B 20.727771 B 18.677838 B 21.008002 You now know how to create this table. Have a look at the code below and make sure you understand it. We have one column called group where we create As and Bs through rep(), and one column called Y, our data, all in our tibble(): Note: We have not stated times = in the below code but because of how the rep() function works it knows that the c(5,5) should be read as times = c(5,5). This is not magic. Again it is about defaults and order or arguments. Just like you saw with rnorm() and not having to state mean and sd each time. tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(5, 5)), Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20))) Be sure to play around with the code chunk above to get used to it. Try adding a third group or even a third column? Perhaps you want to give every participant a random age with a mean of 18, and an sd of 1; or even a participant number. Helpful Hint Try row_number() to create participant numbers. Dont forget, if you wanted to store your tibble, you would just assign it to a name, such as my_data: Note: we are using the row_number() function to give everyone a unique ID. my_data &lt;- tibble(ID = row_number(1:10), group = rep(c(&quot;A&quot;, &quot;B&quot;), c(5, 5)), Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20)), Age = rnorm(10, 18, 1)) which would give a table looking like: Table 5.1: Simulated data of two groups: Group A (M = -10, SD = 1) and Group B (M = 20, SD = 1). Each participant has been given a unique ID based on row number and a random age ID group Y Age 1 A -12.21979 17.33688 2 A -10.88400 20.75731 3 A -12.20528 18.35845 4 A -10.77000 18.63691 5 A -10.17820 17.87753 6 B 19.00950 16.78803 7 B 19.25667 18.44292 8 B 18.60762 17.86343 9 B 19.89482 18.59709 10 B 19.21873 17.44738 Fabulous! We are really starting to bring these skills together! Skill 3 out of 6 Complete! 5.2.4 Skill 4: Computing Differences in Group Means OK so now that we have created some data we want to look at answering questions about that data. For example, the difference between two groups. Thinking back to Chapter 2, you have already learned how to calculate group means using group_by() and summarise(). For example, using code we have seen so far, you might want to calculate sample means for a randomly generated dataset like so: my_data &lt;- tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(20, 20)), Y = c(rnorm(20, 20, 5), rnorm(20, -20, 5))) my_data_means &lt;- my_data %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) my_data_means group m A 19.48526 B -21.69116 However, you might want to take that a step further and calculate the differences between means rather than just the means. Say we want to subtract the mean of Group B (M = -21.7) from the mean of Group A (M = 19.5), to give us a single value, the difference: diff = 41.2. Using code to extract a single value and to runs calculations on single values is much more reproducible (and safer assuming your code is correct) than doing it by hand. There are a few ways to do this which we will look at. The pull() and pluck() method We can do this using the dplyr::pull() and purrr::pluck() functions. pull() will extract a single column (e.g. m) from a tibble/dataframe (e.g. my_data means) and turn it into a vector (e.g. vec). vec &lt;- my_data_means %&gt;% pull(m) vec ## [1] 19.48526 -21.69116 We have now created vec which is a vector containing only the group means; the rest of the information in the table has been discarded. vec just contains our two means and we could calculate the mean difference as below, where vec is our vector of the two means and [1] and [2] refer to the two means: vec[1] - vec[2] ## [1] 41.17642 But pluck() can also do this and is more useful. pluck() allows you to pull out a single element (i.e. a value or values) from within a vector (e.g. vec). pluck(vec, 1) - pluck(vec, 2) ## [1] 41.17642 And we could run it a pipeline, as below, where we still pull() the means column, m, and then pluck() each value in turn and subtract them from each other. my_data_means %&gt;% pull(m) %&gt;% pluck(1) - my_data_means %&gt;% pull(m) %&gt;% pluck(2) ## [1] 41.17642 The pivot_wider(), mutate(), pull() approach An alternative way to extract the difference between means which may make more intuitive sense uses the functions pivot_wider(), mutate() and pull(). From Chapter 2, you already know how to calculate a difference between values in the same row of a table using dplyr::mutate(), e.g. mutate(new_column = column1 - column2). So if you can get the observations in my_data_means into the same row, different columns, you could then use mutate() to calculate the difference. First you need to spread the data out. In Chapter 2 you learned the function pivot_longer() to bring columns together. Well the opposite of pivot_longer() is the tidyr::pivot_wider() function to split columns apart - as below. The pivot_wider() function (?pivot_wider) splits the data in column m by the information, i.e. labels, in column group and puts the data into separate columns. It is necessary to include names_from = ... and values_from = ... of the code wont work. names_from says which existing column to get the new columns names from. values_from says which existing column to get the data from. my_data_means %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) A B 19.48526 -21.69116 Now that we have spread the data, we could follow this up with a mutate() call to create a new column which is the difference between the means of A and B. This is shown here: my_data_means %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) %&gt;% mutate(diff = A - B) A B diff 19.48526 -21.69116 41.17642 Quickfire Question And just to check you follow the mutate() idea, what is the name of the column containing the differences between the means of A and B? means group m diff Brilliant. Now we have the difference calculated, if you then wanted to just get the actual diff value and throw away everything else in the table, we would use pull() as follows: Note: This would be useful if a question asked for just a specific single value as opposed to the data or value in a tibble. my_data_means %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) ## [1] 41.17642 Portfolio Point - Reading pipes and verbalising tasks Keep in mind that a very useful technique for establishing what you want to do to a dataframe is to verbalise what you need, or to write it down in words, or to say it out loud. Take this last code chunk. What we wanted to do was to spread, using pivot_wider(), the data in m into the groups A and B. Then we wanted to mutate() a new column that is the difference, diff, of A minus B. And finally we wanted to pull() out the value in diff. Often step 1 of writing code or understanding code is knowing what it is you want to do in the first place. After that, you just need the correct functions. Fortunately for us, tidyverse names its functions based on what they specifically do! Brilliant. Brilliant. Brilliant. You are really starting to learn about simulating data and using it to calculate differences. This is a lot of info and may take a couple of reads to understand but keep the main goal in mind - simulating data can help us learn about analysis and develop our understanding! Skill 4 out of 6 Complete! 5.2.5 Skill 5: Creating Your Own Functions In Skills 1 to 4, we have looked at creating and sampling data, storing it in a tibble, and extracting information from that tibble. Now, say we wanted to do this over and over again. For instance, we might want to generate 100 random datasets just like the one in Skill 4. Why? Why would I ever want to do such a thing? Well, think about replication. Think about running hundreds of subjects. One dataset does not a research project make! Now it would be a pain to have to type out the tibble() function 100 times or even to copy and paste it 100 times if we wanted to replicate our simulated data. Wed likely make an error somewhere and it would be hard to read. So, to help us, we can create a custom function that performs the action you want; in our case, creating a tibble of random data. Remember, a function is just a procedure that takes an input and gives you the same output each time - like a toaster! All the functions we have used so far have been created by others. Now we want to make our own function to create our own data. A function has the following format: name_of_function &lt;- function(arg1, arg2, arg3) { ## body of function goes between these curly brackets; i.e. what the function does for you. ## Note that the last value calculated will be returned if you call the function. } Breaking down that code a bit: first you give your function a name (e.g. name_of_function) and then define the names of the arguments it will take (arg1, arg2, ) - an argument is the information that you feed into your function, e.g. data, mean, sd, etc. finally, you state the calculations or actions of the function in the body of the function (the portion that appears between the curly braces). Looking at some functions might help. One of the most basic possible functions is one that takes no arguments and just prints a message. Here is an example below. Copy it into a script or your console window and run it to create the function. hello &lt;- function() { print(&quot;Hello World!&quot;) } So this function is called hello. After you have created the function (by running the above lines), the function itself can then be used (run) by typing hello() in your console. If you do that you will see it gives the output of Hello World! every single time you run it; it has no other actions or information. Test this in the console now by typing: hello() ## [1] &quot;Hello World!&quot; Awesome right? Ok, so not very exciting. Lets make it better by adding an argument, name, and have it say Hello name. Copy the below code into your script and run it to create the function. hello &lt;- function(name = &quot;World!&quot;) { paste(&quot;Hello&quot;, name) } This new function is again called hello() and replaces the one you previously created. This time however you are supplying a default argument, (i.e. name = \"World!\"). You have seen many default arguments before in other functions like the mean and sd in rnorm() in Skill 1. The new function we have created still has the same default action as the previous function of putting Hello and World! together. So if you run it you get Hello World! Try it yourself! hello() ## [1] &quot;Hello World!&quot; Howver, the difference this time is that because you have added an argument to the input, you can change the information you give the argument and therefore change the output of the function. More flexible. More exciting. Quickfire Questions Test your understanding by answering these questions: Typing hello(\"Phil\") in the console with this new function will give: Hello Heather Hello Phil Hello Carolina Hello Eugene Typing the argument as \"is it me you are looking for\" will give: Hello is it me you are looking for I just called to say Hello You had me at Hello Hello seems to be the hardest word What argument would you type to get Hello Dolly! as the output:  Dolly Molly Holly Dolly! Great. From this basic function you are beginning to understand how functions work in general. However, most of the time we want to create a function that computes a value or constructs a table. For instance, lets create a function that returns randomly generated data from two samples, as we learned in the Skills 1-3. All we are doing is taking the tibble we created in Skill 4 and putting it in the body (between the curly brackets) of the function. gen_data &lt;- function() { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(20, 20)), Y = c(rnorm(20, 20, 5), rnorm(20, -20, 5))) } This function is called gen_data() and when we run it we get a randomly generated table of two groups, each with 20 people, one with M = 20, SD = 5, the other with M = -20, sd = 5. Try running this gen_data() function in the console a few times; remember that as the data is random, the numbers will be different each time you run it. And just to reiterate, all we have done is take code from Skill 3 and put it into our function. Nothing more. But say we want to modify the function to allow us to get a table with smaller or larger numbers of observations per group. We can add an argument n and modify the code as below Create this function and run it a couple of times through gen_data(). The way to think about this is that every place that n appears in the body of the function (between the curly brackets) it will have the value of whatever you gave it in the arguments, i.e. in this case, 20. gen_data &lt;- function(n = 20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, 20, 5), rnorm(n, -20, 5))) } Ok here are some questions to see how well you understand what this function does. Quickfire Questions How many total participants would there be if you ran gen_data(n = 2)? 2 4 20 40 What would you type to get 100 participants per group? gen_data(50) gen_data(10) gen_dota(100) gen_data(100) Explain This! - I dont get these answers. The value that you give the argument for the function replaces the n every time it appears in the function. So saying gen_data(2) really changes the tibble() line from tibble(group = rep(c(A, B), c(n, n)) to tibble(group = rep(c(A, B), c(2, 2)). This creates two groups with 2 people in each group and therefor a total of 4 people. If you wanted 100 participants per group then really you want the tibble line to say tibble(group = rep(c(A, B), c(100, 100)). As such you would be replacing the n with 100 and as such the function would be called as gen_data(100). Challenge Question: Keeping in mind that functions can take numerous arguments, and that each group in your function have separate means, can you modify the function gen_data to allow the user to change the means for the two calls to rnorm? Have a try before revealing the solution below. Solution To Challenge Question gen_data &lt;- function(n = 20, m1 = 20, m2 = -20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, m1, 5), rnorm(n, m2, 5))) } # m1 is the mean of group A # m2 is mean of group B # The function would be called by: gen_data(20, 20, -20) # Giving 20 participants in each group, # The first group having a mean of 20, # The second group having a mean of -20. # Likewise, a call of: gen_data(4, 10, 5) # Would give two groups of 4, # The first having a mean of 10, # The second having a mean of 5. Portfolio Point - Two important facts about functions Here are two important things to understand about functions. Functions obey lexical scoping. What does this mean? Its like what they say about Las Vegas: what happens in the function, stays in the function. Any variables created inside of a function will be discarded after the function executes and will not be accessible to the outside calling process. So if you have a line, say a variable my_var &lt;- 17 inside of a function, and try to print my_var from outside of the function, you will get an error: object my_var not found. Although the function can read variables from the environment that are not passed to it through an argument, it cannot change them. So you can only write a function to return a value, not change a value. Functions return the last value that was computed. You can compute many things inside of a function but only the last thing that was computed will be returned as part of the calling process. If you want to return my_var, which you computed earlier but not as the final computation, you can do so explicitly using return(my_var) at the end of the function (before the second curly bracket). You are doing an amazing job, but dont worry if you dont quite follow everything. It will take some practice. Just remember, we simulate data, and we create a function to make replicating it easier! Skill 5 out of 6 Complete! 5.2.6 Skill 6: Replicating Operations The last skill (we promise it is quick) you will need for the upcoming lab is knowing how to repeat a function/action (or expression) multiple times. You saw this in Chapter 4 so we will only briefly recap here. Here, we use the base function replicate(). For instance, say you wanted to calculate the mean from rnorm(100) ten times, you could write it like this: ## bad way rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() But its far easier and more readable to wrap the expression in the replicate() function where the first argument is the number of times you want to repeat the function/expression, and the second argument is the actual function/expression, i.e. replicate(times, expression). Below, we replicate the mean of 100 randomly generated numbers from the normal distribution, and we do this 10 times: replicate(10, rnorm(100) %&gt;% mean()) Also youll probably want to store the results in a variable, for example, ten_samples: ten_samples &lt;- replicate(10, rnorm(100) %&gt;% mean()) ten_samples ## [1] 0.120979389 0.083494606 -0.075267313 0.073840347 0.133399957 ## [6] -0.056600599 -0.015981205 0.003498656 0.063554283 -0.074233798 Each element (value) of the vector within ten_samples is the result of a single call to rnorm(100) %&gt;% mean(). Quickfire Questions Assuming that your hello() function from Skill 5 still exists, and it takes the argument name = Goodbye, what would happen in the console if you wrote, replicate(1000, hello(\"Goodbye\"))? Hello World would appear a thousand times hello Goodbye would appear a thousand times Hello Goodbye would appear a thousand times. Try it and see if it works! Solution To Quickfire Question # the function would be: hello &lt;- function(name = &quot;World!&quot;){ paste(&quot;Hello&quot;, name) } # and would be called by: replicate(1000, hello(&quot;Goodbye&quot;)) Excellent. And to very quickly combine all these skills together, have a look at the below code and see if you follow it. We will be creating something like this in the InClass activity so spending a few minutes to think about this will really help you. gen_data &lt;- function(n = 20, m1 = 20, m2 = -20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, m1, 5), rnorm(n, m2, 5))) %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) } replicate(10, gen_data()) ## [1] 37.81662 36.61922 41.26746 38.03652 39.95342 37.37615 38.55536 39.32070 ## [9] 39.69043 41.70738 Every of the 10 values in this output is the difference between between two simulated groups of 20 participants where Group A had a mean of 20, and Group B had a mean of -20. Skill 6 out of 6 Complete! Job Done - Activity Complete! To recap, we have shown you the following six skills: Skill 1: Generating random numbers with base::rnorm() Skill 2: Permuting values with base::sample() Skill 3: Creating a tibble (a type of data table) using tibble::tibble() Skill 4: Computing and extracting a difference in group means using dplyr::pull() and purrr::pluck() Skill 5: Creating your own custom functions using base::function() Skill 6: Repeating operations using base::replicate() You will need these skills in the InClass activity to help you perform a real permutation test. Through these skills and the permutation test you have learnyt about simulating data which in turn will help you learn about null hypothesis significance testing. If you have any questions please post them on the available forums for discussion or ask a member of staff. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Great work today. That is all for now. 5.3 InClass Activity A common statistical question when comparing two groups might be, Is there a real difference between the group means? From this we can establish two contrasting hypotheses: The null hypothesis which states that the group means are equivalent and is written as: \\(H_0: \\mu_1 = \\mu_2\\) where \\(\\mu_1\\) is the population mean of group 1 and \\(\\mu_2\\) is the population mean of group 2 Or the alternative hypothesis which states that the group means are not equivalent and is written as: \\(H_1: \\mu_1 \\ne \\mu_2\\). Using the techniques you read about in the PreClass and in previous chapters, today you will learn how to test the null hypothesis of no difference between two independent groups. We will first do this using a permutation test before looking at other tests in later chapters. 5.3.1 Permutation Tests of Hypotheses A permutation test is a basic inferential procedure that involves a reshuffling of group labels or values (PreClass Skill 2) to create new possible outcomes of the data you collected to see how your original mean difference (PreClass Skill 4) compares to a distribution of possible outcomes (InClass). Permutation tests can in fact be applied in many situations, this is just one, and it provides a good starting place for understanding hypothesis testing. The steps for the InClass exercises below, and really the logic of a permutation test for two independent groups, are: Calculate the real difference \\(D_{orig}\\) between the means of two groups (e.g. Mean of A minus Mean of B). Randomly shuffle the group labels (i.e. which group each participant belonged to - A or B) and re-calculate the difference, \\(D&#39;\\). Repeat Step 2 \\(N_{r}\\) times, where \\(N_r\\) is a large number (typically greater than 1000), storing each \\(D_i&#39;\\) value to form a null hypothesis distribution. Locate the difference you observed in Step 1 (the real difference) on the null hypothesis distribution of possible differences created in Step 3. Decide whether the original difference is sufficiently extreme to reject the null hypothesis of no difference (\\(H_0\\)) - related to the idea of probability in Chapter 4. This logic of the test works because if the null hypothesis (\\(H_0\\)) is true (there is no difference between the groups, \\(\\mu_1 = \\mu_2\\)) then the labeling of the observations/participants into groups is arbitrary, and we can rearrange the labels in order to estimate the likelihood of our original difference under the \\(H_0\\). In other words, if you know the original value of the difference between two groups (or the true difference) falls in the middle of your permuted distribution then there is no significant difference between the two groups. If, however, the original difference falls in the tail of the permuted distribution then there might be a significant difference depending on how far into the tail it falls. Again, think back to Chapter 4 and using pbinom() and pnorm() to ask what is the probability of a certain value or observation - e.g. a really tall student. Unlikely values appear in the tails of distributions. We are goign to use all the skills we have learnt up to this point to visualise this concept and what it means for making inferences about populations in our research. Lets begin! 5.3.2 Step 1: Load in Add-on Packages and Data 1.1. Open a new script and call tidyverse into your library. 1.2. Now type the statement set.seed(1409) at the top of your script after your library call and run it. (This seeds the random number generator in R so that you will get the same results as everyone else. The number 1409 is a bit random but if everyone uses it then we all get the same outcome. Different seeds give different outcomes) Note: This lab was written under R version 4.1.0 (2021-05-18). If you use a different RVersion (e.g. R 3.6.x) or set a different seed (e.g. 1509) you may get different results. 1.3. Download the data file from here and store the data in perm_data.csv in a tibble called dat using read_csv(). 1.4. Lets give every participant a participant number by adding a new column to dat. Something like this would work: mutate(subj_id = row_number()) Helpful Hint 1.1 - Something to do with library() 1.2 - set.seed(1409) 1.3 - Something to do with read_csv() 1.4 - pipe (%&gt;%) dat into the mutate line shown Portfolio Point - Different uses of row_number You will see that, in the example here, to put a row number for each of the participants we do not have to state the number of participants we have. In Preclass Activity SKill 3, however, we did. What is the difference? In the PreClass Activity we were making a new tibble from scratch and trying to create a column in that tibble using row_numbers. If you want to do that you have to state the number of rows, e.g. 1:20. However, in this example in the lab today the tibble already exists, we are just adding to it. If that is the case then you can just mutate on a column of row numbers without stating the number of participants. In summary: When creating the tibble, state the number of participants in row_numbers(). If tibble already exists, just mutate on row_numbers(). No need for specific numbers. Have a look at the resulting tibble, dat. The columns refer to: The column group is your independent variable (IV) - Group A or Group B. The column Y is your dependent variable (DV) The column subj_id is the participant number. group Y subj_id A 112.98233 1 A 90.99374 2 A 89.24606 3 A 110.44834 4 A 118.46742 5 A 103.99662 6 A 100.23478 7 A 94.08834 8 A 94.83061 9 A 92.45023 10 A 94.86514 11 A 102.17296 12 A 102.62185 13 A 96.82137 14 A 84.18164 15 A 108.37726 16 A 87.84597 17 A 109.07707 18 A 77.66668 19 A 101.79243 20 A 100.96560 21 A 123.15635 22 A 87.69904 23 A 100.51099 24 A 111.80215 25 A 79.97282 26 A 98.53688 27 A 106.39774 28 A 98.01146 29 A 66.38682 30 A 111.36834 31 A 120.01310 32 A 132.58026 33 A 103.48150 34 A 95.12308 35 A 93.78817 36 A 96.87146 37 A 120.69336 38 A 100.29758 39 A 105.66415 40 A 93.11102 41 A 133.70746 42 A 107.09334 43 A 102.85333 44 A 126.57195 45 A 73.85866 46 A 110.33882 47 A 104.51367 48 A 103.16211 49 A 93.27530 50 B 85.33467 51 B 130.87269 52 B 108.25974 53 B 68.42739 54 B 106.10006 55 B 117.21891 56 B 117.56341 57 B 85.14176 58 B 102.72539 59 B 126.91223 60 B 102.36536 61 B 105.52819 62 B 103.59579 63 B 104.40565 64 B 112.82381 65 B 92.49548 66 B 94.80926 67 B 103.99790 68 B 118.17509 69 B 112.68365 70 B 133.41869 71 B 107.30286 72 B 130.67447 73 B 113.96922 74 B 125.83227 75 B 104.29092 76 B 95.67587 77 B 125.56749 78 B 108.77002 79 B 102.09267 80 B 96.55066 81 B 108.27278 82 B 126.31361 83 B 127.70411 84 B 127.58527 85 B 82.86026 86 B 132.58606 87 B 108.08821 88 B 81.97775 89 B 117.82785 90 B 86.61208 91 B 107.47836 92 B 94.00012 93 B 128.34191 94 B 101.94476 95 B 140.78313 96 B 122.38445 97 B 93.53927 98 B 93.72602 99 B 118.77979 100 5.3.3 Step 2: Calculate the Original Mean Difference - \\(D_{orig}\\) We now need to write a pipeline of five functions (i.e. commands) that calculates the mean difference between the groups in dat, Group A minus Group B. Just like we did in PreClass Skill 4 starting from group_by() %&gt;% summarise(). Broken down into steps this would be: 2.1.1. Starting from dat, use a pipe of two dplyr one-table verbs (e.g. from Chapter 2) to create a tibble where each row contains the mean of one of the groups. Name the column storing the means as m. 2.1.2. Continue the pipe to spread your data from long to wide format, based on the names in column group and values in m. You should use pivot_wider(). 2.1.3. Now add a pipe that creates a new column in this wide dataset called diff which is the value of group As mean minus group Bs mean. 2.1.4. Pull out the value in diff (the mean of group A minus the mean of group B) to finish the pipe. Helpful Hint dat %&gt;% group_by(?) %&gt;% summarise(m = ?) %&gt;% pivot_wider(names_from = group, values_from = m) %&gt;% mutate(diff = ? - ?) %&gt;% pull(?) Quickfire Questions Check that your value for \\(D_{orig}\\) is correct, without using the solution, by typing your \\(D_{orig}\\) value to two decimal places in the box. Include the sign, e.g. -1.23. The box will go green if you are correct. Just as we did in Skill 4 in the PreClass Activity, the above steps have created a pipeline of five functions to get one value. Nice! Now, using the knowledge from Skills 2 and 5 of the PreClass Activity, we now need to turn this into a function because we are going to be permuting the data set (specifically the grouping labels) and re-calculating the difference many, many times. This is how we are going to create a distribution of values. 2.2. Now, using what you learnt in PreClass Activity Skill 5, wrap your pipeline (from above) in a function (between the curly brackets) called calc_diff. However, instead of starting your pipeline with dat, start it with x - literally change the word dat at the start of your pipeline to x. This function will take a single argument named x, where x is any tibble that you want to calculate group means from. As in the previous step, the function will atill return a single value which is the difference between the group means. The start of your function will look like this below: calc_diff &lt;- function(x){ x %&gt;%.... } Helpful Hint calc_diff &lt;- function(x) { x %&gt;% group_by(group) %&gt;% the_rest_of_your_pipe... %&gt;% pull(diff) } 2.3. Now call your new function where x = dat as the argument and store the result in a new variable called d_orig. Make sure that your function returns the same value for \\(D_{orig}\\) as you got above and that your function returns a single value rather than a tibble. You can test whether something is a tibble or a value by: is.tibble(d_orig) should give you FALSE and is.numeric(d_orig) should give you TRUE. Helpful Hint d_orig &lt;- function_name(x = data_name) # or d_orig &lt;- function_name(data_name) # Then type the following in the Console and look at the answer: is.tibble(d_orig) # True (is a tibble) or False (is not a tibble) is.numeric(d_orig) # True (is numeric) or False (is not numeric; it is a character or integer instead.) Great. To recap, using what we learnt in PreClass Skills 4 &amp; 5, we now have a function called calc_diff() that, when we give it our data (dat), it returns the original difference between the groups - which we have stored in d_orig. Ok, so we now know the original difference between Groups A and B. Also, we know at the start of this InClass Activity we said we wanted to determine if there was a significant difference between Groups A and B by looking at the probability of obtaining the \\(D_{orig}\\) value from the distribution of all possible outcomes for this data, right? So in order to do that, using what we learnt in Skills 2 and 6 of the PreClass Activity, we need to create a distribution of all possible differences between Groups A and B to see where our original difference lies in that distribution. And our first step in that process is shuffling the group (A or B) in our dataset and finding the differencea few hundred times! 5.3.4 Step 3: Permute the Group Labels 3.1. Create a new function called permute() that: Takes as input a dataset (e.g. x) and returns the same dataset transformed such that the group labels (the values in the column group) are shuffled: started below for you. This will require using the sample() function within a mutate() function. You have used mutate() twice already today and you saw how to sample() letters in the PreClass Activity Skill 2. Get this working outside the function first, e.g. in your console windown, and then add it to the function. permute &lt;- function(x){ x %&gt;%..... } Helpful Hint Might be easier to think of these steps in reverse. Start with a mutate() function that rewrites the column group every time you run it, e.g. dat %&gt;% mutate(variable = sample(variable)) Note: Be accurate on the spelling within the mutate() function. If your original column is called group then you should sample(group). Now put that into your permute() function making the necessary adjustments to the code so it starts x %&gt;% instead of dat %&gt;%. Again x should be in the function and not dat. 3.2. Try out your new permute() function by calling it on dat (i.e. x = dat) a few times. You should see the group labels in the group column changing randomly. The most common mistake is that people mutate a new column by mispelling group. You want to overwrite/change the information in the group column not make a new one, so be careful with the spelling. Group Discussion Point Now would be an excellent time to spend five minutes as a group recapping what you are doing. You have the original difference between groups. You have a function that calculates and stores this difference. You have a function that reshuffles the labels of the group. You understand probability is based on where a specific value occurs on the distribution of all possible values You know extreme differences are significant. Do you understand why? If not, go back to the principles of the permutation test at the start of the lab then read on 5.3.5 Step 4: Create the Null-Hypothesis Distribution (NHD) for the Difference Now that we have the original difference and our two functions, one to shuffle group labels (permute) and one to calculate the difference between two groups (calc_diff), we need to actually create the distribution of possible differences and see where the original difference lies in it. 4.1.1. Write a a single pipeline that takes dat as the input, permutes the group labels with a call to your function permute(), and then calculates the difference in means between these new groups with a call to your function calc_diff(). 4.1.2. Run this line manually a few times and watch the resulting value change every time you run it. This is because on each run the order of A and B labels within the data is getting changed and a new difference is calculated. Helpful Hint Think about verbalising your pipelines. In a single pipeline: I want to permute the data into two new groups. Then I want to calculate the difference between these two new groups. The functions you have created do these steps. You just have to put them in order and pipe the data through it: dat %&gt;% function1() %&gt;% function2() 4.2. Now take your pipeline of functions and repeat it 1000 times using the replicate() function as you saw in PreClass Activity Skill 6. Store the output in a variable called nhd. nhd will contain 1000 values where each value is the mean difference of each of the 1000 random permutations of the data. (Warning: This will probably take a while to run, perhaps 10 seconds.) This step will give you 1000 possible values of the difference between the permuted groups A and B - your permuted distribution. Helpful Hint This is going to look something like nhd &lt;- replicate(times, expression) Replace expression with the pipeline you created in 4.1.1 Replace times with how many times you want to run it the expression 4.3 Now that we have all the values for our distribution in nhd, lets visualise this distribution in a histogram. This shows us the likelihood of various mean differences under \\(H_0\\). We have started the code for you but one thing to note however is that nhd is not a tibble and ggplot needs it to be a tibble. You need to convert it just like you did in Preclass Activity Skill 3. You might start by do something like: ggplot(data = tibble(x = NULL), aes(x)) + NULL Helpful Hint Remember that ggplot works as: ggplot(data, aes(x)) + geom. Here you need to convert nhd into a tibble and put that in as your data. Look at the example above and keep in mind that, in this case, the first NULL could be replaced with the data in nhd. Group Discussion Point Looking at the histogram, visually locate where your original value would sit on this distribution. Would it be extreme, in the tail, or does it look rather common, in the middle? is in the middle so looks common is in the tail so looks extreme Before moving on stop to think about what this means - that the difference between the two original groups is rather uncommon in this permuted distribution, i.e. is in the tails! Again, if unsure, go back to the principles of NHST, or the ideas raised about Probability in Chapter 4, or discuss it with your tutor! 5.3.6 Step 5: Compare the Observed Mean Difference to the NHD Right! We know \\(D_{orig}\\) and we know all the values of the simulated distribution, \\(D_i&#39;\\). If the null hypothesis is false \\(\\mu1 \\ne \\mu2\\), and there is a real difference between the groups, then the difference in means we observed for the original data (\\(D_{orig}\\)) should be somewhere in either tail of the null-hypothesis distribution we just estimated, \\(D_i&#39;\\) - i.e. \\(D_{orig}\\) should be an extreme value. How can we test this beyond a visual inspection? First, we have to decide on a false positive (Type I error) rate which is the rate at which we will falsely reject \\(H_0\\) when it is true. This rate is referred to by the Greek letter \\(\\alpha\\) (alpha). Lets just use the conventional level used in Psychology: \\(\\alpha = .05\\). So the question we must ask is, if the null hypothesis was true, what would be the probability of getting a difference in means as extreme as the one we observed in the original data? We will label this p for probability. Group Discussion Point Take a few moments as a group to see if you can figure out how you might compute p from the data before we show you how. We will then show you the process in the next few, final, steps. 5.1. Replace the NULLs in the code below to create a logical vector which states TRUE for all values of nhd greater than or equal to d_orig regardless of sign. Note: A logical vector is one that returns TRUE when the expression is true and FALSE when the expression is false. Hint: You have two NULLs to replace and two things to replace them with - d_orig or nhd. Hint: You want lvec to show every where that the values in nhd are greater than or equal to the d_orig value. lvec &lt;- abs(NULL) &gt;= abs(NULL) Portfolio Point - abs and the case of one or two tails In the code above, the function abs() says to ignore the sign and use the absolute value. For instance, if d_orig = -7, then abs(d_orig) = 7. Why do we do this here? Can you think why you want to know how extreme your value is in this distribution regardless of whether the value is positive or negative? The answer relates to whether you are testing in one or two tails of your distribution; the positive side, the negative side, or both. You will have heard in your lectures of one or two-tailed tests. Most people would say to run two-tailed tests. This means looking at the negative and positive tails of the distribution to see if our original value is extreme, and the simplest way to do this is to ignore the sign of the values and treat both sides equally. If you wanted to only test one-tail, say that your value is extreme to the negative side of the tail, then you would not use the abs() and set the expression to make sure you only find values less than your original value. To test only on the positive side of the distribution, make sure you only get values higher than the original. But for now we will mostly look at two-tailed tests. 5.2. Replace the NULL in the code below to sum() the lvec vector to get the total number of values equal to or greater than our original difference, d_orig. Fortunately, R is fine with summing TRUEs and FALSEs so you do not have to convert the data at all. Hint: This step is easier than you think. sum(...) n_exceeding_orig &lt;- NULL 5.3. Replace the NULL in the code below to calculate the probability of finding a value of d_orig in our nhd distribution by dividing n_exceeding_orig, the number of values greater than or equal to your original value, by the length() of your whole distribution nhd. Note: the length of nhd is the same as the number of replications we ran. Using code reduces the chance of human error. Hint: We saw the length() function in the PreClass Activity Skill 1. p &lt;- NULL 5.4. Finally, complete the sentence below determining if the original value was extreme or not in regards to the distribution. Use inline coding, shown in Chapter 1, to replace the XXXs. For example, if you were to write, `r length(nhd)`, when knitted would appear as 1000. \" The difference between Group A and Group B (M = XXX) was found to be have a probability of p = XXX. This means that the original mean difference was  and the null hypothesis is ..\" Job Done - Activity Complete! Well done in completing this lab. Lets recap before finishing. We had two groups, A and B, that we had tested in an experiment. We calculated the mean difference between A and B and wanted to know if this was a significant difference. To test this, we created a distribution of possible differences between A and B using the premise of permutation tests and then found the probability of our original value in that permuted distribution. The more extreme the value in a distribution, the more likely that the difference is significant. And that is exactly what we found, \\(p &lt; .05\\). Next time we will look at using functions and inferential tests to perform this analysis but by understanding the above you now know how probability is determined and have a very good understanding of null-hypothesis significance testing (NHST); one of the main analytical approaches in Psychology. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the available forums or ask a member of staff. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 5.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 5.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 5.5.1 InClass Activities 5.5.1.1 Step 1 library(&quot;tidyverse&quot;) set.seed(1409) dat &lt;- read_csv(&quot;perm_data.csv&quot;) %&gt;% mutate(subj_id = row_number()) Note: The data for this activity was created in exactly the same fashion as we saw in the PreClass Activity Skill 1. If you wanted to create your own datasets this is the code we used. dat &lt;- tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), each = 50), Y = c(rnorm(50, 100, 15), rnorm(50, 110, 15))) write_csv(dat, &quot;perm_data.csv&quot;) Return to Task 5.5.1.2 Step 2 Step 2.1.1 - the basic dat pipeline dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) group m A 101.4993 B 108.8877 Step 2.1.2 - using pivot_wider() to separate the groups dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) A B 101.4993 108.8877 Step 2.1.3 - mutate() the column of mean differences dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) %&gt;% mutate(diff = A - B) A B diff 101.4993 108.8877 -7.388401 Step 2.1.4 - pull() out the difference dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) ## [1] -7.388401 As such \\(D_{orig}\\) = -7.39 Step 2.2 - setting up the calc_diff() function calc_diff &lt;- function(x) { x %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% pivot_wider(names_from = &quot;group&quot;, values_from = &quot;m&quot;) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) } Step 2.3 - Calculating d_orig using calc_diff() d_orig &lt;- calc_diff(dat) is_tibble(d_orig) is_numeric(d_orig) ## [1] FALSE ## [1] TRUE Return to Task 5.5.1.3 Step 3 permute &lt;- function(x) { x %&gt;% mutate(group = sample(group)) } permute(dat) group Y subj_id A 112.98233 1 A 90.99374 2 B 89.24606 3 B 110.44834 4 A 118.46742 5 A 103.99662 6 A 100.23478 7 B 94.08834 8 B 94.83061 9 A 92.45023 10 B 94.86514 11 A 102.17296 12 A 102.62185 13 A 96.82137 14 A 84.18164 15 B 108.37726 16 B 87.84597 17 A 109.07707 18 A 77.66668 19 A 101.79243 20 A 100.96560 21 B 123.15635 22 A 87.69904 23 A 100.51099 24 A 111.80215 25 B 79.97282 26 B 98.53688 27 B 106.39774 28 B 98.01146 29 B 66.38682 30 B 111.36834 31 B 120.01310 32 A 132.58026 33 A 103.48150 34 A 95.12308 35 A 93.78817 36 B 96.87146 37 A 120.69336 38 B 100.29758 39 B 105.66415 40 A 93.11102 41 B 133.70746 42 B 107.09334 43 B 102.85333 44 A 126.57195 45 B 73.85866 46 A 110.33882 47 B 104.51367 48 B 103.16211 49 A 93.27530 50 B 85.33467 51 A 130.87269 52 A 108.25974 53 A 68.42739 54 B 106.10006 55 A 117.21891 56 B 117.56341 57 B 85.14176 58 B 102.72539 59 A 126.91223 60 A 102.36536 61 B 105.52819 62 A 103.59579 63 B 104.40565 64 B 112.82381 65 B 92.49548 66 B 94.80926 67 B 103.99790 68 B 118.17509 69 A 112.68365 70 A 133.41869 71 A 107.30286 72 B 130.67447 73 A 113.96922 74 B 125.83227 75 B 104.29092 76 B 95.67587 77 B 125.56749 78 A 108.77002 79 B 102.09267 80 B 96.55066 81 A 108.27278 82 A 126.31361 83 B 127.70411 84 B 127.58527 85 B 82.86026 86 A 132.58606 87 A 108.08821 88 B 81.97775 89 B 117.82785 90 B 86.61208 91 A 107.47836 92 A 94.00012 93 A 128.34191 94 A 101.94476 95 A 140.78313 96 B 122.38445 97 A 93.53927 98 A 93.72602 99 A 118.77979 100 Return to Task 5.5.1.4 Step 4 Step 4.1 - the pipeline dat %&gt;% permute() %&gt;% calc_diff() ## [1] 7.232069 Step 4.2 - creating nhd nhd &lt;- replicate(1000, dat %&gt;% permute() %&gt;% calc_diff()) Step 4.3 - plotting nhd ggplot(tibble(x = nhd), aes(x)) + geom_histogram(binwidth = 1) Figure 5.1: The simulated distribution of all possible differences Return to Task 5.5.1.5 Step 5 Step 5.1 - The logical vector This code establishes all the values in nhd that are equal to or greater than the value in d_orig It returns all these values as TRUE and all other values as FALSE abs() tells the code to ignore the sign of the value (i.e. assumes everything is positive) lvec = abs(nhd) &gt;= abs(d_orig) Step 5.2 - Sum up all the TRUE values This gives the total number of values greater or equal to d_orig n_exceeding_orig &lt;- sum(lvec) Step 5.3 - Calculate the probability The probability of obtaining d_orig or greater is calculated by the number of values equal to or greater than d_orig, divided by the full size of nhd (or in other words, its length) p &lt;- n_exceeding_orig / length(nhd) As such the probability of finding a value of \\(D_{orig}\\) or larger was p = 0.016 To write up the sentence, with inline coding you would write: The difference between Group A and Group B (M = `r round(d_orig, 2)`) was found to be have a probability of p = `r p`. This means that the original mean difference was significant and the null hypothesis is rejected. Which when knitted would produce: The difference between Group A and Group B (M = -7.39) was found to be have a probability of p = 0.016. This means that the original mean difference was significant and the null hypothesis is rejected. Return to Task Chapter Complete! 5.6 Additional Material Below is some additional material that might help you understand permutations a bit more and some additional ideas. More on pivot_wider() Much in the same way that pivot_longer() has become the new go-to function for gathering data, replacing the gather() function that many struggled with, pivot_wider() is likewise a new kid on the block. Prior to using pivot_wider(), this book used the very confusing spread() function to change data from long format to wide format - i.e. splitting low columns into more shorter columns. spread() was an interesting function that somehow always managed to confuse people and it seemed that getting it to work was largely chance - pivot_wider() was introduced to reduce these issues and as you saw inclass it works similar to pivot_longer() in that it requires clear statement of names_from and values_from. pivot_wider() should make spreading data wider a lot easier and as such we have aimed to replace every occurrence of it in this book. However, as before, we are still human and mistakes happen. Also, you will still see it being used in codes that you find on things like StackOverflow etc, so below is a very brief example that mimics the InClass activity, showing how spread() works. For ease, we would always recommend using pivot_wider(). The spread() function This example mimics Skill 4 (Section 5.2.4) of the Preclass Activity where we are splitting the data in my_data_means - shown below: Remember the values will be different from the PreClass because we are using a function that creates a set of random numbers each time, rnorm() group m A 19.63478 B -19.78001 In order to split the table so that we have one column for Group A and another columns for Group B, we would use the below code. The spread function (?spread) splits the data in column m by the information, i.e. labels, in column group and puts the data into separate columns. my_data_means %&gt;% spread(group, m) A B 19.63478 -19.78001 So the spread() function is not that different from using pivot_wider() but the inclusion of names_from and values_from really does make using pivot_wider() a lot clearer. The rest of the actions in that step of the Activity would follow as in the PreClass Activity - adding a mutate() as shown below: my_data_means %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) More on simulating your own data Often we get asked for data to allow people to practice their hand calculations. One of the reasons for this chapter is to show people how to simulate their own data and to give them the functions to do so. We are going to use a chi-square test to show you, in a bit more concrete fashion, how you might go about this. We dont really cover much about chi-squares in this book so it is a nice addition. So lets say we have run an observation test with two groups (A and B) and we ask them the classic Huey Lewis &amp; the News question, Do you believe in love? giving them three response options, Yes, No, Unsure. Lets say that we run 50 people in each group as well. Putting that altogether we will end up with a cross-tabulation table that is 2 rows by 3 columns, with each row adding up to 50. Using everything we have learnt in this Chapter here is how we might simulate that data: Going to need at least tidyverse library(tidyverse) The below code is how we might created a tibble (my_chi_data) where you have two columns: Groups showing whether participants are in Group A or B Responses showing the simulated response for that participant We wont walk you through this code as you should know enough from Chapters 4 &amp; 5 to follow it and edit it, but if you really struggle, get in touch my_chi_data &lt;- tibble(Groups = rep(c(&quot;A&quot;,&quot;B&quot;), # Names of your Groups times = c(50, 50)), # Participants per Group Responses = sample(c(&quot;Yes&quot;, # Response Options to Sample &quot;No&quot;, &quot;Unsure&quot;), size = 50 + 50, # Total number of responses replace = TRUE)) # with replacement If you wanted to save your data as a .csv file for later use or to send to a colleague, you could use: write_csv(my_chi_data, &quot;my_data.csv&quot;) Or with a bit of data wrangling, you can convert my_chi_data into a cross-tabulation table that would look like as follows: Again this wouldnt use any new functions group_by() %&gt;% count() %&gt;% pivot_wider() group by both columns as you want to count how many times each response was used by each group. Table 5.2: Cross-Tabulation Table of Simulated Data for learning about chi-squares Groups No Unsure Yes Total A 17 18 15 50 B 18 14 18 50 More on Permutations - a blog There is a lot in this chapter and some find all the different components quite tricky. As such, this is a very paired down version of the crux of the chapter to see if removing all the blurb, keeping it to just the essential, helps. The main concept of Null Hypothesis Significance Testing (NHST) is that you are testing that there is no significant difference between two values - lets say two means for the moment. The null hypothesis states that there is no meaningful difference between the two groups of interest. And as such, any test that you do on the difference between the means of those two groups is trying to determine the probability of finding a difference of the size you found, or larger, in your experiment, if there is actually no real difference between the two groups. In the InClass activity we have our two groups, A and B, and we calculated the difference between the means of those two groups to be \\(D_{orig}\\) = -7.39. Putting that in terms of the Null Hypothesis (\\(H_{0}\\)) we are asking what is the probability of finding a difference between means of -7.39 (or larger) if there is no real difference between our two groups. In order to test the above question (our null hypothesis) we need to compare our original difference against a distribution of possible differences to see how likely the original difference is in that distribution - remember, for example, extreme values are in the tails of the Normal Distribution. As we dont know where our data comes from we dont really know what distribution to check our \\(D_{orig}\\) against. In that situation we can create a distribution based on the values we have already collected using the permutation test we talked about InClass. Basically, in this test, we shuffle which group people belonged to (A or B) and then calculate the difference between these shuffled groups. And we do that 1000 times or more to create a distribution. The reason we are allowed to shuffle whether people were in group A or B is because we are deliberately wanting to test if the group people were originally in drives the difference we see. If the original grouping of participants is important then the original difference (\\(D_{orig}\\)) will be an extreme value on the distribution we create. If the group people were originally in isnt important, then the original difference (\\(D_{orig}\\)) will not be an extreme value in the distribution; it will be a common value. Here again is the distribution we simulated using the permutation test described InClass and above: ggplot(tibble(x = nhd), aes(x)) + geom_histogram(binwidth = 1) Figure 5.2: The simulated distribution of all possible differences Now lets highlight the areas in this distribution where the values in the distribution are equal to or larger than our \\(D_{orig}\\) value - these are the red areas. The blue areas are those where the values in the distribution are smaller than our \\(D_{orig}\\) value. We have changed the binwidth of the plot a little from above to help see the different areas clearly. ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = ## &quot;none&quot;)` instead. Figure 5.3: The simulated distribution of all possible differences. Red areas show values greater than or equal to the original difference. Blue areas show values smaller than the original difference. Going back to what we said about Null Hypothesis Significance Testing, the question is what is the probability of finding the original difference (\\(D_{orig}\\)) if there is no real effect of being in either Group A or Group B. As you can see from the plot, there are very few areas on the distribution where the values are greater than or equal to the original difference - i.e. \\(D_{orig}\\) is an unlikely difference to obtain if there was really no difference between the two groups. From the InClass you will recall that the probability of finding a difference greater than or equal to \\(D_{orig}\\) if there was no difference between the two groups was p = 0.017. This is below the standard cut-off we use in Psychology, \\(p &lt;= .05\\). As such we would reject our null hypothesis and suggest that there is a significant difference between the two groups. Hopefully this gives a bit more insight to the InClass activities but feel free to post any questions on the forums for discussion or ask a member of staff. End of Additional Material! "],["nhst-and-one-sample-t-tests.html", "Lab 6 NHST and One-sample t-tests 6.1 Overview 6.2 PreClass Activity 6.3 InClass Activity 6.4 Test Yourself 6.5 Solutions to Questions 6.6 Additional Material", " Lab 6 NHST and One-sample t-tests 6.1 Overview In the previous chapters we have talked a lot about probability, comparing values across groups, and the inference from a sample to a population. In effect this is the essence of a lot of quantitative research that uses Null Hypothesis Significance Testing (NHST). You collect a sample, calculate a summary statistic about that sample, and use probability to establish the likelihood of that statistic occurring given certain situations. However these concepts and ideas are hard to grasp at first and take playing around with data a few times to help get a better understanding of them. As such, to demonstrate these ideas further, and to start introducing commonly used tests and approaches to reproducible science, we will look at data related to sleep - a very important practice for the consolidation of learning that most of us do not get enough of! The study we will look at, to explore NHST more, is by one of our team and makes use of a well known task in Psychology, the Posner Paradigm: Woods et al (2009) The clock as a focus of selective attention in those with primary insomnia: An experimental study using a modified Posner paradigm . In Chapter 6, through the activities, we will: Recap on testing a hypothesis through null hypothesis significance testing (NHST). Learn about approaches to reproducible experiments. Learn about binomial tests, as well as one-sample and independent t-tests. Learn about Posner paradigms and attention (PreClass), and the recency effect (InClass). Portfolio Point - Additional background on The Posner Paradigm You dont need to read this to complete the activities it might help it make more sense. The Posner paradigm (Posner, 1980), or the Posner Cueing task, is an attentional shift task, often used in a variety of fields to test spatial attention and how this is impacted by disorders or injury. It works by having participants look at a fixation cross in the center of a screen. To either side is an empty box. After a short delay, a cue (e.g. an arrow, an asterisk, or some other attention grabbing image) appears in one of the boxes (e.g. the box to the left of the fixation). This stays on screen for a few hundred milliseconds and is then replaced by a second image called the target (e.g. a different shape or image). Participants then have to respond left or right depending on which side of the fixation the target appeared. The dependent variable (DV) is the time taken to respond to the target appearing. Key to the task is that in most trials the target will appear on the same side as the cue - the cue facilitates the target - and so participants will be quicker to respond. These are called valid trials. However, on some occasions the target will appear on the other side from the cue - e.g. the cue is on the left but the target appears on the right - and these are called invalid trials; participants will be slower to respond here as the cue misleads the participant. From that, you should be starting to get an idea of how a Posner paradigm can help to measure attention and how it can help determine if people have issues in shifting attention (particularly from the invalid trials). Reference: Posner, M. (1980) Orienting of attention. Quarterly Journal of Experimental Psychology, 32(1), 3-25 6.2 PreClass Activity ManyLabs - an approach to reproducible science As you will learn from reading papers around the reproduciblity crisis, findings from experiments tend to be more reproducible when we increase participant numbers as this increases the power of the studys design; the likelihood of an experimental design detecting an effect, of a given size, when there is an effect to detect. Portfolio Point - The power of what? Power is a rather tricky concept in research that essentially amounts to the probability of your design being able to detect a significant difference when there is actually a significant difference to detect. Power is an interplay between three other aspects of research design: alpha - your critical p-value (normally .05); the sample size (n); the effect size - how big is the difference (measured in a number of ways). If you know any three of these four elements (power, alpha, effect size, n) you can calculate the fourth. We will save further discussion of power until Chapter 8 but if you want to read ahead then this blog is highly recommended: The Power Dialogues. However, running several hundred participants in your one study can be a significant time and financial investment. Fortunately, the idea of a ManyLabs project can solve this problem. In this scenario the same experiment is run in various locations, all using the same procedure, and then the data is collapsed together and analysed as one. You can see a nice example of a Many Labs project in the paper Investigating Variation in Replicability (Klein et al., 2014). See how many labs and researchers are involved? Perhaps this is a better approach than lots of researchers working individually? You think this all sounds a great idea so in your quest to be a collaborative reproducible researcher, and as a high-five to #TeamScience, you have joined a ManyLabs study replicating the findings of Woods et al. (2009). And that study is the premis for todays activities so lets start by having a quick-run through of the background of the experiment. The Background Woods and colleagues (2009) were interested in how the attention of people with poor sleep (Primary Insomnia - PI) was more tuned towards sleep-related stimuli than the attention of people with normal sleep (NS). Woods et al., hypothesised that participants with poor sleep would be more attentive to images related to a lack of sleep (i.e. an alarm clock showing 2AM) than participants with normal sleep would be. To test their hypothesis, the authors used a modified Posner paradigm, shown in Figure 1 of the paper, where images of an alarm clock acted as the cue on both valid and invalid trials, with the symbol ( .. ) being the target. As can be seen in Figure 3 of Woods et al., the authors found that, on valid trials, whilst Primary Insomnia participants were faster in responding to the target, suggesting a slight increase in attention to the sleep related cue compared to the Normal Sleepers, there was no difference between groups. In contrast, for invalid trials, where poor sleep participants were expected to be distracted by the cue, the authors did indeed find a significant difference between groups consistent with their alternative hypothesis \\(H_{1}\\). Woods et al., concluded that poor sleepers (Primary Insomnia participants) were slower to respond to the target on invalid trials, compared to Normal Sleepers, due to the attention of the Primary Insomnia participants being drawn to the misleading cue (the alarm clock) on the invalid trials. This increased attention to the sleep-related cue led to an overall slower reponse to the target on these invalid trials. As we said above, your lab is now part of a ManyLabs project that is looking to replicate this finding from Woods et al., (2009). As a pilot study, to test recruitment procedures, as well as the experimental paradigm and analyses pipeline, each lab gathers data from 22 Normal Sleepers. It is common to use only the control participants in a pilot (in this cas the NS participants) as they are more plentiful in the population than the experimental group (in this case PI participants) and saves using participants from the PI group which may be harder to obtain in the long run. After gathering your data, we want to check the recruitment process and whether or not you have been able to draw a sample of normal sleepers similar to the sample drawn by Woods et al. To keep things straightforward, allowing us to understand the analyses better, we will only look at valid trials today, in NS participants, but in effect you could perform this test on all groups and conditions. Are These Participants Normal Sleepers (NS)? Below is the data from the 22 participants you have collected in your pilot study. Their mean reaction time for valid trials (in milliseconds) is shown in the right hand column, valid_rt. Table 6.1: Pilot Data for 22 Participants on a Sleep-Related Posner Paradigm. ID is shown in participant column and mean reaction time (ms) on valid trails is shown in valid_rt column. participant valid_rt 1 631.2 2 800.8 3 595.4 4 502.6 5 604.5 6 516.9 7 658.0 8 502.0 9 496.7 10 600.3 11 714.6 12 623.7 13 634.5 14 724.9 15 815.7 16 456.9 17 703.4 18 647.5 19 657.9 20 613.2 21 585.4 22 674.1 If you look at Woods et al (2009) Figure 3 you will see that, on valid trials, the mean reaction time for NS participants was 590 ms with a SD = 94 ms. As above, as part of our pilot study, we want to confirm that the 22 participants we have gathered are indeed Normal Sleepers. We will use the mean and SD from Woods et al., to confirm this. Essentially we are asking if the participants in the pilot are responding in a similar fashion as NS participants in the original study. You will know from Chapter 5 that when using Null Hypothesis Significance Testing (NHST) we are working with both a null hypothesis (\\(H_{0}\\)) and an alternative hypothesis (\\(H_{1}\\)). Thinking about this study it makes some logical sense to think about it in terms of the null hypothesis (\\(\\mu1 = \\mu2\\)). So we could phrase our hypothesis as, we hypothesise that there is no significant difference in mean reaction times to valid trials on the modified Posner experiment between the participants in the pilot study and the participants in the original study by Woods et al. There is actually a few analytical ways to test our null hypothesis. Today we will show you how to do two of these. In tasks 1-3 we will use a binomial test and in tasks 4-8 we will use a one-sample t-test Portfolio Point - Binomial test and the one-sample t-test The Binomial test is a very simple test that converts all participants to either being above or below a cut-off point, e.g. a mean value, and looking at the probability of finding that number of participants above that cut-off. The one-sample t-test is similar in that it compares participants to a cut-off but it compares the mean and standard deviation of the collected sample to an ideal mean and standard deviation. By comparing the difference in means, divided by the standard deviation of the difference (a measure of the variance), we can determine if the sample is similar or not to the ideal mean. 6.2.1 The Binomial Test The Binomial test is one of the most basic tests in null hypothesis testing in that it uses very little information. The binomial test is used when a study has two possible outcomes (success or failure) and you have an idea about what the probability of success is. This will sound familiar from the work we did in Chapter 4 and the Binomial distribution. A binomial test tests if an observed result is different from what was expected. For example, is the number of heads in a series of coin flips different from what was expected. Or in our case for this chapter, we want to test whether our normal sleepers are giving reaction times that are the same or different from those measured by Woods et al. The following tasks will take you through the process. 6.2.2 Task 1: Creating a Dataframe First we need to create a tibble with our data so that we can work with it. Enter the data for the 22 participants displayed above into a tibble and store it in ns_data. Have one column showing the participant number (called participant) and another column showing the mean reaction time, called valid_rt. We saw how to enter data into tibbles in Chapter 5 PreClass Skill 3. You could type each value out or copy and paste them from the hint below. Helpful Hint You can use this code structure and replace the NULL values: ns_data &lt;- tibble(participant = c(NULL,NULL,), valid_rt = c(NULL,NULL,)) The values are: 631.2, 800.8, 595.4, 502.6, 604.5, 516.9, 658.0, 502.0, 496.7, 600.3, 714.6, 623.7, 634.5, 724.9, 815.7, 456.9, 703.4, 647.5, 657.9, 613.2, 585.4, 674.1 6.2.3 Task 2: Comparing Original and New Sample Reaction Times Our next step is to establish how many participants from our pilot study are above the mean in the original study by Woods et al.  In the original study the mean reaction time for valid trials was 590 ms. Store this value in woods_mean. Now write code to calculate the number of participants in the new sample (ns_data created in Task 1) that had a mean reaction time greater than the original papers mean. Store this single value in n_participants. The function nrow() may help here. nrow() is similar to count() or n() but nrow() returns the number as a single value and not in a tibble. Be sure whatever method you use you end up with a single value, not a tibble. You may need to use pull() or pluck() Helpful Hint Part 1 woods_mean &lt;- value Part 2 A few ways to achieve this. Here are a couple you could try ns_data %&gt;% filter(x ? y) %&gt;% count() %&gt;% pull(?) or ns_data %&gt;% filter(x ? y) %&gt;% summarise(n = ?) %&gt;% pull(?) or ns_data %&gt;% filter(x ? y) %&gt;% nrow() or dim[] %&gt;% pluck() Quickfire Questions The number of participants that have a mean reaction time for valid trials greater than that of the original paper is: 6 10 16 17 6.2.4 Task 3: Calculating Probability Our final step for the binomial test is to compare our value from Task 2, 16 participants, to our hypothetical cut-off. We will work under the assumption that the mean reaction time from the original paper, i.e. 590 ms, is a good estimate for the population of good sleepers (NS). If that is true then each new participant that we have tested should have a .5 chance of being above this mean reaction time (\\(p = .5\\) for each participant). To phrase this another way, the expected number of participants above the cut-off would be \\(.5 \\times N\\), where \\(N\\) is the number of participants, or \\(.5 \\times 22\\) = 11 participants. Calculate what would be the probability of observing at least 16 participants out of your 22 participants that had a valid_rt greater than the Woods et al (2009) mean value. hint: We looked at very similar questions in Chapter 4 using dbinom() and pbinom() hint: The key thing is that you are asking about obtaining X or more successes. You will need to think back about cut-offs and lower.tails. Helpful Hint Think back to Chapter 4 where we used the binomial distribution. This question can be phrased as, what is the probability of obtaining X or more succeses out of Y trials, given the expected probability of Z. How many Xs? (see question) How many Ys? (see question) What is the probability of being either above or below the mean/cut-off? (see question) You can use a dbinom() %&gt;% sum() for this or maybe a pbinom() Quickfire Questions Using the Psychology standard \\(\\alpha = .05\\), do you think these NS participants are responding in a similar fashion as the participants in the original paper? Select the appropriate answer: No Yes According to the Binomial test would you accept or reject the null hypothesis that we set at the start of this test? Reject Accept Explain This - I dont get this answer The probability of obtaining 16 participants with a mean reaction time greater than the cut-off of 590 ms is p = .026. This is smaller than the field norm of p = .05. As such we can say that, using the binomial test, the new sample appears to be significantly different from the old sample as there is a significantly larger number of participants above the cut-off (M = 590ms) than would be expected if the new sample and the old sample were responding in a similar fashion. We would therefore reject our null hypothesis! 6.2.5 The One-Sample t-test In Task 3 you ran a binomial test of the null hypothesis testing that here was no significant difference in mean reaction times to valid trials on the modified Posner experiment between the participants in the pilot study and the participants in the original study by Woods et al. However, the binomial test did not use all the available information in the data because each participant was simply classified as being above or below the mean of the original paper, i.e. yes or no. Information about the magnitude of the discrepancy from the mean was discarded. This information is really interesting and important however and if we wanted to maintain that information then we would need to use a one-sample \\(t\\)-test. In a one-sample \\(t\\)-test, you test the null hypothesis \\(H_0: \\mu = \\mu_0\\) where: \\(H_0\\) is the symbol for the null hypothesis, \\(\\mu\\) (pronounced mu - like few with an m) is the unobserved population mean, and \\(\\mu_0\\) (mu-zero) is some other mean to compare against (which could be an alternative population or sample mean or a constant). And we will do this by calculating the test statistic \\(t\\) which comes from the \\(t\\) distribution - more on that distribution below and in the lectures. The formula to calculate the observed test statistic \\(t\\) for the one-sample \\(t\\)-test is: \\[t = \\frac{\\mu - \\mu_0}{s\\ / \\sqrt(n)}\\] \\(s\\) is the stanard deviation of the sample collected, and \\(n\\) is the number of participants in the sample. Portfolio Point - The Null Hypothesis equation The eagle-eyed of you may have spotted that in Chapter 5 the null hypothesis was stated as: \\[H_0: \\mu_1 = \\mu_2\\] whereas in this Chapter we are stating it as: \\(S\\)H_0: = _0$$ Whats the difference? Conceptually there is no real difference. The null hypothesis in both situations is stating that there is no difference between the means. The only difference is that in Chapter 5 we know the actual mean of the groups of interest. In Chapter 6 we only really know the mean we want to compare against - \\(\\mu_0\\). We do not know the actual mean of the population of interest (we only know the sample we have collected) - so this is written as \\(\\mu\\). Main thing to recognise is that the null hypothesis \\(H_0\\) always states that there is no significant difference between thw two means of interest - i.e. the two means are equivalent. For the current problem: \\(\\mu\\) is the unobserved true mean of all possible participants. We dont know this value. Our best guess is the mean of the sample of 22 participants so we will use that mean here. As such will substitute this value into our formula, which we call \\(\\bar{X}\\) (pronounced X-bar), instead of \\(\\mu\\) \\(\\mu_0\\) is the mean to compare against. For us this is the mean of the original paper which we observed to be 590 ms. So in other words we are testing the null hypothesis that \\(H_0: \\bar{X} =\\) 590. As such the formula for our one-sample \\(t\\)-test becomes: \\[t = \\frac{\\bar{X} - \\mu_0}{s\\ / \\sqrt(n)}\\] Now we just need to fill in the numbers. 6.2.6 Task 4: Calculating the Mean and Standard Deviation Calculate the mean and standard deviation of valid_rt for our 22 participants (i.e., for all participant data at the top of this lab). Store the mean in ns_data_mean and store the standard deviation in ns_data_sd. Make sure to store them both as single values! Helpful Hint In the below code, replace NULL with the code that would find the mean, m, of ns_data. ns_data_mean &lt;- summarise(NULL) %&gt;% pull(NULL) Replace NULL with the code that would find the standard deviation, sd, of ns_data. ns_data_sd &lt;- summarise(NULL) %&gt;% pull(NULL) 6.2.7 Task 5: Calculating the Observed Test Statistic From Task 4, you found out that \\(\\bar{X}\\), the sample mean, was 625.464 ms, and \\(s\\), the sample standard deviation, was 94.307 ms. Now, keeping in mind that \\(n\\) is the number of observations/participants in the sample, and \\(\\mu_0\\) is the mean from Woods et al (2009): Use the one-sample t-test formula above to compute your observed test statistic. Store the answer in t_obs . E.g. t_obs &lt;- (x - y)/(s/sqrt(n)) Answering this question will help you in this task as youll also need these numbers to substitute into the formula: The mean from Woods et al (2009) was 595 590 580 585, and the number of participants in our sample is: (type in numbers) . Remember the solutions at the end of the chapter if you are stuck. To check that you are correct without looking at the solutions though - the observed \\(t\\)-value in t_obs, to two decimal places, is 1.66 1.76 1.86 1.96 Helpful Hint Remember BODMAS and/or PEDMAS when given more than one operation to calculate. (i.e. Brackets/Parenthesis, Orders/Exponents, Division, Multiplication, Addition, Subtraction) t_obs &lt;- (sample mean - woods mean) / (sample standard deviation / square root of n) 6.2.8 Task 6: Comparing the Observed Test Statistic to the t-distribution using pt() Now you need to compare t_obs to the t-distribution to determine how likely the observation (i.e. your test statistic) is under the null hypothesis of no difference. To do this you need to use the pt() function. Use the pt() function to get the \\(p\\)-value for a two-tailed test with \\(\\alpha\\) level set to .05. The test has \\(n - 1\\) degrees of freedom, where \\(n\\) is the number of observations contributing to the sample mean \\(\\bar{X}\\). Store the \\(p\\) value in the variable pval. Do you reject the null? Hint: The pt() function works similar to pbinom() and pnorm(). Hint: Because we want the p-value for a two-tailed test, multiply pt() by two. Helpful Hint Remember to get help you can enter ?pt in the console. The pt() function works similar to pbinom() and pnorm(): pval &lt;- pt(test statistic, df, lower.tail = FALSE) * 2 Use the absolute value of the test statistic; i.e. ignore minus signs. Remember, df is equal to n-1. Use lower.tail = FALSE because we are wanting to know the probability of obtaining a value higher than the one we got. Reject the null at the field standard of p &lt; .05 6.2.9 Task 7: Comparing the Observed Test Statistic to the t-distribution using t.test() Now that you have done this by hand, try using the t.test() function to get the same result. Take a moment to read the documentation for this function by typing ?t.test in the console window. No need to store the t-test output in a dataframe but do check that the p-value matches the pval in Task 6. The structure of the t.test() function is t.test(column_of_data, mu = mean_to_compare_against) Helpful Hint The function requires a vector, not a table, as the first argument. You can use the pull() function to pull out the valid_rt column from the tibble ns_data with pull(ns_data, valid_rt). You also need to include mu in the t.test(), where mu is equal to the mean you are comparing to. Quickfire Questions To make sure you are understanding the output of the t-test, try to answer the following questions. To three decimal places, type in the p value for the t-test in Task 7 As such this one sample t-test is significant not significant The outcome of the binomial test and the one sample t-test produce the same a different answer 6.2.10 Task 8: Drawing Conclusions about the new data Given these results, what do you conclude about how similar these 22 participants are to the original participants in Woods et al (2009) and whether or not you have managed to recruit sleepers similar to that study? Think about which test used more of the available information? Also, how reliable is the finding if the two tests give different answers? We have given some of our thoughts at the end of the chapter. Job Done - Activity Complete! Thats all! There is quite a bit in this lab in terms of theory of Null Hypothesis Significance Testing (NHST) so you might want to go back and add any informative points to your Portfolio. Post any questions on the forums 6.3 InClass Activity The PreClass activity looked at the situation where you had gathered one sample of data (from one group of participants) and you want to compare that one group to a known value, e.g. a standard value. An extension of this design is where you gather data from two samples. Two-sample designs are very common in Psychology as often we want to know whether there is a difference between groups on a particular variable. We looked at a similar idead in Chapter 5 where we did this comparison through simulation. Today we will look at through the \\(t\\)-test analysis introduced in Chapter 6 Comparing the Means of Two Samples First thing to not is that there are different types of two-sample designs depending on whether or not the two groups are independent (e.g. different participants on different conditions) or not (e.g. same participants on different conditions). In todays exercise we will focus on independent samples, which typically means that the observations in the two groups are unrelated - usually meaning different people. In Chapter 7 you will examine cases where the observations in the two groups are from pairs (paired samples) - most often the same people but could also be a matched-pairs design. Portfolio Point - All the different names for one thing! Now that we are progressing through this book, we will start to reduce the pointers for things that you should make a note of. By now you should have a really good idea yourself about what you need to remember. That said, one of the really confusing things about research design is that there are many names for the same type of design. This is definitely something you should be writing out in your own words, to remember it better, if it is something you struggle with. Here is a very brief summary: independent and between-subjects design typically mean the same thing - different participants in different conditions. within-subjects, dependent samples, paired samples, and repeated measures tend to mean the same participants in all conditions matched-pairs design means different people in different conditions but you have matched participants across the conditions so that they are effectively the same person (e.g. age, IQ, Social Economic Status, etc). These designs are analysed as though they were a within-subjects design. mixed design is when there is a combination of within-subjects and between-subjects designs in the one experiment. For example, say you are looking at attractiveness and dominance of male and female faces. Everyone might see both male and female faces (within) but half of the participants do ratings of attractiveness and half do ratings of trustworthiness (between). The paper we are looking at in this InClass activity technically uses a mixed design at times but we will use the between-subjects element to show you how to run independent t-tests. Spend some time when reading articles to really figure out the design they are using. Background For this lab we will be revisiting the data from Schroeder and Epley (2015), which you first encountered as part of the homework for Chapter 5. You can take a look at the Psychological Science article here: Schroeder, J. and Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidates appeal. Psychological Science, 26, 277891. The abstract from this article explains more about the different experiments conducted (we will be specifically looking at the dataset from Experiment 4, courtesy of the Open Stats Lab): A persons mental capacities, such as intellect, cannot be observed directly and so are instead inferred from indirect cues. We predicted that a persons intellect would be conveyed most strongly through a cue closely tied to actual thinking: his or her voice. Hypothetical employers (Experiments 1-3b) and professional recruiters (Experiment 4) watched, listened to, or read job candidates pitches about why they should be hired. These evaluators (the employers) rated a candidate as more competent, thoughtful, and intelligent when they heard a pitch rather than read it and, as a result, had a more favorable impression of the candidate and were more interested in hiring the candidate. Adding voice to written pitches, by having trained actors (Experiment 3a) or untrained adults (Experiment 3b) read them, produced the same results. Adding visual cues to audio pitches did not alter evaluations of the candidates. For conveying ones intellect, it is important that ones voice, quite literally, be heard. To recap on Experiment 4, 39 professional recruiters from Fortune 500 companies evaluated job pitches of M.B.A. candidates (Masters in Business Administration) from the University of Chicago Booth School of Business. The methods and results appear on pages 887889 of the article if you want to look at them specifically for more details. The original data, in wide format, can be found at the Open Stats Lab website for later self-directed learning. Today however, we will be working with a modfied version in tidy format which can be downloaded from here. I you are unsure about tidy format, refer back to the inclass activities of Chapter 2. Todays Goal! The overall goal today is to learn about running a \\(t\\)-test on between-subjects data, as well as learning about analysing actual data along the way. As such, our task today is to reproduce a figure and the results from the article (p. 887-888). The two packages you will need are tidyverse, which we have used a lot, and broom, which is new to you but will become your friend. One of the main functions we use in broom is broom::tidy() - this is an incredibly useful function that converts the output of an inferential test in R from a combination of text and lists, that are really hard to work with - technically called objects - into a tibble that you are very familiar with and that you can then use much more easily. Might that be worth making a note of? We will show you how to use this function today and then ask you to use it over the coming chapters If you are using the Boyd Orr labs, broom is already installed and just needs called to the library(). If you are using your own machine, you will need to install it one time to begin with if you have never installed it before. 6.3.1 Task 1: Evaluators Open a new script or R Markdown file and use code to call broom and tidyverse into your library. Note: Order is important when calling multiple libraries - if two libraries have a function named the same thing, R will use the function from the library loaded in last. We recommend calling libraries in an order that tidyverse is called last as the functions in that library are used most often. The file called evaluators.csv contains the demographics of the 39 raters. After downloading and unzipping the data, and of course setting the working directory, load in the information from this file and store it in a tibble called evaluators. Now, use code to: calculate the overall mean and standard deviation of the age of the evaluators. count how many male and how many female evaluators were in the study. Note: Probably easier doing this task in seperate lines of code. Remeber the solutions are at the end of the Chapter. Note: there are NAs in the data so you will need to include a call to na.rm = TRUE. Helpful Hint Remember to load the libraries you need! Also make sure youve downloaded and saved the data in the folder youre working from. You can use summarise() and count() or a pipeline with group_by() to complete this task. When analysing the number of male and female evaluators it isnt initially clear that 1 represents males and 2 represents females. We can use recode() to convert the numeric names to indicate something more meaningful. Have a look at?recode to see if you can work out how to use it. Itll help to use mutate() to create a new variable to recode() the numeric names for evaluators. This website is also incredibly useful and one to save for anytime you need to use recode(): https://debruine.github.io/posts/recode/ For your own analysis and future reproducible analyses, its a good idea to make these representations clearer to others. Quickfire Questions Fill in the below answers to check that your calculations are correct. Remember that the solutions are at the end of the chapter: What was the mean age of the evaluators in the study? Type in your answer to one decimal place: What was the standard deviation of the age of the evaluators in the study? Type in your answer to two decimal places: How many participants were noted as being female: How many participants were noted as being male: Group Discussion Point The paper claims that the mean age of the evaluators was 30.85 years (SD = 6.24) and that there were 9 male and 30 female evaluators. Do you agree? Why might there be differences? Explain This - Why is there a discrepancy? This paper claimed there were 9 males, however looking at your results you can see only 4 males, with 5 NA entries making up the rest of the participant count. It looks like the NA and male entries have been combined! That information might not be clear to a person re-analysing the data. This is why its important to have reproducible data analyses for others to examine. Having another pair of eyes examining your data can be very beneficial in spotting any discrepancies - this allows for critical evaluation of analyses and results and improves the quality of research being published. All the more reason to emphasize the importance of conducting replication studies! #ReproducibleScience 6.3.2 Task 2: Ratings We are now going to calculate an overall intellect rating given by each evaluator. To break that down a bit, we are going to calculate how intellectual the evaluators (the raters) thought candidates were overall, depending on whether or not the evaluators read or listened to the candidates resume pitches. This is calculated by averaging the ratings of competent, thoughtful and intelligent for each evaluator held within ratings.csv. Note: we are not looking at ratings to individual candidates; we are looking at overall ratings for each evaluator. This is a bit confusing but makes sense if you stop to think about it a little. You can think about it in terms of do raters rate differently depending on whether they read or listen to a resume pitch. We will then combine the overall intellect rating with the overall impression ratings and overall hire ratings for each evaluator, all ready found in ratings.csv. In the end we will have a new tibble - lets call it ratings2 - which has the below structure: eval_id shows the evaluator ID. Each evaluator has a different ID. So all the 1s are the same evaluator. Category shows the scale that they were rating on - intellect, hire, impression Rating shows the overall rating given by that evaluator on a given scale. condition shows whether that evaluater listened to (e.g. evaluators 1, 2 and 3), or read (e.g. evaluator 4) the resume. eval_id Category Rating condition 1 hire 6.000 listened 1 impression 7.000 listened 1 intellect 6.000 listened 2 hire 4.000 listened 2 impression 4.667 listened 2 intellect 5.667 listened 3 hire 5.000 listened 3 impression 8.333 listened 3 intellect 6.000 listened 4 hire 4.000 read 4 impression 4.667 read 4 intellect 3.333 read The following steps describe how to create the above tibble but you might want to have a bash yourself without reading them first. The trick when doing data analysis and data wrangling is to first think about what you want to achieve - the end goal - and then what function do I need to use. You know what you want to end up with - the above table - now how do you get there? Steps 1-3 calculate the new intellect rating. Steps 4 and 5 combine this rating to all other information. Load the data found in ratings.csv into a tibble called ratings. filter() only the relevant variables (thoughtful, competent, intelligent) into a new tibble (call it what you like - in the solutions we use iratings), and calculate a mean Rating for each evaluator. Add on a new column called Category where every entry is the word intellect. This tells us that every number in this tibble is an intellect rating. Now create a new tibble called ratings2 and filter into it just the impression and hire ratings from the original ratings tibble. Next, bind this tibble with the tibble you created in step 3 to bring together the intellect, impression, and hire ratings, in ratings2. Join ratings2 with the evaluator tibble that we created in Task 1. Keep only the necessary columns as shown above and arrange by Evaluator and Category. Dont forget to use the hints below or the solution at the end of the Chapter if you are stuck. One thing to think about though is that the above steps dont help, again just go back to what you want to achieve and ask yourself how would you do that, and think backwards. Helpful Hint Make sure youve downloaded and saved the data into the folder youre working from. filter(Category %in% c()) might work and then use group_by() and summarize() to calculate a mean Rating for each evaluator. Use mutate() to create a new column. bind_rows() from Chapter 2 will help you to combine these variables from two separate tibbles. Use inner_join() with the common column in both tibbles. select() and arrange() will help you here too. 6.3.3 Task 3: Creating a Figure To recap, we now have ratings2 which contains an overall Rating score for each evaluator on the three Category (within: hire, impression, intellect) depending on which condition that evaluator was in (between: listened or read). Great! Now we have all the information we need to replicate Figure 7 in the article (page 888), shown here: Figure 6.1: Figure 7 from Schroeder and Epley (2015) which you should try to replicate. Replace the NULLs below to create a very basic version of this figure. You did something like this for the Chapter 5 assignment and again in the Chapter 3 Visualisation tasks. group_means &lt;- group_by(ratings2, NULL, NULL) %&gt;% summarise(Rating = mean(Rating)) ggplot(group_means, aes(NULL, NULL, fill = NULL)) + geom_col(position = &quot;dodge&quot;) Group Discussion Point Improve Your Figure: Discuss with others how you could improve this plot. What other geom_() options could you try? Are barcharts that informative or would something else be better? How would you add or change the labels of your plot? Could you change the colours in your figure? Next, have a look at the possible solution below to see a modern way of presenting this information. There are some new functions in this solution that you should play about with to understand what they do. Remember it is a layering system, so remove lines and see what happens. Note how in the solution the Figure shows the raw data points as well as the means in each condition; this gives a better impression of the true data as just showing the means can be misleading. You can continue your further exploration of visualisations by reading this paper later when you have a chance: Weissberger et al., 2015, Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm Solution and Possible Alternative Filling in the code as below will create a basic figure as shown: group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(Rating = mean(Rating)) ## `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. ggplot(group_means, aes(Category, Rating, fill = condition)) + geom_col(position = &quot;dodge&quot;) Figure 6.2: A basic solution to Figure 7 Or alternatively, for a more modern presentation of the data: group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(Rating = mean(Rating)) ## `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. ggplot(ratings2, aes(condition, Rating, color = condition)) + geom_jitter(alpha = .4) + geom_violin(aes(fill = condition), alpha = .1) + facet_wrap(~Category) + geom_point(data = group_means, size = 2) + labs(x = &quot;Category&quot;, y = &quot;Recruiters&#39; Evaluation of Candidates&quot;) + coord_cartesian(ylim = c(0, 10), expand = FALSE) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + theme_bw() Figure 6.3: A possible alternative to Figure 7 6.3.4 Task 4: t-tests Brilliant! So far we have checked the descriptives and the visualisations, and the last thing now is to check the inferential tests; the t-tests. You should still have ratings2 stored from Task 2. From this tibble, lets reproduce the t-test results from the article and at the same time show you how to run a t-test. You can refer back to the lectures to understand the maths a between-subjects t-test but essentially it is a measure between the difference in means over the variance about those means. Here is a paragraph from the paper describing the results (p. 887): The pattern of evaluations by professional recruiters replicated the pattern observed in Experiments 1 through 3b (see Fig. 7). In particular, the recruiters believed that the job candidates had greater intellectwere more competent, thoughtful, and intelligentwhen they listened to pitches (M = 5.63, SD = 1.61) than when they read pitches (M = 3.65, SD = 1.91), t(37) = 3.53, p &lt; .01, 95% CI of the difference = [0.85, 3.13], d = 1.16. The recruiters also formed more positive impressions of the candidatesrated them as more likeable and had a more positive and less negative impression of themwhen they listened to pitches (M = 5.97, SD = 1.92) than when they read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p &lt; .01, 95% CI of the difference = [0.55, 3.24], d = 0.94. Finally, they also reported being more likely to hire the candidates when they listened to pitches (M = 4.71, SD = 2.26) than when they read the same pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p &lt; .01, 95% CI of the difference = [0.41, 3.24], d = 0.86. We are going to run the t-tests for Intellect, Hire and Impression; each time comparing evaluators overall ratings for the listened group versus overall ratings for the read group to see if there was a significant difference between the two conditions: i.e. did the evaluators who listened to pitches give a significant higher or lower rating than evaluators that read pitches. In terms of hypotheses, we could phrase the null hypothesis for this tests as there is no significant difference between overall ratings on the {insert trait} scale between evaluators who listened to resume pitches and evaluatoris who read the resume pitches (\\(H_0: \\mu_1 = \\mu2\\)). Alternatively, we could state it as there will be a significant difference between overall ratings on the {insert trait} scale between evaluators who listened to resume pitches and evaluatoris who read the resume pitches (\\(H_1: \\mu_1 \\ne \\mu2\\)). Portfolio Point - A vs B or B vs A in a t-test? Now would be a good time to add to your notes about what is the difference between a positive and negative value as the outcome to a t-test? Remember? It just tells you which group had the bigger mean - the absolute value will be the same. Most commonly, t-tests are reported as a positive value. To clarify, we are going to run three between-subjects t-tests in total; one for intellect ratings; one for hire ratings; one for impression ratings. We will show you how to run the t-test on intellect ratings and then ask you to do the remaining two t-tests yourself. To run this analysis on the intellect ratings you will need the function t.test() and you will use broom::tidy() to pull out the results from each t-test into a tibble Below, we show you how to create the group means and then run the t-test for intellect. Run these lines and have a look at what they do. First we calculate the group means: group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(m = mean(Rating), sd = sd(Rating)) ## `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. And we can call them and look at them by typing: group_means condition Category m sd listened hire 4.714286 2.261479 listened impression 5.968254 1.917477 listened intellect 5.634921 1.608674 read hire 2.888889 2.054805 read impression 4.074074 2.233306 read intellect 3.648148 1.911343 Now to just look at intellect ratings we need to filter them into a new tibble: intellect &lt;- filter(ratings2, Category == &quot;intellect&quot;) And then we run the actual t-test and tidy it into a table. t.test() requires two vectors as input pull() will pull out a single column from a tibble, e.g. Rating from intellect tidy() takes information from a test and turns it into a tibble. Try running the t.test with and without piping into tidy() to see what it does differently. intellect_t &lt;- t.test(intellect %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), intellect %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() Now lets look at the intellect_ttibble we have created (assuming you piped into tidy()): Table 6.2: The t-test output of those in the intellect condition. estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative 1.987 5.635 3.648 3.526 0.001 37 0.845 3.128 Two Sample t-test two.sided From the resultant tibble, intellect_t, you can see that you ran a Two Sample t-test (meaning between-subjects) with a two-tailed hypothesis test (two.sided). The mean for the listened condition, estimate1, was 5.635, whilst the mean for the read condition, estimate2 was 3.648 - compare these to the means in group_means as a sanity check. So an overall the was a difference between the two means of 1.987. The degrees of freedom for the test, parameter, was 37. The observed t-value, statistic, was 3.526, and it was significant as the p-value, p.value, was p = 0.0011, which is lower than the field standard Type 1 error rate of \\(\\alpha = .05\\). As you will know from your lectures, a t-test is presented as t(df) = t-value, p = p-value. As such, this t-test would be written up as: t(37) = 3.526, p = 0.001. Thinking about interpretation of this finding, as the effect was significant, we can reject the null hypothesis that there is no significant difference between mean ratings of those who listened to resumes and those who read the resumes, for intellect ratings. We can go further than that and say that the overall intellect ratings for those that listened to the resume was significantly higher (mean diff = 1.987) than those who read the resumes, t(37) = 3.526, p = 0.001, and as such we accept the alternative hypothesis. This would suggest that hearing people speak leads evaluators to rate the candidates as more intellectual than when you merely read the words they have written. Now Try: Running the remaining t-tests for hire and for impression. Store them in tibbles called hire_t and impress_t respectively. Bind the rows of intellect_t, hire_t and impress_t to create a table of the three t-tests called results. It should look like this: Table 6.3: Output of all three t-tests Category estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative intellect 1.987 5.635 3.648 3.526 0.001 37 0.845 3.128 Two Sample t-test two.sided hire 1.825 4.714 2.889 2.620 0.013 37 0.414 3.237 Two Sample t-test two.sided impression 1.894 5.968 4.074 2.851 0.007 37 0.548 3.240 Two Sample t-test two.sided Quickfire Questions Check your results for hire. Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places): Mean estimate1 (listened condition) = Mean estimate2 (read condition) = t() = , p = Looking at this result, True or False, this result is significant at \\(\\alpha = .05\\)? TRUE FALSE Check your results for impression. Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places): Meanestimate1 (listened condition) = Mean estimate2 (read condition) = t() = , p = Looking at this result, True or False, this result is significant at \\(\\alpha = .05\\)? TRUE FALSE Helpful Hint Your t-test answers should have the following structure: t(degrees of freedom) = t-value, p = p-value, where: degrees of freedom = parameter, t-value = statistic, and p-value = p.value. Remember that if a result has a p-value lower (i.e. smaller) than or equal to the alpha level then it is said to be significant. So to recap, we looked at the data from Schroeder and Epley (2015), both the descriptives and inferentials, we plotted a figure, and we confirmed that, as in the paper, there are significant differences in each of the three rating categories (hire, impression and intellect), with the listened condition receiving a higher rating than the read condition on each rating. All in, our interpretation would be that people rate you hire when they hear you speak your resume as opposed to them just reading your resume! Job Done - Activity Complete! Well done for completing this inclass activity on independent samples t-tests! And now, combined with the information in the preclass activity, you are able to run a t-test on one sample data, comparing it to a standard/criterion values, and on two samples in a between-subjects design. Are there any useful points in this activity about t-tests or plots that you think could be useful to include in your portfolio? Make sure to include them now! For instance, we havent really looked at the assumptions of a between-subjects t-test, just the analysis. You will have covered the assumptions in your lecture series so why not combine that information with the information you learnt here! Also, if you have more time, you might want to visit this website that will give you a better understanding of the relationship between the \\(t\\) distribution and the normal distribution: gallery.shinyapps.io/tdist You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the forums. 6.4 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. Lab 6: Independent samples t-test Assignment In order to complete this assignment, you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester1_Lab6.Rmd. This can be downloaded within a zip file from the link below. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Background For this assignment we will be using real data from the following paper: Nave, G., Nadler, A., Zava, D., and Camerer, C. (2017). Single-dose testosterone administration impairs cognitive reflection in men. Psychological Science, 28, 13981407. The full data for these exercises can be downloaded from the Open Science Framework repository but for this assignment we will just use the .csv file in the zipped folder: CRT_Data.csv. You may also want to read the paper, at least in part, to help fully understand this analysis if at times you are unsure. Here is the articles abstract: In nonhumans, the sex steroid testosterone regulates reproductive behaviors such as fighting between males and mating. In humans, correlational studies have linked testosterone with aggression and disorders associated with poor impulse control, but the neuropsychological processes at work are poorly understood. Building on a dual-process framework, we propose a mechanism underlying testosterones behavioral effects in humans: reduction in cognitive reflection. In the largest study of behavioral effects of testosterone administration to date, 243 men received either testosterone or placebo and took the Cognitive Reflection Test (CRT), which estimates the capacity to override incorrect intuitive judgments with deliberate correct responses. Testosterone administration reduced CRT scores. The effect remained after we controlled for age, mood, math skills, whether participants believed they had received the placebo or testosterone, and the effects of 14 additional hormones, and it held for each of the CRT questions in isolation. Our findings suggest a mechanism underlying testosterones diverse effects on humans judgments and decision making and provide novel, clear, and testable predictions. The critical findings are presented on p. 1403 of the paper under the heading The influence of testosterone on CRT performance. Your task today is to attempt to try and reproduce some of the main results from the paper. NOTE: Being unable to get the exact same results as the authors doesnt necessarily mean you are wrong! The authors might be wrong, or might have left out important details. Present what you find. Before starting lets check: The .csv file is saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Semester1_Lab6.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 6.4.1 Task 1A: Libraries In todays assignment you will need both the tidyverse and broom packages. Enter code into the t1A code chunk below to load in both of these libraries. ## load in the tidyverse and broom packages 6.4.2 Task 1B: Loading in the data Use read_csv() to replace the NULL in the t1B code chunk below to load in the data stored in the datafile CRT_Data.csv. Store the data in the variable crt. Do not change the filename of the datafile. crt &lt;- NULL 6.4.3 Task 2: Selecting only relevant columns Have a look at crt. There are three variables in crt that you will need to find and extract in order to perform the t-test: the subject ID number (hint: each participant has a unique number); the independent variable (hint: each participant has the possibility of being in one of two treatments coded as 1 or 0); and the dependent variable (hint: the test specifically looks at which answers people get correct). Identify those three variables. It might help to look at the first few sentences under the heading The influence of testosterone on CRT performance and Figure 2a in the paper for further guidance on the correct variables. Having identified the important three columns, replace the NULL in the t2 code chunk below to select out only those three columns from crt and store them in the tibble crt2. Check your work: If correct, crt2 should be a tibble with 3 columns and 243 rows. crt2 &lt;- NULL NOTE: For the remainder, of this assignment you should use crt2 as the main source tibble and not crt. 6.4.4 Task 3: Verify the number of subjects in each group The Participants section of the article contains the following statement: 243 men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = 125) or placebo (n = 118). In the t3 code block below, replace the NULLs with lines of code to calculate: The number of men in each Treatment. This should be a tibble called cond_counts containing a column called Treatment showing the two groups and a column called n which shows the number of men in each group. The total number of men in the sample. This should be a single value, not a tibble, and should be stored in n_men. You know the answer to both of these tasks already. Make sure that your code gives the correct answer! cond_counts &lt;- NULL n_men &lt;- NULL Now replace the strings in the statements below, using inline R code, so that it reproduces the sentence from the paper exactly as it is shown above. In other words, in the statement below, anywhere it says \"(your code here)\", replace that string (including the quotes), with inline R code. To clarify, when looking at the .Rmd file you should see R code, but when looking at the knitted file, you should see values. Look back at Chapter 1 if you are unsure of how to use inline code. Hint: One solution is to do something with cond_counts similar to what we did with filter() and pull() in the in-class exercises of this Chapter. \"(your code here)\" men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = \"(your code here)\") or placebo (n = \"(your code here)\"). 6.4.5 Task 4: Reproduce Figure 2a Here is Figure 2A from the original paper: Figure 6.4: Figure 2A from Nave, Nadler, Zava, and Camerer (2017) which you should replicate Write code in the t4 code chunk to reproduce a version of Figure 2a - shown above. Before you create the plot, replace the NULL to make a tibble called crt_means with the mean and standard deviation of the number of CorrectAnswers for each group. Use crt_means as the source data for the plot. Hint: you will need to check out recode() to get the labels of treatments right. Again this webpage is highly recommended: https://debruine.github.io/posts/recode/ Dont worry about including the error bars (unless you want to) or the line indicating significance in the plot. Do however make sure to pay attention to the labels of treatments and of the y-axis scale and label. Reposition the x-axis label to below the Figure. You can use colour if you like. crt_means &lt;- NULL ## TODO: add lines of code using ggplot 6.4.6 Task 5: Interpreting your Figure Always good to do a slight recap at this point to make sure you are following the analysis. Replace the NULL in the t5 code chunk below with the number of the statement that best describes the data you have calculated and plotted thus far. Store this single value in answer_t5: The Testosterone group (M = 2.10, SD = 1.02) would appear to have fewer correct answers on average than the Placebo group (M = 1.66, SD = 1.18) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. The Testosterone group (M = 1.66, SD = 1.18) would appear to have more correct answers on average than the Placebo group (M = 2.10, SD = 1.02) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. The Testosterone group (M = 1.66, SD = 1.18) would appear to have fewer correct answers on average than the Placebo group (M = 2.10, SD = 1.02) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. The Testosterone group (M = 2.10, SD = 1.02) would appear to have more correct answers on average than the Placebo group (M = 1.66, SD = 1.18) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. answer_t5 &lt;- NULL 6.4.7 Task 6: t-test Now that we have calculated the descriptives in our study we need to run the inferentials. In the t6 code chunk below, replace the NULL with a line of code to run the t-test taking care to make sure that the output table has the Placebo mean under Estimate1 (group 0) and Testosterone mean under Estimate2 (group 1). Assume variance is equal and use broom::tidy() to sweep and store the results into a tibble called t_table. t_table &lt;- NULL 6.4.8 Task 7: Reporting results In the t7A code chunk below, replace the NULL with a line of code to pull out the df from t_table. This must be a single value stored in t_df. t_df &lt;- NULL In the t7B code chunk below, replace the NULL with a line of code to pull out the t-value from t_table. Round it to three decimal places. This must be a single value stored in t_value. t_value &lt;- NULL In the t7C code chunk below, replace the NULL with a line of code to pull out the p-value from t_table. Round it to three decimal places. This must be a single value stored in p_value. p_value &lt;- NULL In the t7D code chunk below, replace the NULL with a line of code to calculate the absolute difference between the mean number of correct answers for the Testosterone group and the Placebo group. Round it to three decimal places. This must be a single value stored in t_diff. t_diff &lt;- NULL If you have completed t7A to t7D accurately, then when knitted, one of these statements below will produce an accurate and coherent summary of the results. In the t7E code chunk below, replace the NULL with the number of the statement below that best summarises the data in this study. Store this single value in answer_t7e The testosterone group performed significantly better ( fewer correct answers) than the placebo group, t() = , p = . The testosterone group performed significantly worse ( fewer correct answers) than the placebo group, t() = , p = . The testosterone group performed significantly better ( more correct answers) than the placebo group, t() = , p = . The testosterone group performed significantly worse ( fewer correct answers) than the placebo group, t() = , p = . answer_t7e &lt;- NULL Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solutions at the end of this Chapter. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers, it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the available forums or ask a member of staff. On to the next chapter! 6.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 6.5.1 PreClass Activities 6.5.1.1 PreClass Task 1 ns_data &lt;- tibble(participant = 1:22, valid_rt = c(631.2,800.8,595.4,502.6,604.5, 516.9,658.0,502.0,496.7,600.3, 714.6,623.7,634.5,724.9,815.7, 456.9,703.4,647.5,657.9,613.2, 585.4,674.1)) Return to Task 6.5.1.2 PreClass Task 2 woods_mean &lt;- 590 n_participants &lt;- ns_data %&gt;% filter(valid_rt &gt; woods_mean) %&gt;% nrow() Giving an n_participants value of 16 Return to Task 6.5.1.3 PreClass Task 3 You can use the density function: sum(dbinom(n_participants:nrow(ns_data), nrow(ns_data), .5)) ## [1] 0.0262394 Or, the cumulative probability function: pbinom(n_participants - 1L, nrow(ns_data), .5, lower.tail = FALSE) ## [1] 0.0262394 Or, If you were to plug in the numbers directly into the code: sum(dbinom(16:22,22, .5)) ## [1] 0.0262394 Or, finally, remembering we need to specify a value lower than our minimum participant number as lower.tail = FALSE. pbinom(15, 22, .5, lower.tail = FALSE) ## [1] 0.0262394 It is better practice to use the first two solutions, which pull the values straight from ns_data, as you run the risk of entering an error into your code if you plug in the values manually. Return to Task 6.5.1.4 PreClass Task 4 For ns_data_mean use summarise() to calculate the mean and then pull() the value. For ns_data_sd use summarise() to calculate the sd and then pull() the value. # the mean ns_data_mean &lt;- ns_data %&gt;% summarise(m = mean(valid_rt)) %&gt;% pull(m) # the sd ns_data_sd &lt;- ns_data %&gt;% summarise(sd = sd(valid_rt)) %&gt;% pull(sd) NOTE: You could print them out on the screen if you wanted to \\n is the end of line symbol so that they print on different lines cat(&quot;The mean number of hours was&quot;, ns_data_mean, &quot;\\n&quot;) cat(&quot;The standard deviation was&quot;, ns_data_sd, &quot;\\n&quot;) ## The mean number of hours was 625.4636 ## The standard deviation was 94.30693 Return to Task 6.5.1.5 PreClass Task 5 t_obs &lt;- (ns_data_mean - woods_mean) / (ns_data_sd / sqrt(nrow(ns_data))) Giving a t_obs value of 1.7638067 Return to Task 6.5.1.6 PreClass Task 6 If using values straight from ns_data, and multiplying by 2 for a two-tailed test, you would do the following: pval &lt;- pt(abs(t_obs), nrow(ns_data) - 1L, lower.tail = FALSE) * 2L Giving a pval of 0.0923092 But you can also get the same answer by plugging the values in yourself - though this method runs the risk of error and you are better off using the first calculation as those values come straight from ns_data. : pval2 &lt;- pt(t_obs, 21, lower.tail = FALSE) * 2 Giving a pval of 0.0923092 Return to Task 6.5.1.7 PreClass Task 7 The t-test would be run as follows, with the output shown below: t.test(pull(ns_data, valid_rt), mu = woods_mean) ## ## One Sample t-test ## ## data: pull(ns_data, valid_rt) ## t = 1.7638, df = 21, p-value = 0.09231 ## alternative hypothesis: true mean is not equal to 590 ## 95 percent confidence interval: ## 583.6503 667.2770 ## sample estimates: ## mean of x ## 625.4636 Return to Task 6.5.1.8 PreClass Task 8 According to the one-sample t-test these participants are responding in a similar manner as the participants from the original study, and as such, we may be inclined to assume that the recruitment process of our pilot experiment is working well. However, according to the binomial test the participants are responding differently from the original sample. So which test result should you take as the finding? Keep in mind that the binomial test is very rough and categorises participants into yes or no. The one-sample t-test uses much more of the available data and to some degree would give a more accurate answer. However, the fact that two tests give really different answers may give you reason to question whether or not the results are stable and potentially you should look to gather a larger sample to get a more accurate representation of the population. Return to Task 6.5.2 InClass Activities 6.5.2.1 InClass Task 1 library(&quot;tidyverse&quot;) library(&quot;broom&quot;) # you&#39;ll need broom::tidy() later evaluators &lt;- read_csv(&quot;evaluators.csv&quot;) evaluators %&gt;% summarize(mean_age = mean(age, na.rm = TRUE)) evaluators %&gt;% count(sex) # If using `recode()`: evaluators %&gt;% count(sex) %&gt;% mutate(sex_names = recode(sex, &quot;1&quot; = &quot;male&quot;, &quot;2&quot; = &quot;female&quot;)) The mean age of the evaluators was 30.9 The standard deviatoin of the age of the evaluators was 6.24 There were 4 males and e_count %&gt;% filter(sex_names == \"female\") %&gt;% pull(n) females, with 5 people not stating a sex. Return to Task 6.5.2.2 InClass Task 2 load in the data ratings &lt;- read_csv(&quot;ratings.csv&quot;) First pull out the ratings associated with intellect iratings &lt;- ratings %&gt;% filter(Category %in% c(&quot;competent&quot;, &quot;thoughtful&quot;, &quot;intelligent&quot;)) Next calculate means for each evaluator imeans &lt;- iratings %&gt;% group_by(eval_id) %&gt;% summarise(Rating = mean(Rating)) Mutate on the Category variable. This way we can combine with impression and hire into a single table which will be very useful! imeans2 &lt;- imeans %&gt;% mutate(Category = &quot;intellect&quot;) And then combine all the information in to one single tibble. ratings2 &lt;- ratings %&gt;% filter(Category %in% c(&quot;impression&quot;, &quot;hire&quot;)) %&gt;% bind_rows(imeans2) %&gt;% inner_join(evaluators, &quot;eval_id&quot;) %&gt;% select(-age, -sex) %&gt;% arrange(eval_id, Category) Return to Task 6.5.2.3 InClass Task 4 First we calculate the group means: group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(m = mean(Rating), sd = sd(Rating)) ## `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. And we can call them and look at them by typing: group_means Now to just look at intellect ratings we need to filter them into a new tibble: intellect &lt;- filter(ratings2, Category == &quot;intellect&quot;) And then we run the actual t-test and tidy it into a table. t.test() requires two vectors as input pull() will pull out a single column from a tibble, e.g. Rating from intellect tidy() takes information from a test and turns it into a table. Try running the t.test with and without piping into tidy() to see what it does differently. intellect_t &lt;- t.test(intellect %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), intellect %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() Now we repeat for HIRE and IMPRESSION hire &lt;- filter(ratings2, Category == &quot;hire&quot;) hire_t &lt;- t.test(hire %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), hire %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() And for Impression impress &lt;- filter(ratings2, Category == &quot;impression&quot;) impress_t &lt;- t.test(impress %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), impress %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() Before combining all into one table showing all three t-tests results &lt;- bind_rows(&quot;hire&quot; = hire_t, &quot;impression&quot; = impress_t, &quot;intellect&quot; = intellect_t, .id = &quot;id&quot;) results Return to Task 6.5.2.4 Going Further with your coding An alternative solution to Task 4: There is actually a quicker way to do this analysis of three t-tests which you can have a look at below if you have the time. This uses very advanced coding with some functions we wont really cover in this book. Do not worry if you cant quite follow it though; the main thing is to understand what we covered in the main chapter activities - the outcome is the same. ratings2 %&gt;% group_by(Category) %&gt;% nest() %&gt;% mutate(ttest = map(data, function(x) { t.test(Rating ~ condition, x, var.equal = TRUE) %&gt;% tidy() })) %&gt;% select(Category, ttest) %&gt;% unnest(cols = c(ttest)) Category estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative hire 1.825397 4.714286 2.888889 2.620100 0.0126745 37 0.4137694 3.237024 Two Sample t-test two.sided impression 1.894180 5.968254 4.074074 2.850766 0.0070911 37 0.5478846 3.240475 Two Sample t-test two.sided intellect 1.986773 5.634921 3.648148 3.525933 0.0011444 37 0.8450652 3.128480 Two Sample t-test two.sided 6.5.3 Test Yourself Activities 6.5.3.1 Assignment Task 1A: Libraries library(broom) library(tidyverse) Return to Task 6.5.3.2 Assignment Task 1B: Loading in the data Use read_csv() to read in data! crt &lt;- read_csv(&quot;data/06-s01/homework/CRT_Data.csv&quot;) crt &lt;- read_csv(&quot;CRT_Data.csv&quot;) Return to Task 6.5.3.3 Assignment Task 2: Selecting only relevant columns The key columns are: ID Treatment CorrectAnswers Creating crt2 which is a tibble with 3 columns and 243 rows. crt2 &lt;- select(crt, ID, Treatment, CorrectAnswers) Return to Task 6.5.3.4 Assignment Task 3: Verify the number of subjects in each group The Participants section of the article contains the following statement: 243 men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = 125) or placebo (n = 118). In the t3 code block below, replace the NULLs with lines of code to calculate: The number of men in each Treatment. This should be a tibble/table called cond_counts containing a column called Treatment showing the two groups and a column called n which shows the number of men in each group. The total number of men in the sample. This should be a single value, not a tibble/table, and should be stored in n_men. You know the answer to both of these tasks already. Make sure that your code gives the correct answer! For cond_counts, you could do: cond_counts &lt;- crt2 %&gt;% group_by(Treatment) %&gt;% summarise(n = n()) Or alternatively cond_counts &lt;- crt2 %&gt;% count(Treatment) For n_men, you could do: n_men &lt;- crt2 %&gt;% summarise(n = n()) %&gt;% pull(n) Or alternatively n_men &lt;- nrow(crt2) Solution: When formatted with inline R code as below: `r n_men` men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = `r cond_counts %&gt;% filter(Treatment == 1) %&gt;% pull(n)`) or placebo (n = `r cond_counts %&gt;% filter(Treatment == 0) %&gt;% pull(n)`). should give: 243 men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = 125) or placebo (n = 118). Return to Task 6.5.3.5 Assignment Task 4: Reproduce Figure 2A You could produce a good representation of Figure 2A with the following approach: crt_means &lt;- crt2 %&gt;% group_by(Treatment) %&gt;% summarise(m = mean(CorrectAnswers), sd = sd(CorrectAnswers)) %&gt;% mutate(Treatment = recode(Treatment, &quot;0&quot; = &quot;Placebo&quot;, &quot;1&quot; = &quot;Testosterone Group&quot;)) ggplot(crt_means, aes(Treatment, m, fill = Treatment)) + geom_col() + theme_classic() + labs(x = &quot;CRT&quot;, y = &quot;Number of Correct Answers&quot;) + guides(fill = &quot;none&quot;) + scale_fill_manual(values = c(&quot;#EEEEEE&quot;,&quot;#AAAAAA&quot;)) + coord_cartesian(ylim = c(1.4,2.4), expand = TRUE) Figure 6.5: A representation of Figure 2A Return to Task 6.5.3.6 Assignment Task 5: Interpreting your Figure Option 3 is the correct answer given that: The Testosterone group (M = 1.66, SD = 1.18) would appear to have fewer correct answers on average than the Placebo group (M = 2.10, SD = 1.02) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. answer_t5 &lt;- 3 Return to Task 6.5.3.7 Assignment Task 6: t-test You need to pay attention to the order when using this first approach, making sure that the 0 group are entered first. This will put the Placebo groups as Estimate1 in the output. In reality it does not change the values, but the key thing is that if you were to pass this code on to someone, and they expect Placebo to be Estimate1, then you need to make sure you coded it that way. t_table &lt;- t.test(crt2 %&gt;% filter(Treatment == 0) %&gt;% pull(CorrectAnswers), crt2 %&gt;% filter(Treatment == 1) %&gt;% pull(CorrectAnswers), var.equal = TRUE) %&gt;% tidy() Alternatively, you could use what is known as the formula approach as shown below. Here you state the DV ~ IV and you say the name of the tibble in data = .... You just need to make sure that the columns you state as the DV and the IV are actually in the tibble! t_table &lt;- t.test(CorrectAnswers ~ Treatment, data = crt2, var.equal = TRUE) %&gt;% tidy() Return to Task 6.5.3.8 Assignment Task 7: Reporting results The degrees of freedom (df) is found under parameter t_df &lt;- t_table$parameter An alternative option for this would be as follows, using the pull() method. This would work for B to D as well t_df &lt;- t_table %&gt;% pull(parameter) The t-value is found under statistic t_value &lt;- t_table$statistic %&gt;% round(3) The p-value is found under p.value p_value &lt;- t_table$p.value %&gt;% round(3) The absolute difference between the two means can be calculated as follows: t_diff &lt;- (t_table$estimate1 - t_table$estimate2) %&gt;% round(3) %&gt;% abs() If you have completed t7A to t7D accurately, then when knitted, Option 4 would be stated as such The testosterone group performed significantly worse (0.438 fewer correct answers) than the placebo group, t(241) = 3.074, p = 0.002 and would therefore be the correct answer! answer_t7e &lt;- 4 Return to Task Chapter Complete! 6.6 Additional Material Below is some additional material that might help you understand the tests in this Chapter a bit more as well as some additional ideas. More on t.test() - vectors vs. formula A quick note on running the t-test in two different ways. In the lab we showed you how to run a t-test on a between-subjects design. This is the Welchs t-test version of the code from the lab: t_table &lt;- t.test(crt2 %&gt;% filter(Treatment == 0) %&gt;% pull(CorrectAnswers), crt2 %&gt;% filter(Treatment == 1) %&gt;% pull(CorrectAnswers), var.equal = FALSE) %&gt;% tidy() This is sometimes referred to as the vector approach, and what the code is doing is taking each groups data as a vector input. For example, if you were to just look at the data for the Treatment 0 group then that line of code: crt2 %&gt;% filter(Treatment == 0) %&gt;% pull(CorrectAnswers) shows just these values: ## [1] 2 2 3 3 3 2 1 0 2 0 2 1 3 2 2 3 1 3 2 3 2 0 2 3 3 3 3 2 2 2 3 0 3 3 3 0 0 ## [38] 3 3 1 3 1 2 3 3 1 3 3 3 3 2 3 2 2 2 2 3 3 1 3 1 0 3 1 3 1 2 3 3 0 2 2 2 3 ## [75] 1 2 3 0 3 1 1 3 3 0 1 0 3 2 2 3 1 3 3 2 3 1 3 2 3 3 3 3 3 3 2 3 3 2 2 0 3 ## [112] 0 1 2 2 3 3 3 And likewise the Treatment 1 line of code just gives these values: ## [1] 2 0 0 1 3 0 0 2 2 3 1 3 3 1 0 2 1 0 0 3 2 0 3 0 3 3 3 0 2 0 3 3 3 1 0 0 1 ## [38] 2 3 2 2 2 3 3 1 0 3 0 1 1 1 0 0 1 2 3 3 0 2 2 3 3 2 3 3 3 3 3 3 1 2 3 1 1 ## [75] 2 3 3 0 2 2 1 1 2 3 0 0 3 0 1 2 0 0 0 0 3 1 3 2 2 2 1 2 3 3 0 3 1 0 1 2 0 ## [112] 2 2 2 3 2 3 3 3 0 3 0 1 2 3 And the t.test() function is just saying this is group 1s data, this is group 2s data, and I am comparing them. However the eagle-eyed of you will have seen in the solution an alternative code, that looks like: t_table &lt;- t.test(CorrectAnswers ~ Treatment, data = crt2, var.equal = FALSE) %&gt;% tidy() This is the formula method and if you look at the structure of crt2 (the first 6 rows are shown below) you get an idea of how the formula approach works: ID Treatment CorrectAnswers 11 1 2 12 1 0 13 1 0 14 1 1 15 1 3 16 1 0 The formula approach works as t.test(DV ~ IV, data = my_data) where: DV is the name of the column with your responses (e.g. mean reaction time) IV is the name of the column with your groups (in categorical form - e.g. 1 vs 0) ~ means by or based on or split up by. So test the DV based on the IV grouping and my_data is the name of your tibble. Whichever approach you use, the observed t-value, p-value, and df (parameter) should be the same! What might change is what the t-test considers as group 1 and what it considers group 2. And from your knowledge of the t-test formula, this will affect whether your t-test is positive or negative. For example: t_table &lt;- t.test(crt2 %&gt;% filter(Treatment == 0) %&gt;% pull(CorrectAnswers), crt2 %&gt;% filter(Treatment == 1) %&gt;% pull(CorrectAnswers), var.equal = FALSE) %&gt;% tidy() Gives the t-value of t = 3.09, whereas if you switch the order of inputting the Treatment groups as such: t_table &lt;- t.test(crt2 %&gt;% filter(Treatment == 1) %&gt;% pull(CorrectAnswers), crt2 %&gt;% filter(Treatment == 0) %&gt;% pull(CorrectAnswers), var.equal = FALSE) %&gt;% tidy() You get a t-value of t = -3.09. Same value (or magnitude) just one is positive and the other is negative, because the order of who is Group 1 and who is group 2 is switched. And just for comparion, the formula approach: t_table &lt;- t.test(CorrectAnswers ~ Treatment, data = crt2, var.equal = FALSE) %&gt;% tidy() Gives a t-value of t = 3.09. So again the same magnitude but you are not controlling who is Group 1 and who is Group 2. Long story short is that both methods, if used appropriately, run the same analysis. The benefit of the vector approach is that you can dictate in your analysis who is Group 1 and who is Group 2. And that is really the only difference. Misleading and Appropriate Barplots Misleading Barplots The data used in the PreClass activities allows us to show you something that you might find interesting. As we mentioned back in Chapter 3 on visualisation, the barplot is becoming less frequently used, as summarised in this blog: One simple step to improve statistical inferences. The data we have used today demonstrate the point that a simple barplot can actually be somewhat misleading about the data. Have a look at the figure below. Both bars represent the data from our 22 Normal Sleep participants. The column on the left, hide_data, is a standard representation (albeit without error bars) whereas the column on the right, show_data, demonstrates the issue. Looking at the column on the left, the assumption is that all the data is around the peak of the column. However, looking at the column on the right, we can see that this is not the case and there are participants both above and below the mean by approximately 100 ms. This misleading perception, when the data is hidden, was tested and shown to exist in participants viewing these figures by Newman and Scholl (2012) which you can read up on if you like. Figure 6.6: How representative are barplots of the actual spread of the data! The function we use to show the data points is geom_jitter() and it gets added to a visualisation pipeline just like other geom_?()s that we have used; your code will look like something like this: ggplot(my_data, aes(x = my_x_variable, y = my_y_variable)) + geom_col() + geom_jitter(width = .1) Look out for it in the coming chapters but the main thing to keep in mind is that barplots can be misleading and displaying the individual data may be more informative - particularly when the response measured creates a spread of data. Appropriate Barplots That said, barplots do have their place in data analysis and research methods. The best time to use a barplot is when the value is a count - i.e. when the value had no variability in it. A great example of this is in observational research using categorical data. Think about the table we simulated for the chi-squre data as part of the Additional Materials in Chapter 5. Here is that table again: Table 6.4: Cross-Tabulation Table of Simulated Data from Chapter 5 Additional Materials Groups No Unsure Yes Total A 17 18 15 50 B 18 14 18 50 We have two groups with three possible responses and each participants can only give one response. What that means, for example, is that there is no variation in the number of people who in Group A said No - it is exactly 17 people. Likewise, exactly 14 people in Group B said unsure. So here a barplot actually makes sense to use as it accurately reflects the value in that group/condition and no data is hidden. The plot for the above table might look like this: Figure 6.7: A barplot works well with categorical data In short, the main message about any visualisation is about accurately and clearly conveying that information to your reader. If there is a spread of data within a variable then try to show that more clearly with error bars and data points, perhaps even using a violin plot instead of a barplot. If however there is no spread of data within a variable then a barplot works well. Analysing chi-squares Finally, to round of this additional material, we thought it might be handy to show how to run a chi-square analysis. We cover the chi-square more in lectures but adding it here might help some people. We wont really look at the hand calculations, but you will know that the formula for the chi-square is: \\(x_c^2 = \\sum\\frac{(O_i - E_i)^2}{E_i}\\) where \\(x_c^2\\) is the chi-square symbol - often simplified to \\(x_2\\) \\(O_i\\) is the observed value of group {i} and \\(E_i\\) is the expected value of group {i} And we can use the data from above to show how to quickly run the chi-square analysis on the table. First we need to tidy up the table to get rid of the Groups and Total columns, leaving us with a tibble called chi_table that has this structure: Table 6.5: Cross-Tabulation Table Data for analysis No Unsure Yes 17 18 15 18 14 18 Now to analyse the data we use the chisq.test() function. There are really only two arguments you need to think about: your data - for us stored in chi_table and whether to set correct as TRUE or FALSE. This is Yates Continuity Correction. The default is set as TRUE but a lot of people set it as FALSE so we will select that today. And we run the analysis as such: chisq_result &lt;- chisq.test(chi_table, correct = FALSE) %&gt;% tidy() Giving the following result: Table 6.6: Chi-Square Result statistic p.value parameter method 0.8012987 0.6698849 2 Pearsons Chi-squared test And could be written up as \\(x_c^2\\)(df = 2, N = 100) = 0.8, p = 0.67. And if you wanted to check the output and calculate the data by hand, or needed to do further analysis, you can see the expected values by doing the analysis but not tidying it, as follows: chisq_untidy &lt;- chisq.test(chi_table, correct = FALSE) chisq_untidy$expected ## No Unsure Yes ## [1,] 17.5 16 16.5 ## [2,] 17.5 16 16.5 And if you wanted the expected values for just one of the groups - say the second group, Group B: chisq_untidy$expected[2,] ## No Unsure Yes ## 17.5 16.0 16.5 Or even just the observed values to check you entered the data correctly: chisq_untidy$observed ## No Unsure Yes ## [1,] 17 18 15 ## [2,] 18 14 18 We will leave that there but hopefully this gives you some additional insight into how to analyse a chi-square test. End of Additional Material! "],["within-subjects-t-test.html", "Lab 7 Within-Subjects t-test 7.1 Overview 7.2 PreClass Activity 7.3 InClass Activity 7.4 Assignment 7.5 Solutions to Questions 7.6 Additional Material", " Lab 7 Within-Subjects t-test 7.1 Overview In the previous chapters we have looked at one-sample t-tests and between-samples t-tests. In this chapter we will continue to look at t-tests in general, with the PreClass Activity looking really at the assumption of variance in a between-subjects t-test, and the InClass Activity looking at the remaining type of t-test; the within-subjects t-test (sometimes called the dependent sample or paired-sample t-test). The within-subjects t-test is a statistical procedure used to determine whether the mean difference between two sets of observations from the same or matched participants is zero. As in all tests, the within-subjects t-test has two competing hypotheses: the null hypothesis and the alternative hypothesis. The null hypothesis assumes that the true mean difference between the paired samples is zero: \\[H_0: \\mu1 = \\mu_2\\]. The alternative hypothesis assumes that the true mean difference between the paired samples is not equal to zero: \\[H_1: \\mu1 \\ne \\mu_2\\]. In this chapter we are going to look at running the within-subjects t-test but to begin with we will do a little work on the checks of your data that you need to perform prior to analysis. Assumptions of tests So far we have focussed your skills on data-wrangling, visualisations and probability, and now we are moving more towards the actual analysis stage of research. However, as you will know from your lectures, all tests, and particularly parametric tests, make a number of assumptions about the data that is being tested, and that you, as a responsible researcher, need to check these assumptions are held as any violation of the assumptions may make your results invalid. For t-tests the assumptions change for between-subjects and within-subjects designs (the one-sample and matched-pairs designs can be thought of as within-subjects designs). The assumptions of the between-subjects t-test are: All data points are independent. The variance across both groups/conditions should be equal. The dependent variable must be continuous (interval/ratio). The dependent variable should be normally distributed. And the assumptions of the within-subjects t-test are: All participants appear in both conditions/groups. The dependent variable must be continuous (interval/ratio). The dependent variable should be normally distributed. Before beginning any analysis, using your data-wrangling skills, you must check to see if the data deviates from these assumptions, and whether it contains any outliers, in order to assess the quality of the results. So in this lab we will: Understand about assumptions of t-tests in general Learn about the assumption of equal variance in between-subjects t-tests Run all assumption checks and analysis in an experiment with a within-subjects design. 7.2 PreClass Activity A bit of a change of pace in this PreClass Activity. In order to give you a bit more of an understanding of the assumptions of the between-subjects t-test, and a viable alternative to the standard Students t-test, we ask that you read the following blog (and even the full paper if you have time) and then try out the couple of tasks below. 7.2.1 Reading Read the following blog on using Welchs t-test for between-subjects designs. Blog: Always use Welchs t-test instead of Students t-test by Daniel Lakens. For further reading you can look at the paper that resulted from this blog: Paper: Delacre, M., Lakens, D., &amp; Leys, C. (in press) Why Psychologists Should by Default Use Welchs t-test Instead of Students t-test. International Review of Social Psychology. 7.2.2 Task Copy the script within the blog into an R script and try running it to see the difference between Welchs t-test (the recommended test in the blog) and Students t-test (the standard test in the field). Note: You will need the car package. This is installed already in the Boyd Orr labs so if doing this in the labs, do not install the package, just call it to the library with library(car). If yuo are using your own machine then you will need to install the car package. Note: The code will take a minute or two to run as there is a stage of simulating data (just like we did in Chapter 5) that will take a little time to run. Dont worry if you dont yet understand all the code. It is highly commented, with each line stating what it does, but it is tricky. The key thing is to try and run it and to look at the figures that come out of it - particularly the third figure that you see in the blog, the one with the red line on it that compares p-values in the two tests. Look at how many tests (dots) are significant on one test and not the other. We give an explanation of the blog below but it is worth trying it out yourself first. Now try changing the values for n1, n2, sd1 and sd2 at the top of the script to see what effect this has on the Type 1 Error rate (alpha = .05). Again look at the figure with the red line, comparing significance on one test versus significance on the other. This is what should change depending on the n of each sample and whether the variance is equal or not. Think about the overall point of this blog and which test we should use when conducting a t-test. Once you have thought about this, read the blog below and see if you follow it. We will look at this more again in later chapters and lectures. Understanding the Blog and the assumption of variance What the blog and paper are trying to help us recognise is that if the two groups you have sampled have equal number of participants and equal variance, then both the Students t-test and the Welchs t-test will return the same t-value and, therefore, p-value. This means that you would reject the null hypothesis an equal number of times regardless of which test you used. You can see this in the first figure below with significant results shown as blue circles and non-significant resutls show as orange circles. We have used an adapted version of the code from the blog to create this figure - the settings we have used are in the box shown and you can change your code to match them (remember to set the seed to get the same values) set.seed(1409) n1 &lt;- 38 n2 &lt;- 38 sd1 &lt;- 1.85 sd2 &lt;- 1.85 nSims &lt;- 500 Figure 7.1: Scatterplot illustrating that with equal number of participants and equal variance, Welchs t-test and Students t-test find the same results as significant (Blue Circles) and the same results as non-significant (Orange Circles). The horizontal and vertical black lines represent the alpha = .05 for both tests. Dots falling on diagonal red line show tests with the same p-value for Welchs and Students t-tests. However, and the key point of the blog, if the two groups have unequal variance and/or an unequal number of participants, the two tests start to give different findings. This is shown in the below figure where findings that are significant in both tests are shown in blue, findings that are non-significant in both tests are shown in orange, and findings that are significant using the Students t-test but non-significant by using Welchs t-test are shown in pink. If we read the blog, especially about how p-values work when there is no actual difference between two groups, then we can come to the conclusion that Welchs t-test is working better than the Students t-test in this scenario. To put it in ther words, the Students t-test is finding more tests as significant than it should, and as such has a false positive rate (Type 1 error rate) much higher than the expected \\(\\alpha = .05\\). Have a look at the figure and see if you can understand it. set.seed(1409) n1 &lt;- 38 n2 &lt;- 25 sd1 &lt;- 1.15 sd2 &lt;- 1.85 nSims &lt;- 500 Figure 7.2: Figure illustrates that with unequal number of participants and/or unequal variance, Welchs t-test and Students t-test work differently, returning conflciting findings. Findings that are significant in both tests are shown in blue, findings that are non-significant in both tests are shown in orange, and findings that are significant using the Students t-test but non-significant by using Welchs t-test are shown in pink. The horizontal and vertical black lines represent the alpha = .05 for both tests. Dots falling on diagonal red line show tests with the same p-value for Welchs and Students t-tests. Dots above (below) the red line shown tests where p-value is smaller (larger) in the Students t-test. So what is the difference between the two tests? The answer relates to the assumption of variance. What is considered the common/standard t-test in the field, Students t-test, has the assumption of equal variance, whereas Welchs t-test has no assumption of equal variace - it does however have all the other same assumptions as the Students t-test. What this blog shows is that if the groups have equal variance then both tests return the same finding. However if the assumption of equal variance is violated, i.e. the groups have unequal variance, then Welchs test produce the more accurate finding, based on the data. This is important as often the final decision on whether assumptions are held or violated is subjective; i.e. it is down to the researcher to fully decide. Nearly all data will show some level of unequal variance (with perfectly equal variance across multiple conditions actually once revealing fraudulent data). Researchers using the Students t-test regularly have to make a judgement about whether the variance across the two groups is equal enough. As such, this blog shows that it is always better to run a Welchs t-test to analyse a between-subjects design as a) Welchs t-test does not have the assumption of equal variance, b) Welchs t-test gives more accurate results when variance is not equal, and c) Welchs t-test performs exactly the same as the Student t-test when variance is equal across groups. In short, Welchs t-test takes a level of ambiguity (or what may be called a researcher degree of freedom) out of the analysis and makes the analysis less open to bias or subjectivity. As such, from now on, unless stated otherwise, you should run a Welchs t-test. In practice it is very easy to run the Welchs t-test, and you can switch between the tests as shown: to run a Students t-test you set var.equal = TRUE to run a Welchs t-test you set var.equal = FALSE For example, if we created some simulated data of two groups (A vs B) with twenty data points in each group: my_data &lt;- tibble(my_iv = rep(c(&quot;A&quot;, &quot;B&quot;), each = 20), my_dv = c(rnorm(20,0,1),rnorm(20,0,1))) This would run a Students t-test: t.test(my_dv ~ my_iv, data = my_data, var.equal = TRUE) ## ## Two Sample t-test ## ## data: my_dv by my_iv ## t = 0.74956, df = 38, p-value = 0.4581 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.5021683 1.0926848 ## sample estimates: ## mean in group A mean in group B ## 0.32491425 0.02965598 This would run the Welchs test: t.test(my_dv ~ my_iv, data = my_data, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: my_dv by my_iv ## t = 0.74956, df = 37.933, p-value = 0.4581 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -0.5022146 1.0927312 ## sample estimates: ## mean in group A mean in group B ## 0.32491425 0.02965598 And two ways to know that you have run the Welchs t-test are: The output says you ran the Welch Two Sample t-test The df is likely to have decimal places in the Welchs t-test whereas it will be a whole number in the Students t-test. Always run the Welchs test in a between-subjects design when using R! Dont worry if you dont yet fully understand this blog. We will have some practice on it in coming chapters but in short Welchs t-test is better as it does not require the assumption of equal variance. Conversely, there is no concern with variance in a within-subjects t-test because, as you will know from lectures, the top half of the equation of the formula (the numerator) is the mean difference between the two conditions, and so it is only one set of values and there is nothing to equate it to. We are going to explore the assumptions of the within-subjects t-test in much more depth next in the InClass Activities! Job Done - Activity Complete! Thats it for the PreClass Activity! This is a bit of a change to the PreClass activities you have done so far, and you will start to see this approach more in the later chapters of this book - you being asked to read blogs, papers, and chapters from other books. Dont forget though that it is really important to summarise information in your own words to help you really understand it, so you should always be going back and adding informative points to your Portfolio. And, as always, post any questions you have on the available forums or ask a member of staff. 7.3 InClass Activity Up to now we have covered the one-sample t-test and the between-subjects t-test, and we have talked a little about the assumptions of the between-subjects t-test. We are now going to expand on that by looking at the final t-test we need to cover, the within-subjects t-test; used when you have the same participants in both conditions, or you have two groups of people that are very closely matched on various demographics such as age, IQ, verbal acuity, etc. In explorting the within-subjects t-test we will also look more at checking the assumptions of t-tests. If you refer back to the earlier activities you will know that many of the assumptions of the different t-tests are largely similar (apart from equal variance, for exmple), so by looking at these assumption check here you can apply them to other tests. For this activity we will look at a replication of Furnham (1986) that the School of Psychology, University of Glasgow, carried out in 2016 - 2017. It would be worth familiarising yourself with the original study at some point for more information regarding the concepts of the study, but it is not essential in order to complete the assignment: Furnham, A. (1986), The Robustness of the Recency Effect: Studies Using Legal Evidence. We will explain a little about the study before carrying out some tasks to check the assumptions and to analyse the data. Juror Decision Making: Does the order of information affect juror judgements of guilt or innocence? The overall aim of the original experiment was to investigate whether the decision a jury member makes about the innocence or guilt of a defendant could be influenced by something as simple as when crucial evidence is presented during a trial. During the experiment participants (Level 2 Psychology students) listened to a series of recordings that recreated the 1804 trial of a man known as Joseph Parker who was accused of assuming two identities and marrying two women; i.e. bigamy. Each participant listened to the same recordings of evidence, presented by both prosecution and defence witnesses, and were asked to judge how guilty they thought Mr. Parker was at 14 different TimePoints during the experiment on a scale of 1 to 9: 1 being innocent and 9 being guilty. The 14 TimePoints came immediately after certain pieces of evidence. The manipulation in the experiment was that the order of evidence was altered so that half the participants received one order of evidence and the other half received the second order of evidence. Key to the order change was the time at which a critical piece of evidence was presented. This critical evidence proved that the defendant was innocent. The middle group heard this evidence at Timepoint 9 of the trial whereas the late group heard this evidence at Timepoint 13. You will have an opportunity to look at all the data in due course but, for todays exercise, we will only focus on the late group. In this exercise, your task is to analyse the data to examine whether the participants ratings of guilt significantly changed before and after the presentation of the critical evidence in the late condition. If the critical evidence, which proved the defendants innocence, had the desired effect then you should see a significant drop in ratings of guilt after hearing this evidence (Timepoint 13) compared to before (Timepoint 12). Or in other words, we hypothesised that there would be a significant decrease in ratings of guilt, caused by presentation of the critical evidence, from Timepoint 12 to Timepoint 13. 7.3.1 Task 1: Load the Data Download the data for this experiment from here. Unzip the data and save it into a folder you have access to and set that folder as your working directory. Open a new R script. Today you will need the broom and tidyverse libraries. Load these in this order. Remember that the order you load in libraries matters. Using read_csv(), load in the data from the experiment contained in GuiltJudgements.csv and store it in a tibble called ratings. 7.3.2 Task 2: Wrangle the Data As above, you are only interested in the 75 participants of the Late group for this assignment and only for Timepoints 12 (rating before key evidence) and 13 (rating after key evidence). But having had a look at ratings you will see that the Timepoints are in wide format (columns 1 to 14 - each a different timepoint) and the Evidence column contains the Middle group as well. Hmmmm! We need to do some wrangling to make the data look like it is shown in Table 7.1 below. The stept to do this are as follows: filter() only those participants from the Late condition. select() only the Timepoints 12 and 13. rename() these Timepoints as Twelve and Thirteen as numerical names are hard to deal with. pivot_longer() to gather the data so that you have the below structure. Note that only the first four rows are shown. Do this all as one pipe and store it in a tibble called lates. Your tibble, lates will have 150 rows and 4 columns. Check that your table looks like the table below. Table 7.1: How your table should look from Task 2 Participant Evidence Timepoint GuiltRating 1 Late Twelve 7 1 Late Thirteen 5 2 Late Twelve 5 2 Late Thirteen 3 Helpful Hint You need to specify the column you want to filter from, stating which variable (i.e. Late) that this column is equal to (i.e. ==) Other than the two columns representing Timepoints 12 and 13, there are two other columns you need to keep in order to identify the participant and group. Use the table as a guide. When renaming, first state the new variable name and then designate this to the old variable name. i.e. rename(data, new_column_name = old_column_name). If the old column is a number, put it in backticks e.g. Five = backtick 5 backtick (but be sure to use `s). We have seen how to recode a few times now; where have you made a note of the blog from Lisa DeBruine. The structure shown has two new columns: Timepoint and GuiltRating, which are both created from columns Twelve to Thirteen. You should state these new column names when using pivot_longer(), as well as the columns you used to create them. Think about completing this: cols = X:Y, names_to = Where, values_to = Where Quickfire Questions To check you have completed this Task correctly, enter the appropriate values into the boxes. This dataset has: columns by rows and participants. 7.3.3 Task 3: Look at the Histogram for Normality We are now going to show you how to start checking data in terms of assumptions such as Normality. We will do this through creating some visualisations of the data and then spending a few minutes thinking about what these visualisations tell us about the data. First we will create a histogram for each of the two timepoints to see if their individual distributions appear normally distributed. Use your visualisation skills to plot a histogram for each TimePoint. Have the two histograms side-by-side in the one figure and set the histogram binwidth to something reasonable for this experiment. Helpful Hint ggplot() + geom_? A histogram only requires you to state aes(x) and not aes(x, y). We are examining the differences in guilt rating scores across participants. Which column from lates should be x? It should be your categorical Independent Variable. binwidth is an argument you can specify within geom_histogram() such as geom_historgram(binwidth = ). Think about an appropriate binwidth. Your guilt rating scale runs from 1 to 9 in increments of 1. You have used something like facet_?() to display a categorical variables (i.e. Timepoint) according to the different levels it contains. You need to specify the variable you want to use, using ~ before the variable name. Beyond this point, you can think about adding appropriate labels and color if you like. 7.3.4 Task 4: A Boxplot of Outliers We can also check for outliers in the different conditions. Outliers can obviously create skewed data and make the mean value misleading. Create a boxplot for each Timepoint by GuiltRating and check for outliers. Helpful Hint This time when using ggplot() to create a boxplot, you need to specify both x, which is the discrete/categorical variable, and y, which is the continuous variable. geom_boxplot() - see Chapter 3 for an example. Quickfire Questions How many outliers do you see? 0 1 2 3 too many to count Remember that outliers are normally represented as dots or stars beyond the whiskers of the boxplot. As you will see in the solution the data contains an outlier. We wont deal with outliers today but it is good to be able to spot them at the moment. It would be worth thinking about how you could deal with outliers and maybe discuss it as a group. There are numerous methods such as replacing with a given value or removing the participants. Remember though that this decision, how to deal with outliers, and any deviation from normality, should be considered and written down in advance as part of your preregistration protocol. 7.3.5 Task 5: The Violin Plot So far so visualising good! Boxplots and histograms tell you slightly different information about the same data, which we will discuss in a minute, and you are developing your skills of plotting and interpreting them. But we have already introduced you to a new type of figure that we can combine the information from a boxplot and a histograom into the one figure. We do this using a violin plot and can be created using the geom_violin() function. Take the code youve written above for the boxplot (Task 4) and add on geom_violin as another layer. You may need to rearrange your code (i.e. the boxplot and violin plot functions) so that the violin plots appear underneath the boxplot. Think layers of the visualisation! Helpful Hint ggplot() works on layers - the first layer (i.e. the first plot you call) is underneath the second layer. This means that to get a boxplot showing on top of a violin plot, the violin must come first (i.e. you need to call geom_violin() before you call geom_boxplot()) We have embellished the figure a little in the solution that you can have a look at once you have the basics sorted. Things like adding a width call to the boxplot, or an alpha call to the violin. Do you see how the violin plot relates to the histogram you created earlier? In your head, rotate your histograms so that the bars are pointing to the left and imagine there is a mirror-image of them (making a two-sided histogram). This should look similar to your violin plot. Do you see it? And be sure to be able to still locate the outlier in th Thirteen condition. WAIT JUST A SECOND!!!!!! As you will know from your lectures and previous discussions of the assumptions of a within-subjects t-test, when dealing with a within-subejcts design, and a paired t-test, normality is actually determined based on the difference between the two conditions. That is, we should in fact check that the distribution of the scores of the difference between the two conditions is normally distributed. We have been looking at it in terms of separate conditions, to some degree to show you the process of the visualisations, but to really follow the within-subjects t-test need to look at the normality of the difference. The code below will create a violin and boxplot visualisation of the difference between the two conditions and you should now be able to understand this code. Have a look at the output, and at the code, and think about whether the scores of the difference between the two conditions is normally distributed. Note that in the code below outliers will appear as red circles and inidividual data points will appear as blue Xs. Can you see what is controlling this? Why is this an important step? We will say why in a second but it is worth thinking about yourself for a minute or two. lates %&gt;% spread(Timepoint, GuiltRating) %&gt;% mutate(diff = Thirteen - Twelve) %&gt;% ggplot(aes(x = Evidence, y = diff)) + geom_violin() + geom_boxplot(fill = &quot;red&quot;, width = .5, alpha = .1, outlier.colour = &quot;red&quot;) + geom_jitter(color = &quot;blue&quot;, width = .1, shape = 4) + theme_classic() Figure 7.3: A violin and boxplot showing the distribution of the scores of the difference between the Thirteen and Twelve conditions. Individual participant data show as blue stars. Positive values would indicate that the rating in the Thirteen condition is higher than the rating in the Twelve condition. Oultiers will show as red circles. Group Discussion Point We have now checked our assumptions but we sort of still need to make a decision regarding normality. Having had a look at the figures we have created, spend a few minutes thinking about whether the data is normally distributed or not. Also, spend a few minutes thinking about why this data shows no outliers but the individaul conditions did show an outlier. Lastly, think about why it is important to code different presentations of outliers from the individaul data points. Normal or not - Outliers vs DataPoints - Our Answer Is the data normally distributed or not What you are looking for is a nice symmetry around the topp and bottom half of the violin and boxplot. You are looking for the median (thick black line to be roughly in the middle of the boxplot), and the whiskers of the boxplot to be roughly equal length. Also, you want the violin to have a the bulge in the middle of the figure and to taper off at the top and at the bottom. You also want to have just one bulge on the violin (symbolising where most of the scores are). Two bulges in the violin may be indicative of two different response patterns or samples within the one dataset. Now we need to make our judgememt on normality. Remember that real data will never have the textbook curve that the normal distribution has. It will always be a bit messier than that and to some degree a judgement call is needed or you need to use a test to compare your sample distribution to the normal distribution. Tests such as a permutation bootstrap test that you saw in Chapter 5 (convert to z-scores and create a distribution of the difference of permuted means to the normal mean), or tests such as the Kolmogorov-Smirnov and the Shapiro-Wilks tests are sometimes recommended. However, these last two tests are not that reliable depending on sample size and often we revert back to the judgement call. Again this is really important why steps are documented so a future researcher can check and confirm the process. Overall, the data looks normally distributed - at least visually. Outliers vs DataPoints You have to be careful when using geom_jitter() to show individual data points. You can overcomplicate a figure and if you have a large number of participants then it might not make sense to include the individual data points as it can just create a noisy figure. A second point is that you want to make sure that your outliers are not confused with individual data points; all outliers are datapoints but not all data points are outliers. So, in order not to confuse data and outliers, you need to make sure you are properly controlling the colors and shapes of different information in a figure. And be sure to explain in the figure legend what each element shows. Lastly, you will have noted that when we plotted the original boxplots for the individual conditions, the Thirteen group had an outlier. However now when we plot the difference as a boxplot we see no outliers. This is important and reinforces why you have to plot the corret data in order to check the assumptions. The assumptions for a within-subjects t-test is based on the difference between the scores of individual participants. What we see here is that when we calculate the difference of scores there is no outlier, even though the original data point was an outlier. That is fine within consideration of the assumption - as the assumption only looks at the difference. That said, you might also want to check that the original outlier was an acceptable value on the rating scale (e.g. between 1 and 9) and not some wild value that has come about through bad data entry (e.g. a rating of 13; say if the rating and the condition got mixed up somehow). Great. We have run some data checks on the assumptions of the within-subjects t-test. To remind you, the assumptions of a within-subjects t-test are: All participants appear in both conditions/groups. The dependent variable must be continuous (interval/ratio). The dependent variable should be normally distributed. We checked the data for normal distribution, using a violin plot and a boxplot, looking for skewed data and outliers. And from our understanding of the data, and of arguments in the field, whilst the rating scale used could be called ordinal, many, including Furnham treat it as interval. Finally, we havent spotted any issues in our code to suggest that any participant didnt give a response in both conditions but we can check that again in the descriptives - both conditions should have an n = 75. We will do that check and also run some descriptives to start understanding the relationship between the two levels of interest: Timepoint 12 and Timepoint 13. 7.3.6 Task 6: Calculating Descriptives Calculate the mean, standard deviation, and Lower and Upper values of the 95% Confidence Interval for both levels of the Independent Variable (the two timepoints). You will need to also calculate the n() - see Chapter 2 - and the Standard Error to complete this task. Store all this data in a tibble called descriptives. The solution shows the values you should obtain but be sure to have a go first. Be sure to confirm that both groups have 75 people in it! Answering these questions may help on calculating the CIs: Quickfire Questions From the options, which equation would use to calculate the LowerCI? mean - 1.96 * sd mean * 1.96 - se mean - 1.96 * se mean * 1.96 - sd From the options, which equation would use to calculate the UpperCI? mean + 1.96 * sd mean * 1.96 + se mean + 1.96 * se mean * 1.96 + sd Helpful Hint group_by() the categorical column Timepoint. This is the column you want to compare groups for. summarise() Different calculations can be used within the same summarise() function as long as they are calculated in the order which you require them. For example, you first need to calculate the participant number, n = n(), and the standard deviation, sd = sd(variable), in order to calculate the standard error, se = sd/sqrt(n), which is required to calculate your Confidence Intervals. For the 95% Confidence Interval, you need to calculate an LowerCI and a UpperCI using the appropriate formula. Remember it will be mean + 1.96se and mean - 1.96se. If you dont include the mean then you are just calculating how much higher and lower than the mean the CI should be. We want the actual interval Portfolio Point - What is 1.96? Lets think about your task. Youre looking to calculate the 95% Confidence Interval for normally distributed data. To do this you require a z-score which tells you how many standard deviations you are from the mean. 95% of the area under a normal distribution curve lies within 1.96 standard deviations from the mean; i.e. 1.96SD above and below the mean. If you were looking to calculate a 99% Confidence Interval you would instead use a z-score of 2.576. This takes into account a greater area under the normal distribution curve and so you are further away from the mean (i.e. closer to the tail ends of the curve), resulting in a higher z-score. 7.3.7 Task 7: Visualising Means and Descriptives Using the data in descriptives, produce a plot that visualises the mean and 95% Confidence Intervals. One way would be a basic barplot, shown in previous chapters, with error bars indicating the 95% CI. To add the error bars you could add a layer like below. Feel free to embellish the figure as you see fit. We have shown a couple of options in the solution that you should look at and try adjusting, after you have tried your own plot. geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), position = &quot;dodge&quot;, width = .15) Helpful Hint We recommend using geom_col() Remember to add (+) the geom_errorbar() line above to your code! Dont pipe it. In the above code for error bars, the aesthetic, aes(), allows you to set the min and max values of the error bars - these will be the max and min of your CIs. position = dodge does the same as position = position_dodge() and position = position_dodge(width = .9). There are a number of ways to use a position call and they all do the same thing. Important to remember: as we have mentioned in previous labs, barplots are not that informative in themselves. Going ahead in your research, keep in mind that you should look to use plots that incorporate a good indication of the distribution/spread of the individual data points as well, when needed. Barplots give a good representation on categorical counts like in a chi-square test but not so much on ordinal or interval data where there is likely to be a spread. Group Discussion Point Now that we have some descriptives to look at we need to think what they tell us - or really how we interpret them. First thing is to think back to the hypothesis as every interpretation is phrased around the hypothesis. We hypothesised that there would be a significant decrease in ratings of guilt, caused by presentation of the critical evidence, from Timepoint 12 to Timepoint 13. Spend a few minutes talking to your group about whether you think there will be a significant difference between the two timepoints. What evidence do you have? Think about the overlap of confidence intervals! Remember the key thing at this stage is that it is a subjective impression - It appears that there might be. or words to that effect. 7.3.8 Task 8: The t-test Now we have checked our assumptions and ran our dscriptives, the last thing we need to do is to perform the within-subjects t-test to test the differences between the two time points. To perform the within-subjects t-test you use the same t.test function as you did in Chapter 6. However, this time you add the argument, paired = TRUE, as this is what tells the code yes, this is a paired t-test. Perform a paired-sample t-test between guilt ratings at the crucial time points (Twelve and Thirteen) for the subjects in the late group. Store the data (e.g. tidy) in a tibble called results. Helpful Hint From your work in earlier chapters you will know two ways to use the t.test() function. It might help to read the additional materials at the end of Chapter 6 if that is still unclear. But basically the two options are: The formula approach t.test(x ~ y, data, paired = TRUE/FALSE, alternative =two.sided/greater/less) where x is the columns containing your DV and y is typically your grouping variable (i.e. your independent variable). The vector approach t.test(data %&gt;% filter(condition1) %&gt;% pull(data),data %&gt;% filter(condition2) %&gt;% pull(data), paired = TRUE) To pull out the Twelve and Thirteen columns to pass as condition1 and condition2, you can use: lates %&gt;% pull(Twelve) and lates %&gt;% pull(Thirteen). Regardless of method Do not forget to state paired = TRUE or you will run a between-subjects t-test Once youve calculated results, dont forget to tidy() - you can add this using a pipe! If you dont quite understand the use of tidy() yet, run your t.test() without tidy() and see what happens! Note: Both options of running the t-test will give the same result. The only difference will be whether the t-value is positive or negative. Remember that the vector approach allows you to state what is condition 1 and what is condition 2. The formula approach just runs conditions alphabetically. Group Discussion Point Now look at the output of your test within the tibble results. In your group, spend some time breaking down the results you can see. Do you understand what all the values mean and where they come from. You may have to match up some of your knowledge from your lectures. Was there a significant difference or not? We are about to write it up so best we know for sure. How can you tell? One you have had some time to discuss the output, try to complete the next task. 7.3.9 Task 9: The Write-up Fill in the blanks below to complete this paragraph, summarising the results of the study. You will need to refer back to the information within results and descriptives to get the correct answers and to make sure you understand the output of the t-test. Enter all values to two decimal places and present the absolute t-value. The solutions contain a completed version of this paragraph for you to compare against. \"A paired-samples t-test one-sample t-test between-samples t-test matched-pairs t-test was ran to compare the change in guilt ratings before (M = , SD = ) and after (M = , SD = ) the crucial evidence was heard. A significant non-significant difference was found (t() = , p = .05 &gt; .05 = .001 &lt; .001) with Timepoint 13 having an average rating units lower than Timepoint 12. This tells us that the critical evidence did have an influence on the rating of guilt by jury members that the critical evidence did not have an influence on the rating of guilt by jury members that the critical evidence and the rating of guilt by jury members are unconnected something but I am not quite sure right now, I best ask!` Helpful Hint t-tests take the following format: t(df) = t-value, p = p-value your results states degrees of freedom as parameter, and your t-value as statistic. estimate is your mean difference between ratings at Timepoints Twelve and Thirteen. The conf.low and conf.high values are the 95% Confidence Intervals for the mean difference between the two conditions. We havent included them in the write-up here but you could do. This could be written as something like, there was a difference between the two groups (M = -1.76, 95% CI = [-2.19, -1.33]). Note: If you were to write up the above for your own report, you can make your write-up reproducible as well by using the output of your tibbles and calling specific columns. For example, t(`r results$parameter`) = `r results$statistic %&gt;% abs()`, p &lt; .001, when knitted will become t(74) = 8.23, p &lt; .001. So code can prevent mistakes in write-ups! However working with rounding up p-values can be tricky and we have offered a code to show you how, in the solutions. Note: Another handy function when writing up is the round() function for putting values to a given number of decimal places. For example in the above if we wanted to round the absolute t-value to two decimal places we might do results %&gt;% pull(statistic) %&gt;% abs() %&gt;% round(2) which would give t = 8.23. Or maybe we want three decimal places: results$statistic %&gt;% abs() %&gt;% round(3) which would give t = 8.232. So really handy function and follows the format of round(value_to_round, number_of_decimal_places) Job Done - Activity Complete! Excellent work! You have now completed the PreClass and InClass activities for this chapter! You can see how performing the t-test is only a small part of the entire process: wrangling the data, calculating descriptives, and plotting the data to check the distributions and assumptions is a major part of the analysis process. Over the past Chapters, you have been building all of these skills and so you should be able to see them being put to good use now that we have moved onto more complex data analysis. Running the inferential part is usually just one line of code. If youre wanting to practice your skills further, you could perform a t-test for the middle group where the crucial evidence was presented on time point 9. Otherwise, you should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the available forums for discussion or ask a member of staff. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. 7.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 7.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 7.5.1 InClass Activities 7.5.1.1 InClass Task 1 library(broom) library(tidyverse) ratings &lt;- read_csv(&quot;GuiltJudgements.csv&quot;) Return to Task 7.5.1.2 InClass Task 2 lates &lt;- ratings %&gt;% filter(Evidence == &quot;Late&quot;) %&gt;% select(Participant, Evidence, `12`, `13`) %&gt;% rename(Twelve = `12`, Thirteen = `13`) %&gt;% pivot_longer(cols = Twelve:Thirteen, names_to = &quot;Timepoint&quot;, values_to = &quot;GuiltRating&quot;) If you have carried this out correctly, lates will have 150 rows and 4 columns. This comes from 75 participants giving two responses each - TimePoint 12 and TimePoint 13. Return to Task 7.5.1.3 InClass Task 3 lates %&gt;% ggplot(aes(GuiltRating)) + geom_histogram(binwidth = 1) + facet_wrap(~Timepoint) + labs(x = &quot;GuiltRating&quot;, y = NULL) + theme_bw() Figure 7.4: Potential Solution to Task 3 Return to Task 7.5.1.4 InClass Task 4 The Task only asks for the boxplot. We have added some additional functions to tidy up the figure a bit that you might want to play with. lates %&gt;% ggplot(aes(x = Timepoint, y = GuiltRating)) + geom_boxplot() + scale_y_continuous(breaks = c(1:9)) + coord_cartesian(xlim = c(.5, 2.5), ylim = c(1,9), expand = TRUE) + theme_bw() Figure 7.5: Potential Solution to Task 4 You can see that there is one outlier in the Thirteen condition. It is represented by the the single dot far above the whiskers of that boxplot. Return to Task 7.5.1.5 InClass Task 5 We have added color but that was not necessary: lates %&gt;% ggplot(aes(x=Timepoint,y=GuiltRating))+ geom_violin(aes(fill = Timepoint), alpha = .2) + geom_boxplot(width = 0.5) + scale_y_continuous(breaks = c(1:9)) + coord_cartesian(ylim = c(1,9), expand = TRUE) + theme_bw() Figure 7.6: Potential Solution to Task 5 You can still see the outlier at the top of the figure as a solid black dot. You could even add the geom_jitter to have all the data points: lates %&gt;% ggplot(aes(x=Timepoint,y=GuiltRating))+ geom_violin(aes(fill = Timepoint), alpha = .2) + geom_boxplot(width = 0.5) + geom_jitter(aes(fill = Timepoint), width = .1, alpha = .2) + scale_y_continuous(breaks = c(1:9)) + coord_cartesian(ylim = c(1,9), expand = TRUE) + theme_classic() Figure 7.7: Alternative Potential Solution to Task 5 Return to Task 7.5.1.6 InClass Task 6 descriptives &lt;- lates %&gt;% group_by(Timepoint) %&gt;% summarise(n = n(), mean = mean(GuiltRating), sd = sd(GuiltRating), se = sd/sqrt(n), LowerCI = mean - 1.96*se, UpperCI = mean + 1.96*se) This would show the following data: knitr::kable(descriptives, align = &quot;c&quot;, caption = &quot;Descriptive data for the current study&quot;) Table 7.2: Descriptive data for the current study Timepoint n mean sd se LowerCI UpperCI Thirteen 75 4.04 1.934327 0.2233569 3.602221 4.477779 Twelve 75 5.80 1.497746 0.1729448 5.461028 6.138972 Return to Task 7.5.1.7 InClass Task 7 A basic barplot with 95% Confidence Intervals. We have embellished the figure a little but you can mess around with the code to see what each bit does. ggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + geom_col(colour = &quot;black&quot;) + scale_fill_manual(values=c(&quot;#999000&quot;, &quot;#000999&quot;)) + scale_x_discrete(limits = c(&quot;Twelve&quot;,&quot;Thirteen&quot;)) + labs(x = &quot;Timepoint of Evidence&quot;, y = &quot;GuiltRating&quot;) + guides(fill=&quot;none&quot;) + geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), position = &quot;dodge&quot;, width = .15) + scale_y_continuous(breaks = c(1:9), limits = c(0,9)) + coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) + theme_classic() Figure 7.8: Possible Solution to Task 7 One thing to watch out for with the above code is the scale_y_continuous() function which helps us set the length and tick marks (-) on the y-axis. Rather oddly, if you set the limits = ... to the same values as the ylim = ... in coord_cartesian() then your figure will behave oddly and may disappear. coord_cartesian() is a zoom function and must be set within the limits of the scale, set by scale_y_continuous(). An alternative way to display just the means and errorbars would be to use the pointrange approach. This image shows again the 95% CI ggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + geom_pointrange(aes(ymin = LowerCI, ymax = UpperCI))+ scale_x_discrete(limits = c(&quot;Twelve&quot;,&quot;Thirteen&quot;)) + labs(x = &quot;Timepoint of Evidence&quot;, y = &quot;GuiltRating&quot;) + guides(fill=&quot;none&quot;)+ scale_y_continuous(breaks = c(1:9), limits = c(0,9)) + coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) + theme_bw() Figure 7.9: Alternative Solution to Task 7 Return to Task 7.5.1.8 InClass Task 8 Remember to set paired = TRUE to run the within-subjects t-test results &lt;- t.test(GuiltRating ~ Timepoint, data = lates, paired = TRUE, alternative = &quot;two.sided&quot;) %&gt;% tidy() estimate statistic p.value parameter conf.low conf.high method alternative -1.76 -8.232202 0 74 -2.185995 -1.334005 Paired t-test two.sided Alternatively, using the filter() and pull() functions to make force in a given condition as the first condition. Here, below, we are forcing condition Thirteen as the first condition and so the values match the above approach. If you forced condition Twelve as the first condition then the only difference would be that the t-value would change polarity (positive to negative or vice versa). results &lt;- t.test(lates %&gt;% filter(Timepoint == &quot;Thirteen&quot;) %&gt;% pull(GuiltRating), lates %&gt;% filter(Timepoint == &quot;Twelve&quot;) %&gt;% pull(GuiltRating), paired = TRUE, alternative = &quot;two.sided&quot;) %&gt;% tidy() estimate statistic p.value parameter conf.low conf.high method alternative -1.76 -8.232202 0 74 -2.185995 -1.334005 Paired t-test two.sided The reason that the two outputs are the same is because the formula (top) method (x ~ y) is actually doing the same process as the second approach, but you are just not sure which is the first condition. This second approach (bottom) just makes it clearer. Note: The conf.low and conf.high values are the 95% Confidence Intervals for the mean difference between the two conditions. This could be written as something like, there was a difference between the two groups (M = -1.76, 95% CI = [-2.19, -1.33]). Return to Task 7.5.1.9 InClass Task 9 A potential write-up for this study would be as follows: A paired-samples t-test was ran to compare the change in guilt ratings before (M = 5.8, SD = 1.5) and after (M = 4.04, SD = 1.93) the crucial evidence was heard. A significant difference was found (t(74) = 8.23, p = 4.7113406^{-12}) with Timepoint 13 having an average rating 1.76 units lower than Timepoint 12. This tells us that the critical evidence did have an influence on the rating of guilt by jury members. Working with rounding p-values When rounding off p-values that are less than .001, rounding will give you a value of 0 which is technically wrong - the probability will be very low but not 0. As such, and according to APA format, values less than .001 would normally be written as p &lt; .001. To create a reader-friendly p-value, then you could try something like the following in your code: ifelse(results$p.value &lt; .001, &quot;p &lt; .001&quot;, paste0(&quot;p = &quot;, round(results$p.value,3))) So instead of writing t(74) = 8.23, p = 4.7113406^{-12}, you would write t(74) = 8.23, p &lt; .001 The in-line coding for these options would look like: p = `r results %&gt;% pull(p.value)` for p = 4.7113406^{-12} &amp; `r ifelse(results$p.value &lt; .001, p &lt; .001, paste0(p =, round(results$p.value,3)))` for p &lt; .001 Return to Task Chapter Complete! 7.6 Additional Material Below is some additional material that might help you understand the tests in this Chapter a bit more as well as some additional ideas. Non-Parametric tests In this chapter we have really been focussing on between-subjects and within-subjects t-tests which fall under the category of parametric tests. One of the main things you will know about these tests is that they have a fair number of assumptions that you need to check first to make sure that your results are valid. We looked at how to do this in the main body of the chapter, and you will get more practice at this as we progress, but one question you might have is, what do you do if the assumptions arent met (or are violated\" as it is termed)? So what options are there? Well actually you have already seen one - in Chapter 5. We could use permutation tests and bootstrapping (replication) techniques to compare two conditions. This is a nice approach as it has very few assumptions about the data - merely that the shape of the sample distribution is the shape of the population distribution. However permutation tests are a relatively new approach and require really thorough analytical skills to make sure you are doing them correctly when designs get complicated. Alternatively, there are tests known as non-parametric tests that have fewer assumptions than the parametric tests and can be run quite quickly using the same approach as we have seen with the t-tests. The non-parametric t-tests generally dont require any assumption of normality and tend to work on either the medians of the data (as opposed to the mean values) or the rank order of the data - i.e. what was the highest value, the second highest, the lowest - as opposed to the actual value. And just like there is slightly different versions of t-tests there are different non-parametric tests for between-subjects designs and within-subjects designs as such: The Mann-Whitney U-test is the non-parametric equivalent of the between-subjects t-test The Wilcoxon Signed-Ranks Test is the non-parametric equivalent of the within-subjects t-test. Note: Bootstrapping and permutation tests would also be considered non-parametric tests. However if you were to ask someone about a non-parametric version of a t-test they would likely think of the Mann-Whitney or the Wilcoxon Signed-Ranks Test depending on your design. Note: The Mann-Whitney and the Wilcoxon Signed-Ranks tests are now a bit antiquated as they were designed to be done by hand when computer processing power was limited. However they are still used in Psychology and you will still see them in older papers, so it is worth seeing one in action at least. So for example, if you were concerned that your data was really far from being normally distributed, and werent quite sure about permutation tests, you might use the Mann-Whitney or the Wilcoxon Signed-Ranks Test depending on your design. Here we will run through a Mann-Whitney U-test and then you can try out a Wilcoxon Signed-Ranks Test in your own time as it uses the same function - it is again just a matter of saying paired = TRUE. Our Scenario Aim: To examine the influence of perceived reward on problem solving. Procedure: 14 Participants in 2 groups (7 per group) are asked to solve a difficult lateral thinking puzzle. One group is offered a monetary reward for completing it as quick as possible. One group is offered nothing; just the internal joy of getting the task completed and correct. Task: The participants are asked to solve the following puzzle. Man walks into a bar and asks for a glass of water. The barman shoots at him with a gun. The man smiles, says thanks, and leaves. Why? IV: Reward group vs. No Reward group DV: Time taken to solve puzzle measured in minutes. Hypothesis: We hypothesise that participants who are given a monetary incentive for solving a puzzle will solve the puzzle significantly faster, as measured in minutes to solve the puzzle, than those that are given no incentive. Our Analysis Here is our data and a boxplot of the data to try and visualise what is happening in the data. Table 7.3: Table showing the time taken to complete the puzzle for the Reward and No Reward groups Participant Group Time 1 No Reward 1.32 2 No Reward 3.56 3 No Reward 7.55 4 No Reward 8.05 5 No Reward 8.54 6 No Reward 10.18 7 No Reward 15.55 8 Reward 3.25 9 Reward 5.54 10 Reward 7.66 11 Reward 9.02 12 Reward 10.45 13 Reward 13.21 14 Reward 21.37 ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. Please use `guides(&lt;scale&gt; = ## &quot;none&quot;)` instead. Figure 7.10: Boxplots showing the time taken to solve the puzzle for the two conditions, Reward vs No Reward. Outliers are represented by solid blue dots. Looking at the boxplots there is potentially some issues with skew in the data (see the No Reward group in particular) and both conditions are showing as having at least one outlier. As such we are not convinced our assumption of normality is held so we will run the Mann-Whitney U-test - the non-parametric equivalent of the between-subjects t-test (i.e. independent groups) - as it does not require the assumption of normal data. The descriptives Next, as always, we should look at the descriptives as well to make some subjective, descriptive inference about the pattern of the results. One thing to note is that the Mann-Whitney analysis is based on the rank order of the data regardless of group. In this code and table below we have put the data in order from lowest to highest and added on a rank order column. We have used the rank() function to create the ranks and setting the ties.method = \"average\". We wont go into why that is the case here but you can read about it through ?rank. scores_rnk &lt;- scores %&gt;% arrange(Time) %&gt;% mutate(ranks = rank(Time, ties.method = &quot;average&quot;)) Table 7.4: Table showing the time taken to complete the puzzle for the Reward and No Reward groups and the rank order of these times. Participant Group Time ranks 1 No Reward 1.32 1 8 Reward 3.25 2 2 No Reward 3.56 3 9 Reward 5.54 4 3 No Reward 7.55 5 10 Reward 7.66 6 4 No Reward 8.05 7 5 No Reward 8.54 8 11 Reward 9.02 9 6 No Reward 10.18 10 12 Reward 10.45 11 13 Reward 13.21 12 7 No Reward 15.55 13 14 Reward 21.37 14 Here is a table of descriptives for this dataset an the code we used to create it. ByGrp &lt;- group_by(scores_rnk, Group) %&gt;% summarise(n_Pp = n(), MedianTime = median(Time), MeanRank = mean(ranks)) Table 7.5: Descriptive (N, Medians and Mean Ranks) for the two groups (Reward vs No Reward) in time taken to solve the puzzle. Group n_Pp MedianTime MeanRank No Reward 7 8.05 6.714286 Reward 7 9.02 8.285714 Based on figure and descriptive data we can suggest that there appears to be no real difference between the two groups in terms of time taken to solve the puzzle. The group that was offered a reward have a slightly higher spread of data than the no reward group. However the medians and mean ranks are very comparable. The inferential test We will now run the Mann-Whitney U-test to see if the difference between the two groups is significant or not. To do this, somewhat confusingly, we us the wilcox.test() function. The code to do the analysis on the current data (with the tibble scores, the DV in the column Time, and the IV in the column Group) is shown below. It works just like the t-test() functio in that you can use either vectors or the fomula approach. Note: There are a couple of additional calls in this function that you can read about using the ?wilcox.test() approach. Note: We could just have easily used scores_rnk as our tibble in the wilcox.test() as opposed to scores. We are using scores to show you that you dont need to put the ranks into the wilcox.test() function, it will create them itself when it runs the analysis. We only created them to run some descriptives. result &lt;- wilcox.test(Time ~ Group, data = scores, alternative = &quot;two.sided&quot;, exact = TRUE, correct = FALSE) %&gt;% tidy() And here is the output of the test after is has been tidied into a tibble using tidy() Table 7.6: Output of the Mann-Whitney U-test statistic p.value method alternative 19 0.534965 Wilcoxon rank sum exact test two.sided The main statistic (the test-value) of the Mann-Whitney test is called the U-value and is shown in the above table as statistic; i.e. U = 19 and you can see from the results that the difference was non-significant as p = 0.535. Note: The eagle-eyed of you will spot that the test actually says it is a Wilcoxon rank sum test. That is fine. The Mann-Whitney U is calculated from the sum of the ranks (shown in the table above). The Wilcoxon rank sum test is just that, a sum of the ranks. The U-value is then created from the summed ranks. Note: Also, dont mistake the Wilcoxon rank sum test mentioned here - for between-subjects - with the Wilcoxon Signed-Ranks test for within-subjects mentioned above. They are different tests However, one thing to note about the U is that it is an unstandardised value - meaning that it is dependent on the values sampled and it cant be compared to other U values to look at the magnitude of one effect versus another. The second thing to note about the U-value is that wilcox.test() will return a diferent U-value depending on which condition is stated as Group 1 or Group 2. Compare the outputs of these two tests where we have switched the order of the conditions: Version 1: result_v1 &lt;- wilcox.test(scores %&gt;% filter(Group == &quot;Reward&quot;) %&gt;% pull(Time), scores %&gt;% filter(Group == &quot;No Reward&quot;) %&gt;% pull(Time), data = scores, alternative = &quot;two.sided&quot;, exact = TRUE, correct = FALSE) %&gt;% tidy() Version 2: result_v2 &lt;- wilcox.test(scores %&gt;% filter(Group == &quot;No Reward&quot;) %&gt;% pull(Time), scores %&gt;% filter(Group == &quot;Reward&quot;) %&gt;% pull(Time), data = scores, alternative = &quot;two.sided&quot;, exact = TRUE, correct = FALSE) %&gt;% tidy() The U-value for these two tests are, for Version 1, U = 30 and for Versoin 2, U = 19. This may seem odd but actually both those test are correct. However, strictly speaking the U-value is the smaller of the two-values given by the different outputs. It is to do with how the U-value is calculated. Both groups have a U-value and the one that is checked for significance is the smaller of the two. So for the reasons mentioned above, when we present the Mann-Whitney U-test we usually also give a Z-statistic, which is the standardised version of the U-value. We also present an effect size, commonly r. Z and r can be calculated as follows: Z = \\(\\frac{U - \\frac{N1 \\times N2}{2}}{\\sqrt\\frac{N1 \\times N2 \\times (N1 + N2 + 1)}{12}}\\) r = \\(\\frac{Z}{\\sqrt(N1 + N2)}\\) Putting these formulas into a coded format would look like this: U &lt;- result$statistic N1 &lt;- ByGrp %&gt;% filter(Group == &quot;Reward&quot;) %&gt;% pull(n_Pp) N2 &lt;- ByGrp %&gt;% filter(Group == &quot;No Reward&quot;) %&gt;% pull(n_Pp) Z &lt;- (U - ((N1*N2)/2))/ sqrt((N1*N2*(N1+N2+1))/12) r &lt;- Z/sqrt(N1+N2) And as such the write-up could be written as: The time taken to solve the problem for the Reward group (n = 7, Mdn Time = 9.02, Mean Rank = 8.29) and the no reward group (n = 7, Mdn Time = 8.05, Mean Rank = 6.71) were compared using a Mann-Whitney U-test. No significance difference was found, U = 19, Z = -0.703, p = 0.535, r = -0.188 Last point on calculating U In the final write-up there we know, because of our codes, that U = 19 is the smallest U-value of the test. However had you put the alternative U, U = 30, when you calculated Z you would have got Z = 0.703, as opposed to Z = -0.703. So your standardised statistic will have the same value but just the opposite polarity (either positive or negative). That is fine though as you can look at the medians and mean ranks to make sure you are interpreting the data correctly. However, what you do have to watch out for when writing up this test is that you are presenting the correct U-value - remembering that technically you should present the smallest of the two U-values (refer back to Version 1 and Version 2 of using the wilcox.test()) above. Fortunately you dont have to run both analyses to figure out which is the smaller U (though you could if you wanted). There is a quicker way using the below formula: \\(U1 + U2 = N1 \\times N2\\) where U1 is the U-value from your wilcox.test() function N1 is the number of people in one group (technically doesnt matter which group) and N2 is the number of people in the other group We actually know both our U-values as we ran both tests; they are U1 = 30 and U2 = 19, and we know our two groups are N1 = 7 and N2 = 7. And if we put those numbers in the formula we get \\(U1 + U2 = N1 \\times N2\\) =&gt; \\(30 + 19 = 7 \\times 7\\) =&gt; \\(49 = 49\\) So both sides equal 49. But say you only know one of the U-values; you of course will know both Ns. Well you can quickly figure out the other U-value based on: \\(U2 = (N1 \\times N2) - U1\\) for example, if you know U1 = 19, N1 = 7 and N = 7 then: \\(U2 = (7 \\times 7) - 19\\) =&gt; \\(U2 = (49) - 19\\) =&gt; \\(U2 = 30\\) And then you just have to present the smallest of the two U-values, in this case U = 19. That is it for this additional materials and hopefully you now have a decent understanding of the Mann-Whitney test. Remember you have the skills to simulate data to run some new examples. You could also try running a Wilcoxon Signed-Ranks Test as well though you might have to read a little on how to present those. It is similar to the Mann-Whitney though and you should be able to get there. However, if stuck, do ask! Oh, and last last point, how to remember which test is which? Is the Mann-Whitney for between-subjects or within-subjects, and what is the Wilcoxon Signed-Ranks test for? You know they do different designs but which is which? Well, as silly as this memory aid is The other name of between-subjects designs, as you know, is independent designs. Add to that the fact that the late great singer Whitney Houston once starred in The Bodyguard which was about maintaining your right to freedom and independence. So whenever you get stuck on knowing which test is which, remember Whitney wanted independence in The Bodyguard and you should be ok. We did not say this was a very good memory aid! End of Additional Material! "],["apes-alpha-power-effect-sizes-sample-size.html", "Lab 8 APES - Alpha, Power, Effect Sizes, Sample Size 8.1 Overview 8.2 PreClass Activity 8.3 InClass Activity 8.4 Test Yourself 8.5 Solutions to Questions 8.6 Additional Material", " Lab 8 APES - Alpha, Power, Effect Sizes, Sample Size 8.1 Overview Up until now we have mainly spent time on data-wrangling, understanding probability, visualising our data, and more recently, running inferential tests, i.e. t-tests. In the lectures, however, you have also started to learn about additional aspects of inferential testing and trying to reduce certain types of error in your analyses. It is this balance of minimising error in our inferential statisitcs that we will focus on today. First thing to remember is that there are two types of hypotheses in Null Hypothesis Significance Testing (NHST) and what you are trying to establish is the probability of the null hypothesis not being accepted. Those two hypotheses are: The null hypothesis which states that the compared values are equivalent and, when referring to means, is written as: \\(H_0: \\mu_1 = \\mu_2\\) And the alternative hypothesis which states that the compared values are not equivalent and, when referring to means, is written as: \\(H_1: \\mu_1 \\ne \\mu_2\\). Now, each decision about a hypothesis is prone to some degree of error and, as you will learn, the two main types of error that we worry about in Psychology are: Type I error - or False Positives, is the error of rejecting the null hypothesis when it should not be rejected (otherwise called alpha or \\(\\alpha\\)). In other words, you conclude that there is a real effect when in fact there is no effect. The field standard rate of acceptable false positives is \\(\\alpha = .05\\) meaning that in theory 1 in 20 studies may be a false positive. Type II error - or False Negatives, is the error of retaining the null hypothesis when it is false (otherwise called beta or \\(\\beta\\)). In other words, you conclude that there was no real effect when in fact there was one. The field standard rate of acceptable false negatives is \\(\\beta = .2\\) meaning that in theory 1 in 5 studies may be a false negative. Adding to the ideas of hypotheses and errors, we are going to look at the idea of power which you will learn is the long-run probability of correctly rejecting the null hypothesis for a fixed effect size and fixed sample size; i.e. correctly concluding there is an effect when there is a real effect to detect. Power is calculated as \\(power = 1-\\beta\\) and is directly related to the False Negative rate. If the field standard of False Negatives is \\(\\beta = .2\\) then the field standard of power should be \\(power = 1 - .2 = .8\\), for a given effect size and sample size (though some papers, including Registered Reports are often required to have a power of at least \\(power &gt;= .9\\)). As such, \\(power = .8\\) means that the majority of studies should find an effect if there is one to detect, assuming that your study maintains these rates of error and power. Unfortunately, however, psychological research has been criticised for neglecting power and \\(\\beta\\) when planning studies resulting in what are called underpowered or low powered studies - meaning that your error rates are higher than you think they are, your power is lower than you think it is, and the study is unreliable. Note that as \\(\\beta\\) increases (the false negative rate increases), power decreases; power and false positive rates are also related, though less directly. In fact, low powered studies, combined with undisclosed analytical flexibility and publication bias, is thought to be a key issue in the replication crisis within the field. As such there may be a large number of studies where the null hypothesis has been rejected when it should not have been, and unpublished studies that have not been written up because they did not find an effect when they should have. In turn, when that is the case, the field becomes noisy and you are unsure which studies will replicate. It is issues like this that led us to redevelop our courses and why we really want you to understand power as much as possible. So this chapter is all about power, error rates, effect sizes, and sample sizes. We will learn: the relationship between power, alpha, effect sizes and sample sizes how to calculate certain effect sizes how to determine appropriate sample sizes in given scenarios and how to interpret power analyses. 8.2 PreClass Activity As in the last chapter, the Preclass activities involve reading and watching. We have written and selected this material to help give you a better understanding of power and how it interacts with effect size, sample size, and alpha. We have also suggested some optional material that you can look at and play with to get a rounder view. 8.2.1 Reading Read the following blog on Power and then the section we have written on Power and Design. This blog is a fictional conversation between a professor and a student on the importance of power. Grab a coffee and have a read. Dont worry about reading all the additional papers unless you want to; just the blog is fine to get an understanding. What you are trying to understand from this blog is the relationship between sample size and effect sizes, and whether a result from a study is likely to replicate or not based on the power of the original study. Blog: The Power Dialogues by PIGEE at the University of Illinois. Using power to design your study To reiterate, power is defined as the probability of correctly rejecting the null hypothesis for a fixed effect size and fixed sample size. As such, power is a key decision when you design your study, under the premis that the higher the power of your planned study, the better. Two relationships you will learn in this chapter are that: for a given sample size and \\(\\alpha\\), the power of your study is higher if the effect you are looking for is assumed to be a large effect as opposed to a small effect; large effects are easier to detect. and, for a given effect size and \\(\\alpha\\), the power of your study is higher when you increase your sample size. From these relationships we see that, because you have little control over the size of the effect you are trying to detect (it lives in the real world which you dont control), you can instead increase the power of your study by increasing the size of your sample (and also reducing sources of noise and measurement error in your study). As such, when planning a study, any good researcher will consider the following four key elements - the APES: alpha - (the false positive rate - Type 1 error) most commonly thought of as the significance level; usually set at \\(\\alpha = .05\\) power - the probability of rejecting the null hypothesis for a given effect size and sample size, with \\(power = .8\\) usually cited as the minimum power you should aim for based on the false negative rate being set at \\(\\beta = .2\\); effect size - size of the asssociation or difference you are trying to detect; sample size - the number of observations (usually, participants, but sometimes also stimuli) in your study. Note: because power depends on several variables, it is useful to think of power as a function with varying value rather than as a single fixed quantity. Now the cool thing about the APES is that if you know any three of these elements then you can calculate the fourth. In reality, the two most common approaches when designing a study would be: to determine the appropriate sample size required to reject your null hypothesis, with high probability, for the effect size that you are interested in. That is, you decide your \\(\\alpha\\), \\(power\\), and effect size, and from that you calculate for the sample size required in your study. Generally, the smaller the assumed effect size, the more participants you will need, assuming power and alpha are held constant. to determine the smallest effect size you can reliably detect given your sample size. That is, you know everything except the effect size. For example, say you are using an open dataset and you know they have run 100 participants, you cant add any more participants, and you want to know what is the minimum effect size you could detect from this dataset if you set \\(power\\) and \\(\\alpha\\) at the field standards. Hopefully that gives you an idea of how we use power to determine sample sizes for studies - and that the sample size should not just be pulled out of thin air. Both of these approaches described above are called a priori power analyses as you are stating the power level you want before (a priori means before) the study. However, you may now be thinking though, if everything is connected, then can we use the effect size from our study and the sample size to determine the power of the study after we have run it? No! Well, you can but it would be wrong to do so. This is actually called Observed or Post-Hoc power and most papers would discourage you from calculating it on the grounds that the effect size you are using is not the true effect size of the population you are interested in; it is just the effect size of your sample. As such any indication of power from this analysis is misleading. Avoid doing this. You can read more about why, here, in your own time if you like: Lakens (2014) Observed Power, and what to do if your editor asks for post-hoc power analyses. In short, stick to using only a priori power analyses approaches and use them to determine your required sample size or achievable reliable effect size. 8.2.2 Watch You should now also watch this short but nonetheless highly informative video by Daniel Lakens on Power and Sample Size. It will help consolidate the above points. And his shirt is amazing! Video: Power Analysis and Sample Size Decisions by Daniel Lakens Remember to make notes about power, effect sizes, and sample sizes, as processing the concepts into your own words will really help you to understand them better. 8.2.3 Optional Finally, there are a number of great webpages and blogs that will help you understand the concepts in this chapter. Here are some that we think might be good for you to look at. You dont have to look at all of these to understand this chapter but do come back to them as they will really help you as you progress in becoming a responsinble researcher. We are deliberately giving you a number of options here as for everyone there is that one analogy that will work best for you and that one paper that will make everything click into place. That example will be different from person to person so having a variety of explanations will help. A YouTube video by Dan Quintana (University of Oslo) showing how to use the pwr package to calculate power in t-tests, correlations, and one-way ANOVAs https://www.youtube.com/watch?v=ZIjOG8LTTh8 A shiny app created by Lisa Debruine (University of Glasgow) on guessing the effect size between two conditions http://shiny.psy.gla.ac.uk/guess/ A blog by Daniel Lakens (Eindhoven University of Technology) on determining the smallest effect size you are interested in. This is often referred to as the Smallest Effect Size of Interest (SESOI) http://daniellakens.blogspot.com/2017/05/how-power-analysis-implicitly-reveals.html An interactive webpage by Kristoffer Magnusson (Karolina Instituet, Stockholm) on interpreting Cohens d effect size https://rpsychologist.com/d3/cohend/ A shiny app by Hause Lin (University of Toronto) showing the conversion of one effect size into another http://escal.site/ A Frontiers in Psychology paper by Daniel Lakens on calculating various effect sizes for t-tests and ANOVAs https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full A blog by Daniel Lakens on what Type I and Type II errors are acceptable. In short, justify everything http://daniellakens.blogspot.com/2019/05/justifying-your-alpha-by-minimizing-or.html Chapter 9 of Ian Walkers Research Methods and statistics which is availble to read online through the University Library. This is a short chapter all about hypothesis testing and power really brining everything from the last few chapters together. And dont forget Chapter 3 of The 7 Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice is very good to read on the topic of power and unreliable research. The book is available in the University Library or can be bought at all reputable bookshops and online repositories. A really nice paper by Marjan Bakker and colleagues on whether people put power analyses in their ethics proposals. It has a nice introduction on power and the results show how many different ways researchers actually calculate sample sizes https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079 A paper by Marcus Munafo and colleagues that we have mentioned many times but it might help to read again now as the concepts and ideas will be more famility now. https://www.nature.com/articles/s41562-016-0021 A paper by Schafer and Schwarz (2019) aimed at helping people make meaningful interpretation of effect sizes in Psychology. It also explores differences of commonly found effect sizes in sub-disciplines of Psychology. https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/full Finally, a paper by Brysbaert (2019) that shows just how many participants you would need in a variety of common designs in Psychology studies. You will be shocked by the difference between the number of participants needed compared to the number of participants used in published studies https://www.journalofcognition.org/article/10.5334/joc.72/ Job Done - Activity Complete! Hopefully this has given you a good basis to understanding power, sample sizes, alpha, and effect sizes. These are difficult concepts to grasp and it will take a lot of time thinking about them and interacting with them before they really start to sink in. Hopefully however, if nothing else, the least you come away with is the idea that the number of participants you should run in a study is not an arbitrary decision but is in fact a relationship between the effect size you want to test for and the level of error (Type I or Type II) you are willing to accept. As always, the best way to understand something is to put it into your own words so dont forget to go back and add any informative points to your Portfolio. Post any questions on the available forums for discussion or ask a member of staff if you are unsure. 8.3 InClass Activity Hopefully you now have a decent understanding at least of the four APES that need to be considered when designing a study: \\(\\alpha\\), \\(power\\), effect size and sample size. We are going to look more at calculating and understanding these elements today. You dont have to fully understand everything about power to complete this chapter - believe us when we say many seasoned researchers struggle with parts - you just need to get the general gist that there is always a level of acceptable error in hypothesis testing and we are trying to minimise that for a given effect size (i.e. the magnitude of the difference, relationship, association). So lets jump into this a bit now and start running some analyses to help further our understanding of alpha, power, effect sizes and sample size! To help our understanding we will focus on t-tests for this chapter which you will know well from previous chapters. Effect Sizes - Cohens \\(d\\) There are a number of different effect sizes that you can choose to calculate but a common one for t-tests is Cohens d: the standardised difference between two means (in units of SD) and is written as d = effect-size-value. The key point is that Cohens d is a standardised difference, meaning that it can be used to compare against other studies regardless of how the measurement was made. Take for example height differences in men and women which is estimated at about 5 inches (12.7 cm). That in itself is an effect size, but it is an unstandardised effect size in that for every sample that you test, that difference is dependent on the measurement tools, the measurement scale, and the errors contained within them (Note: ask Helena about the time she photocopied some rulers). As such using a standardised effect size allows you to make comparisons across studies regardless of measurement error. In standardised terms, the height difference above is considered a medium effect size (d = .5) which Cohen (1988, as cited by Schafer and Schwarz (2019)) defined as representing an effect likely to be visible to the naked eye of a careful observer. Cohen (1988) in fact stated three sizes of Cohens d that people could use as a guide: Effect size Cohens d value small .2 to .5 medium .5 to .8 large &gt; .8 You may wish to read this paper later about different effect sizes in psychology - Schafer and Schwarz (2019) The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases. One thing to note is that the formula for Cohens d is slightly different depending on the type of t-test used. And even within a type of t-test the formula can sometimes change depending on who you read. For today, and this chapter, lets go with the following formulas: One-sample t-test &amp; within-subjects (paired-sample) t-test: \\[d = \\frac{t}{sqrt(N)}\\] Between-subjects (Independent) t-test: \\[d = \\frac{2t}{sqrt(df)}\\] Lets now try using these formulas in order to calculate the effect sizes for given scenarios; we will work up to calculating power later in the chapter. 8.3.1 Task 1: Effect size from a one-sample t-test You run a one-sample t-test and discover a significant effect, t(25) = 3.24, p = .003. Calculate d and determine whether the effect size is small, medium or large. Helpful Hint Use the appropriate formula from above for the one-sample t-tests. You have been given a t-value and df (degrees of freedom), you still need to determine n before you calculate d. According to Cohen (1988), the effect size is small (.2 to .5), medium (.5 to .8) or large (&gt; .8). Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter, in digits, how many people were run in this study: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be considered: small medium large 8.3.2 Task 2: Effect size from between-subjects t-test You run a between-subjects t-test and discover a significant effect, t(30) = 2.9, p = .007. Calculate d and determine whether the effect size is small, medium or large. Helpful Hint Use the appropriate formula above for between-subjects t-tests. remember that df = (N-1) + (N-1) for a between-subjects t-test. According to Cohen (1988), the effect size is small (.2 to .5), medium (.5 to .8) or large (&gt; .8). Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter, in digits, how many people were run in this study: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be considered: small medium large 8.3.3 Task 3: Effect Size from matched-pairs t-test You run a matched-pairs t-test between an ASD sample and a non-ASD sample and discover a significant effect t(39) = 2.1, p &lt; .05. How many people are there in each group? Calculate d and determine whether the effect size is small, medium or large. Helpful Hint You need the df value to determine N. A matched pairs is treated like a paired-sample t-test but with two separate groups. Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter, in digits, how many people were in each group in this study. Note, not the total number of participants: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be considered: small medium large Explain This - I dont understand the number of people in each group answer! df in a paired-samples and in a matched-pairs t-test is calculated as df = N - 1. Conversely, to find the total number of participants: N = df + 1 so N = 39 + 1 = 40. Given that this is a matched-pairs t-test, by design there has to be an equal number of participants in each group. Therefore 40 participants in each group. 8.3.4 Task 4: t-value and effect size for a between-subjects Experiment You run a between-subjects design study and the descriptives tell you: Group 1, M = 10, SD = 1.3, n = 30; Group 2, M = 11, SD = 1.7, n = 30. Calculate t and d for this between-subjects experiment. Helpful Hint Before you can calculate d (using the appropriate formula for a between-subjects experiment), you need to first calculate t using the formula: t = (Mean1 - Mean2)/sqrt((var1/n1) + (var2/n2)) var stands for variance in the above formula. Variance is not the same as the standard deviation, right? Variance is measured in squared units. So for this equation, if you require variance to calculate t and you have the standard deviation, then you need to remember that \\(var = SD^2\\) (otherwise written as \\(var = SD \\times SD\\). Now you have your t-value, but for calculating d you also need degrees of freedom. Think about how you would calculate df for a between-subjects experiment, taking n for both Group 1 and Group 2 into account. Remember that convention is that people report the t as a positive. As such, convention also dictates that d is reported as a positive value. Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter the correct t-value for this test, rounded to two decimal places: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Based on the above t-value above, enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be described as: small medium large Excellent! Now that you are comfortable with calculating effect sizes, we will look at using them to establish the sample size for a required power. One thing you will realise as we progress is that the true effect size in a population is something we do not know, but we need to justify one for our design. A clever approach is laid out by Daniel Lakens in the blog from the PreClass on the Smallest Effect Size of Interest (SESOI) - you set the smallest effect that you would be interested in! This can be determined through theoretical analysis, through previous studies, through pilot studies, or through rules of thumb like Cohen (1988). However, also keep in mind that the lower the effect size, the larger the sample size you will need. Everything is a trade-off. Power Calculations Today we are going to use the function pwr.t.test() to run our calculations from the pwr library. This is a really useful library of functions for various tests but we will just use it for t-tests right now. If you are using the Boyd Orr machines the pwr package is already installed and you will just need to call it like all other packages, e.g. library(pwr). Do not attempt to install it yourself on the Boyd Orr machines. If you are using your own laptop then feel free to install it. Remember that for more information on the function pwr.t.test(), simply do ?pwr.t.test in the console. Or you can have a look at these webpages to get in idea (or bad ideas if you spot where they erroneously calculate post-hoc power!): A quick-R summary of the pwr package - https://www.statmethods.net/stats/power.html the pwr package vignette - https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html From these you will see that pwr.t.test() takes a series of inputs: n - observations/participants, per group for the independent samples version, or the number of subjects or matched pairs for the paired and one-sample designs. d - the effect size of interest sig.level or \\(\\alpha\\) power or \\(1-\\beta\\) type - the type of t-test; e.g. \"two.sample\", \"one.sample\", \"paired\" alternative - the type of hypothesis; \"two.sided\", \"one.sided\" And it works on a leave one out principle. You give it all the info you have and it returns the element you are missing. So, for example, say you needed to know how many people per group you would need to detect an effect size as low as d = .4 with power = .8, alpha = .05 in a two.sample (between-subjects) t-test on a two.sided hypothesis test. You would do: pwr.t.test(d = .4, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) Which will show you the following output, which, if you look at it, tells you that you need 99.0803248 people per condition. ## ## Two-sample t test power calculation ## ## n = 99.08032 ## d = 0.4 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group But you only get whole people and we like to be conservative on our estimates so we would actually run 100 per condition. That is a lot of people!!! One problem though is that the output of the pwr.t.test() is an object and not that easy to work with in terms of getting values out from it to be reproducible. However, we have already seen a function in Chapter 5 that we can use to pluck values from objects - purr::pluck(). And the code would look like this n_test &lt;- pwr.t.test(d = .4, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) %&gt;% pluck(&quot;n&quot;) So when we call n_test we get the same answer as above, but it is saved as a single value and easier to work with: n_test ## [1] 99.08032 And we could use the ceiling() funtion to round up to whole people: n_test %&gt;% ceiling() ## [1] 100 Note: ceiling() is better to use than round() when dealing with people as it always rounds up. For example, ceiling(1.1) gives you 2. round() on the other hand is useful for rounding an effect size, for example, to two decimal places - e.g. d = round(.4356, 2) would give you d = 0.44 We will use this approach pwr.t.test() %&gt;% pluck() and pwr.t.test() %&gt;% pluck() %&gt;% ceiling() throughout the rest of this chapter to get used to it. But before you start with this next task, you will need to make sure you have loaded in the tidyverse. 8.3.5 Task 5: Sample size for standard power one-sample t-test Assuming the smallest effect size of interest is a Cohens d of d = .23, what would be the minimum number of participants you would need in a one-sample t-test, assuming \\(power = .8\\), \\(\\alpha = .05\\), on a two-sided hypothesis? Using a pipeline, store the answer as a single value called sample_size (e.g. think pluck()) and round up to the nearest whole participant. Helpful Hint Use the list of inputs above as a kind of checklist to clearly determine which inputs are known or unknown. This can help you enter the appropriate values to your code. The structure of the pwr.t.test() would be very similar to the one shown above except two.sample would become one.sample You will also need to use pluck(n) to help you obtain the sample size and %&gt;% ceiling() to round up to the nearest whole participant. Quickfire Questions Answer the following question to check your answers. The solutions are at the end of the chapter to check against: Enter the minimum number of participants you would need in this one-sample t-test: 8.3.6 Task 6: Effect size from a high power between-subjects t-test Assuming you run a between-subjects t-test with 50 participants per group and want a power of .9, what would be the minimum effect size you can reliably detect? Assume the field standard \\(\\alpha = .05\\) and alternative hypothesis settings (two-tailed). Using a pipeline, store the answer as a single value called cohens and round to two decimal places. Helpful Hint Again, use the list of inputs above as a kind of checklist to clearly determine which inputs are known or unknown. This can help you enter the values to your code. You will also need to use pluck() to obtain Cohens d, and round() so the value is rounded to two decimal places. Dont forget the quotes when using pluck(). i.e. pluck(value) and not pluck(value) Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Based on the information given, what will you set type as in the function? one.sample two.sample Based on the output, enter the minimum effect size you can reliably detect in this test, rounded to two decimal places: According to Cohen (1988), the effect size for this t-test is small medium large Say you run the study and find that the effect size determined is d = .50. Given what you know about power, select the statement that is most accurate: the study is sufficiently powered as the analysis indicates you can only reliably detect effect sizes smaller than d = .65 the study is potentially underpowered as the analysis indicates you can really only reliably detect effect sizes larger than d = .65 8.3.7 Task 7: Power of Published Research Thus far we have used hypothetical situations - now go look at the paper on the Open Stats lab website called Does Music Convey Social Information to Infants?. You can download the pdf and look at it, but here we will determine the power of the significant t-tests reported in Experiment 1 under the Results section on Pg489. There is a one-sample t-test and a paired-samples t-test to consider, summarised below. Assume testing was at power = .8, alpha = .05. Based on your calculations are either of the stated effects underpowered? one-sample: t(31) = 2.96, p = .006 paired t-test: t(31) = 2.42, p = .022 Helpful Hint A one-sample t-test and a paired t-test use the same formula for Cohens d. To calculate n: n = df + 1. Calculate the achievable Cohens d for the studies and then calculate the established Cohens d for the studies. Group Discussion Point Based on what you have found out, think about the following questions and discuss them in your groups: Which of the t-tests do you believe to be potentially underpowered? Why do you think this may be? Additional information about this discussion can be found in the solutions at the end of this chapter. One caveat to Tasks 6 and 7: We have to keep in mind that here we are looking at single studies using one sample from a potentially huge number of samples within a population. As such there will be a degree of variance in the true effect size within the population regardless of the effect size of one given sample. What that means is we have to be a little bit cautious when making claims about a study. Ultimately the higher the power the better as you can detect smaller effect sizes! Job Done - Activity Complete! Great! So hopefully you are now starting to see the interaction between alpha, power, effect sizes, and sample size. We should always want high powered studies and depending on the size of the effect we are interested in (small to large), and our \\(\\alpha\\) level, this will determine the number of observations we need to make sure our study is well powered. Points to note: Lowering the \\(\\alpha\\) level (e.g. .05 to .01) will reduce the power. Lowering the effect size (e.g. .8 to .2) will reduce the power. Increasing power (.8 to .9) will require more participants. It is also possible to increase power for a fixed sample size by reducing sources of noise in the study. A high-powered study looking to detect a small effect size at a low alpha may require a large number of participants! Another point probably to consider for the future: what about studies with multiple observations per participant? How do you calculate power for this? This is a very common situation. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the forums! 8.4 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. Lab 8: APES Assignment In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester1_Lab8.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top Download the assignment zip file from here. NOTE: in nearly all of the problems below, you will need to replace NULL with a value or a pipeline of code that computes a value. Please pay special attention as to what the question is asking for as the output, e.g. value or a tibble; when asked for a value as an output, make sure it is a single value and not a value stored in a tibble. Finally, when altering code inside the code blocks, please do not re-order or rename the code blocks (T1, T2,  etc.). If you do, this may impact your grade! Its also recommended that you Knit a report to be able to see what youve accomplished and spot potential errors. A great thing to do is close the whole programme, restart it, and then knit your code. This will test whether you have remembered to include essential elements, such as libraries, in your code. APES: Alpha, Power, Effect Size, and Sample Size In the lab we have been looking at the interplay between the four components of Alpha, Power, Effect Size, and Sample Size. This is a very important part of experimental design to understand as it will help you understand which studies are worth paying attention to and it will help you design your own studies in the coming years so that you know just how many people to run and what to make of the effect that you find. If you have not yet done so, we highly recommend reading the blog suggested as PreClass reading material and carrying out the activities in the inclass activity. These will help you will both the practicalities and the interpretation of the following assignment. Remember that this assignment is formative but the knowledge gained from the practical activities in this lab will be super important to your future-self! Before starting lets check: The .Rmd file is saved in your working directory. For assessments we ask that you save it with the format GUID_Level2_Semester1_Lab8.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. Libraries You will need to use the tidyverse and broom libraries in this assignment, so load them in the library code chunk below. Hint: library(package) Basic Calculations 8.4.1 Assignment Task 1 You set up a study so that it has a power value of \\(power = .87\\). To two decimal places, what is the Type II error rate of your study? Replace the NULL in the T1 code chunk below with either a single value, or with mathematical notation, so that error_rate returns the actual value of the Type II error rate for this study. By mathematical notation we mean you to use the appropriate formula but insert the actual values. error_rate &lt;- NULL 8.4.2 Assignment Task 2 You run an independent t-test and discover a significant effect, t(32) = 3.26, p &lt; .05. Using the appropriate formula, given in the inclass activity, calculate the effect size of this t-test. Replace the NULL in the T2 code chunk below with mathematical notation so that effect1 returns the value of the effect size. Do not round the value. effect1 &lt;- NULL 8.4.3 Assignment Task 3 You run a dependent t-test and discover a significant effect, t(43) = 2.24, p &lt; .05. Using the appropriate formula, given in the inclass activity, calculate the effect size of this t-test. Replace the NULL in the T3 code chunk below with mathematical notation so that effect2 returns the value of the effect size. Do not round the value. effect2 &lt;- NULL Using the Power function 8.4.4 Assignment Task 4 Replace the NULL in the T4 code chunk below with a pipeline combining pwr.t.test(), pluck() and ceiling(), to determine how many participants are needed to sufficiently power a paired-samples t-test at \\(power = .9\\) with \\(d = .5\\)? Assume a two-sided hypothesis with \\(\\alpha = .05\\). Ceiling the answer to the nearest whole participant and store this value in participants. Hint: Remember the quotes on the pluck participants &lt;- NULL 8.4.5 Assignment Task 5 Using a pipeline similar to Task 4, what is the minimum effect size that a one-sample t-test study (two-tailed hypothesis) could reliably detect given the following details : \\(\\beta = .16, \\alpha = 0.01, n = 30\\). Round to two decimal places and replace the NULL in the T5 code chunk below to store this value in effect3. Hint: Remember you are going to round() and not ceiling() effect3 &lt;- NULL 8.4.6 Assignment Task 6 Study 1 You run a between-subjects study and establish the following descriptives: Group 1 (M = 5.1, SD = 1.34, N = 32); Group 2 (M = 4.4, SD = 1.27, N = 32). Replace the NULL in the T6 code chunk below with the following formula, substituting in the appropriate values, to calculate the t-value of this test. Calculate as Group1 minus Group2. Store the t-value in tval. Do not round tval and do not include the t = part of the formula. \\[ t = \\frac {{\\bar{x_{1}}} - \\bar{x_{2}}}{ \\sqrt {\\frac {{s_{1}}^2}{n_{1}} + \\frac {{s_{2}}^2}{n_{2}}}}\\] tval &lt;- NULL 8.4.7 Assignment Task 7 Using the tval calculated in Task 6, calculate the effect size of this study and store it as d1 in the T7 code chunk below, replacing the NULL with the appropriate formula and values. Do not round d1. Hint: Think between-subjects d1 &lt;- NULL 8.4.8 Assignment Task 8 Assuming \\(power = .8\\), \\(\\alpha =.05\\) on a two-tailed hypothesis, based on the d1 value in Task 7 and the smallest achievable effect size of this study, which of the below statements is correct. The smallest effect size that this study can determine is d = .71. The detected effect size, d1, is larger than this and as such this study is potentially suitably powered The smallest effect size that this study can determine is d = .17. The detected effect size, d1, is larger than this and as such this study is potentially suitably powered The smallest effect size that this study can determine is d = .17. The detected effect size, d1, is smaller than this and as such this study is potentially suitably powered The smallest effect size that this study can determine is d = .71. The detected effect size, d1, is smaller than this and as such this study is potentially not suitably powered Replace the NULL in the T8 code chunk below with the number of the statement that is a true summary of this study. It may help you to calculate and store the smallest achievable effect size of this study in poss_d. Hint: use poss_d to calculate the smallest possible effect size of this study to help you answer this question. poss_d &lt;- NULL answer_T8 &lt;- NULL 8.4.9 Assignment Task 9 Study 2 Below is a paragraph from the results of Experiment 4 from Schroeder, J., &amp; Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidates appeal. Psychological Science, 26, 877-891. We saw this paper in Lab 5 but you can find out more details at &lt;a href=https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities, target = \"_blank\"&gt;Open Stats Lab. Recruiters believed that the job candidates had greater intellect - were more competent, thoughtful, and intelligent - when they listened to pitches (M = 5.63, SD = 1.61, n = 21) than when they read pitches (M = 3.65, SD = 1.91, n = 18), t(37) = 3.53, p &lt; .01, 95% CI of the difference = [0.85, 3.13], d1 = 1.16. The recruiters also formed more positive impressions of the candidates - rated them as more likeable and had a more positive and less negative impression of them - when they listened to pitches (M = 5.97, SD = 1.92) than when they read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p &lt; .01, 95% CI of the difference = [0.55, 3.24], d2 = 0.94. Finally, they also reported being more likely to hire the candidates when they listened to pitches (M = 4.71, SD = 2.26) than when they read the same pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p &lt; .01, 95% CI of the difference = [0.41, 3.24], d3 = 0.86. Using the pwr.t.test() function, what is the minimum effect size that this paper could have reliably detected? Test at \\(power = .8\\) for a two-sided hypothesis. Use the \\(\\alpha\\) stated in the paragraph and the smallest n stated; store the value as effect4 in the T9 code chunk below. Replace the NULL with your pipeline and round the effect size to two decimal places. effect4 &lt;- NULL 8.4.10 Assignment Task 10 Given the value of effect4 calculated in Task 9, and the stated alpha in the paragraph and the smallest n of the two groups, which of these statements is true. This study has enough power to reliably detect effects at the size of d3 and larger. This study has enough power to reliably detect effects at the size of only d1. This study has enough power to reliably detect effects at the size of d2 and larger, but not d3. This study does not have enough power to reliably detect effect sizes at d1 or lower. Replace the NULL in the T10 code chunk below with the number of the statement that is TRUE, storing the single value in answer_t10. answer_t10 &lt;- NULL 8.4.11 Assignment Task 11 Last but not least: Read the following statements. In general, increasing sample size will increase the power of a study. In general, smaller effect sizes require fewer participants to detect at \\(power = .8\\). In general, lowering alpha (from .05 to .01) will decrease the power of a study. Now look at the below four summary statements of the validity of the statements a, b and c. Statements a, b and c are all TRUE. Statements a and c are both TRUE. Statements b and c are both TRUE. None of the statements are TRUE. Replace the NULL in the T11 code chunk below with the number of the statement that is correct, storing the single value in answer_t11. answer_t11 &lt;- NULL 8.4.12 The pwr package An alternative solution to Task 9 would be to use the pwr.t2n.test() function from the pwr package (Champely 2020). This would allow you to enter the n of both groups as there is an n1 and an n2 argument. Were you to use this, entering n1 = 18, n2 = 21, alpha = .01, the d drops just a little, changing the interpretation of Task 10. Feel free to try this analysis and see if you can figure out what would be the alternative answer to Task 10. Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solution file which can be found at the end of this chapter. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers, it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the forums. 8.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 8.5.1 InClass Activities 8.5.1.1 InClass Task 1 d &lt;- 3.24 / sqrt(25 +1) Giving an effect size of d = 0.64 and as such a medium to large effect size according to Cohen (1988) Return to Task 8.5.1.2 InClass Task 2 d &lt;- (2*2.9) / sqrt(30) Giving a effect size of d = 1.06 and as such a large effect size according to Cohen (1988) Return to Task 8.5.1.3 InClass Task 3 N = 39 + 1 d &lt;- 2.1 / sqrt(N) Giving an N = 40 and an effect size of d = 0.33. This would be considered a small effect size according to Cohen (1988) Return to Task 8.5.1.4 InClass Task 4 t = (10 - 11)/sqrt((1.3^2/30) + (1.7^2/30)) d = (2*t)/sqrt((30-1) + (30-1)) Giving a t-value of t = 2.56 and an effect size of d = 0.67. Remember that convention is that people tend to report the t and d as positive values. Return to Task 8.5.1.5 InClass Task 5 sample_size &lt;- pwr.t.test(d = .23, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;one.sample&quot;) %&gt;% pluck(&quot;n&quot;) %&gt;% ceiling() Giving a sample size of n = 151 Return to Task 8.5.1.6 InClass Task 6 cohens &lt;- pwr.t.test(n = 50, power = .9, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) %&gt;% pluck(&quot;d&quot;) %&gt;% round(2) Giving a Cohens d effect size of d = 0.65 Return to Task 8.5.1.7 InClass Task 7 Example 1 ach_d_exp1 &lt;- pwr.t.test(power = .8, n = 32, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot;, sig.level = .05) %&gt;% pluck(&quot;d&quot;) %&gt;% round(2) exp1_d &lt;- 2.96/sqrt(31+1) Giving an achievable effect size of 0.51 and they found an effect size of 0.52. This study seems ok as the authors could achieve an effect size as low as .51 and found an effect size at .52 Example 2 ach_d_exp2 &lt;- pwr.t.test(power = .8, n = 32, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;, sig.level = .05) %&gt;% pluck(&quot;d&quot;) %&gt;% round(2) exp2_d &lt;- 2.42/sqrt(31+1) Giving an achievable effect size of 0.51 and they found an effect size of 0.43. This effect might not be reliable given that the effect size found was much lower than the achievable effect size. The issue here is that the researchers established their sample size based on a previous effect size and not on the minimum effect size that they would find important. If an effect size as small as .4 was important then they should have powered all studies to that level and ran the appropriate n ~52 babies (see below). Flipside of course is that obtaining 52 babies isnt easy; hence why some people consider the Many Labs approach a good way ahead. ONE CAVEAT to the above is that before making the assumption that this study is therefore flawed, we have to keep in mind that this is one study using one sample from a potentially huge number of samples within a population. As such there will be a degree of variance in the true effect size within the population regardless of the effect size of one given sample. What that means is we have to be a little bit cautious when making claims about a study. Ultimately the higher the power the better. Below you could calculate the actual sample size required to achieve a power of .8: sample_size &lt;- pwr.t.test(power = .8, d = .4, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;, sig.level = .05) %&gt;% pluck(&quot;n&quot;) %&gt;% ceiling() Suggesting a sample size of n = 52 would be appropriate. Return to Task 8.5.2 Test Yourself Activities Libraries library(pwr) library(tidyverse) 8.5.2.1 Assignment Task 1 error_rate &lt;- 1 - .87 The Type II error rate of your study would be \\(\\beta\\) = 0.13. Return to Task 8.5.2.2 Assignment Task 2 effect1 &lt;- (2*3.26)/sqrt(32) The effect size would be d = 1.1525841 Return to Task 8.5.2.3 Assignment Task 3 effect2 &lt;- 2.24/sqrt(43+1) The effect size would be d = 0.3376927 Return to Task 8.5.2.4 Assignment Task 4 participants &lt;- pwr.t.test(power = .9, d = .5, sig.level = 0.05, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;) %&gt;% pluck(&quot;n&quot;) %&gt;% ceiling() Given the detailed scenario, the appropriate number of participants would be n = 44 Return to Task 8.5.2.5 Assignment Task 5 effect3 &lt;- power.t.test(power = 1-.16, n = 30, sig.level = 0.01, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(2) Given the detailed scenario, we would be able to detect an effect size of d = 0.69 Return to Task 8.5.2.6 Assignment Task 6 tval &lt;- (5.1 - 4.4) / sqrt((1.34^2/32) + (1.27^2/32)) Given the stated means and standard deviations, the t-value for this study would be t = 2.1448226 Return to Task 8.5.2.7 Assignment Task 7 d1 &lt;- (2*tval)/sqrt((32-1)+(32-1)) Given the t-value in Task 6, the effect size of this study would be d = 0.5447855. Return to Task 8.5.2.8 Assignment Task 8 poss_d &lt;- pwr.t.test(power = .8, n = 32, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;) %&gt;% pluck(&quot;d&quot;) %&gt;% round(2) answer_T8 &lt;- 4 The smallest effect size that this study can determine is d = 0.71. The detected effect size, d1, is smaller than this (d1 = 0.5447855) and as such this study is not suitably powered. Given that outcome, the 4th statement is the most suitable answer - answer_T8 = 4. Return to Task 8.5.2.9 Assignment Task 9 effect4 &lt;- pwr.t.test(power = .8, n = 18, sig.level = .01, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) %&gt;% pluck(&quot;d&quot;) %&gt;% round(3) The smallest stated n is n = 18 and the stated \\(\\alpha\\) is \\(\\alpha\\) = .01 Given these details, the minimum effect size that this paper could have reliably detected was d = 1.198 Return to Task 8.5.2.10 Assignment Task 10 answer_t10 &lt;- 4 This study does not have enough power to detect effect sizes at d1 or lower and as such answer_t10 = 4 However, it is worth keeping in mind that we are only looking at one study here which drew one sample from a population of samples. This means that there is always uncertainty about the true effect size of a difference or association - taking a different sample may have given a different effect size. As such, the comparison we are making here is not entirely valid and we should see it more as a reminder that we should always think of power as more in the planning of studies rather than in the search for criticism. Return to Task 8.5.2.11 Assignment Task 11 answer_t11 &lt;- 2 In general, increasing sample size will increase the power of a study whereas lowering alpha (from .05 to .01) will decrease the power of a study. As such, statements a and c, answer_t11 = 2. Return to Task Chapter Complete! 8.6 Additional Material Below is some additional material that might help you understand APES a bit more and some additional ideas. A different power function - power.t.test() First thing we wanted to mention was that you can still do this chapter if you dont have the pwr library installed. You could instead use the power.t.test() function which is a function available in base R, meaning that it is included when you install R, so you do not need to install any additional package. This is handy to know when you are using a computer that you cant install libraries on to. The pwr library offers more functions and is easier to follow but but for now lets just use the base function power.t.test(). Again, remember that for more information on this function, simply do ?power.t.test in the console. On doing this you will see that power.t.test() takes a series of inputs: n - observations/participants, per group for the independent samples version, or the number of subjects or matched pairs for the paired and one-sample designs. delta - the difference between means sd - standard deviation; note: if sd = 1 then delta = Cohens d sig.level or \\(\\alpha\\) power or \\(1-\\beta\\) type - the type of t-test; e.g. \"two.sample\", \"one.sample\", \"paired\" alternative - the type of hypothesis; \"two.sided\", \"one.sided\" And it works on a leave one out principle, just like in the main chapter - except there is the added inclusion of sd and delta instead of d. But other than those differences it is very similar to what we did in the main activities of the chapter. You give it all the info you have and it returns the element you are missing. So, returning to the example from the main chapter, say you needed to know how many people per group you would need to detect an effect size as low as d = .4 with power = .8, alpha = .05 in a two.sample (between-subjects) t-test on a two.sided hypothesis test. You would do: power.t.test(delta = .4, sd = 1, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) Which gives the following output: ## ## Two-sample t test power calculation ## ## n = 99.08057 ## delta = 0.4 ## sd = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group And it would tell you that you would need 99.080565 people per condition. Which matches the 100 per condition that we saw in the chapter. That really is a lot of people!!! And just in case you were wondering, you can also use pluck() to pull out individual values as follows: n_test &lt;- power.t.test(delta = .4, sd = 1, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) %&gt;% pluck(&quot;n&quot;) And when you call n_test You again see: ## [1] 99.08057 So hopefully that shows you an alternative if there is an issue with your pwr library. We would recommend using the pwr library where possible. Cohens d to r As we said in the chapter there are actually a lot of different effect sizes that you could calculate and that Cohens d is only one of them. An alternative is \\(r\\). You will see \\(r\\) a lot more as we progress through the book, and in particular around correlations, but it is fair to say that it is becoming more of a standard effect size for t-test as well. One thing that people do not like about Cohens d is that it can actually be a very large number - well above 1 - and that can be difficult to compare across studies. \\(r\\) on the other hand cant go above 1 and is therefore is considered easier to compare to across studies. The good news is that \\(r\\) and Cohens d can be calculated from each other using the below formulas: \\(r = \\frac{d}{\\sqrt(d^2 + 4)}\\) and \\(d = \\frac{2 \\times r}{\\sqrt(1-r^2)}\\) You can present either for a t-test in the format of: t(df) = t-value, p = p-value, d = d-value or t(df) = t-value, p = p-value, r = r-value How to choose an effect size A really quick analogy from Ian Walkers Research Methods and statistics, is say your test is not a stats test but a telescope. And say you have a telescope that is specifically designed only for spotting animals that are the size of elephants or larger (similar to saying a cohens d of .8 or greater for example - very big effect). If your telescope can only reliably detect something down to the size of an elephant but when you look through it you see something smaller that you think might be a mouse, you cant say that the object\" is definitely is a mouse as you dont have enough power in your telescope - it is too blurry. But likewise you cant rule out that it isnt a mouse as that would be something you dont know for sure - both of these are true because your telescope was only designed to spot things the size of an elephant or larger. You only bought a telescope that was able to spot elephants because that was all your were interested in. Had you been interested in spotting mice you would have had to have bought a more powerful telescope. And that is the point of Lakens SESOI - you power to the minimum effect size (minimum object size) you would be interested in. This is why it is imperative that you decide before your study what effect you are interested in - and you can base this on previous literature or theory. Interpreting and writing up power A few points on interpreting power to consolidate things a bit. Firstly, it is great that you are now thinking about power and effect sizes in the first place. It is important that this becomes as second nature as thinking about the design of your study and in future years and future studies the first question you should ask yourself when designing your study/secondary analysis is what size are my APES - Alpha, Power, Effect Size and Sample. And remember that a priori power analysis is the way ahead. The power and alpha are determined in advance of the study and you are using them to determine the effect size or the sample size. Power is stated more and more commonly again in papers now and you will start to notice it in the Methods or Results sections. You will see something along the lines of Based on a power =.. and alpha =., given we had X voices in our sample, a power analysis (pwr package citation) revealed d =  as the minimum effect sizes we could reliably determine. But how do you interpret a study in terms of power? Well, lets say you run a power analysis for a t-test (or for a correlation), and you set the smallest effect size of interest as d = .4 (or the equivalent r-value). If you then run your analysis and find d = .6 and the effect is significant, then your study had enough power to determine this effect. The effect that you found was bigger than the effect you could have found. You can have some confidence that you have a reliable effect at that given power and alpha values. However, say that instead of d = .6 you found a significant effect but with an effect size just below .4, say d = .3 - the effect size you found is smaller than the smallest effect you could reliably find. In this case you have to be cautious as you are still unclear as to whether there actually is an effect or whether you have found an effect by chance due to your study not having enough power to reliably detect an effect size that small. You cant say for sure that there is an effect or that there isnt an effect. You need to consider both stances in your write up. Remember though that you have sampled a population, so how representative that sample is of your population will also influence the validity of your power. Each sample will give a slightly different effect size. Alternatively, and probably quite likely in many undergraduate projects due to time constraints, say you find a non-significant effect at an effect size smaller than what you predicted; say you find a non-significant effect with an effect size of d = .2 and your power analysis said you could only reliably detect an effect as small as d = .4. The issue you have here is that you cant determine solely based on this study if you a) have a non-significant effect because you are under powered or b) that you have a non-significant effect because there is actually no effect in the first place. Again in your discussion you would need to consider both stances. What you can however say is that the effect that you were looking for is not any bigger than \\(d = .4\\). That is still useful information. Ok you dont know how small the effect really is, but you can rule out any effect size bigger than your original d-value. In turn this helps future researchers plan their studies better and can guide them better in knowing how many participants to run. See how useful it would be if we published null findings! Basically, when your test finds an effect size smaller than you can detect, you dont know what it but you know what it isnt - we arent sure if it is a mouse but we know it is not an elephant. Instead you would use previous findings to support the object being a mouse or not but caveat the conclusion with the suggestion that the test isnt really sensitive to finding a mouse. Similar to a finding that has an effect size smaller than you can detect. You can use previous literature to support their not being an effect but you cant rule it out for sure. You might have actually found an effect had you had a more powerful test. Just like you might have been able to determine that it was a mouse had you had a more powerful telescope. Taking this a bit further in some studies there really is enough power (in terms of N - say a study of 25000 participants) to find a flea on the proverbial mouse, but where nevertheless there is a non-significant finding. In this case you have the fortunate situation where you have a well-powered study and so can say with some degree of confidence that your hypothesis and design is unlikely to ever produce a replicable significant result. That is probably about as certain as you can get in our science or as close as you can get to a fact, a very rare and precious thing. However, incredibly high powered studies, with lots of participants, tend to be able to find any difference as a significant difference. A within-subjects design with 10000 participants (\\(power = .8, \\alpha = .05\\)) can determine reliably detect an incredibly small effect size of d = .04. The question at that stage is whether that effect has any real world significance or meaning. So the take-home message here is that your discussion should always consider the result in relation to the hypothesis, integrating previous research and theory, and if there is an additional issue of power, then your discussion could also consider the result in relation to whether you can truly determine the effect and how that might be resolved (e.g. re-assessing the effect size, changing the design (withins are more powerful), low sample, power to high (e.g. .9), alpha to low (e.g. .01)). This issue of power would probably be a small part in the generalisability/limitation section. Note: In all of the above you can swap effect and relationship, d and r, and other analyses accordingly. End of Additional Material! References "],["reflection-chpts-1-9.html", "Lab 9 Reflection - Chpts 1-9 9.1 Overview 9.2 PreClass Activity 9.3 InClass Activity 9.4 Assignment 9.5 Solutions to Questions", " Lab 9 Reflection - Chpts 1-9 9.1 Overview We have covered a lot of material in these first few chapters and now would be a good time to stop, recap, and reflect on what we have learnt. As such, this chapter is more about looking back at what you have learnt, testing your skills, resolving issues, and looking at other cool applications of R that have not been covered in this lab series. 9.2 PreClass Activity As we are reflecting on what we have covered so far, your preclass activities this time are: Review the previous chapters and note any issues you have with the elements covered - both in terms of concepts and code. Post these issues on available discussion channels and we can look at them together at our next meeting. 9.3 InClass Activity Like the PreClass, we want to spend some time reflecting on what we have learnt and as such this InClass is about looking at ideas, concepts, and codes, that you have had issue with and seeing if we can resolve those issues. In class, we will spend some time looking at any issues you have had along the way. We will also look at some other interesting things you can do in R should you wish to expand your own knowledge and skills, such as: Popping out the Source Window to make working easier - Using Source Windows Analysing Twitter data with the rtweet package (R-rtweet?) Animating plots with the ggganimate package (R-gganimate?) Creating quickfire quizzes with the webex (Barr and DeBruine 2021) Make your own memes using the meme package (R-meme?) The Hex Sticker Memory game and the background behind it Creating interactive plots using ggplot and plotly Even funkier visualtions using the ggforce package - check out facet_zoom() (R-ggforce?) Many diverse fields are now using R and this is a good example: R for Journalists Using the knitr::read_chunk() function to call R script code through R Markdown (Xie 2021) The statcheck package for checking that you and others are presenting their inferential analyses accurately (in terms of df and p-value) and in APA format (R-statcheck?) A great repository of R resources for teaching, learning, writing webpages, CVs, and doing very cool things with webpages 9.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 9.5 Solutions to Questions Instructions on how to access the solution to this lab will be made available during the course. References "],["correlations.html", "Lab 10 Correlations 10.1 Overview 10.2 PreClass Activity 10.3 InClass Activity 10.4 Assignment 10.5 Solutions to Questions 10.6 Additional Material", " Lab 10 Correlations 10.1 Overview As you will read in Miller and Haden (2013) as part of the preclass reading, correlations are used to detect and quantify relationships among numerical variables. In short, you measure two variables and the correlation analysis tells you whether or not they are related in some manner - positively (i.e. one increases as the other increases) or negatively (i.e. one decreases as the other increases). Schematic examples of three extreme bivariate relationships (i.e. the relationship between two (bi) variables) are shown below: Figure 10.1: Schematic examples of extreme bivariate relationships However, unfortunately, you may have heard people say lots of negative things about correlations: they do not prove causality they suffer from the bidirectional problem (A vs B is the same as B vs A) any relationship found may be the result of an unknown third variable (e.g. murders and ice-creams) But whilst these are true, correlations are an incredibly useful and commonly used measure in the field, so dont be put off from using them or designing a study that uses correlations. In fact, they can be used in a variety of areas. Some examples include: looking at reading ability and IQ scores such as in Miller and Haden Chapter 11, which we will also look at today; exploring personality traits in voices (see Phil McAleers work on Voices); or traits in faces (see Lisa DeBruines work); brain-imaging analysis looking at activation in say the amygdala in relation to emotion of faces (see Alex Todorovs work at Princeton); social attidues, both implicit and explicit, as Helena Paterson will discuss in her Social lectures this semester; or a whole variety of fields where you simply measure the variables of interest. To actually carry out a correlation is very simple and we will show you that in a little while: you can use the cor.test() function. The harder part of correlations is really wrangling the data (which youve learned to do in the first part of this book) and interpreting the data (which we will focus more on this chapter). So, we are going to run a few correlations, showing you how to do one, and asking you to peform others to give you some good practice at running and interpreting the relationships between two variables. Note: When dealing with correlations it is better to refer to relationships and NOT predictions. In a correlation, X does not predict Y, that is really more regression which we will look at later on the book. In a correlation, what we can say is that X and Y are related in some way. Try to get the correct terminology and please feel free to pull us up if we say the wrong thing in class. It is an easy slip of the tongue to make! In this lab we will: Introduce the Miller and Haden book, which we will be using throughout the rest of the book Show you the thought process of a researcher running a correlational analysis. Give you practice in running and writing up a correlation. 10.2 PreClass Activity The majority of the PreClass activities from now on will involve reading a chapter or two from Miller and Haden (2013) and trying out a couple of tasks. This is an excellent free textbook that we will use to introduce you to the General Linear Model, a model that underlies all the majority of analyses you have seen and will see this year - e.g. t-tests, correlations, ANOVAs (Analysis of Variance), Regression are all related and are all part of the General Linear Model (GLMs). We will start off this chapter some introduction to correlations. Have a read at the chapter, use the visualiser, play the game, and we will see you in the lab. 10.2.1 Read Chapter Read Chapters 10 and 11 of Miller and Haden (2013). Both chapters are really short but give a good basis to understanding correlational analysis. Please note, in Chapter 10 you might not know some of the the terminology yet, e.g. ANOVA means Analysis of Variance and GLM means General Linear Model (Reading Chapter 1 of Miller and Haden might help). We will go into depth on these terms in the coming chapters. 10.2.2 Watch Visualisation Have a look at this visualisation of correlations by Kristoffer Magnusson: https://rpsychologist.com/d3/correlation/. After having read Miller and Haden Chapter 11, use this visualisation page to visually replicate the scatterplots in Figures 11.3 and 11.4 - use a sample of 100. After that, visually replicate the scatterplots in Figure 11.5. Each time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of relationship between the two variables. The greater the shared variance, the stronger the relationship. Also, try setting the correlation to r = .5 and then moving a sinlge dot to see how one data point, a potential outlier, can change the stated correlation value between two variables 10.2.3 Play Guess the correlation Now that you are well versed in interpreting scatterplots (scattergrams) have a go at this online app on guessing the correlation: https://www.rossmanchance.com/applets/GuessCorrelation.html. This is a very basic applet that allows you to see how good you are at recognising different correlation strengths from the scatterplots. We would recommend you click the Track Performance tab so you can keep an overview of your overall bias to underestimate or overestimate a correlation. Is this all just a bit of fun? Well, yes because stats is actually fun, and no, becuase it serves a purpose of helping you determine if there correlations you see in your own data are real, and to help you see if correlations in published research match with what you are being told. As you will from the above examples one data point can lead to a misleading relationship and even what might be considered a medium to strong relationship may actually have only limited relevance in the real world. One only needs to mention Anscombes Quartet to be reminded of the importance of visualising your data. Anscombe (1973) showed that four sets of bivariate data (X,Y) that have the exact same means, medians, and relationships: Table 10.1: Four bivariate datasets that have the same means, medians, standard deviations, and relationships Group meanX_rnd sdX_rnd meanY_rnd sdY_rnd corrs_rnd I 9 3.317 7.501 2.032 0.816 II 9 3.317 7.501 2.032 0.816 III 9 3.317 7.500 2.030 0.816 IV 9 3.317 7.501 2.031 0.817 Can look very different when plotted: ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.2: Though all datasets have a correlation of r = .82, when plotted the four datasets look very different. Grp I is a standard linear relationship where a pearson correlation would be suitable. Grp II would appear to be a non-linear relationship and a non-parametric analysis would be appropriate. Grp III again shows a linear relationship (approaching r = 1) where an outlier has lowered the correlation coefficient. Grp IV shows no relationship between the two variables (X, Y) but an oultier has inflated the correlation higher. All in this is a clear example of where you should visualise your data and not to rely on just the numbers. You can read more on Anscombes Quartet in your own time, with wikipedia (https://en.wikipedia.org/wiki/Anscombe%27s_quartet) offering a good primer and the data used for the above example. Job Done - Activity Complete! 10.3 InClass Activity We are going to jump straight into this one! To get you used to running a correlation we will use the examples you read about in Miller and Haden (2013), Chapter 11, looking at the relationship between four variables: reading ability, intelligence, the number of minutes per week spent reading at home, and the number of minutes per week spent watching TV at home. This is again a great example of where a correlation works best because it would be unethical to manipulate these variables so measuring them as they exist in the environment is most appropriate. Click here to download the data for today. 10.3.1 Task 1 - The Data After downloading the data folder, unzip the folder, set your working directory appropriately, open a new script, and load in the Miller and Haden data (MillerHadenData.csv), storing it in a tibble called mh. As always, the solutions are at the end of the chapter. Note 1: Remember that in reality you could store the data under any name you like but to make it easier for a demonstrator to debug with you it is handy if we all use the same names. Note 2: You will find that the instructions for tasks are sparser from now on, compared to earlier in the book. We want you to really push yourself and develop the skills you learnt in Semester 1. Dont worry though, we are always here to help, so if you get stuck, ask! Note 3: You will see an additional data set on vaping. We will use that later on in this chapter so you can ignore it for now. Task 1: Hints for loading in Data Hint 1: We are going to need the following libraries: tidyverse, broom Hint 2: mh &lt;- read_csv() Remember that, in this book, we will always ask you to use read_csv() to load in data. Avoid using other functions such as read.csv(). Whilst these functions are similar, they are not the same. Lets look at your data in mh - we showed you a number of ways to do this in Semester 1. As in Miller and Haden, we have 5 columns: the particpant (Participant), Reading Ability (Abil), Intelligence (IQ), number of minutes spent reading at home per week (Home), and number of minutes spent watching TV per week (TV). For the lab we will focus on the relationship between Reading Ability and IQ but for further practice you can look at other relationships in your free time. A probable hypothesis for today could be that as Reading Ability increases so does Intelligence (but do you see the issue with causality and direction). Or phrasing the alternative hypothesis (\\(H_1\\)) more formally, we hypothesise that the reading ability of school children, as measured through a standardized test, and intelligence, again measured through a standardized test, show a linear positive relationship. This is the hypothesis we will test today but remember that we could always state the null hypothesis (\\(H_0\\)) that there is no relationship between reading ability and IQ. First, however, we must check some assumptions of the correlation tests. As we have stated that we hypothesise a linear relationship, this means that we are going to be looking at the Pearsons product-moment correlation which is often just shortened to Pearsons correlation and is symbolised by the letter r. The main assumptions we need to check for the Pearsons correlation, and which we will look at in turn, are: Is the data interval, ratio, or ordinal? Is there a data point for each participant on both variables? Is the data normally distributed in both variables? Does the relationship between variables appear linear? Does the spread have homoscedasticity? Assumption 1: Level of Measurement 10.3.2 Task 2 - Interval or Ordinal Group Discussion Point If we are going to run a Pearson correlation then we need interval or ratio data. An alternative correlation, a non-parametric equivalent of the Pearson correlation is the Spearman correlation and it can run with ordinal, interval or ratio data. What type of data do we have? Discuss with your group for a few minutes and then answer the following question. A discussion is in the solutions at the end of the chapter. Check your thinking: the type of data in this analysis is most probably ratio interval ordinal nominal as the data is continuous discrete and there is unlikely to be a true zero Hints on data type are the variables continuous? is the difference between 1 and 2 on the scale equal to the difference between 2 and 3? Assumption 2: Pairs of Data All correlations must have a data point for each participant in the two variables being correlated. This should make sense as to why - you cant correlate against an empty cell! Check that you have a data point in both columns for each participant. After that, try answering the question in Task 3. A discussion is in the solutions at the end of the chapter. Note: You can check for missing data by visual inspection - literally using your eyes. A missing data point will show as a NA, which is short for not applicable, not available, or no answer. An alternative would be to use the is.na() function. This can be really handy when you have lots of data and visual inspection would just take too long. If for example you ran the following code: is.na(mh) If you look at the output from that function, each FALSE tells you that there is a data-point in that cell. That is because is.na() asks is that cell a NA; is it empty. If the cell was empty then it would come back as TRUE. As all cells have data in them, they are all showing as FALSE. If you wanted to ask the opposite question, is their data in this cell, then you would write !is.na() which is read as is not NA. The exclamation mark ! turns the question into the opposite. 10.3.3 Task 3 - Missing Data It looks like everyone has data in all the columns but lets test our skills a little whilst we are here. Answer the following questions: How is missing data represented in a cell of a tibble? an empty cell NA a large number don't know Which code would leave you with just the participants who were missing Reading Ability data in mh: filter(mh, is.na(Ability) filter(mh, is.na(Abil) filter(mh, !is.na(Ability) filter(mh, !is.na(Abil) Which code would leave you with just the participants who were not missing Reading Ability data in mh: filter(mh, is.na(Ability) filter(mh, is.na(Abil) filter(mh, !is.na(Ability) filter(mh, !is.na(Abil) Hints on removing missing data points filter(dat, is.na(variable)) versus filter(dat, !is.na(variable)) Assumption 3: The shape of my data The remaining assumptions are all best checked through visualisations. You could use histograms to check that the data (Abil and IQ) are both normally distributed, and you could use a scatterplot (scattergrams) of IQ as a function of Abil to check whether the relationship is linear, with homoscedasticity, and without outliers! An alternative would be to use z-scores to check for outliers with the cut-off usually being set at around \\(\\pm2.5SD\\). You could do this using the mutate function (e.g. mutate(z = (X - mean(X))/SD(X))), but today we will just use visual checks. 10.3.4 Task 4 - Normality Create the following figures and discuss the outputs with your group. A discussion is in the solutions at the end of the chapter. A histogram for Ability and a histogram for IQ. Are they both normally distributed? A scatterplot of IQ (IQ) as a function of Ability (Abil). Do you see any outliers? Does the relationship appear linear? Does the spread appear ok in terms of homoscedasticity? Hints to create figures ggplot(mh, aes(x = )) + geom_histogram() ggplot(mh, aes(x = , y = )) + geom_point() Normality: something to keep in mind is that there are only 25 participants, so how normal do we expect relationships to be homoscedasticity is that the spread of data points around the (imaginary) line of best fit is equal on both sides along the line; as opposed to very narrow at one point and very wide at others. Remember these are all judgement calls! Descriptives of a correlation A key thing to keep in mind is that the scatterplot is actually the descriptive of the correlation. Meaning that in an article, or in a report, you would not only use the scatterplot to determine which type of correlation to use but also to describe the potential relationship in regards to your hypothesis. So you would always expect to see a scatterplot in the write-up of this type of analysis 10.3.5 Task 5 - Descriptives Group Discussion Point Looking at the scatterplot you created in Task 4, spend a couple of minutes discussing and describing the relationship between Ability and IQ in terms of your hypothesis. Remember this is a descriptive analysis at this stage, so nothing is confirmed. Does the relationship appear to be as we predicted in our hypothesis? A discussion is in the solutions at the end of the chapter. Hints on discussing descriptives Hint 1: We hypothesised that reading ability and intelligence were positively correlated. Is that what you see in the scatterplot? Hint 2: Keep in mind it is subjective at this stage. Hint 3: Remember to only talk about a relationship and not a prediction. This is correlational work, not regression. Hint 4: Can you say something about both the strength (weak, medium, strong) and the direction (positive, negative)? The correlation Finally we will run the correlation using the cor.test() function. Remember that for help on any function you can type ?cor.test in the console window. The cor.test() function requires: the name of the tibble and the column name of Variable 1 the name of the tibble and the column name of Variable 2 the type of correlation you want to run: e.g. pearson, spearman the type of NHST tail you want to run: e.g. one.sided, two.sided For example, if your data is stored in dat and you are wanting a two-sided pearson correlation of the variables (columns) X and Y, then you would do: cor.test(dat$X, dat$Y, method = &quot;pearson&quot;, alternative = &quot;two.sided&quot;) where dat$X means the column X in the tibble dat. The dollar sign ($) is a way of indexing, or calling out, a specific column. 10.3.6 Task 6 - Pearson or Spearman? Based on your answers to Task 5, spend a couple of minutes deciding with your group which correlation method to use (e.g. pearson or spearman) and the type of NHST tail to set (e.g. two.sided or one.sided). Now, run the correlation between IQ and Ability and save it in a tibble called results (hint: broom::tidy()). The solution is at the end of the chapter. Hints to correlation Hint 1: the data looked reasonably normal and linear so the method would be? Hint 2: results &lt;- cor.test(mh$Abil, method = .., alternative.) %&gt;% tidy() Interpreting the Correlation You should now have a tibble called results that gives you the output of the correlation between Reading Ability and IQ for the school children measured in Miller and Haden (2013) Chapter 11. All that is left to do now is interpret the output of the correlation. 10.3.7 Task 7 - Interpretation Look at results. Locate your correlation value, e.g. results %&gt;% pull(estimate), and then with your group, answer the following questions. Again a discussion can be found at the end of the chapter. The direction of the relationship between Ability and IQ is: positive negative no relationship The strength of the relationship between Ability and IQ is: strong medium weak Based on \\(\\alpha = .05\\) the relationship between Ability and IQ is: significant not significant Based on the output, given the hypothesis that the reading ability of school children, as measured through a standardized test, and intelligence, again through a standardized test, are positively correlated, we can say that the hypothesis: is supported is not supported is proven is not proven Hints to interpretation Hint1: If Y increases as X increases then the relationship is positive. If Y increases as X decreases then the relationship is negative. If there is no change in Y as X changes then there is no relationship Hint2: Depending on the field most correlation values greater than .5 would be strong; .3 to .5 as medium, and .1 to .3 as small. Hint3: The field standard says less than .05 is significant. Hint4: Hypotheses can only be supported or not supported, never proven. Recap so far Great, so far we have set a hypothesis for a correlation, checked the assumptions, run the correlation and interpreted it appropriately. So as you can see running the correlation is the easy bit. As in a lot of analyses it is getting your data in order, checking assumptions, and interpreting your output that is the hard part. We have now walked you through one analysis but you can always go run more with the Miller and Haden dataset. There are six in total that could be run but watch out for Multiple Comparisons - where your False Positive Error rate (Type 1 error rate) is inflated and as such the chance of finding a significant effect is inflated by simply running numerous tests. Alternatively, we have another data set below that we want you to run a correlation on yourself but first we want to show you something that can be very handy when you want to view lots of correlations at once. 10.3.8 Advanced 1: Matrix of Scatterplots Above we ran one correlation and if we wanted to do a different correlation then we would have to edit the cor.test() line and run it again. However, when you have lots of variables in a dataset, to get a quick overview of patterns, one thing you might want to do is run all the correlations at the same time or create a matrix of scatterplots at the one time. You can do this with functions from the Hmisc library - already installed in the Boyd Orr Labs. We will use the Miller and Haden data here again which you should still have in a tibble called mh. First, we need to get rid of the Participant column as we dont want to correlate that with anything. It wont tell us anything. Copy and run the below line of code library(&quot;Hmisc&quot;) library(&quot;tidyverse&quot;) mh &lt;- read_csv(&quot;MillerHadenData.csv&quot;) %&gt;% select(-Participant) Now run the following line. The pairs() function from the Hmisc library creates a matrix of scatterplots which you can then use to view all the relationships at the one time. pairs(mh) Figure 10.3: Matrix of Correlation plots of Miller and Haden (2013) data And the rcorr() function creates a matrix of correlations and p-values. But watch out, it only accepts the data in matrix format. Run the following two lines of code. mh_mx &lt;- as.matrix(mh) rcorr(mh_mx, type = &quot;pearson&quot;) ## Abil IQ Home TV ## Abil 1.00 0.45 0.74 -0.29 ## IQ 0.45 1.00 0.20 0.25 ## Home 0.74 0.20 1.00 -0.65 ## TV -0.29 0.25 -0.65 1.00 ## ## n= 25 ## ## ## P ## Abil IQ Home TV ## Abil 0.0236 0.0000 0.1624 ## IQ 0.0236 0.3337 0.2368 ## Home 0.0000 0.3337 0.0005 ## TV 0.1624 0.2368 0.0005 10.3.9 Task 8 - The Matrix After running the above lines, spend a few minutes answering the following questions with your group. The solutions are at the end of the chapter. The first table outputted is the correlation values and the second table is the p-values. A discussion is at the end of the chapter. Why do the tables look symmetrical around a blank diagonal? What is the strongest positive correlation? What is the strongest negative correlation? Hints to Matrix of correlations Hint1: There is no hint, this is just a cheeky test to make sure you have read the correlation chapter in Miller and Haden, like we asked you to! :-) If you are unsure of these answers, the solutions are at the end of the chapter. 10.3.10 Advanced 2: Attitudes towards Vaping Great work so far! Now we really want to see what you can do yourself. As we mentioned earlier, in the data folder there is another file called VapingData.csv. This data comes from a lab we used to run looking at implicit and explicit attitudes towards vaping. Explicit attitudes were measured via a questionnaire where higher scores indicated a positive attitude towards vaping. Implicit attitudes were measured through an Implicit Association Test (IAT) using images of Vaping and Kitchen utensils and associating them with positive and negative words. The IAT works on the principal that associations that go together (that are congruent, e.g. warm and sun) should be quicker to respond to than associations that do not go together (that are incongruent, e.g. warm and ice). You can read up more on the procedure at a later date on the Noba Project https://nobaproject.com/modules/research-methods-in-social-psychology which has a good description of the procedure under the section Subtle/Nonsconscious Research Methods. For this exercise, you need to know that Block 3 in the experiment tested reaction times and accuracy towards congruent associations, pairing positive words with Kitchen utensils and negative words with Vaping. Block 5 in the experiment tested reaction times and accuracy towards incongruent associations, pairing positive words with Vaping and negative words with Kitchen Utensils. As such, if reaction times were longer in Block 5 than in Block 3 then people are considered to hold the view that Vaping is negative (i.e. congruent associations are quicker than incongruent associations). However, if reaction times were quicker in Block 5 than in Block 3 then people are considered to hold the view that Vaping is positive (i.e. incongruent associations were quicker than congruent associations). The difference between reaction times in Block5 and Block3 is called the participants IAT score. 10.3.11 Task 9 - Attitudes to Vaping Load in the data in VapingData.csv and analyse it to test the hypothesis that Implicit and Explicit attitudes towards Vaping are positively related. Here are some pointers, hints and tips. Again, the solutions walk you through each step but it will help you to try the steps yourself first. Think through each step. Sometimes you will need to think backwards, what do I want my tibble to look like. You can do all of these steps. This is nothing new. After loading in the data, start by looking at the data. You have 8 columns. Reaction times and Accuracy scores for Blocks 3 and 5 as well as the Explicit Vaping Questionnaire scores, Sex and Age, for each participant. Accuracy was calculated as proportion and as such cant go above 1. Participants entered their own data so some might have made a mistake. Remove participants who had an accuracy greater than 1 in either Block 3 or Block 5 as we are unclear on the accuracy of these values. We also only want participants that were paying attention so best remove anybody whose average accuracy score across Blocks 3 and 5 was less than 80%. Note - this value is aribtrary and if you wanted, in your own experiment, you could use a more relaxed or strict cut-off based on other studies or guidance. Note that these decisions should be set out at the start of your research as part of your pre-registration or as part of your Registered Report. Finally, in this instance, remember, the values are in proportions not percentages (so 80% will be .8). Now create an IAT score for participants by subtracting Block 3 reaction times (RT) away from Block 5 reaction times e.g \\(Block5 - Block3\\). Use the information above to understand how the scores relate to attitudes. Create a descriptives summary of the number of people, the mean RT and Vaping Questionnaire Score. Why might these averages be useful? Why are averages not always useful in correlations? Check your assumptions of correlations as we did above and descriptives, thinking about how it compares to the hypothesis. Run the appropriate correlation based on your assumptions and interpret the output. Hints for Vaping libraries might include tidyverse and broom Hint Step 1: read_csv() Hint Step 2: filter(Accuracy &lt; 1 OR Accuracy &lt;= 1 OR Accuracy &gt; 1 OR Accuracy &gt;= 1) Hint Step 3: average accuracy: mutate(data, name = (1 + 2)/2) %&gt;% filter(name &gt; ) Hint Step 4: RT: mutate(data, nom = 1 - 2) Hint Step 5: descriptives &lt;- summarise() Hint Step 6: assumptions would be type of data, normality, linear relationship, homoscedasicity, data points for everyone! Hint Step 7: results &lt;- cor.test(method = pearson)? Job Done - Activity Complete! Excellent work, who would have thought that about Explicit and Implicit attitudes towards Vaping?! One last thing: Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. We wont incorporate Portfolio points this semester as by now you should know what sort of information you need to make a note of for yourself, and the more independent you are in your learning the better it will be, but please dont think they are no longer relevant! 10.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 10.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 10.5.1 InClass Activities 10.5.1.1 InClass Task 1 Loading in the data and the two libraries needed Good point to remind you that: we use read_csv() to load in data the order that libraries are read in is important. If there are any conflicts in terms of libraries then the last library that is loaded will be the functions you are using. library(&quot;broom&quot;) library(&quot;tidyverse&quot;) mh &lt;- read_csv(&quot;MillerHadenData.csv&quot;) Return to Task 10.5.1.2 InClass Task 2 Actually the information within the textbook is unclear as to whether the data is interval or ordinal so we have accepted both as you could make a case for both arguments. A quick google search will show just as many people who think that IQ is interval as think it is oridinal. In terms of Reading Ability, again we probably dont know enough information about this scale to make a clear judgement but it is at least ordinal and could well be interval. Return to Task 10.5.1.3 InClass Task 3 Missing data is represented by NA. It stands for Not Available but is a very good way of improving your Scottish accent. For example, is that a number can be replied with NA! If you want to keep everybody from the whole dataset that has a score for Ability you would use: filter(mh, !is.na(Abil)) Participant Abil IQ Home TV 1 61 107 144 487 2 56 109 123 608 3 45 81 108 640 4 66 100 155 493 5 49 92 103 636 6 62 105 161 407 7 61 92 138 463 8 55 101 119 717 9 62 118 155 643 10 61 99 121 674 11 51 104 93 675 12 48 100 127 595 13 50 95 97 673 14 50 82 140 523 15 67 114 151 665 16 51 95 112 663 17 55 94 102 684 18 54 103 142 505 19 57 96 127 541 20 54 104 102 678 21 52 98 124 564 22 48 117 87 787 23 61 100 141 582 24 54 101 117 647 25 48 94 111 448 Alternatively, if you want to keep everybody from the whole dataset that does not have a score for Ability you would use: filter(mh, is.na(Abil)) Participant Abil IQ Home TV Remember that you would need to store the output of this step, so really it would be something like mh &lt;- filter(mh, !is.na(Abil)) Return to Task 10.5.1.4 InClass Task 4 Reading ability data appears as normal as expected for 25 participants. Hard to say how close to normality something should look when there are so few participants. ggplot(mh, aes(x = Abil)) + geom_histogram(binwidth = 5) + theme_bw() Figure 10.4: Histogram showing the distribution of Reading Ability Scores from Miller and Haden (2013) IQ data appears as normal as expected for 25 participants ggplot(mh, aes(x = IQ)) + geom_histogram(binwidth = 5) + theme_bw() Figure 10.5: Histogram showing the distribution of IQ Scores from Miller and Haden (2013) The relationship between reading ability and IQ scores appears appears linear and with no clear outliers. Data also appears homeoscedastic. We have added the geom_smooth() function to help clarify the line of best fit (also know as the regression line or the slope). ggplot(mh, aes(x = Abil, y = IQ)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 10.6: Scatterplot of IQ scores as a function of Reading Ability from Miller and Haden (2013) data Return to Task 10.5.1.5 InClass Task 5 Based on the scatterplot we might suggest that as reading ability scores increase, IQ scores also increase and as such it would appear that our data is inline with our hypothesis that the two variables are positively correlated. This appears to be a medium strength relationship. Return to Task 10.5.1.6 InClass Task 6 We are going to run a pearson correlation as we would argue the data is interval and the relationship is linear. The correlation would be run as follows - tidying it into a nice and useable table. results &lt;- cor.test(mh$Abil, mh$IQ, method = &quot;pearson&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy() The output of the table would look as follows: Table 10.2: The correlation output of the Reading Ability and IQ relationship. estimate statistic p.value parameter conf.low conf.high method alternative 0.451 2.425 0.024 23 0.068 0.718 Pearsons product-moment correlation two.sided Return to Task 10.5.1.7 InClass Task 7 In the Task 6 output: The correlation value (r) is stored in estimate The degrees of freedom (N-2) is stored in parameter The p-value is stored in p.value And statistic is the t-value associated with this analysis as correlations use the t-distribution (same as in Chapters 6, 7 and 8) to determine probability of an outcome. Remember that we can use the pull() function to get individual values as shown here. pvalue &lt;- results %&gt;% pull(p.value) %&gt;% round(3) df &lt;- results %&gt;% pull(parameter) correlation &lt;- results %&gt;% pull(estimate) %&gt;% round(2) And we can use that information to write-up the following using inline coding for accuracy: A pearson correlation found reading ability and intelligence to be positively correlated with a medium to strong relationship, (r(`r df`) = `r correlation`, p = `r pvalue`). As such we can say that our hypothesis is supported and that there appears to be a relationship between reading ability and IQ in that as reading ability increases so does intelligence. Which when knitted would read as: A pearson correlation found reading ability and intelligence to be positively correlated with a medium to strong relationship, (r(23) = 0.45, p = 0.024). As such we can say that our hypothesis is supported and that there appears to be a relationship between reading ability and IQ in that as reading ability increases so does intelligence. Return to Task 10.5.1.8 InClass Task 8 The table looks the same across the diaganol because the correlation of e.g. Abil vs Abil is not shown, and the correlation of Abil vs Home is the same as the correlation of Home vs Abil The strongest positive correlation is between the number of minutes spend reading at home (Home) and Reading Ability (abil), r(23) = .74, p &lt; .001 The strongest negative correlation is between the number of minutes spend reading at home (Home) and minutes spent watching TV per week (TV), r(23) = -.65, p &lt; .001 Return to Task 10.5.1.9 InClass Task 9 Step 1 Reading in the Vaping Data using read_csv() dat &lt;- read_csv(&quot;VapingData.csv&quot;) Steps 2 to 4 The main wrangle of parts 2 to 4 dat &lt;- dat %&gt;% filter(IAT_BLOCK3_Acc &lt;= 1) %&gt;% filter(IAT_BLOCK5_Acc &lt;= 1) %&gt;% mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %&gt;% filter(IAT_ACC &gt; .8) %&gt;% mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT) Step 5 It is always worth thinking about which averages are informative and which are not. Knowing the average explicit attitude towards vaping could well be informative. In contrast, if you are using an ordinal scale and people use the whole of the scale then the average may just tell you the middle of the scale you are using - which you already know and really isnt that informative. So it is always worth thinking about what your descriptives are calculating. descriptives &lt;- dat %&gt;% summarise(n = n(), mean_IAT_ACC = mean(IAT_ACC), mean_IAT_RT = mean(IAT_RT), mean_VPQ = mean(VapingQuestionnaireScore, na.rm = TRUE)) Step 6 A couple of visual checks of normality through histograms ggplot(dat, aes(x = VapingQuestionnaireScore)) + geom_histogram(binwidth = 10) + theme_bw() ## Warning: Removed 11 rows containing non-finite values (stat_bin). Figure 10.7: Histogram showing the distribution of Scores on the Vaping Questionnaire (Explicit) ggplot(dat, aes(x = IAT_RT)) + geom_histogram(binwidth = 10) + theme_bw() Figure 10.8: Histogram showing the distribution of IAT Reaction Times (Implicit) A check of the relationship between reaction times on the IAT and scores on the Vaping Questionnaire Remember that, often, the scatterplot is considered the descriptive of the correlation, hence why you see them including in journal articles to support the stated relationship. The scatterplot can be used to make descriptive claims about the direction of the relationship, the strength of the relationship, whether it is linear or not, and to check for outliers and homeoscedasticity. ggplot(dat, aes(x = IAT_RT, y = VapingQuestionnaireScore)) + geom_point() + theme_bw() ## Warning: Removed 11 rows containing missing values (geom_point). Figure 10.9: A scatterplot showing the relationship between implicit IAT reaction times (x) and explicit Vaping Questionnaire Scores (y) A quick look at the data reveals some people do not have a Vaping Questionnaire score and some dont have an IAT score. The correlation only works when people have a score on both factors so we remove all those that only have a score on one of the factors. dat &lt;- dat %&gt;% filter(!is.na(VapingQuestionnaireScore)) %&gt;% filter(!is.na(IAT_RT)) Step 7 The analysis: results &lt;- cor.test(dat$VapingQuestionnaireScore, dat$IAT_RT, method = &quot;pearson&quot;) %&gt;% tidy() And extracting the values: correlation &lt;- results %&gt;% pull(estimate) df &lt;- results %&gt;% pull(parameter) pvalue &lt;- results %&gt;% pull(p.value) With inline coding: Testing the hypothesis that there would be a relaionship beween implicit and explicit attitudes towards vaping, a pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(`r df`) = `r correlation`, p = `r pvalue`. Overall this suggests that there is no direct relationship between implicit and explicit attitudes when relating to Vaping and as such our hypothesis was not supported; we cannot rejet the null hypothesis. and appears as when knitted: Testing the hypothesis that there would be a relaionship beween implicit and explicit attitudes towards vaping, a pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(143) = -0.1043797, p = 0.2115059. Overall this suggests that there is no direct relationship between implicit and explicit attitudes when relating to Vaping and as such our hypothesis was not supported; we cannot rejet the null hypothesis. Remember though that r-values and p-values are often rounded to three decimal places, so a more appropriate write up would be: Testing the hypothesis that there would be a relaionship beween implicit and explicit attitudes towards vaping, a pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(143) = -.104, p = .212. Overall this suggests that there is no direct relationship between implicit and explicit attitudes when relating to Vaping and as such our hypothesis was not supported; we cannot rejet the null hypothesis. Chapter Complete! 10.6 Additional Material Below is some additional material that might help you understand correlations a bit more and some additional ideas. Checking for outliers with z-scores We briefly mentioned in the inclass activities that you could use z-scores to check for outliers, instead of visual inspection. We have covered this in lectures and it works for any interval or ratio dataset, and not just ones for correlations, but we will demonstrate it here using the IQ data from Miller and Haden. First, lets get just the IQ data. mh_IQ &lt;- mh %&gt;% select(IQ) A z-score is just a standardised value based on the mean (\\(\\mu\\)) and standard deviation (SD or \\(\\sigma\\), proonounced sigma) of the sample it comes from. The formula for a z-score is: \\[z = \\frac{x - \\mu}{\\sigma}\\] Using z-scores is away of effectively converting your data onto the Normal Distribution. As such, because of known cut-offs from the Normal Distribution (e.g. 68% of data between \\(\\pm1SD\\), etc - See Chapter 4), we can use that information to determine a value as an outlier. To do it, you convert all the data within your variable into their respective z-score and if any of the values fall above or below your cut-off then they are considered an outlier. mh_IQ_z &lt;- mh_IQ %&gt;% mutate(z_scores = (IQ - mean(IQ))/sd(IQ)) Which will give us the following data (showing only the first 6 rows): Table 10.3: Raw IQ data and z-scored IQ data from Miller and Haden IQ z_scores 107 0.7695895 109 0.9907359 81 -2.1053138 100 -0.0044229 92 -0.8890086 105 0.5484431 92 -0.8890086 And if you run through the data you will see a whole range of z-scores ranging from -2.1053138 to 1.9858948. During the class we said that we would use a cut-off of \\(\\pm2.5SD\\) and you can see from the range of the IQ z-scores that there is no value above that, so we have no outliers. Which you could confirm with the following code: mh_IQ_z %&gt;% filter(abs(z_scores) &gt; 2.5) %&gt;% count() %&gt;% pull(n) Which if you run that code, you can see we have 0 outliers. This also demonstrates however why you must stipulate your cut-offs in advance - otherwise you might fall foul of adjusting your data to fit your own predictions and hiding your decision making in the numerous researcher degrees of freedom that exist. For example, say you run the above analysis and see no outliers but dont find the result you want/expect/believe should be there. A questionable research practice would be now to start adjusting your z-score cut-off to different values to see what difference that makes. For instance, had we said the cut-off was more stringent at \\(\\pm2SD\\) we would have found 1 outlier. So the two take-aways from this are: How to spot outliers using z-scores. You must set your exclusion criteria in advance of running your analysis. A different approach to making a correlation table In the inclass activities, Task 8, we looked at making a table of correlation functions. It was a bit messy and there is actually an alternative approach that we can use, with some useful additional functions. This makes use of the corrr package which you may need to install onto your laptop if you want to follow along. Here is the code below that you can try out and explore yourself. The main functions are correlate(), shave(), and fashion(): correlate() runs the correlations and can be changed between pearson, spearman, etc. The quiet argument just removes some information reminders that we dont really need. Switch it to FALSE to see what happens. A nice aspect of this function is that the default is to only use complete rows but you can alter that. shave() can be used to convert one set of correlation values that are showing the same test to NAs. For example, the bottom half of the table just reflects the top half of the table so we can convert one half. The default is to convert the upper half of the table. fashion() can be used to tidy up the table in terms of removing NAs, leading zeros, and for setting the number of decimal places. The code would look like this: library(corrr) mh &lt;- read_csv(&quot;MillerHadenData.csv&quot;) mh %&gt;% select(-Participant) %&gt;% correlate(method = &quot;pearson&quot;, quiet = TRUE) %&gt;% shave() %&gt;% fashion(decimals = 3, leading_zeros = FALSE) Which if you then look at the ouput you get this nice clean correlation matrix table shown below. term Abil IQ Home TV Abil IQ .451 Home .744 .202 TV -.288 .246 -.648 So that is a bit of a nicer approach than the one shown in the lab, and gives you more control over the output of your correlation matrix table. However, the downside of this approach, and the reason we didnt use it in the lab is because this approach does not show you the p-values that easily. In fact, it doesnt show you them at all and you need to do a bit more work. You can read more about using the corrr package in this webpage by the author of the package: https://drsimonj.svbtle.com/exploring-correlations-in-r-with-corrr Comparing Correlations One step that is often overlooked when working with correlations and a data set with numerous variables, is to compare whether the correlations are significantly different from each other. For example, say you are looking at whether there is a relationship between height and attractiveness in males and in females. You run the two correlations and find one is a stronger relationship than the other. Many would try to conclude that that means there is a significantly stronger relationship in one sex than the other. However that has not been tested. It can be tested though. We will quickly show you how, but this paper will also help the discussion: Diedenhofen and Musch (2015) cocor: A Comprehensive Solution for the Statistical Comparison of Correlations. PLOS ONE 10(6): e0131499. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0121945 As always we need the libraries first. You will need to install the cocor package on to your laptop. library(cocor) library(tidyverse) Now we are going to walk through three different examples just to show you how it would work in different experimental designs. You will need to match the values in the code with the values in the text to make full sense of these examples. We will use examples from work on faces, voices and personality traits that you will encounter later in this book. Comparing correlation of between-subjects designs (i.e. independent-samples) Given the scenario of males voice pitch and height (r(28) = .89) and female voice pitch and height (r(28) = .75) can you say that the difference between these correlations is significant? As these correlations come from two different groups - male voices and female voices - we used the cocor.indep.groups() function. Note: As the df of both groups is 28, this means that the N is 30 of both groups (based on N = df-2) compare1 &lt;- cocor.indep.groups(r1.jk = .89, r2.hm = .75, n1 = 30, n2 = 30) Gives the output of: ## ## Results of a comparison of two correlations based on independent groups ## ## Comparison between r1.jk = 0.89 and r2.hm = 0.75 ## Difference: r1.jk - r2.hm = 0.14 ## Group sizes: n1 = 30, n2 = 30 ## Null hypothesis: r1.jk is equal to r2.hm ## Alternative hypothesis: r1.jk is not equal to r2.hm (two-sided) ## Alpha: 0.05 ## ## fisher1925: Fisher&#39;s z (1925) ## z = 1.6496, p-value = 0.0990 ## Null hypothesis retained ## ## zou2007: Zou&#39;s (2007) confidence interval ## 95% confidence interval for r1.jk - r2.hm: -0.0260 0.3633 ## Null hypothesis retained (Interval includes 0) You actually get two test outputs with this function: Fishers Z and Zous Confidence Interval. The one most commonly used is Fishers Z so we will look at that one. As the p-value of the comparison between the two correlations, p = .1, is greater than p = .05, then this tells us that we have cannot reject the null hypothesis that there is no significant difference between these two correlations and they are of similar magnitude - i.e. the correlation is similar in both groups. You would report this outcome along the lines of, despite the correlation between male voice pitch and height was found to be stronger than the same relationship in female voices, Fishers Z suggested that this was not a significant difference (Z = 1.65, p = .1) Within-Subjects (i.e. Dependendent Samples) with common variable What about the scenario where you have 30 participants rating faces on the scales of trust, warmth and likeability, and we want to know if the relationship between trust and warmth (r = .89) is significantly different than the relationship between trust and likeability (r = .8). As this data comes from the same participants, and there is crossover/overlap in traits - i.e. one trait appears in both correlations of interest - then we will use the cocor.dep.groups.overlap(). To do this comparison we also need to know the relationship between the remaining correlation of warmth and likeability (r = .91). Note: The order of input of correlations matters. See ?cocor.dep.groups.overlap() for help. compare2 &lt;- cocor.dep.groups.overlap(r.jk = .89, r.jh = .8, r.kh = .91, n = 30) Gives the output of: ## ## Results of a comparison of two overlapping correlations based on dependent groups ## ## Comparison between r.jk = 0.89 and r.jh = 0.8 ## Difference: r.jk - r.jh = 0.09 ## Related correlation: r.kh = 0.91 ## Group size: n = 30 ## Null hypothesis: r.jk is equal to r.jh ## Alternative hypothesis: r.jk is not equal to r.jh (two-sided) ## Alpha: 0.05 ## ## pearson1898: Pearson and Filon&#39;s z (1898) ## z = 1.9800, p-value = 0.0477 ## Null hypothesis rejected ## ## hotelling1940: Hotelling&#39;s t (1940) ## t = 2.4208, df = 27, p-value = 0.0225 ## Null hypothesis rejected ## ## williams1959: Williams&#39; t (1959) ## t = 2.4126, df = 27, p-value = 0.0229 ## Null hypothesis rejected ## ## olkin1967: Olkin&#39;s z (1967) ## z = 1.9800, p-value = 0.0477 ## Null hypothesis rejected ## ## dunn1969: Dunn and Clark&#39;s z (1969) ## z = 2.3319, p-value = 0.0197 ## Null hypothesis rejected ## ## hendrickson1970: Hendrickson, Stanley, and Hills&#39; (1970) modification of Williams&#39; t (1959) ## t = 2.4208, df = 27, p-value = 0.0225 ## Null hypothesis rejected ## ## steiger1980: Steiger&#39;s (1980) modification of Dunn and Clark&#39;s z (1969) using average correlations ## z = 2.2476, p-value = 0.0246 ## Null hypothesis rejected ## ## meng1992: Meng, Rosenthal, and Rubin&#39;s z (1992) ## z = 2.2410, p-value = 0.0250 ## Null hypothesis rejected ## 95% confidence interval for r.jk - r.jh: 0.0405 0.6061 ## Null hypothesis rejected (Interval does not include 0) ## ## hittner2003: Hittner, May, and Silver&#39;s (2003) modification of Dunn and Clark&#39;s z (1969) using a backtransformed average Fisher&#39;s (1921) Z procedure ## z = 2.2137, p-value = 0.0268 ## Null hypothesis rejected ## ## zou2007: Zou&#39;s (2007) confidence interval ## 95% confidence interval for r.jk - r.jh: 0.0135 0.2353 ## Null hypothesis rejected (Interval does not include 0) So this test actually produces the output of a lot of tests; many of which we probably dont know whether to use or not. However, what you can do is run the analysis and choose just a specific test by adding the argument to the function: test = steiger1980 or test = pearson1898 for example. These are probably the two most common. Lets run the analysis again using just Pearsons Z (1898) compare3 &lt;- cocor.dep.groups.overlap(r.jk = .89, r.jh = .8, r.kh = .91, n = 30, test = &quot;pearson1898&quot;) Gives the output of: ## ## Results of a comparison of two overlapping correlations based on dependent groups ## ## Comparison between r.jk = 0.89 and r.jh = 0.8 ## Difference: r.jk - r.jh = 0.09 ## Related correlation: r.kh = 0.91 ## Group size: n = 30 ## Null hypothesis: r.jk is equal to r.jh ## Alternative hypothesis: r.jk is not equal to r.jh (two-sided) ## Alpha: 0.05 ## ## pearson1898: Pearson and Filon&#39;s z (1898) ## z = 1.9800, p-value = 0.0477 ## Null hypothesis rejected As you can see from the output, this test is significant (p = .047) suggesting that there is a significant difference between the correlation of trust and warmth (r = .89) and the correlation of trust and likeability (r = .8). Within-Subjects (i.e. Dependendent Samples) with no common variable Ok last scenario. You have 30 participants rating faces on the scales of trust, warmth, likeability, and attractiveness, and we want to know if the relationship between trust and warmth (r = .89) is significantly different than the relationship between likeability and attractiveness (r = .93). As the correlations of interest have no crossover in variables but do come from the same participants, in this example we use the cocor.dep.groups.nonoverlap(). Note: In order to do this we need the correlations of all other comparisons: trust and likeability (.88) trust and attractiveness (.91) warmth and likeability (.87) warmth and attractiveness (.92) Note: The order of input of correlations matters. See ?cocor.dep.groups.nonoverlap() for help. compare5 &lt;- cocor.dep.groups.nonoverlap(r.jk = .89, r.hm = .93, r.jh = .88, r.jm = .91, r.kh = .87, r.km = .92, n = 30, test = &quot;pearson1898&quot;) Gives the output of: ## ## Results of a comparison of two nonoverlapping correlations based on dependent groups ## ## Comparison between r.jk = 0.89 and r.hm = 0.93 ## Difference: r.jk - r.hm = -0.04 ## Related correlations: r.jh = 0.88, r.jm = 0.91, r.kh = 0.87, r.km = 0.92 ## Group size: n = 30 ## Null hypothesis: r.jk is equal to r.hm ## Alternative hypothesis: r.jk is not equal to r.hm (two-sided) ## Alpha: 0.05 ## ## pearson1898: Pearson and Filon&#39;s z (1898) ## z = -1.1424, p-value = 0.2533 ## Null hypothesis retained As you can see from the output, this test is non-significant (p = .253) suggesting that there is no significant difference between the correlation of trust and warmth (r = .89) and the correlation of likeability and attractiveness (r = .8). End of Additional Material! "],["introduction-to-glm-one-factor-anova.html", "Lab 11 Introduction to GLM: One-factor ANOVA 11.1 Overview 11.2 PreClass Activity 11.3 InClass Activity 11.4 Test Yourself 11.5 Solutions to Questions", " Lab 11 Introduction to GLM: One-factor ANOVA 11.1 Overview A key way that we attempt to learn from data is to build a statistical model that captures relationships among variables. You are actually already familiar with this approach but it just hasnt been phrased as such - this is what t-test, correlations, etc, do. In this chapter we will formalise this approach and introduce you to the General Linear Model (GLM) which you will read about in the Miller and Haden (2013) textbook (Chapters 1-3) as part of the PreClass. The GLM is a very common model in statistics in Psychology and it encapsulates a range of common analytical techniques that you are already familiar with and will become even more familiar with throughout this book as we will spend some of the next few lessons looking at the GLM and reading about it. The GLM covers all the t-tests and correlations you have looked at, and the ANOVA and regression we are going to come on to. Basically, the General Linear Model (GLM) is the foundation of a lot of that statistical tests we use. Over the next few chapters, and building for future years of study, we will introduce the GLM here, through working with the model by hand on a simulated dataset, as this one of the best ways to learn about linear models. You will also notice a slight change in the assignments for the next few chapters in that you are required to do a little more computation than before. Keep in mind though that all the skills you need will of course be shown to you first or you have already learnt them. The previous chapters have been aimed at developing your general practical data skills and now we want to develop your understanding of the analysis and data you are working with. As such, the goals of this chapter are: to recap and practice entering data into a tibble (tidyverse data frame - as introduced Chapter 5); to learn how to estimate model parameters from a dataset; to learn how to derive/generate a decomposition matrix that expresses each observation/participant as a linear sum of model components and error. These terms will become more familiar to you over the following chapters and from reading Miller and Haden, but remember to make notes for yourself to help your solidify your learnin and, as always, ask as many questions as you like! 11.2 PreClass Activity The PreClass Activity for this chapter is reading. It is quite a bit of reading but dont worry if you dont understand it all the first time round. The best way to deal with this is to read through the information as prep, get the gist, then use it to support the activities, and then re-read to consolidate knowledge for the assessment. And ask questions. This will seem difficult at first but by working with the ideas and examples, you can understand the concepts introduced here. 11.2.1 Read Chapters As preparation for the inclass activities, please read Chapters 1 to 3 of Miller and Haden (2013). The inclass activities will be working up to and including the concept of Sums of Squares which is around Section 3.4 of Miller and Haden (2013), so at least read up to and including that section. When reading Miller and Haden it will help to remember that: Factor is another name for Variable Level is another name for Condition E.g. a between-subjects experiment with one independent variable (sleep quality) and two conditions (Good vs Poor) can be said to have one factor with two levels. The key terms we want you to be becoming familiar with from these chapters, though perhaps you dont quite fully understand them, are: ANOVA - short for Analysis of Variance. A statistical test used to compare the spread of the variance across two or more conditions and/or two or more factors. estimation equations - formulas (or equations) that we use to determine our best guess (estimates) at parameters (values) of a population of interest from our sample parameters (values) decomposition matrices - a table (matrix) that breaks down information into individual components. In this instance, the decomposition matrix breaks down how each individual observation/participant is fitted into the GLM based on the estimation equations. sums of squares - an estimate of the total spread of the data around a parameter (such as the mean). We have seen sums of squares before in lectures, as the top part of the variance equation for example: \\[\\sum(x - \\bar{x})^2\\] The activities we will look at next will help you understand these terms better. In short, over the next two chapters, we will show you how to take data from a simulated experiment and apply the estimation equations to create a decomposition matrix summarising this data. This decomposition matrix will show how each participant and condition is fitted to the General Linear Model. Then, from that matrix, we will calculate the relevant sums of squares for each condition, which will in turn be used in our ANOVA calculation to determine if there is a significant difference between conditions. It is going to be a lot more fun than it sounds! If you want to get ahead, you should try the next activities. The first part reiterates and expands on the Miller and Haden information. Job Done - Activity Complete! 11.3 InClass Activity We are going to start with a step-by-step example of building a decomposition matrix for an ANOVA (Analysis of Variance) and then ask you to perform the steps yourself on a different dataset. If you feel comfortable with the examples in Chapter 3 of Miller and Haden, feel free to skim worked example and move onto the exercises below. You can find further examples and step-by-step walkthroughs at the end of Miller and Haden (2013), Chapter 3. One-factor ANOVA: Worked example An ANOVA is a method of analysis for analysing data where you have two or more conditions (levels) for an independent variable (factor), and/or you have more than one independent variable (factors). Thinking back, a t-test is where you normally have two conditions (levels) with one independent variable (factor), right? Well an ANOVA is just an extension of that. For example, in the classic Professor Priming experiments you may have read about, instead of just comparing IQ scores for professors vs. hooligans (t-test), you can compare IQ scores for professors vs. hooligans vs. politicians (ANOVA). Actually, an experiment that has one factor with two levels can be analysed with a t-test or an ANOVA as they are both based on the General Linear Model (GLM). So lets assume that you have data from a one-factor design with three-levels (i.e. one independent variable with three conditions (Grp1, Grp2, Grp3)). To make this example concrete, lets pretend you are studying how consuming food before an exam affects student performance. You randomly assign 12 participants to three separate conditions (four participants per group) (We chose a small number of participants to simplify the computations; obviously if you were going to do this study in real life, youd need far more than 12 participants to make this worthwhile). The three conditions are as follow: no food, glass of water only (Control) all-you-can-eat buffet (Buffet); side salad (Salad). Your dependent variable is operationalised as the number of questions answered correctly on a difficult exam (100 points possible). The exam is administered right after consuming the meal (or drinking water, for the control group). And just in case you arent sure, this would be a one-factor design because there is a single factor, which we might call pre-exam consumption. And this factor has three different levels: water, buffet, and salad. It is a between-subjects design because there are different people in each group (4 per group). In textbooks you might see this referred to as a one-way between-subjects ANOVA. Yes stats does have numerous names for the same thing - that is why making notes is really important. Depending on training someone might use a different word from you but mean the same thing. You can bridge that gap by knowing the alternative names. And finally, for our analysis, we want to test whether there is any difference in exam performance across the levels of the factor. We wont complete the analysis today but we will look at setting up our model which we would then take on further to see if there is a difference between groups. Heres how the exam performance looks like for each of the three groups with each value representing an individual participants score: ## Warning in RNGkind(&quot;Mersenne-Twister&quot;, &quot;Inversion&quot;, &quot;Rounding&quot;): non-uniform ## &#39;Rounding&#39; sampler used Control: 37, 80, 64, 51 Buffet: 33, 47, 55, 41 Salad: 59, 23, 50, 60 Quickfire Questions To make sure you understand the design of this study try to answer the following questions. All the answers are in the information above: Factor is another name for a level condition control variable of the experiment Level is another name for a factor condition control variable of the experiment In this experiment we have 1 factor with 3 levels 1 level with 3 factors 3 variables with 1 condition Because each group contains different participants then this is a within-subjects design between-subjects design mixed design The fourth participant in the Control condition scored 60 41 51 64 on the exam Estimating model components Great so we understand our experiment! Now, and based on your reading, the General Linear Model (GLM) that we will fit to our data is: \\[Y_{ij} = \\mu + A_i + S(A)_{ij}\\] Where: \\(Y_{ij}\\) is the observed value for observation \\(j\\) of group \\(i\\) - i.e. a given participants score (\\(j\\)) in a given group (\\(i\\)); \\(\\mu\\) (pronounced mu) is the population grand mean (estimated by the sample grand mean) - grand just means overall; \\(A_i\\) is the deviation of the population mean of group \\(i\\) from the population grand mean - i.e. how different a groups mean is from the overall population mean; Now before the last part of the formula, you need to know that the sum of \\(\\mu + A_i\\) is known as the fitted value or the typical value or the predicted value of a participant in a condition and is written as: \\(\\hat{Y}_{ij}\\). So: \\[\\hat{Y}_{ij} = \\mu + A_i\\] The party hat that \\({Y}_{ij}\\) is wearing in this part, i.e. \\(\\hat{Y}_{ij}\\), is there to remind us that it is not the actual score of your participant (in a given condition), but an estimate of that value. So, when we are working in predicted values (values we havent actually collected, just predicted) then we stick the party hat on the symbol. If we are working in real values (that we have collected) then no party hat! Finally, back to the GLM equation, \\(S(A)_{ij}\\) is the error or residual, defined as the observed value (\\(Y_{ij}\\)) minus the model prediction (\\(\\hat{Y}_{ij}\\)), or the actual value minus the predicted value. This can be thought of as how different an indivual observation/participant is from their condition mean. So: \\[S(A)_{ij} = \\hat{Y}_{ij} - {Y}_{ij}\\] Now the joy of an analysis such as this is that it can be hard to understand just from the words but makes much more sense when you start to run the numbers - which we will now do. We begin by applying the estimation equations. Our estimate of the population grand mean \\(\\mu\\), will be based on the grand mean of the sample. We will call this \\(\\hat{\\mu}\\) (notice once again, the party hat). The estimation equations for our model (seen in Table 3.3 on page 18 of Miller and Haden (2013)) are: \\(\\hat{\\mu} = Y_{..}\\) \\(\\hat{A}_i = Y_{i.} - \\hat{\\mu}\\) \\(\\widehat{S(A)}_{ij} = Y_{ij} - (\\hat{\\mu} + \\hat{A}_i) = Y_{ij} - \\hat{\\mu} - \\hat{A}_i\\) where \\(Y_{..}\\) is the mean of all 12 observations in the sample a.k.a. the overall mean or the baseline \\(Y_{i.}\\) is the mean of the 4 observations in group \\(i\\). \\(\\hat{A}_i\\) is the respective group mean minus the sample mean a.k.a. between-subjects variance \\(\\widehat{S(A)}_{ij}\\) is the individual error term for a given participant, or how much they deviate from the group contribution. Applying these estimation equations to the data above yields the following decomposition matrix: Table 11.1: Decomposition Matrix of our data ID i j Yij mu Ai err 1 1 1 37 50 8 -21 2 1 2 80 50 8 22 3 1 3 64 50 8 6 4 1 4 51 50 8 -7 5 2 1 33 50 -6 -11 6 2 2 47 50 -6 3 7 2 3 55 50 -6 11 8 2 4 41 50 -6 -3 9 3 1 59 50 -2 11 10 3 2 23 50 -2 -25 11 3 3 50 50 -2 2 12 3 4 60 50 -2 12 In the above table: the column ID can be used to locate individual rows, the column mu represents the value of \\(\\hat{\\mu}\\), the columns Ai represents the value of \\(\\hat{A}_i\\), and the column err represents the value of \\(\\widehat{S(A)}_{ij}\\). Spend a few moments understanding how this table expresses each of the 12 observed values in our example (the \\(Y_{ij}\\)s) in terms of the linear model: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\). For example, if the Control group is group \\(i\\) = 1, then for the first participant, \\(j\\) = 1, you would get: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\) \\(Y_{ij} = mu + Ai + err\\) \\(37 = 50 + 8 + -21\\) Meaning that the overall mean of the whole sample is 50. The unique contribution of the control group is \\(+8\\), so the predicted value of a member of the control group would be 58. However the first participant has an error of \\(-21\\) from the predicted as there actual score is 37. We will start to understand how the difference (the variance) between conditions and within conditions leads to our analysis later but hopefully you are beginning to understand some of the above. Quickfire Questions To make sure you understand the above equations before going on to calculate your own, answer the following questions about the above table which expresses the GLM decomposition matrix, and then check your answers. In which column of the table are the 12 observed values - i.e. the 12 original scores from the participants? Answer The column named Yij What is the estimated grand mean of this sample? (hint: \\(\\hat{\\mu}\\)) Answer The estimate grand mean of the sample is 50 Which rows of the table contain the data and model estimates for the Buffet group if they are group 2? Answer The rows where i is equal to 2; in other words, rows 5-8 What is the value of \\(\\hat{A}_1\\) and in what rows does it appear? Answer The value would be 8 This value appears in column Ai, rows 1-4, where i equals 1. This can be thought of as the difference between that given group and the sample mean that is applied to all participants within that group. In other words this would be the value of the typical participant in that group - as opposed to an individual participant in that group. Note that, conceptually this gives you the effect of your manipulation (food consumption style) and the different i allow you do so for each of the different styles you look at. What is the value of \\(\\widehat{S(A)}_{32}\\)? (hint: this can be read as where i = 3 and j = 2) Answer The value would be -25 This value appears in column err, where i equals 3 and j equals 2. This can be thought of the unique difference for that participant from the sample mean and group mean, meaning that we take into consideration that each subject is unique. What is the models prediction for a typical participant in the Salad group (\\(\\hat{A}_3\\))? (hint: (\\(\\hat{Y}_{ij}\\) = mu + Ai) Answer The prediction would be \\(\\hat{Y}_{ij} = \\hat{\\mu} + \\hat{A}_3\\) = 50 + -2 = 48 A typical participant is one where the residual is 0. The model prediction is also known as the fitted value for this group. Where in the table are the differences found between this typical participant prediction and the observed values in the salad group? Answer These are the called the residuals (\\(\\widehat{S(A)}_{ij}\\)) and are found in the err column of the table in rows 9-12. As above they are the difference between that specific individual participant and the typical participant for that group. 11.3.1 Recreate decomposition matrix from the raw data So we have shown you where all the parts of the table come from and how to calculate them. Now, for this part, your task is to reproduce the decomposition matrix tibble shown above, reproduced here: Table 11.2: Decomposition Matrix of our data ID i j Yij mu Ai err 1 1 1 37 50 8 -21 2 1 2 80 50 8 22 3 1 3 64 50 8 6 4 1 4 51 50 8 -7 5 2 1 33 50 -6 -11 6 2 2 47 50 -6 3 7 2 3 55 50 -6 11 8 2 4 41 50 -6 -3 9 3 1 59 50 -2 11 10 3 2 23 50 -2 -25 11 3 3 50 50 -2 2 12 3 4 60 50 -2 12 You will do this by typing the observed values into a tibble, and then writing code to add columns with estimates of the individual components. At the end, your table should look exactly like the one above. You already know how to do all the data-wrangling elements, so today really try to focus on understanding what the values mean. And remember that for each step there is a solution at the end of the chapter. 11.3.2 Step 1: Create the basic tibble Create a tibble named dmx (short for decomposition matrix). It will eventually contain all of the columns in the one above, but for now, just create the columns i, j, and Yij as they appear above. You already know how to create a tibble (dont forget to load the tidyverse package first). In case you need to refresh your memory, see page 2 of this cheatsheet on data input or refer back to the preclass activities of Chapter 5. You should just type in the values for Yij but try to use the rep() function for columns i (the group) and j (the participant). Step 1 Hints You will need some wrangling functions so dont forget to load in tidyverse Create a tibble as dmx &lt;- tibble(i = NA, j = NA, Yij = NA) When using the rep() function remember that you can use each or times as calls in rep: rep(1:3, each = 4) When typing in numbers the c() function will allow you to put in numbers such as Column = c(37, 80, 64, 31) 11.3.3 Step 2: Estimate the Grand Mean \\(\\hat{\\mu}\\) Great, we have our group numbers \\(i\\), our participant numbers \\(j\\), and our participant scores \\(Yij\\). Now we need to start expanding out dmx. First thing we need is the grand mean: \\(\\hat{\\mu}\\) In a new tibble called dmx2, add a column to the dmx tibble, called mu representing \\(\\hat{\\mu}\\). Remember that you can add a column to a table using mutate(). \\(\\hat{\\mu}\\) is the mean() of all participants. Step 2 Hints When calculating mu keep in mind that each value of mu should be the grand mean of the sample; the mean of all participants regardless of group. dmx2 &lt;- dmx %&gt;% mutate(mu = ???) 11.3.4 Step 3: Entering the estimates \\(\\hat{A}_1\\), \\(\\hat{A}_2\\), \\(\\hat{A}_3\\) Good! Now we need to add on a column showing the typical effect of being a member of a particular group; the Ai column. In a new tibble called dmx3, add a column to the tibble dmx2 called Ai, with the three estimates for \\(\\hat{A}_i\\). Store the resulting tibble in dmx3. \\(\\hat{A}_i\\) is the difference between the grand mean \\(\\hat{\\mu}\\) and the mean of individual groups. This means that you will need to group people by the group they belong to. Add the ungroup() function to the end of your pipeline as you wont need this grouping after. Step 3 Hints To calculate the column \\(\\hat{A}_i\\) would be something like: dmx3 &lt;- dmx2 %&gt;% group_by(something) %&gt;% mutate(Ai = something - something) %&gt;% ungroup() 11.3.5 Step 4: Calculate Residuals \\(\\widehat{S(A)}_{ij}\\) Well done, youre almost there! We just need to add on the final column, err, which is called the residuals or in other words, the difference between the score of a typical participant for that group (mu + Ai) and a given individual participant score (Yij). In a new tibble called dmx4 add a column to the dmx3 tibble called err that contain the residuals - the difference between the observed (Yij) and fitted (typical) values. Step 4 Hints For calculating err you would use: \\(\\widehat{S(A)}_{ij} = Y_{ij} - \\hat{Y}_{ij}\\) where: \\(\\hat{Y}_{ij} = \\hat{\\mu} + \\hat{A}_i\\) 11.3.6 Step 5: Sums of squares Great! Last step for today. Once you have your final dmx, dmx4, you can start calculating the sums of squars. Sums of squares are used in calculations for performing tests on model components, which we will learn more about soon, but you can practice them for now as shown in Section 3.4 of Miller and Haden (2013). The steps are as follows: Step1 - Square all the individual values in the columns Yij, mu, Ai, and err in dmx4, Step2 - Now sum up the squared values for each of these columns. Step3 - Save these in a variable called sstbl. The simplest way to square a column, for example called x, would be: mutate(squared_x = x^2) The ^2 means take x to the power of 2. So typing 3^2 in the console will give you 9 (try it if youre unsure). It also works for columns! Give that a go. If you have done it correctly you should see the below table: .dmx %&gt;% mutate(Yij2 = Yij^2, mu2 = mu^2, Ai2 = Ai^2, err2 = err^2) %&gt;% select(Yij2, mu2, Ai2, err2) %&gt;% summarise(ss_Y = sum(Yij2), ss_mu = sum(mu2), ss_Ai = sum(Ai2), ss_err = sum(err2)) %&gt;% knitr::kable(align = &quot;c&quot;, caption =&quot;Sums of Squares for this analysis&quot;) Table 11.3: Sums of Squares for this analysis ss_Y ss_mu ss_Ai ss_err 32580 30000 416 2164 Where: ss_Y represents the Sums of Squares of Yij. This is referred to as the Sums of Squares total or \\(SS_{total}\\). ss_mu represents the Sums of Squares of mu. This is referred to as the Sums of Squares of the grand mean or \\(SS_{\\mu}\\) and sometimes called the intercept. ss_Ai represents the Sums of Squares of Ai. This is referred to as the Sums of Squares of A or \\(SS_{A}\\) and sometimes called \\(SS_{between}\\). ss_err represents the Sums of Squares of err. This is referred to as the Sums of Squares error or \\(SS_{error}\\) and sometimes called \\(SS_{within}\\). And the following statement is true: \\[SS_{total} = SS_{\\mu} + SS_{A} + SS_{error}\\] Step 5 Hints Mutate on the squared values: dmx4 %&gt;% mutate(Yij2 = Yij^2, ) And sum up using: summarise(ss_Y = sum(Yij2), ss_mu = ) Job Done - Activity Complete! Well done! How did you do? If the values in your dmx4 do not match the table above then you need to go back and look at where it has gone wrong. Alternatively you should look at the solutions at the end of the chapter. To recap, what we are doing here is setting up the ANOVA to compare the three groups to see if there is a significant difference between each group. Doing it this way allows us to get an understanding of where the numbers come from, and it highlights that the analysis is about comparing variance within groups and variance between groups. We will cover this in the next chapter but in short, if the variance between groups \\(SS_{A}\\) is larger than the variance within groups \\(SS_{error}\\) then it is likely that your experimental manipulation has had an effect. Conversely, if the variance within groups is larger than the variance between groups then there is likely to be no effect of your experimental manipulation. One last thing: Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become. 11.4 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. Lab 11: Introduction to GLM: One-factor ANOVA In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester2_Lab2.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Single Answer and Multiple Choice Questions For this assignment you will answer a series of short single and Mulitple Choice Questions, followed by a calculation of a decomposition matrix in the final task. In order to complete this formative assignment you will need to have completed the inclass activity and have read Miller and Haden Chapter 3. Before starting lets check: The .Rmd file is saved in a folder and that you have set your working directory to that folder. For assessments we ask that you save it with the format GUID_Level2_Semester2_Lab2.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. Lets Begin! 11.4.1 Question 1 Consider the following description of a study. You are investigating whether there is seasonal variation in students bodyweight. In other words, is there any evidence that bodyweight differs across the four seasons (Winter, Spring, Summer, and Fall - #AllYouGotToDoIsCall)? Which of the models shown below would be the the general linear model corresponding to this study? \\(Y_{ij} = \\mu + A_{i} + S(A)_{ij}\\) \\(Y_{ijkm} = \\mu + A_{i} + B_{j} + C_{k} + D_{m} + S_{ijkm}\\) \\(Y_{ij} = \\beta_0 + \\beta_1 X_1 + e_{ij}\\) \\(Y_{ijkm} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + e_{ijkm}\\) Replace the NULL in the Q1 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). mcq1 &lt;- NULL For the next few questions consider the decomposition matrix for a one-factor design with three groups, shown below . \\(i\\) \\(j\\) \\(Y_{ij}\\) \\(\\hat{\\mu}\\) \\(\\hat{A}_{i}\\) \\(\\widehat{S(A)}_{ij}\\) 1 1 4 6 -1 -1 1 2 6 6 -1 1 2 1 4 6 0 -2 2 2 8 6 0 2 3 1 2 6 1 -5 3 2 12 6 1 5 11.4.2 Question 2 According to the above decomposition matrix, the population grand mean is estimated to be: 0 6 36 cant answer; not observed Replace the NULL in the Q2 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). mcq2 &lt;- NULL 11.4.3 Question 3 According to the above decomposition matrix, the value of \\(\\hat{A}_3\\) is: 6 0 1 cant answer; not observed Replace the NULL in the Q3 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). mcq3 &lt;- NULL 11.4.4 Question 4 According to the above decomposition matrix, the predicted value for a participant in group 1 is what? Hint: this is the fitted or typical for that group (\\(\\hat{Y}_{ij}\\)) as opposed to the actual value (\\(Y_{ij}\\)) Replace the NULL in the Q4 code chunk with the actual value of the correct answer (e.g a number). Q4 &lt;- NULL # replace NULL with your answer (a number) 11.4.5 Question 5 Which observation or observations has/have the largest residual(s)? \\(Y_{21}\\) \\(Y_{21}\\) and \\(Y_{22}\\) \\(Y_{31}\\) \\(Y_{31}\\) and \\(Y_{32}\\) Replace the NULL in the Q5 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). Q5 &lt;- NULL 11.4.6 Question 6 From your reading of Miller and Haden Chapter 3, and from the inclass activity Section 5, based on the above decomposition matrix, what would \\(SS_{total}\\) be for this model? Replace the NULL in the Q6 code chunk with the actual value of the correct answer (e.g a number). Q6 &lt;- NULL # replace NULL with your answer (a number) 11.4.7 Question 7 From your reading of Miller and Haden Chapter 3, and from the inclass activity Section 5, based on the above decomposition matrix, what would \\(SS_{error}\\) be for this model? Replace the NULL in the Q7 code chunk with the actual value of the correct answer (e.g a number). Q7 &lt;- NULL # replace NULL with your answer (a number) 11.4.8 Question 8 From reading Miller and Haden Chapter 3, and from the inclass activity Section 5, a study with a one-factor design with GLM \\(Y_{ij} = \\mu + A_{ij} + S(A)_{ij}\\) is found to have the following SS: \\(SS_{total} = 280\\), \\(SS_{\\mu} = 40\\), and \\(SS_{error} = 60\\). Given those values, what is the value of \\(SS_{A}\\)? hint: \\(SS_{total}\\) = \\(SS_{\\mu}\\) + \\(SS_{A}\\) + \\(SS_{error}\\) Replace the NULL in the Q8 code chunk with the actual value of the correct answer (e.g a number). Q8 &lt;- NULL # replace NULL with your answer (a number) 11.4.9 Question 9: Create your own decomposition matrix Finally, this last task tests your ability to set up a decomposition matrix as shown inclass. The code chunk below creates the basic table structure you will need to complete this task. Run the code and have a look at the table, but DO NOT CHANGE IT! ## run this block, have a look at the structure of dsetup, ## but don&#39;t change anything library(&quot;tidyverse&quot;) dsetup &lt;- tibble(i = rep(1:4, each = 3), j = rep(1:3, times = 4), Yij = NA, mu = NA, Ai = NA, err = NA) In the code chunk below, flesh out the values in dsetup to create a decomposition matrix for the data shown below (a one-factor design with four levels), but with the actual numeric values replacing the NA values. Group 1: 84, 86, 61 Group 2: 83, 71, 95 Group 3: 56, 95, 92 Group 4: 68, 76, 93 IMPORTANT! Make sure the final table with your result has the name dmx. Check spelling and capitalization. The values should be computed based on the Yij values such that if the Yij values were to change then your code would still produce the correct decomposition matrix. DO NOT change the column names, the column ordering, and make sure it has the right number of rows and columns. You should have 12 rows by 6 columns. Make sure your code runs without error in a fresh R session, and make sure no warnings are generated by the code chunk named dmx_warning, which validates your response. # TODO: DO STUFF WITH dsetup # you can change or remove the line below, # but make sure your final table is called dmx dmx &lt;- NULL Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solutions which can be found at the end of this Chapter. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the available forums or speak to a member of the team. On to the next chapter! 11.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 11.5.1 InClass Activities 11.5.1.1 InClass Step 1 The basic tibble would be created as follows. When it comes to \\(Y_{ij}\\), simply typing in the values in order was what was needed. library(&quot;tidyverse&quot;) dmx &lt;- tibble(i = rep(1:3, each = 4), j = rep(1:4, times = 3), Yij = c(37, 80, 64, 51, 33, 47, 55, 41, 59, 23, 50, 60)) Return to Task 11.5.1.2 InClass Step 2 The Grand Mean can be added as follows: dmx2 &lt;- dmx %&gt;% mutate(mu = mean(Yij)) And would appear as: Table 11.4: Decomposition Matrix with Grand Mean added i j Yij mu 1 1 37 50 1 2 80 50 1 3 64 50 1 4 51 50 2 1 33 50 2 2 47 50 2 3 55 50 2 4 41 50 3 1 59 50 3 2 23 50 3 3 50 50 3 4 60 50 Return to Task 11.5.1.3 InClass Step 3 The estimates \\(\\hat{A}_1\\), \\(\\hat{A}_2\\), \\(\\hat{A}_3\\), or in other words the unique contribution of each group, are calculated as follows. The key point is grouping by i so that each group is accounted for individually. dmx3 &lt;- dmx2 %&gt;% group_by(i) %&gt;% mutate(Ai = mean(Yij) - mu) %&gt;% ungroup() And would appear as: Table 11.5: Decomposition Matrix with Group Estimates added i j Yij mu Ai 1 1 37 50 8 1 2 80 50 8 1 3 64 50 8 1 4 51 50 8 2 1 33 50 -6 2 2 47 50 -6 2 3 55 50 -6 2 4 41 50 -6 3 1 59 50 -2 3 2 23 50 -2 3 3 50 50 -2 3 4 60 50 -2 Return to Task 11.5.1.4 InClass Step 4 The residuals are calculated as follows: dmx4 &lt;- dmx3 %&gt;% mutate(err = Yij - (mu + Ai)) And would appear as: Table 11.6: Decomposition Matrix with Residuals added i j Yij mu Ai err 1 1 37 50 8 -21 1 2 80 50 8 22 1 3 64 50 8 6 1 4 51 50 8 -7 2 1 33 50 -6 -11 2 2 47 50 -6 3 2 3 55 50 -6 11 2 4 41 50 -6 -3 3 1 59 50 -2 11 3 2 23 50 -2 -25 3 3 50 50 -2 2 3 4 60 50 -2 12 Return to Task 11.5.1.5 InClass Step 5 (version 1) mutate() on the squared values column select() only those columns summarise(sum) those columns sstbl &lt;- dmx4 %&gt;% mutate(Yij2 = Yij^2, mu2 = mu^2, Ai2 = Ai^2, err2 = err^2) %&gt;% select(Yij2, mu2, Ai2, err2) %&gt;% summarise(ss_Y = sum(Yij2), ss_mu = sum(mu2), ss_Ai = sum(Ai2), ss_err = sum(err2)) 11.5.1.6 InClass Step 5 (version 2) There is an alternative way to do the above in a supercool, superquick, two lines of code using dplyrs scoping technique. Have a look at ?dplyr::scoped and ?dplyr::summarise_all. Dont worry if you dont understand this yet, as it is pretty advanced, but as you can see it gives the same output as we created in class. sstbl &lt;- dmx4 %&gt;% select(Yij:err) %&gt;% summarise_all(list(name = ~ sum(.^2))) Return to Task 11.5.2 Test Yourself Activities 11.5.2.1 Assignment Question 1 The correct model for this scenario would be: \\(Y_{ij} = \\mu + A_{i} + S(A)_{ij}\\) As such the correct answer is: mcq1 &lt;- 1 Return to Task 11.5.2.2 Assignment Question 2 The population grand mean for the shown decomposition matrix is \\(\\hat{\\mu}\\) = 6 As such the correct answer is: mcq2 &lt;- 2 Return to Task 11.5.2.3 Assignment Question 3 The value for the shown decomposition matrix is \\(\\hat{A}_3\\) = 1 As such the correct answer is: mcq3 &lt;- 3 Return to Task 11.5.2.4 Assignment Question 4 The fitted or typical value for a participant in Group 1 would be \\(\\hat{Y}_{ij}\\) = \\(\\mu\\) + \\(A_i\\) As such the correct answer is: Q4 &lt;- 5 Return to Task 11.5.2.5 Assignment Question 5 The participants/observations with the largest residuals are \\(Y_{31}\\) and \\(Y_{32}\\) As such the correct answer is: Q5 &lt;- 4 Return to Task 11.5.2.6 Assignment Question 6 The \\(SS_{total}\\) for this model would be calculated as: Q6 &lt;- 4^2 + 6^2 + 4^2 + 8^2 + 2^2 + 12^2 As such giving a \\(SS_{total}\\) of 280 Return to Task 11.5.2.7 Assignment Question 7 The \\(SS_{error}\\) for this model would be calculated as: Q7 &lt;- (-1)^2 + 1^2 + (-2)^2 + 2^2 + (-5)^2 + 5^2 As such giving a \\(SS_{error}\\) of 60 Return to Task 11.5.2.8 Assignment Question 8 From reading Miller and Haden Chapter 3, and from the inclass activity Section 5, a study with a one-factor design with GLM \\(Y_{ij} = \\mu + A_{ij} + S(A)_{ij}\\) is found to have the following SS: \\(SS_{total} = 280\\), \\(SS_{\\mu} = 40\\), \\(SS_{error} = 60\\). \\(SS_{total} = SS_{\\mu} + SS_{A} + SS_{error}\\) And so: \\(SS_{A} = SS_{total} - (SS_{\\mu} + SS_{error})\\) Or, in other words: \\(SS_{A} = SS_{total} - SS_{\\mu} - SS_{error}\\) As such, given the above values and formula the value of \\(SS_{A}\\) would be \\(SS_{A}\\) = 180 As such the correct answer is: Q8 &lt;- 180 Return to Task 11.5.2.9 Assignment Question 9 Entering the following values: Group 1: 84, 86, 61 Group 2: 83, 71, 95 Group 3: 56, 95, 92 Group 4: 68, 76, 93 dmx can be created as shown: dsetup &lt;- tibble(i = rep(1:4, each = 3), j = rep(1:3, times = 4), Yij = NA, mu = NA, Ai = NA, err = NA) dmx &lt;- dsetup %&gt;% mutate(Yij = c(84, 86, 61, 83, 71, 95, 56, 95, 92, 68, 76, 93), mu = mean(Yij)) %&gt;% group_by(i) %&gt;% mutate(Ai = mean(Yij) - mu) %&gt;% ungroup() %&gt;% mutate(err = Yij - (mu + Ai)) Producing the following output: Table 11.7: Decomposition Matrix of Ch11 Assignment Task 9 i j Yij mu Ai err 1 1 84 80 -3 7 1 2 86 80 -3 9 1 3 61 80 -3 -16 2 1 83 80 3 0 2 2 71 80 3 -12 2 3 95 80 3 12 3 1 56 80 1 -25 3 2 95 80 1 14 3 3 92 80 1 11 4 1 68 80 -1 -11 4 2 76 80 -1 -3 4 3 93 80 -1 14 If you have set-up dmx correctly then you will see the below message at the bottom of your knitted html file. The tibble dmx has been defined properly meaning that the column names (including capitalization), column data types, and tibble structure are as expected. However this does not guarantee that the values are correct and you should check these against the solution. # Don&#39;t change anything in this code chunk; it is just here to help you check that you&#39;ve defined a table called `dmx` correctly. Pay attention to any warnings that appear when you run this chunk. When the warning has gone away, that means that the structure of `dmx` is correct (although it doesn&#39;t mean that the values are correct!) if (identical(names(dmx), names(dsetup)) &amp;&amp; identical(dim(dmx), dim(dsetup))) { warning(&quot;The tibble &#39;dmx&#39; has been defined properly meaning that the column names (including capitalization), column data types, and tibble structure are as expected. However this does not guarantee that the values are correct and you should check these against the solution.&quot;) } If however you see the below, then you need to look again at the structure of dmx You have not yet defined the tibble dmx properly, and in future assignments this may affect your overall grade. Check column names (including capitalization), column data types, and table structure. Return to Task Chapter Complete! "],["continuing-the-glm-one-factor-anova.html", "Lab 12 Continuing the GLM: One-factor ANOVA 12.1 Overview 12.2 PreClass Activity 12.3 InClass Activity 12.4 Assignment 12.5 Solutions to Questions 12.6 Additional Material", " Lab 12 Continuing the GLM: One-factor ANOVA 12.1 Overview In the previous chapter you learned how to decompose a dependent variable into components of the general linear model, expressing the values in terms of a decomposition matrix, before finishing up with calculating the sums of squares. In this chapter, we will take it a step further and look at running the ANOVA from those values. Through this we will start exploring the relationships between sums of squares (SS), mean squares (MS), degrees of freedom (df), and F-ratios. In this chapter we will show you how you go from the decomposition matrix we created in the previous chapter to actually determining if there is a significant difference or not. In the last chapter we had you work through the calculations step-by-step by hand to gain a conceptual understanding, and we will continue that in the first half of the inclass activities. However, when you run an ANOVA, typically the software does all of these calculations for you. As such, in the second part of the activities, well show you how to run a one-factor ANOVA using the aov_ez() function in the afex add-on package. From there you will see how the output of this function maps onto the concepts youve been learning about. Some key terms from this chapter are: Sums of Squares (SS) - an estimate of the total spread of the data (variance) around a parameter (such as the mean). We saw these in Chapter 11 degrees of freedom (df) - the number of observations that are free to vary to produce a known output. Again we have seen these before in all previous tests. The df impacts on the distribution that is used to compare against for probability Mean Square (MS) - an average estimate of the spread of the data (variance) calculated by \\(MS = \\frac{SS}{df}\\) F-ratio - the test statistic of the ANOVA from the F-distribution. Calculated as \\(F = \\frac{MS_{between}}{MS_{within}}\\) or \\(F = \\frac{MS_{A}}{MS_{err}}\\). Again, these terms will become more familiar as we work through this Chapter so dont worry if you dont understand them yet. The main thing to understand is that we go from the indivual data to the decomposition matrix to the Sums of Squares to the Mean Squares to the F-ratio. But in summary this is what we are doing: ANOVA PATH: \\(Data \\to Decomp. Matrix \\to SS \\to MS \\to F\\) As such, the goals of this chapter are to: to demonstrate how Sums of Squares leads to an F-value, finishing off the decomposition matrix to determine the probability of a F-value for given degrees of freedom (df) to explore using the aov_ez() function and how the outcome compares to your decomposition matrix calculations. Note: The afex package is already installed on the Boyd Orr machines and only needs read into the library in the normal fashion. Do not install it on the Boyd Orr Lab machines. If you are using your own laptop you will need to make sure you have installed the afex package. 12.2 PreClass Activity As per the previous Chapter, the PreClass activity for this lab is reading (re-reading in fact) and trying out some of the Miller and Haden activities. We will go over similar activities in the remainder of this chapter so it will really help you to spend a few minutes trying out the Miller and Haden activities and thinking about them. 12.2.1 Read Chapters Re-read Chapter 3 of Miller and Haden (2013) and make sure you are understanding it. Particularly focus on how the decomposition matrix leads to the ANOVA output through sums of squares, dfs, and mean squares. 12.2.2 Try Activities Test your understanding by working through Computational Exercise #1 in section 3.12 of Miller and Haden (page 31) all the way to the summary table. The answer is in Miller and Haden section 3.13 but be sure to work through the example first. Dont worry about sketching the data. What you want to do is work the values through the decomposition matrix all the way to the F-value. The decomposition matrix is very similar to the one we looked at in Chapter 11, but with less conditions. The sums of squares would be calculated the same way as we did in Chapter 11. Use the formulas shown to calculate F. \\[MS = \\frac{SS}{df} \\space\\space\\space\\space\\space F = \\frac{MS_{between}}{MS_{within}}\\] It may also help to review Chapter 4 on probability as we move onto understanding F distributions and p-values. Job Done - Activity Complete! 12.3 InClass Activity One-factor ANOVA: Worked example Just like we did in Chapter 11, lets start with some simulated data corresponding to a between-subjects design with three groups (conditions/levels) on one factor (variable). In this hypothetical study, youre investigating the effects of ambient noise on concentration. You have participants transcribe a handwritten document onto a laptop and count the number of typing errors (DV = typos) each participant makes under their respective different conditions: while hearing ambient conversation such as you would find in a busy cafe (cafe condition); while listening to mellow jazz music (jazz condition); or in silence (silence condition). Again for practice we will only use small, highly under-powered groups. You have three different participants in each condition. As such, your data are as follows: cafe: 111, 102, 111 jazz: 89, 127, 90 silence: 97, 85, 88 Below is the decomposition matrix for this data set, based on the GLM: \\[Y_{ij} = \\mu + A_i + S(A)_{ij}\\] Here we are copying what we did in the last chapter in both the inclass and assignment and what you did for the homework activity - with just less participants. You can have a go at creating the decomposition matrix yourself from scratch if you like, as good practice, or, in the interests of time, feel free to reveal the code and run that code to create the dmx. Note that we have also included a column called sub_id with a unique identifier for each participant. This is not that important for the decomposition matrix but we will definitely need it later for running the ANOVA using the afex::aov_ez() function, so lets just include it now so we dont forget. Reveal DMX code dmx &lt;- tibble(sub_id = 1:9, i = rep(1:3, each = 3), j = rep(1:3, times = 3), typos = c(111, 102, 111, 89, 127, 90, 97, 85, 88), sound = rep(c(&quot;cafe&quot;, &quot;jazz&quot;, &quot;silence&quot;), each = 3)) %&gt;% mutate(mu = mean(typos)) %&gt;% group_by(i) %&gt;% mutate(Ai = mean(typos) - mu) %&gt;% ungroup() %&gt;% mutate(err = typos - (mu + Ai)) Table 12.1: Decomposition Matrix for Typos Example sub_id i j typos sound mu Ai err 1 1 1 111 cafe 100 8 3 2 1 2 102 cafe 100 8 -6 3 1 3 111 cafe 100 8 3 4 2 1 89 jazz 100 2 -13 5 2 2 127 jazz 100 2 25 6 2 3 90 jazz 100 2 -12 7 3 1 97 silence 100 -10 7 8 3 2 85 silence 100 -10 -5 9 3 3 88 silence 100 -10 -2 We finished off last week by calculating the Sums of Squares for the different columns - a measure of the variance attributable to that part of the model (or that column). Remember that to calculate the Sums of Squares (or often shortend to \\(SS\\)) we square all the values in a column and sum them up. So for example: \\[SS_{\\mu} = \\sum\\mu^2 = (\\mu_{11} \\times \\mu_{11}) + (\\mu_{12} \\times \\mu_{12}) + \\space ... \\space (\\mu_{33} \\times \\mu_{33})\\] We also said at the end of the last Chapter that the Sums of Squares of the different columns were all linked through the following relationship: \\[SS_{total} = SS_{\\mu} + SS_{A} + SS_{error}\\] Have a go at calculating the SS of the above decomposition matrix (dmx) using the code we showed you towards the end of the inclass activity in the last chapter. If unsure, then the code can be revealed below: Show how to calculate Sums of Squares dat_ss &lt;- dmx %&gt;% summarise(total = sum(typos^2), ss_mu = sum(mu^2), ss_sound = sum(Ai^2), ss_err = sum(err^2)) Which would give: Table 12.2: Sums of Squares for Typos Example total ss_mu ss_sound ss_err 91574 90000 504 1070 We can then check that we have calculated everything correctly by using the following relationship: \\[SS_{total} = SS_{\\mu} + SS_{A} + SS_{error}\\] then: \\[91574 = 90000 + 504 + 1070\\] 12.3.1 Task 1 - Quick Checks QuickFire Questions Answer the following questions. The solutions are at the end of the chapter. If the corrected total Sum of Squares is the \\(SS_{total}\\) minus the part of the total attributable to the intercept (i.e., the grand mean, \\(SS_{\\mu}\\)), calculate the corrected total Sum of Squares for the above example. hint: \\(SS_{corrected} = SS_{total} - SS_{\\mu}\\) What proportion of the corrected total sum of squares is attributable to the main effect of sound? hint: \\(SS_{sound} = SS_{A}\\) hint: \\(\\frac{SS_{sound}}{SS_{corrected}}\\) What proportion of the corrected total sum of squares is attributable to residual error? hint: \\(SS_{error}\\) 12.3.2 Task 2 - Mean squares and degrees of freedom Great, so now we know how to create our decomposition matrix and how to calculate our sums of squares. The only thing left to do is to calculate the F-ratio (the test statistic for the ANOVA, otherwise called the F-value) to determine if there is a significant effect between our groups. But, as it is always good to have a view of the whole picture, lets not forget that the whole purpose here is to show you where the numbers come from in our quest to determine if there is a significant difference between our groups, or in other words, is there an effect of listening condition on concentration! You will remember from your lectures and from your reading of Miller &amp; Haden (2013) that the F-value is a ratio of two estimates of population variance: \\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{MS_{treatment}}{MS_{error}} = \\frac{MS_{A}}{MS_{S(A)}}= \\frac{MS_{A}}{MS_{error}}\\] And you will also remember that the Mean Square (MS) is the Sums of Squares (SS) divided by its degrees of freedom (df). If you dont remember what degrees of freedom are, go back to pages 21-23 of Miller and Haden (2013). They have a good explanation for it, however, these things are easy to forget, so make sure to qucikly skim back through the book. So the general formula for the Mean Square is: \\[MS = \\frac{SS}{df}\\] Lets start trying to put all this information together! If we know the SS of our group/treatment (\\(SS_{A} = 504\\) - also called the between variance) and we know the SS of our error/residuals (\\(SS_{error} = 1070\\) - also called the within variance), then we can convert both of the \\(SS\\)s to Mean Squares (MS) (i.e. the average variance for that condition) by dividing them by their respective degrees of freedom (df). We can then calculate F-value (also called F-observed) by: \\[F = \\frac{MS_{A}}{MS_{error}}\\] If the \\(MS_{error}\\) is larger than \\(MS_{A}\\) (the group effect) then F-value will be small and there will be no significant effect of group - any difference in groups is purely due to individual differences (another way of thinking about error). On the other hand, if \\(MS_{A}\\) (the group effect) is larger than \\(MS_{error}\\) then F-value will be large, and depending on how large the F-value is, there may be a significant difference caused by your group variable. With all that in mind, and it may take a couple of readings, try to answer the following questions (consulting Miller &amp; Haden Ch. 3 and your lecture slides where needed). The solutions are at the end of the chapter. QuickFire Questions Stated in terms of \\(\\mu_{jazz}\\), \\(\\mu_{cafe}\\), and \\(\\mu_{silence}\\), what is the null hypothesis for this specific study of the effects of sound on typographic errors? You may need to look back to Chapters 6 and 7 to think about this. Hint: the alternative hypothesis for a two condition experiment would be \\(\\mu_{1} \\ne \\mu_{2}\\) How many degrees of freedom are there for \\(A_{i}\\), the main effect of sound, if \\(dfA_{i}\\) = k - 1? How many degrees of freedom are there for \\(S(A)_{ij}\\), the error term, if \\(dfS(A)_{ij}\\) = N - \\(dfA_{i}\\) - 1? Hint: You can also refer to \\(dfS(A)_{ij}\\) as \\(df_{error}\\) Calculate \\(MS_{A}\\), where \\(A\\) is the factor sound. Note: You can access individual columns in a table using double square brackets [[]]; for instance dat_ss[[ss_mu]] gives you the column ss_mu from dat_ss. This is an alternative to $ that some may know; e.g. dat_ss$mu. Calculate \\(MS_{S(A)}\\). Hint: You can also refer to \\(MS_{S(A)}\\) as \\(MS_{error}\\) Hints for Task 2 Remember that the null says that there are no differences between conditions. Sound, our factor, has three levels. N is the total number of participants \\(MS_{A} = \\frac{SS_{A}}{dfA_{i}}\\) \\(MS_{S(A)} = \\frac{SS_{error}}{dfS(A)_{ij}}\\) 12.3.3 Task 3 - F-ratios Last step, calculating the F-value. As above, if the null hypothesis is true (\\(\\mu_{1} = \\mu_{2} = \\mu_{3}\\)), then both estimates of the population variance (\\(MS_{between}\\) and \\(MS_{within}\\)) should be equal, and the \\(F\\)-ratio should approach 1 (because \\(\\frac{x}{x} = 1\\)). Now, we cant expect these two estimates to be exactly equal because of sampling bias, so to see how unlikely our observed F-ratio is under the null hypothesis, we have to compare it to the F-distribution. To learn a bit about the F-distribution we have created a shiny app to play with, which you can see below. Shiny Apps are interactive webpages and applications made through R. Figure 4.12: F-distribution app The F distribution is a representation of the probability of various values of F under the null hypothesis. It depends upon two parameters: \\(df_{numerator}\\) and \\(df_{denominator}\\). Play around with the sliders corresponding to these two parameters and observe how the shape of the distribution changes. There is also a slider that lets you specify an observed \\(F\\) ratio (to one digit of precision). It is represented on the blue line of the graph. Move this slider around and watch how the p-values change. The p-value is the total area under the curve to the right of the blue line. The red line on the plot denotes the critical value of F required for a significant difference, given the \\(\\alpha\\) (type 1 error rate) and the \\(df_{numerator}\\) and \\(df_{denominator}\\) . QuickFire Questions Try using your data and the app to answer the following questions. The solutions are at the end of the chapter. From your data, calculate the observed F-value (called f_obs) for the effect of sound on typos (concentration). hint: \\(MS_{between} / MS_{within}\\) Using the app shown above, set \\(\\alpha = .05\\), set the degrees of freedom to correspond to those in your study, and set the observed F ratio as close as you can to the value you got in the above question. Now, according to the app, what is the critical value for \\(F\\) (hint: red line)? According to the app, what is the approximate \\(p\\) value associated with your observed \\(F\\) ratio? Based on these values, do you reject or retain the null hypothesis? Tricky question: Note that you can use the distribution functions for \\(F\\) in the same way you did in Chapter 4 for the normal distribution (pnorm(), dnorm(), qnorm()) or for the binomial distribution (pbinom(), dbinom(), qbinom()), keeping in mind however that the F distribution, being continuous, is more analogous to the normal distribution. See ?df for the distribution functions associated with \\(F\\). Using the appropriate distribution function, calculate the \\(p\\) value associated with \\(F_{obs}\\). This will be more precise than the app. Hints for Question 5 look at inputs for the function - ?pf ignore ncp f_obs = q lower.tail? What is the probability of obtaining an F_obs higher than your value. 12.3.4 Task 4 - Using afex::aov_ez() Great, so we have calculated the F-value for this test and made a judgement about whether it is significant or not. But that was quite a long way of doing it, and whilst it is always great to understand where the data comes from, you dont want to have to do that each time you run a test. So now we are going to re-analyse the same dataset but this time we are going to have the computer do all the computational work for us. There are various options for running ANOVAs in R, but the function we will be using for this course is aov_ez() function in the afex add-on package. Note that to use aov_ez() you either have to load in the package using library(afex), or you can call it directly without loading using afex::aov_ez() (the package_name::function syntax). If youre just using the function once, the latter often makes more sense. The afex package is already installed in the Boyd Orr machines so it only needs called to the library. On your own machines you will need to install the package if you havent already done so. Have a quick read through the documentation for aov_ez (type ?aov_ez in the console) and then look at the code below which shows how to run the ANOVA and try to understand it. Pay specific attention to how you stipulate the datafile, the dv, the factor, the participants, etc. library(afex) dat &lt;- select(dmx, sub_id, typos, sound) results &lt;- afex::aov_ez(data = dat, dv = &quot;typos&quot;, id = &quot;sub_id&quot;, type = 3, between = &quot;sound&quot;) You will have seen a couple of messages pop up about converting sound to factor. That is fine. You can see these in the solutions if you want to confirm yours are the same. If we then look at the output of the analysis through the following code: print(results$Anova) We see: ## Anova Table (Type III tests) ## ## Response: dv ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 90000 1 504.6729 5.091e-07 *** ## sound 504 2 1.4131 0.3142 ## Residuals 1070 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And if you ran the following code: print(results$anova_table) Then you would be able to see the effect size for this ANOVA in the form of generalised eta-squared (ges) - \\(\\eta_G^2 = .32\\): ## Anova Table (Type 3 tests) ## ## Response: typos ## num Df den Df MSE F ges Pr(&gt;F) ## sound 2 6 178.33 1.4131 0.3202 0.3142 Looking at the ANOVA ouput, do you see how the numbers match up with what we calculated above? You can ignore the intercept for now (though the Sums of Squares will be familiar). This table doesnt show the Mean Squares but it does show the Sums of Squares and the F-value. What do you conclude about the effects of ambient noise on concentration? Conclusion Not much to be honest with you! The study returns a non-significant finding suggesting that there is no significant effect of ambient noise on concentration, F(2, 6) = 1.413, p = .31, ges = 0.32. However, before you go off and publish this highly underpowered study we should probably look to replicate it with a larger sample (which you could calculate using your skills from Chapter 8). Job Done - Activity Complete! Excellent work today! And super interesting as well, huh? Quick, everyone to the cafe and dont worry about the typos!!!! Only joking, we are all going to the cafe to replicate! One last thing: Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become. 12.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 12.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 12.5.1 InClass Activities 12.5.1.1 InClass Task 1 Question 1 \\(SS_{corrected\\ total} = SS_{total} - SS_{\\mu}\\) \\(SS_{total}\\) = 91574 \\(SS_{\\mu}\\) = 90000 \\(SS_{corrected\\ total}\\) = 91574 - 90000 = 1574 Question 2 \\(SS_{sound}\\) = 504 \\(SS_{corrected\\ total}\\) = 1574 \\(SS_{sound} / SS_{corrected\\ total}\\) = 504 / 1574 = 0.32 Question 3 \\(SS_{err}\\) = 1070 \\(SS_{corrected\\ total}\\) = 1574 \\(SS_{err} / SS_{corrected\\ total}\\) = 1070 / 1574 = 0.68 Return to Task 12.5.1.2 InClass Task 2 Question 1 \\(\\mu_{cafe} = \\mu_{jazz} = \\mu_{silence}\\) Question 2 k = number of conditions, so: df = k - 1 = 3 - 1 = 2 Question 3 N = 9, and \\(dfA_{i}\\) = 2, so df = 9 - 2 - 1 = 6 Question 4 The factor df = 2 ms_a &lt;- dat_ss[[&quot;ss_sound&quot;]] / 2L Giving \\(MS_{A}\\) = 252 Question 5 The residual df = 6 ms_err &lt;- dat_ss[[&quot;ss_err&quot;]] / 6L Giving \\(MS_{S(A)}\\) = 178.3333333 Return to Task 12.5.1.3 InClass Task 3 Question 1 f_obs &lt;- ms_a / ms_err Giving a F-value of F = 1.413 when rounded to three decimal places. Question 2 The red line should be about F = 5.143, meaning that that is the minimum value of F, for those dfs, considered to be statistically significant at \\(\\alpha = .05\\) Question 3 Reading off the app, the approximate p-value for f_obs is p = .317. Question 4 As \\(p\\) &gt; \\(\\alpha\\) we would retain the null hypothesis that there is no significant effect of group in this study Question 5 This could alternatively calculated using coding as such: pf(f_obs, 2, 6, lower.tail = FALSE) Which would give a p-value of p = 0.314 and again could be written up as F(2, 6) = 1.413, p = 0.314, suggesting no significant effect of ambiance or location on concentration as measured through the number of typos. Return to Task 12.5.1.4 InClass Task 4 dat &lt;- select(dmx, sub_id, typos, sound) results &lt;- afex::aov_ez(data = dat, dv = &quot;typos&quot;, id = &quot;sub_id&quot;, type = 3, between = &quot;sound&quot;) ## Converting to factor: sound ## Contrasts set to contr.sum for the following variables: sound And when we look at the output table see: Table 12.3: ANOVA output Sum Sq Df F value Pr(&gt;F) (Intercept) 90000 1 504.673 0.000 sound 504 2 1.413 0.314 Residuals 1070 6 From this we can see that the numbers we calculated match up exactly with the values that the function produced - F(2, 6) = 1.41, p = .31. You can also see that there is no significant effect and as such we cannot reject the null hypothesis. Return to Task Chapter Complete! 12.6 Additional Material Below is some additional material that might help you understand ANOVAs a bit more and some additional ideas. 12.6.1 Levenes Test of Homogeneity From previous chapters and from your lectures you will know that every test has a series of assumptions that are required to be held in order for your conclusions to be valid, and the ANOVA is no exception. As always the specific assumptions change depending on the design of the study, but in the instance of the experiment in this chapter - a between-subjects design - the assumptions are as follows: Data should be interval/ratio Data in each condition should be normally distributed Data is independent from each other There is homogeneity of variance across different conditions (i.e. different conditions have equal variances). You will spot that these are actually very similar to a between-subjects t-test and that again just shows that the two tests are highly related. Now we know various ways to check things like normality and variance, with visual inspection being one of them, but a common test that people tend to use is what is called Levenes Test of Homogeneity. In fact, if you think back to Chapter 7, when you run a t-test and set the variance as var.equal = FALSE, if you really wanted to run the test as var.equal = TRUE then it would be Levenes test you should run first to confirm that variance is actually equal. However, the issue with the ANOVA is that it actually assumes variance is equal, so it is best to run a Levenes test whenever you run an ANOVA that has at least one between-subjects condition. That is what we are going to show you here as well as how to interpret it and a little bit of discussion after. The main thing to note about Levenes Test of Homogeniety is that the null hypothesis for this test states that there is no significant difference in variance between conditions: \\[H_{0}: \\sigma_{1}^2 = \\sigma_{2}^2 \\] Conversely, the alternative hypothesis states that there is a difference in variance between conditions: \\[H_{1}: \\sigma_{1}^2 \\ne \\sigma_{2}^2 \\] So a non-significant finding in Levenes test would suggests equal variance across conditions as required by the assumptions of the ANOVA, whereas a significant finding for Levenes Test would suggest unequal variance and you may need to think about the assumptions of your data. Running the test So here is our data again from the main activities, which we have stored in a tibble called dmx: Table 12.1: Decomposition Matrix for Typos Example sub_id i j typos sound mu Ai err 1 1 1 111 cafe 100 8 3 2 1 2 102 cafe 100 8 -6 3 1 3 111 cafe 100 8 3 4 2 1 89 jazz 100 2 -13 5 2 2 127 jazz 100 2 25 6 2 3 90 jazz 100 2 -12 7 3 1 97 silence 100 -10 7 8 3 2 85 silence 100 -10 -5 9 3 3 88 silence 100 -10 -2 Lets run a quick visual inspection of the variance in each condition using a boxplot as such: Figure 12.1: A boxplot of our data to check for homogeneity of variance And as you can see it does not look like there is equal variance. However, maybe we dont really trust our eyes and as such we want to run the Levenes Test. To do that we use the below code: car::leveneTest(typos ~ sound, data = dmx, center = median) ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. Where we are: using the leveneTest() function from the car package. using the formula approach that we saw in t-test where we are saying effectively DV by IV - typos ~ sound, where ~ stands for by. saying the data is in the tibble dmx and we are telling the code to run based on the medians of the distribution. This is a bit of a judgement call and you could use the mean instead of the median, but this paper would seem to suggest the median is optimal: Carroll &amp; Schnieder (1985) A note on levenes tests for equality of variances If we run the leveneTest() code we get the following output: ## Warning in leveneTest.default(y = y, group = group, ...): group coerced to ## factor. Df F value Pr(&gt;F) group 2 0.5165877 0.6208682 6 NA NA And now with your knowledge of understanding F-values and ANOVA tables (see Levenes is really just a sort of ANOVA) you can determine that, F(2, 6) = 0.517, p = .621. This tells us that there is no significant difference between the variance across the different conditions. Which, if you look at the boxplot, is completely the opposite of what we expected! Why is that? Most likely, that is because we have such small numbers of participants in our samples and you will remember from Chapter 8, small samples mean low power and unreliable findings. This is actually a good example why people often use visual inspection as well as some analytical tests to make checks on assumptions and they pull information together to make a judgement. The boxplot would say that variance is unequal. Levenes test would say variance is equal. Here, I would probably suggest taking a cautious approach and run a lot more participants before I consider running my analysis. Using afex to run Levenes One issue with the above example is that it requires another package - car. This isnt a major problem but say you only had the afex package to use. Well fortunately that package also has a function that can check Levenes Test - afex::test_levene(). And it works as follows: First run the ANOVA as shown in the chapter and store the output results &lt;- afex::aov_ez(data = dmx, dv = &quot;typos&quot;, id = &quot;sub_id&quot;, type = 3, between = &quot;sound&quot;) ## Converting to factor: sound ## Contrasts set to contr.sum for the following variables: sound Now use the output of the ANOVA, in this instance results, as the input for the Levenes Test, and again center on the median afex::test_levene(results, center = median) Df F value Pr(&gt;F) group 2 0.5165877 0.6208682 6 NA NA And if we look at that output we see the exact same finding as we did using car::leveneTest()- F(2, 6) = 0.517, p = .621. So the positives of this approach is that you only use one package and it gives the same results as using two packages. The downside is that you have to run the ANOVA first - see how we use the ANOVA output as an input for test_levene() - which is a bit of an odd way of treating this assumption. Normally you would run the assumption check first and then the test. Help I have unequal variance! Right, so you have found that your variance is not equal and you are now worried about what to do in regards your assumptions. Well, there is much debate on how robust the ANOVA is, where robust would mean that your False Positive rate does not necessarily inflate if the assumptions are not held. Some say that the ANOVA is not robust and you should use a non-parametric style of analysis instead in this instance, or break the ANOVA down into a series of Welchs t-tests (assuming unequal variance) and control the False Positive rate through Bonferroni corrections. Some however say that the ANOVA is actually pretty robust. We would recommend reading this paper Blanca et al. (2018) Effect of variance ratio on ANOVA robustness: Might 1.5 be the limit? which suggests that some degree of unequal variance is acceptable. Alternatively, just as there is a Welchs t-test that does not assume equal variance there is also a Welchs ANOVA that does not assume equal variance. This paper Delacre et al., (preprint) Taking Parametric Assumptions Seriously Arguments for the Use of Welchs F-test instead of the Classical F-test in One-way ANOVA goes into great detail about why using Welchs ANOVA is always advisable. You may be wondering how do you run a Welchs ANOVA in R, yeah? That is a very good question that you should ask Phil to write about one day after he figures it out himself. End of Additional Material! "],["continuing-the-glm-two-factor-designs.html", "Lab 13 Continuing the GLM: Two-factor designs 13.1 Overview 13.2 PreClass Activity 13.3 InClass Activity 13.4 Test Yourself 13.5 Solutions to Questions", " Lab 13 Continuing the GLM: Two-factor designs 13.1 Overview For the past couple of weeks we have been building our understanding of the General Linear Model and in particular how it applies to a one-factor between-subjects ANOVA. Remember this is the scenario where you have one IV (categorical) and one DV (continuous) and you want to know if there is a significant effect at the different levels of your factor; where factor is another name for variable (or IV) and level is another name for condition (or group). We started out with the decomposition matrix, calculated our sums of squares, and from there our F-value to determine if there was a significant effect. One thing that is really worth keeping in mind is that the ANOVA is an omnibus test in that it tells you there is a significant effect of that factor, but it doesnt specifically say in which way is that effect manifested; you always have to do a little work there to tease out the pattern of the effect. Say for instance you test a one-way ANOVA on three animal categories on some test (dogs, cats, gerbils). The ANOVA will tell you if there is an overall effect (or difference between groups) but you need to do a little work to find out is the difference between cats and dogs, dogs and gerbils, etc etc. But more on that another time. One-way ANOVAs are great when you only have one IV but the really useful thing about ANOVAs, and the GLM really, is that it can handle much more complex situations; which we are going to look at a little today. You were asked to read up on Chapter 4 of Miller and Haden (2013) looking at two-factor, between-subjects designs. This is the scenario where you have two factors (IVs) and it is different people in each condition. For example, say your IVs were people who can/cant juggle, and people who do/dont have pets. You have 4 groups here as you have people who can juggle and have pets, people who cant juggle and have pets, people who can juggle and dont have pets, and people who cant juggle and who dont have pets (how sad!!!). This would be an example of a two-way between-subjects factorial ANOVA (also a 2x2 ANOVA). And it is this scenario that we will be looking at today. The goals of this chapter are to: extend our knowledge of ANOVAs and GLMs to deal with two factors between-subject designs. understand the concepts of and calculate main effects and interactions. be able to plot and interpret data from factorial ANOVAs. 13.2 PreClass Activity Only one chapter to read this week. Some of the terms will be familiar but some novel so remember to take notes and think of examples which would use the same design but in a different scenario. Once you have read the chapter, try the suggested exercise from the chapter and then the MCQs below to see if you are following things correctly. Anything you are unsure of, post questions on the forum or ask them in the lab. 13.2.1 Read Chapter Read Chapter 4 of Miller and Haden (2013) and try to understand the situation where you have two factors with at least two levels each. In this lab we will look at interactions. 13.2.2 TRY Test your understanding of Miller and Haden (2013) Chapter 4 To test your understanding, work through Computational Exercise #1 in Section 4.9 of Miller and Haden - the answer is in Section 4.10 so check your working but be sure to work through the example first. The concept of interactions should be familiar to you from your statistics lectures this semester. Try these short MCQs on two factor, between-subjects designs: A 2x2 factorial design contains how many cells? two four six eight who do we appreciate! What effect is / effects are tested in a 2x2 ANOVA with factors A and B? the main effects of A and B, and the AB interaction only the main effects of A and B only the AB interaction a correlation between A and B What is a marginal mean? the mean DV at a given level of one factor, averaged over the levels of the other factors the mean DV at a given level of one factor, at a particular level of the other factor a mean that is nearly statistically significant a mean defined based on marginal likelihood What is a cell mean? the mean DV at a given level of one factor, averaged over the levels of the other factors the mean DV at a given level of one factor, at a particular level of the other factor a mean that is nearly statistically significant a mean defined based on marginal likelihood A statistical test for a main effect tests the null hypothesis that simple effects in the sample are equivalent simple effects in the population are equivalent sample marginal means are equivalent population marginal means are equivalent A statistical test for an interaction tests the null hypothesis that the effect of one factor is constant across the levels of the other in the population the effect of one factor is constant across the levels of the other in the sample sample marginal means are equivalent population marginal means are equivalent If you are not sure about the above questions, go back and read the chapter and make sure you understand the difference between a main effect (the effect at one of the IVs) and the interaction (the effect of one factor dependent on the levels of the other factor). Those are the key elements to really wrap your head around in a factorial ANOVA. Note: factorial ANOVAs can get really complex with three, four, or more IVs, so when writing about one, it is often good to state something like two-way (meaning two IVs) or three-way (meaning three IVs) etc. Be clear for your reader. Job Done - Activity Complete! 13.3 InClass Activity 13.3.1 Estimation equations and decomposition matrix We will start today by working with a decomposition matrix for a two-way between-subjects ANOVA and then finish by using the afex::aov_ez() function to show you how you might practically carry out this analysis. Consider the data below from a 2x2 between-subjects design with 3 observations per cell. Keep in mind that each cell is a particular combination of levels of A and B, and each value in a cell, in this instance, is a unique participant. Table 13.1: Data for todays example B1 B2 A1 74, 65, 77 70, 74, 66 A2 67, 67, 64 78, 78, 84 The decomposition matrix for these data is shown below; however, rather unfortunately for us, it is missing the columns AB_ij (\\(\\hat{A}_{ij}\\)) and err (\\(\\widehat{S(AB)}_{ijk}\\)) which we will need to calculate to complete our analysis. Here is a little recap of the columns (plus the two we will add): \\(i\\) - the first factor (here with two levels) \\(j\\) - the second factor (again here with two levels) \\(k\\) - the participant number within that \\(ij\\) combination \\(Y_{ijk}\\) - a participants score on a DV \\(\\mu\\) (mu) - the overall grand mean (or baseline effect) \\(A_i\\) - the effect of the first factor \\(i\\) \\(B_j\\) - the effect of the second factor \\(j\\) \\(\\hat{AB}_{ij}\\) - the effect of the AB interaction \\(\\widehat{S(AB)}_{ijk}\\) - the effect of within-group variability or error, err Table 13.2: Incomplete Decomposition Matrix i j k Y_ijk mu A_i B_j 1 1 1 74 72 -1 -3 1 1 2 65 72 -1 -3 1 1 3 77 72 -1 -3 1 2 1 70 72 -1 3 1 2 2 74 72 -1 3 1 2 3 66 72 -1 3 2 1 1 67 72 1 -3 2 1 2 67 72 1 -3 2 1 3 64 72 1 -3 2 2 1 78 72 1 3 2 2 2 78 72 1 3 2 2 3 84 72 1 3 The code to create the above matrix is in the solutions at the end of this chapter in case you want to create the matrix yourself as practice. If not, copy and paste the code from the solutions into a code chunk of an R Markdown file or into an R script (and make sure you also load tidyverse so that you can use the dplyr functions and pipes.) Run the code and look at decomp to confirm to yourself that it worked. Note the use of the group_by() function so that the values calculated with in mutate() are only calculated for each group. 13.3.2 Adding the missing columns Once you understand the table and code, try writing code to add the two missing columns to our matrix. Store the resulting table in decomp2. Here are some hints but again the code is in the solutions if you cant quite get it - but remember, to paraphrase Dumbledore: Help will always be given at Glasgow to those that look for it. Hints: AB_ij - (\\(\\hat{A}_{ij}\\)) - is what is left of the mean value of all participants in that group once you have removed the effect of the grand mean, the effect of factor one, and the effect of factor two err - (\\(\\widehat{S(AB)}_{ijk}\\)) - is what is left from an individuals score after removing the effect of the grand mean, the effect of factor A, the effect of factor B, and the interaction effect. 13.3.3 Understanding the two-factor decomposition matrix If you have performed the above steps correctly, then the decomp matrix should now look like Table 13.3: The complete decomposition matrix i j k Y_ijk mu A_i B_j AB_ij err 1 1 1 74 72 -1 -3 4 2 1 1 2 65 72 -1 -3 4 -7 1 1 3 77 72 -1 -3 4 5 1 2 1 70 72 -1 3 -4 0 1 2 2 74 72 -1 3 -4 4 1 2 3 66 72 -1 3 -4 -4 2 1 1 67 72 1 -3 -4 1 2 1 2 67 72 1 -3 -4 1 2 1 3 64 72 1 -3 -4 -2 2 2 1 78 72 1 3 4 -2 2 2 2 78 72 1 3 4 -2 2 2 3 84 72 1 3 4 4 So lets now make sure we understand the table that we have and that we can pinpoint different elements of it by answering the following questions. The solutions are at the end of the chapter. From the options, what was the DV-value of participant \\(Y_{212}\\)? 64 65 66 67 Type in the value of \\(SS_{B}\\): Type in the value of \\(SS_{error}\\) is: Type in the value of \\(MS_{B}\\) is: Type in the value of \\(MS_{error}\\) (to one decimal places) is: The value of \\(F_{B}\\) (the F-ratio for the main effect of B) to 3 decimal places is: The numerator and denominator degrees of freedom associated with this \\(F\\) ratio are and respectively The \\(p\\) value associated with this F ratio to three decimal places is (HINT: ?pf): 13.3.4 Get your data ready for analysis It is excellent that you now understand a decomposition matrix and how it relates to an F-ratio. In reality however, rarely will you ever derive a decomposition matrix by hand; the point was to improve your understanding of the calculations behind an ANOVA. Lets continue using the simulated data from above and run through the analysis steps we would normally follow. But first, lets put it in a more useful format. The first thing we might want to do is to add columns that more clearly indicate the levels of our two factors. Right now the levels of A are represented by i and the levels of B are represented by j. But the data should really look more like this below: Table 13.4: Converted Decomposition Matrix id A B Y_ijk 1 A1 B1 74 2 A1 B1 65 3 A1 B1 77 4 A1 B2 70 5 A1 B2 74 6 A1 B2 66 7 A2 B1 67 8 A2 B1 67 9 A2 B1 64 10 A2 B2 78 11 A2 B2 78 12 A2 B2 84 We will need the id column (a unique value for each participant) for when we run afex::aov_ez() later. Again the code to convert decomp into this table (named dat) is in the solutions at the end of the chapter and you can use it if you like, but if you want to practice your skills first and convert the table yourself, that is also fine. Dont spend too long on it though as it is more the output we want to look at. So if you are stuck, copy the code and run it in your session. Well be working with the new table dat for the remaining exercises. 13.3.5 Visualizing 2x2 designs: The interaction plot A critical part of data analysis is visualization. When dealing with factorial data, one of the most important visualizations is the interaction plot showing the cell means. You have already seen some of these in the Miller and Haden chapter and in the previous labs. Remember that before you can make an interaction plot, you need to calculate the cell means. First create a table called cell_means with the cell means stored in the column m. HINT - think group_by and summarise to leave three columns only, each with 4 rows. For example, one row will show A, A, mean-value Next, reproduce the plot below. Dont look at the solution below until youve really tried! You will need two geoms: one to draw the points, one to draw the lines. And think about what is your x and y axes and how do you group the lines. ## `summarise()` has grouped output by &#39;A&#39;. You can override using the `.groups` argument. Figure 13.1: Interaction plot 13.3.6 Running a 2x2 between-subjects ANOVA with aov_ez Excellent! So we have a figure now! In reality, you would want to embellish this figure to make it look more professional, add some error bars, make sure the whole of the y-axis is shown, give proper names to the factors and levels, but it will do for now. You also want to look at the figure and think about what it is telling you. Do you think there will be: A main effect of Factor A? A main effect of Factor B? An interaction between Factors A and B? To some degree these are the three basic hypotheses laid out in any two-way ANOVA. To answer these you can think about: Are the means of \\(A_1\\) and \\(A_2\\) different, disregarding the effect of Factor B? Are the means of \\(B_1\\) and \\(B_2\\) different, disregarding the effect of Factor A? Are the means of \\(A_1\\) and \\(A_2\\) influenced by the effect of Factor B? What do non-parallel (or crossing lines) suggest about an interaction? Looking at the figure you might suggest, no main effect of A, no main effect of B, but that there is an interaction between A and B. Lets test this using the afex::aov_ez() function! Perform the 2x2 ANOVA on dat and store the output in result. Try not to look at the solution until you have tried the ?aov_ez to see how to add more than one condition of the same design. a second hint is that both factors are between, so you want to focus on adding a second between condition set type = 3 We have also placed a short summary of the output of the ANOVA below to give you an idea of the outcome. Think about the outcome for a moment or two before having a go at writing one out and then look at the summary to compare. Summarising the output A two-way between-subjects factorial ANOVA was conducted. A significant interaction was found between Factor A and Factor B, F(1, 8) = 10.97, p = .011, ges = .59. Furthermore, a main effect of Factor B was found, F(1, 8) = 6.17, p = .038, ges = .44, which showed that the mean of \\(B_2\\) (M = 75) was significantly larger than the mean of \\(B_1\\) (M = 69). However, no main effect of Factor A was found, F(1, 8) = 0.686, p = .43, ges = .08. The mean of \\(A_1\\) (M = 71) was similar to the mean of \\(A_2\\) (M = 73). So it turns out we were sort of right and sort of wrong. There was no main effect of Factor A as we predicted. The means at \\(A_1\\) and \\(A_2\\) are very similar when you disregard the effect of Factor B. However, there actually was a main effect of Factor B; i.e. there was a significant difference between the means of \\(B_1\\) and \\(B_2\\) when you disregard the effect of Factor A. And finally, we predicted that there would be a significant interaction and there was one because the effect of Factor A is modulated by Factor B, and vice versa. One thing to point out here, when there are only two conditions in a factor and there is a significant main effect of that Factor (in this example Factor B) then to further qualify that effect you simply have to say which of the two conditions was bigger than the other! Group 2 bigger than Group 1 or Group 1 bigger than Group 2. When there is more than two conditions in a factor (e.g. three) or in the interaction, it is not that straightforward and you need to do further comparisons such as pairwise comparisons, t-test, simple main effects, or TUKEY HSDs, to tease those effects a part. We will cover more of that in the lecture series. 13.3.7 App: Understanding main effects and interactions If time permits (or on your own time), check out the accompanying shiny app, below, on main effects and interactions or by clicking here. This allows you to move sliders and change the sizes of main effects / interactions and see how this affects cell means and effect decompositions. This will help sharpen your intuitions about these concepts. Figure 4.11: Main Effects and Interactions App Job Done - Activity Complete! One last thing: Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become. 13.4 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. Lab 13: Two-Factor ANOVA: Perspective-Taking in Language Comprehension In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester2_Lab4.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Background: Perspective-Taking in Language Comprehension For this assignment, you will be looking at real data from Experiment 2 of Keysar, Lin, and Barr (2003), Limits on Theory of Mind Use in Adults, Cognition, 89, 2941. This study used eye-tracking to investigate peoples ability to take anothers perspective during a kind of communication game. (The data that you will be analysing, while real, did not appear in the original report.) The communication game that participants played was as follows. Each participant sat at a table opposite a confederate participant (a stooge from the lab who pretended to be a naive participant). Between the two participants was an upright set of shelves (see figure below). The participants played a game in which the real participant was assigned the role of the matcher and the confederate the role of the director. The director was given a picture with a goal state for the grid, showing how the objects needed to be arranged. However, the director was not allowed to touch the objects. To get the objects in their proper places, the director needed to give instructions to the matcher to move the objects. For example, the director might say, take the red box in the top corner and move it down to the very bottom row, and the matcher would then perform the action. The matchers eye movements were tracked as they listened to and interpreted the directors instructions. Figure 13.2: Director-Matcher Viewpoints from Keysar, Lin, and Barr (2003) To investigate perspective taking, the instructions given by the director were actually scripted beforehand in order to create certain ambiguities. Most of the objects in the grid, such as the red box, were mutually visible to both participants (i.e., visible from both sides of the grid). However, some of objects, like the brush and the green candle, were occluded from the directors view; the matcher could see them, but had no reason to believe that the director knew the contents of these occluded squares, and thus had no reason to expect her to ever refer to them. However, sometimes the director would refer to a mutually visible object using a description that also happened to match one of the hidden objects. For instance, the director might instruct the matcher to pick up the small candle. Note that for the director, the small candle is the purple candle. A given matcher would see this grid in one of two conditions: In the Experimental condition, the matcher saw an additional green candle in a hidden box that was even smaller than the purple candle (see middle panel of the above figure). This object was called a competitor because it matched the description of the intended referent (the purple candle). In the Baseline condition, the green candle was replaced with an object that did not match the directors description, such as an apple. These ambiguous situations provided the main data for the experiment, and in total there were eight different grids in the experiment that presented analogous situations to the example above. A previous eye-tracking study by the same authors had found the presence of the competitors severely confused the matchers, suggesting that people were surprisingly egocentricthey found it hard to ignore privileged information when interpreting another persons speech. For example, when the director said to pick up the small candle, they spent far more time looking at a hidden green candle than a hidden apple, even though neither one of these objects, being hidden, was a viable referent. We refer to the difference in looking time as the egocentric interference effect. Experiment 2 by Keysar, Lin, and Barr aimed to follow up on this finding. In the previous article, the matcher had reason to believe that the director was merely ignorant of the identity of the hidden objects. But what would happen if the matcher was given reason to believe that the director actually had a false belief about the hidden object? For example, would the matcher experience less egocentric interference if he or she had reason to think that the director thought that the hidden candle was a toy truck? To test this, half of the participants were randomly assigned to a false belief condition, where the matcher was led to believe that the director had a false belief about the identity of the hidden object; the other half participated in the ignorance condition, where as in previous experiments, they were led to believe that the director simply did not know what was in the hidden squares. There were 40 participants in this study, 20 in the false belief condition, and 20 in the ignorance condition. There were also an equal number of male and female participants in the study. To spoil the plot a bit, Keysar, Lin and Barr did not find any effect of condition on looking time. However, they did not consider sex as a potential moderating variable. Thus, we will explore the effects of ignorance vs. false belief on egocentric interference, broken down by the sex of the matcher. Before starting lets check: The .csv file is saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Semester2_Lab4.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 13.4.1 Task 1A: Libraries In todays assignment you will need both the tidyverse and ez packages. Enter code into the t1A code chunk below to load in both of these libraries. # load in the packages 13.4.2 Task 1B: Loading in the data Use read_csv() to replace the NULL in the t1B code chunk below to load in the data stored in the datafile keysar_lin_barr_2003.csv. Store the data in the variable dat. Do not change the filename of the datafile. dat &lt;- NULL Take a look at your data (dat) in the console using glimpse() or View(), or just display it by typing in the name. You will see the following columns: variable description subject unique identifier for each subject sex whether the subject was male or female condition what condition the subject was in looktime egocentric interference We have simplified things from the original experiment by collapsing the baseline vs. experimental conditions into a single DV. Our DV, egocentric interference, is the average difference in looking time for each subject (in milliseconds per trial) for hidden competitors (e.g., small candle) versus hidden non-competitors (e.g., apple). The larger this number, the more egocentric interference the subject experienced. 13.4.3 Task 2: Calculate cell means Today we are going to focus on just the main analysis and write-up, and not the assumptions, but as always before running any analysis you should check that your assumptions hold. One of the elements we will need for our write-up is some descriptives. We want to start by creating some summary statistics for the four conditions. Remember, two factors (sex and condition) with 2 levels each (sex: female vs. male; condition: false belief vs. ignorance) will give you four conditions, and as such in our summary table, four cells created by factorially combining sex and condition. Replace the NULL in the t2 code chunk below to create the four cells created by factorially combining sex and condition, calculating the mean and standard deviation for each cell. Store the descriptives in the tibble called cell_means Call the column for the mean m and the column for the standard deviation sd. Your table should have four rows and four columns as shown below but with your values replacing the XXs Follow the case and spelling exactly. cell_means &lt;- NULL sex condition m sd female false belief XX XX female ignorance XX XX male false belief XX XX male ignorance XX XX 13.4.4 Task 3: Marginal means for sex We will also need to have some descriptives where we just look at the means of a given factor; the marginal means - the means of the levels of one factor regardless of the other factor. Replace the NULL in the t3 code chunk below to calculate the marginal means and standard deviations for the factor sex. Store these descriptives in the tibble marg_sex Call the column for the mean m and the column for the standard deviation sd. Your table should have two rows and three columns as shown below but with your values replacing the XXs Follow the case and spelling exactly. marg_sex &lt;- NULL sex m sd female XX XX male XX XX 13.4.5 Task 4: Marginal means for condition And now do the same for condition. Replace the NULL in the t4 code chunk below to calculate the marginal means and standard deviations for the factor, condition Store these descriptives in the tibble marg_cond Call the column for the mean m and the column for the standard deviation sd. Your table should have two rows and three columns as shown below but with your values replacing the XXs Follow the case and spelling exactly. marg_cond &lt;- NULL condition m sd false belief XX XX ignorance XX XX 13.4.6 Task 5: Interaction plot And finally we are going to need a plot. When you have two factors, you want to show both factors on the plot to give the reader as much information as possible and save on figure space. The best way to do this is through some sort of interaction plot as shown in the lab. It is really a lot easier than it looks and it only requires you to think about setting the aes by the different conditions. Insert code into the t5 code chunk below to replicate the figure shown to you. Pay particular attention to labels, axes dimensions, color and background. Note that the figure must appear when your code is knitted. Note: The figure below is a nice figure but should really have error bars on it if I was including it in an actual paper. Including the error bars may help in clarifying the descriptive statistics and you will see that here, although the means are different, there is huge overlap in terms of error bars which may indicate no overall effect. # to do: something with ggplot to replicate the figure ## `summarise()` has grouped output by &#39;sex&#39;. You can override using the `.groups` argument. Figure 13.3: Replicate this Figure 13.4.7 Task 6: Recap Question 1 Thinking about the above information, one of the below statements would be an acceptable hypothesis for the interaction effect of sex and condition, but which one: In the t6 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t6 We hypothesised that there will be a significant difference between males and females in egocentric interference (mean looking time (msecs)) regardless of condition. We hypothesised that there will be a significant difference between participants in the false belief condition and those in the ignorance condition in terms of egocentric interference (mean looking time (msecs)) regardless of sex of participant. We hypothesised that there would be a significant interaction between condition and sex of participant on egocentric interference (mean looking time (msecs)) We hypothesised that there will be no significant difference between males and females in egocentric interference (mean looking time (msecs)) regardless of condition but that there would be a significant difference between participants in the false belief condition and those in the ignorance condition in terms of egocentric interference (mean looking time (msecs)) regardless of sex of participant. answer_t6 &lt;- NULL 13.4.8 Task 7: Recap Question 2 Thinking about the above information, one of the below statements is a good description of the marginal means for sex, but which one: In the t7 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t7 The female participants have an average longer looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of sex. The female participants have an average shorter looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of condition. The female participants have an average shorter looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of sex. The female participants have an average longer looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of condition. answer_t7 &lt;- NULL 13.4.9 Task 8: Recap Question 3 Thinking about the above information, one of the below statements is a good description of the marginal means for condition, but which one: In the t8 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t8 The participants in the false belief group had an average longer looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of condition. The participants in the false belief group had an average shorter looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of condition. The participants in the false belief group had an average longer looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of sex. The participants in the false belief group had an average shorter looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of sex. answer_t8 &lt;- NULL 13.4.10 Task 9: Running the factorial ANOVA Great, so we have looked at our descriptives and thought about what effects there might be. What we need to do now is run the ANOVA using the aov_ez() function. The ANOVA we are going to run is a two-way between-subjects ANOVA because both conditions are between-subjects variables. You may need to refer back to the lab or to have a look at the help on aov_ez() to see how to add a second variable/factor. Replace the NULL in the t9 code chunk below to run this two-way between-subjects ANOVA. Look at the inclass for guidance. You need the data, the dv, the two between condition, and the participant id. Set the type to type = 3 Do not tidy() the output. Do nothing to the output other than store it in the variable named mod (note: technically it will store as a list). You will see in red in the output that the code will convert the conditions to factors automatically, and set the contrasts. This is fine. mod &lt;- NULL 13.4.11 Task 10: Interpreting the ANOVA output Question Thinking about the above information, one of the below statements is a good summary of the outcome ANOVA, but which one: In the t10 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t10 There is a significant main effect of sex, but no main effect of condition and no interaction between condition and sex. There is a significant main effect of condition, but no main effect of sex and no interaction between condition and sex. There is no significant main effect of sex or condition and there is no significant interaction between condition and sex. There is a significant main effect of sex, a significant main effect of condition, and a significant interaction between condition and sex. answer_t10 &lt;- NULL 13.4.12 Task 11: Report your results Write a paragraph reporting your findings. NOTE: You can use inline code to report the \\(F\\) values, but note that you cannot use broom::tidy() for objects created by ezANOVA(). Here is a hint: mod$ANOVA gives you a table of your results (ezANOVA() returns a list with two elements; mod$ANOVA returns the element of the list called ANOVA that has the results). You can pull() and pluck() whatever you need from this table. Write your summary between the two green lines shown in the Assignment file. Job Done - Activity Complete! Well done, you are finished! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the forums See you in the next lab! 13.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 13.5.1 InClass Activities 13.5.1.1 Estimation equations and decomposition matrix Creating the Decomposition Matrix decomp &lt;- tibble(i = rep(1:2, each = 6), j = rep(rep(1:2, each = 3), times = 2), k = rep(1:3, times = 4), Y_ijk = c(74, 65, 77, 70, 74, 66, 67, 67, 64, 78, 78, 84)) %&gt;% mutate(mu = mean(Y_ijk)) %&gt;% # calculate mu group_by(i) %&gt;% mutate(A_i = mean(Y_ijk) - mu) %&gt;% # calculate A_i group_by(j) %&gt;% mutate(B_j = mean(Y_ijk) - mu) %&gt;% # calculate B_j ungroup() Return to Task 13.5.1.2 Adding the missing columns decomp2 &lt;- decomp %&gt;% group_by(i, j) %&gt;% mutate(AB_ij = mean(Y_ijk) - mu - A_i - B_j) %&gt;% ungroup() %&gt;% mutate(err = Y_ijk - mu - A_i - B_j - AB_ij) Return to Task 13.5.1.3 Understanding the two-factor decomposition matrix Q1 The DV value of participant \\(Y_{212}\\) or the 2nd Participant in \\(I_2\\), \\(J_1\\), is 67 Q2 The Sums of Squares of Factor B is 108 Q3 The Sums of Squares of the Error is 140 Q4 The \\(MS_{B}\\) is 108 Q5 The \\(MS_{error}\\) (to one decimal places) is 17.5 Q6 The F-ratio for the main effect of B to 3 decimal places is 6.171 Q7 The numerator and denominator degrees of freedom associated with this \\(F\\) ratio are 1 and 8 respectively Q8 And based on pf(fratio, 1,8, lower.tail = FALSE) the \\(p\\)-value associated with this F ratio to three decimal places is 0.038 or .038 Return to Task 13.5.1.4 Get your data ready for analysis dat &lt;- decomp %&gt;% mutate(A = paste0(&quot;A&quot;, i), B = paste0(&quot;B&quot;, j), id = row_number()) %&gt;% select(id, A, B, Y_ijk) Return to Task 13.5.1.5 Visualizing 2x2 designs: The interaction plot Creating the Cell means cell_means &lt;- dat %&gt;% group_by(A, B) %&gt;% summarise(m = mean(Y_ijk)) ## `summarise()` has grouped output by &#39;A&#39;. You can override using the `.groups` argument. Reproducing the Plot ggplot(cell_means, aes(A, m, group = B, shape = B, color = B)) + geom_point(size = 3) + geom_line() Figure 13.4: The plot that the code gives Easter Egg Figure Solution The plot above is functional but sometimes you want something a bit more communicative. It is worth working on your figures so, here is an example of what you can think about for your report. Remember to look back through previous labs and homework as well (Semester 1: Lab 3, Lab 7, Lab 6, Lab 5 &amp; 9 assignments, for instance) to see how figures can be improved. The code below adds another few dimensions to the above figure. Copy and run the code in your Rmd, knitting it to HTML, and play with the different parts to see what they do. We have changed the legends to be more descriptive and to have more readable text, fixed the scale for the vertical axis, made the figure black and white without a box, we also added 95% confidence intervals and a figure caption. There are of course various ways to do these changes, in particular the caption, but this is an option. Note: This will only run if you have the tibble dat from earlier in this worksheet cell_means1 &lt;- dat %&gt;% group_by(A, B) %&gt;% summarise(m = mean(Y_ijk), n = n(), sd_scores = sd(Y_ijk), ste_scores = sd_scores/sqrt(n), ci = 1.96 * ste_scores) ## `summarise()` has grouped output by &#39;A&#39;. You can override using the `.groups` argument. ggplot(cell_means1, aes(A, m, group = B)) + geom_point(aes(shape = B), size = 3) + geom_line() + geom_errorbar(aes(ymin = m - ci, ymax = m + ci), width = 0.05, size = .5) + coord_cartesian(ylim = c(0,100)) + labs(x = &quot;Groups in Factor A&quot;, y = &quot;Mean Scores&quot;, caption = &quot;Figure 1. Mean scores from the example data for the two-way \\nbetween-subjects design ANOVA. Error bars indicate 95% \\nconfidence intervals.&quot;) + scale_shape_discrete(&quot;Groups in Factor B&quot;) + theme_classic() + theme(axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), axis.title = element_text(size = 14), legend.title = element_text(size = 14), legend.text = element_text(size = 14), plot.caption = element_text(size = 14, hjust = 0)) Figure 13.5: A nice figure example Return to Task 13.5.1.6 ANOVA Using aov_ez The code result &lt;- aov_ez(data = dat, dv = &quot;Y_ijk&quot;, id = &quot;id&quot;, type = 3, between = c(&quot;A&quot;, &quot;B&quot;)) ## Converting to factor: A, B ## Contrasts set to contr.sum for the following variables: A, B result$anova_table num Df den Df MSE F ges Pr(&gt;F) A 1 8 17.5 0.6857143 0.0789474 0.4316340 B 1 8 17.5 6.1714286 0.4354839 0.0378608 A:B 1 8 17.5 10.9714286 0.5783133 0.0106614 The output Using the anova_table within the output, i.e. result$anova_table, would show us: Table 13.5: The ANOVA output num Df den Df MSE F ges Pr(&gt;F) A 1 8 17.5 0.686 0.079 0.432 B 1 8 17.5 6.171 0.435 0.038 A:B 1 8 17.5 10.971 0.578 0.011 Alternatively, using the Anova within the output, i.e. result$Anova, would show us the Sums of Squares as well: Table 13.6: The ANOVA output Sum Sq Df F value Pr(&gt;F) (Intercept) 62208 1 3554.743 0.000 A 12 1 0.686 0.432 B 108 1 6.171 0.038 A:B 192 1 10.971 0.011 Residuals 140 8 NA NA Return to Task 13.5.2 Test Yourself Activities 13.5.2.1 Task 1A: Libraries In todays assignment you will need both the tidyverse and ez packages. library(afex) library(tidyverse) Return to Task 13.5.2.2 Task 1B: Loading in the data Remember to use read_csv() to load in the data. dat &lt;- read_csv(&quot;keysar_lin_barr_2003.csv&quot;) Return to Task 13.5.2.3 Task 2: Calculate cell means for the cell means. The code below will give the table shown. cell_means &lt;- dat %&gt;% group_by(sex, condition) %&gt;% summarise(m = mean(looktime), sd = sd(looktime)) ## `summarise()` has grouped output by &#39;sex&#39;. You can override using the `.groups` argument. sex condition m sd female false belief 594.5833 899.1660 female ignorance 944.6970 932.6990 male false belief 504.5833 676.7338 male ignorance 611.1111 778.0212 Return to Task 13.5.2.4 Task 3: Marginal means for sex The code below will give the table shown for the marginal means of sex. marg_sex &lt;- dat %&gt;% group_by(sex) %&gt;% summarise(m = mean(looktime), sd = sd(looktime)) sex m sd female 777.9762 911.5331 male 555.0439 707.8138 Return to Task 13.5.2.5 Task 4: Marginal means for condition The code below will give the table shown for the marginal means of condition. marg_cond &lt;- dat %&gt;% group_by(condition) %&gt;% summarise(m = mean(looktime), sd = sd(looktime)) condition m sd false belief 549.5833 775.9108 ignorance 794.5833 861.2306 Return to Task 13.5.2.6 Task 5: Interaction plot The code below will produce the shown figure. ggplot(cell_means, aes(condition, m, shape = sex, group = sex, color = sex)) + geom_line() + geom_point(size = 3) + labs(y = &quot;mean looking time (msecs)&quot;) + scale_y_continuous(limits = c(0, 1000)) + theme_bw() Figure 13.6: You should have produced a similar figure Return to Task 13.5.2.7 Task 6: Recap Question 1 We want the alternative, not the null hypothesis here. So, an acceptable hypothesis for the interaction effect of sex and condition would be: We hypothesised that there would be a significant interaction between condition and sex of participant on egocentric interference (mean looking time (msecs)). As such the correct answer is: answer_t6 &lt;- 3 Return to Task 13.5.2.8 Task 7: Recap Question 2 A good description of the marginal means for sex would be: The female participants have an average longer looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of sex. As such the correct answer is: answer_t7 &lt;- 1 Return to Task 13.5.2.9 Task 8: Recap Question 3 A good description of the marginal means for condition would be: The participants in the false belief group had an average shorter looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of condition. As such the correct answer is: answer_t8 &lt;- 2 Return to Task 13.5.2.10 Task 9: Running the factorial ANOVA The code below will produce the shown ANOVA output mod &lt;- aov_ez(data = dat, dv = &quot;looktime&quot;, id = &quot;subject&quot;, type = 3, between = c(&quot;condition&quot;, &quot;sex&quot;)) knitr::kable(mod$anova_table) num Df den Df MSE F ges Pr(&gt;F) condition 1 36 692778.4 0.7487009 0.0203735 0.3926176 sex 1 36 692778.4 0.6442294 0.0175807 0.4274507 condition:sex 1 36 692778.4 0.2130405 0.0058830 0.6471716 Return to Task 13.5.2.11 Task 10: Interpreting the ANOVA output Question A good summary of the outcome ANOVA would be: There is no significant main effect of sex or condition and there is no significant interaction between condition and sex. As such the correct answer is: answer_t10 &lt;- 3 Return to Task 13.5.2.12 Task 11: Report your results There is no definitive way to write this paragraph but essentially your findings should report both main effects and the interaction, giving appropriate F outputs, e.g. F(1, 36) = .79, p = .38, and give some interpretation/qualification of the results using the means and standard deviations above, e.g. looking time was not significantly different between the false belief task (M = X, SD = XX) or the Ignorance task (M = XX, SD = XX). Something along the following would be appropriate: A two-way between-subjects factorial ANOVA was conducted testing the main effects and interaction between sex (male vs. female) and condition (false belief vs. ignorance) on the average looking time (msecs) on a matching task. Results revealed no significant interaction (F(1, 36) = .21, p = .647) suggesting that there is no modulation of condition by sex of participant in this looking task. Furthermore, there was no significant main effect of sex (F(1, 36) = .64, p = .429) suggesting that male (M = 555.04, SD = 707.81) and female participants (M = 777.98, SD = 911.53) perform similarly in this task. Finally, there was no significant main effect of condition (F(1, 36) = .79, p = .38) suggesting that whether participants were given a false belief scenario (M = 594.58, SD = 775.91) or an ignorance scenario (M = 794.58, SD = 861.23) had no overall impact on their performance. Return to Task Chapter Complete! "],["regression.html", "Lab 14 Regression 14.1 Overview 14.2 PreClass Activity 14.3 InClass Activity 14.4 Assignment 14.5 Solutions to Questions", " Lab 14 Regression 14.1 Overview For the past few weeks we have been looking at designs where you have categorical factors/variables and where you want to see whether there is an effect of a given factor or an interaction between two factors on a continuous DV. And we have looked at this through decomposition matrices and through the afex package and the aov_ez() function. We also briefly mentioned how this approach can be extrapolated into designs with more than two factors such as three-way ANOVAs (three factors) and larger, but also into within-subject designs where every participant sees every stimuli, and mixed-designs where you have at least one between and one within factor. We will look in-depth at these different types of designs next year. Today, however, we want to start looking at predicting relationships from continuous variables through regression. You will already be familiar with many of the terms here from your lecture series. In addition, by looking at a practical example (relating to voice research) in the lab, and by reading about regression in Miller and Haden (2013) in the PreClass, it should start to become more concrete for you. Regression is still part of the GLM and eventually the goal will be to show you how to analyse designs that has both categorical and continuous variables as much of real data is made up like that. But for now we will just look at simple linear regression and multitple linear regression to make you more comfortable with carrying out and interpreting these analyses. The goals of this chapter are to: Introduce the concepts that underpin linear regression. Demonstrate and practice carrying out and interpreting regression analysis with one or more predictor variables. Demonstrate and practice being able to make predictions based on your regression model. 14.2 PreClass Activity As in previous chapters, the PreClass activity is to read the following chapter from Miller and Haden (2013). You may also want to try reading Chapter 14 as well on Multiple Regression but really more to add to your understanding of the general goal, as opposed to the underlying computations. Finally, reviewing your lecture on Simple Linear and Multiple Linear Regression will really help your understanding of this lab. 14.2.1 Read Chapter Read Chapter 12 of Miller and Haden (2013) and try to understand the how the GLM applies to regression. The concept of regression will be familiar to you based on the stats lectures of this semester so some of the terms will just be recapping. Some will be an expansion of your understanding and basing the analysis in terms of the GLM. Optional Read Chapter 14 of Miller and Haden (2013). This covers Multiple Linear Regression. Again the concepts will be familiar to you from the lecture series and reading this chapter would be to enhance your overall understanding, not necessarily the underlying computations. Job Done - Activity Complete! 14.3 InClass Activity You have been reading about regression in Miller and Haden (2013) and have been looking at it in the lectures. Today, to help get a practical understanding of regression, you will be working with real data and using regression to explore the question of whether there is a relationship between voice acoustics and ratings of perceived trustworthiness. The Voice The prominent theory of voice production is the source-filter theory (Fant, 1960) which suggests that vocalisation is a two step process: air is pushed through the larynx (vocal chords) creating a vibration, i.e. the source, and this is then shaped and moulded into words and utterances as it passes through the neck, mouth and nose, and depending on the shape of those structures at any given time you produce different sounds, i.e. the filter. One common measure of the source is pitch (otherwise called Fundamental Frequency or F0 (pronounced F-zero)) (Titze, 1994), which is a measure of the vibration of the vocal chords, in Hertz (Hz); males have on average a lower pitch than females for example. Likewise, one measure of the filter is called formant dispersion (measured again in Hz), and is effectively a measure of the length of someones vocal tract (or neck). Height and neck length are suggested to be negatively correlated with formant dispersion, so tall people tend to have smaller formant dispersion. So all in, the sound of your voice is thought to give some indication of what you look like. More recently, work has focussed on what the sound of your voice suggests about your personality. McAleer, Todorov and Belin (2014) suggested that vocal acoustics give a perception of your trustworthiness and dominance to others, regardless of whether or not it is accurate. One extension of this is that trust may be driven by malleable aspects of your voice (e.g. your pitch) but not so much by static aspects of your voice (e.g. your formant dispersion). Pitch is considered malleable because you can control the air being pushed through your vocal chords (though you have no conscious control of your vocal chords), whereas dispersion may be controlled by the structure of your throat which is much more rigid due to muscle, bone, and other things that keep your head attached. This idea of certain traits being driven by malleable features and others by static features was previously suggested by Oosterhof and Todorov (2008) and has been tested with some validation by Rezlescu, Penton, Walsh, Tsujimura, Scott and Banissy (2015). So the research question today is: can vocal acoustics, namely pitch and formant dispersion, predict perceived trustworthiness from a persons voice? We will only look at male voices today, but you have the data for female voices as well should you wish to practice (note that in the field, tendency is to analyse male and female voices separately as they are effectively sexually dimorphic). As such, we hypothesise that a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. This is what we will analyse. Lets begin. First, to run this analysis you will need to download the data from here. You will see in this folder that there are two datafiles: voice_acoustics.csv - shows the VoiceID, the sex of voice, and the pitch and dispersion values voice_ratings.csv - shows the VoiceID and the ratings of each voice by 28 participants on a scale of 1 to 9 where 9 was extremely trustworthy and 1 was extremely untrustworthy Have a look at the layout of the data and familiarise yourself with it. The ratings data is rather messy and in a different layout to the acoustics but can you tell what is what? Looking at the layout of the acoustics data it appears to be in long wide tidy Looking at the layout of the ratings data it appears to be in long wide It may help to refer back to Chapter 2 of the book on different layouts of data if you are still not quite sure what is the difference between long and tidy. But in terms of today, we are going to need to do some data-wrangling before we do any analysis, so lets crack on with that! 14.3.1 Task 1: Setup Start by opening a new script or .Rmd (depending on how you like to work) and load in the tidyverse, broom, and the two CSV datasets into tibbles called ratings and acoustics. Probably best if the ratings are in ratings and the acoustics in acoustics. Note: Remember that order of packages matter and we recommend always loading in tidyverse last. 14.3.2 Task 2: Restructuring the ratings Next we need to calculate a mean rating score for each voice. We are analysing the voices and not specifically what each participant rated each voice as (that is for another year) so we need to average across all participants, their ratings for each individual voice and get a mean rating for each voice. You will see in your data that the voices are identified in the VoiceID column. Recall the difference between tidy, wide and long data. In wide data, each row represents an individual case, with different observations for that case in separate columns; in long data, each row represents a single observation, and the observations are grouped together into cases based on the value of a variable (for these data, the VoiceID variable). Tidy is a bit like a mix of these ideas but is generally closer to long format, with the main difference being that you are likely to have more than one row relating to a given observation. Before we calculate means, what you need to do is to restructure the ratings data into the appropriate tidy format; i.e., so that it looks like the table below. Table 14.1: Using pivot_longer(), gather the data into tidy format VoiceID participant rating 1 P1 7.0 1 P2 7.5 1 P3 5.5 1 P4 4.5 1 P5 5.0 1 P6 4.0 Write code using pivot_longer() to restructure the ratings data as above and store the resulting tibble as ratings_tidy. Only the first six rows are shown but your tibble will have all the data. In the table above you see the first six ratings of Voice 1, with each rating coming from a different participant. Hint for Task 2 pivot_longer(data, new_column_name, new_column_name, first-column:last-column) dont forget the quotation marks 14.3.3 Task 3: Calculate mean trustworthiness rating for each voice Now that youve gotten your ratings data into a more tidy format, the next step is to calculate the mean rating (mean_rating) for each voice. Remember that each voice is identified by the VoiceID variable. Store the resulting tibble in a variable named ratings_mean. The reason you need the mean rating for each voice is because you have to think what goes into the regression - what is actually being analysed. The analysis requires one trustworthiness rating for each voice, so that means we need to average across all the ratings for each voice. Otherwise we would have far too much data! Hint for Task 3 a group_by and summarise would do the trick remember if there are any NAs then na.rm = TRUE would help 14.3.4 Task 4: Joining the Data together Great! We are so hot at wrangling now we are like hot wrangling irons! But before we get ahead of ourselves, in order to perform the regression analysis, we need to combine the data from ratings_mean (the mean ratings; our Dependent Variable - what we want to predict) with acoustics (the pitch and dispersion ratings; our Predictors). Also, as we said, we only want to analyse Male voices today. Go ahead and join the two tibbles (ratings_mean and acoustics) and filter out the Female voices to keep only the Male voices. Call the resulting tibble joined. The first few rows should look as shown below. If you are not quite sure which join you need then it might be worth having a quick look at the join summary at the end of Chapter 2 Table 14.2: The joined tibble showing the mean trustworthiness rating and the pitch and dispersion value for each voice VoiceID mean_rating sex measures value 1 4.803571 M Pitch 118.6140 1 4.803571 M Dispersion 1061.1148 2 6.517857 M Pitch 215.2936 2 6.517857 M Dispersion 1023.9048 3 5.910714 M Pitch 147.9080 3 5.910714 M Dispersion 1043.0630 Hint for Task 4 inner_join by the common column in both datasets filter to keep just Male voices 14.3.5 Task 5: Scatterplot As always, where possible, it is a good idea to visualise your data. Now that we have all of the variables in one place, reproduce the scatterplot shown below and then try to answer the below questions. Your first version of the figure might look a bit odd as the Hz values of pitch and dispersion are rather different. Remember you can look up the help of different functions to see how to set different axes in facets by using ?facet_wrap but we have also given a hint below. The code for this figure is in the solution. It would be worth taking a few minutes to try it out as it also shows how to change the layout of facet_wrap() using nrow and ncol. ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 14.1: Scatterplot showing the relationship between the voice measures of Dispersion (left) and Pitch (right) and Mean Trustworthiness Rating Answer me: According to the scatterplot, there appears to be a negative relationship positive relationship between both pitch and trustworthiness and dispersion and trustworthiness though the relationship with dispersion pitch seems stronger. Hint for Task 5 The scatterplot ggplot() + geom_point() geom_smooth(method = lm) coord_cartesian or scale_y_continuous facet_wrap(scales = free) did you know also that you can control the number of columns and rows in a facet_wrap by adding nrow and ncol? for example, facet_wrap(~variable, nrow = 1, ncol = 2) The interpretation The first consideration is direction. Remember that a positive relationship means as scores on one variable increase so do scores on the other variable, and negative means as scores on one variables increase, scores on the other variable decrease. The second consideration is strength: strong, medium or weak. 14.3.6 Task 6: Spreading the data with pivot_wider() Ok so we are starting to get an understanding of our data and we want to start thinking about the regression. However, the regression would be easier to work with if Pitch and Dispersion were in separate columns. So I know we just joined the columns together to create the figure and now we are splitting them to do the analyse and that might seem odd. There are ways around this that you will develop in future years but when learning it can help just to keep everything systematic and really highly processed. So sometimes you will see funny moves like this. But remember, if the code works and does what you need, then the only bad code is no code Using the pivot_wider() function, create a new tibble where Pitch and Dispersion data have been split into two columns called Pitch and Dispersion respectively. We used pivot_wider() to spread data in Chapter 5 when creating the difference of two groups so maybe refer back then to see how to input the columns, but there is also information in the hint below. Hint for Task 6 pivot_wider() needs the data, name of the categorical column to spread, and the name of the data to spread pivot_wider(data, names_from = column_name, values_from = column_name) 14.3.7 Task 7: The Regressions Ok we are almost at the point when we need to do the regression. We should always think about assumptions but as that is covered in the lecture series we will mostly leave that there for now. That said, one assumption we can roughly and quickly look at is the correlation between Pitch and Dispersion, remembering the issue of multicollinearity - if predictors are highly correlated (r &gt; .8 for example then it is impossible to tease apart their unique contributions). So, now, run a code using cor.test() to calculate the correlation between Pitch and Dispersion and then fill in this statement. The correlation between Pitch and Dispersion is rho = .239 rho = -.239 rho = .186 rho = -.186 which would suggest that we have no issues with multicollinearity as our two predictors are only slightly correlated. So nothing to worry about there. Right, now lets do some regression analysis. We will first do two simple linear regressions (one for each predictor), and then we will do a multiple linear regression using both predictors. The lm() function in R is the main function we will use to estimate a Linear Model (hence the function name lm). The function takes the format of: lm(dv ~ iv, data = my_data) for simple linear regression lm(dv ~ iv1 + iv2, data = my_data) for multiple linear regression Now, use the lm() function to run the following three regression models. Simple Linear Regression Run the simple linear regression of predicting trustworthiness mean ratings from Pitch and store the model in mod_pitch Run the simple linear regression of predicting trustworthiness mean ratings from Dispersion and store the model in mod_disp Multitple Linear Regression Run the multiple linear regression of predicting trustworthiness mean ratings from Pitch and Dispersion, and store the model in mod_pitchdisp Hint for Task 7 Correlations You could argue that you should use a Spearman correlation for the correlation between Pitch and Dispersion because the scales are very different although measured both in (Hz), though it would not be wrong to use Pearson here either as it is still the same scale. You may need to refer back to Chapter 10 on correlations to remember how to use cor.test(). Regressions The functions would be something like: lm(trust_column ~ pitch_column, data = my_data) for simple linear regression lm(trust_column ~ pitch_column + dispersion_column, data = my_data) for multiple linear regression 14.3.8 Task 8: Model interpretations Now lets look at the results of each model in turn, using the function summary(), e.g. summary(mod_pitch), and try to interpret them based on what you already know about regression outputs from the lectures. Lets start by looking at the mod_pitch model together. summary(mod_pitch) ## ## Call: ## lm(formula = mean_rating ~ Pitch, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52562 -0.30181 0.04361 0.33398 1.20492 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.921932 0.583801 5.005 2.3e-05 *** ## Pitch 0.015607 0.004052 3.852 0.000573 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6279 on 30 degrees of freedom ## Multiple R-squared: 0.3309, Adjusted R-squared: 0.3086 ## F-statistic: 14.83 on 1 and 30 DF, p-value: 0.0005732 From the output we can see that this model, when relating to the population, would predict approximately 30.86% of the variance in trustworthiness ratings (Adjusted-R^2 = 0.3086). We could also say that a linear regression model revealed that pitch significantly predicted perceived trustworthiness scores in male voices in that as pitch increased so does perceived trustworthiness (\\(b\\) = 0.0156, t(30) = 3.852, p &lt; .001). (Remember that these are unstandardised coefficients so the Estimate would mean that a one unit change in pitch would result in a 0.0156 unit change in perceived trust, a rather small change.) Overall, looking at this model with just Pitch as a predictor, we have a regression model of small to medium prediction (i.e. explained variance) but it is significantly better than a model using the mean value of trust ratings as a prediction - as shown by the F-test being significant. Remember from the lectures that the default model is where every predicted value of Y is the mean of Y; we called this the mean model. So our new model is better than using that model based on the mean. Worth also pointing out here that in a simple linear regression the F-test and the t-value for the predictor are the same based on t^2 = F, as seen earlier in this book. For example, looking at the values above, we see that 3.852 \\(\\times\\) 3.852 = 14.83. Just again goes to show that there is a link between the tests we have been looking at. Ok, based on that knowledge, answer the following questions about the two remaining models. The model of just dispersion as a predictor would explain approximately 3% 13% 31% 33% In fact, the model of just dispersion is not significant significant and therefor no use very useful as a model Looking at the multiple linear regression model, the explained variance is 3.05% 13.5% 30.5% 33.5%and as such explains less more variance than the pitch only model. We have added a brief interpretation of the models in the solution to this task as well so it might be worth reading that to make sure you are understanding the outputs. What the above should remind you is that it is not the case that simply putting all the possible predictors into a model will make it a better model. For every predictor you add there is a penalty associated with the Adjusted-R^2 and if the explained variance attributable to the new predictor is not greater than the penalty to overall explained variance then you may actually end up with a worse model despite having more predictors. We touched briefly in the lecture on using the anova(model1, model2) approach to compare models; something like this: anova(mod_pitch, mod_pitchdisp) Which gives the following output: Res.Df RSS Df Sum of Sq F Pr(&gt;F) 30 11.82665 NA NA NA NA 29 11.48600 1 0.3406532 0.8600855 0.3613695 And shows no significant difference between these two models. We will look at model comparison more in the coming months and years but it is always good to keep the rule of parsimony in mind! The simpler the better. 14.3.9 Task 9: Making predictions Congratulations! You have successfully constructed a linear model relating trustworthiness to pitch and dispersion and you can think about applying this knowledge to other challenges - perhaps go look at female voices? However, one last thing you might want to do that we will quickly show you is how to make a prediction using the predict() function. One way you use this, though see the solution to the chapter for alternatives, is: predict(model, newdata) where newdata is a tibble with new observations/values of X (e.g. pitch and/or dispersion) for which you want to predict the corresponding Y values (mean_rating). So lets try that out now. Make a tibble with two columns, one called Pitch and one called Dispersion - exactly as spelt in the model. Give Pitch a value of 150 Hz (quite a high voice) and give Dispersion a value of 1100 Hz - somewhere in the middle. We saw how to create tibbles in Chapter 5 using the tibble() function so you might need to refer back to that. Now put that tibble, newdata, into the predict() function and run it on the mod_pitchdisp. Follow the above format of model as first argument and tibble of new values as second argument. Now, based on the output try to answer this question: To one decimal place, what is the predicted trustworthiness rating of a person with 150 Hz Pitch and 1100 Hz Dispersion - Hint for Task 9 tibble(Pitch = Value, Dispersion = Value) predict(mod_pitchdisp, newdata) Note: Alternative ways to enter the data are in the solution for this task. WAIT! - Didnt we just say that the mod_pitchdisp model is not as good as mod_pitch. Yep. We did and that is actually what the model comparison shows above as well, but we wanted to show you how to enter different predictors into the predict() function. So whilst this is a good teaching aid, you are 100% correct in thinking that in reality we would be better making predictions with just the mod_pitch model as it explains more variance overall. Well done for spotting that! Job Done - Activity Complete! Great! Now you know how to make predictions why not try a few more. Choose some pitch values see what you get! Go record your voice. Extract the pitch using something like PRAAT. Put it in the predict() function. Get a rating of trustworthiness for your voice. Go run a study that has your voice rated for trustworthiness and see how close the model was. Given the explained variance is not great it probably wont be that close but you start to see how, in principle, the idea of regression and prediction of relationships works. The greater the explained variance of your predictors, the better your prediction will be for a novel participant/observation/event! One last thing: Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become. 14.4 Assignment This is a summative assignment and as such, as well as testing your knowledge, skills, and learning, this assignment contributes to your overall grade for this semester. You will be instructed by the Course Lead on Moodle as to when you will receive this assignment, as well as given full instructions as to how to access and submit the assignment. Please check the information and schedule on the Level 2 Moodle page. 14.5 Solutions to Questions Below you will find the solutions to the questions for the Activities in this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 14.5.1 InClass Activities 14.5.2 Task 1 library(&quot;broom&quot;) library(&quot;tidyverse&quot;) ratings &lt;- read_csv(&quot;voice_ratings.csv&quot;) acoustics &lt;- read_csv(&quot;voice_acoustics.csv&quot;) Return to Task 14.5.3 Task 2 We are calling the new tibble ratings_tidy. We did not state what to call it as by now you can make that decision yourself. Just remember that when debugging your analysis paths from now on, the tibble names might not match up so, you may need to do a little bit of backtracking to see where tibbles were created. ratings_tidy &lt;- pivot_longer(ratings, names_to = &quot;participant&quot;, values_to = &quot;rating&quot;, cols = &quot;P1&quot;:&quot;P28&quot;) Return to Task 14.5.4 Task 3 ratings_mean &lt;- ratings_tidy %&gt;% group_by(VoiceID) %&gt;% summarise(mean_rating = mean(rating)) Return to Task 14.5.5 Task 4 joined &lt;- inner_join(ratings_mean, acoustics, &quot;VoiceID&quot;) %&gt;% filter(sex == &quot;M&quot;) Return to Task 14.5.6 Task 5 The following code: ggplot(joined, aes(value, mean_rating)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + scale_y_continuous(breaks = c(1:7), limits = c(1,7)) + facet_wrap(~measures, nrow = 1, ncol = 2, scales = &quot;free&quot;) + theme_classic() Will gives the below figure: ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 14.2: Scatterplot showing the relationship between the voice measures of Dispersion (left) and Pitch (right) and Mean Trustworthiness Rating Return to Task 14.5.7 Task 6 pivot_wider() is the reverse of pivot_longer() It takes in the data and you tell it which values you want to spread and by which column Order is normally, data, names_from, values_from. joined_wide &lt;- joined %&gt;% pivot_wider(names_from = &quot;measures&quot;, values_from = &quot;value&quot;) Return to Task 14.5.8 Task 7 Simple Linear Regression - Pitch The pitch model would be written as such: mod_pitch &lt;- lm(mean_rating ~ Pitch, joined_wide) And gives the following output: summary(mod_pitch) ## ## Call: ## lm(formula = mean_rating ~ Pitch, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52562 -0.30181 0.04361 0.33398 1.20492 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.921932 0.583801 5.005 2.3e-05 *** ## Pitch 0.015607 0.004052 3.852 0.000573 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6279 on 30 degrees of freedom ## Multiple R-squared: 0.3309, Adjusted R-squared: 0.3086 ## F-statistic: 14.83 on 1 and 30 DF, p-value: 0.0005732 Simple Linear Regression - Dispersion The Dispersion model would be written as such: mod_disp &lt;- lm(mean_rating ~ Dispersion, joined_wide) And gives the following output: summary(mod_disp) ## ## Call: ## lm(formula = mean_rating ~ Dispersion, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.87532 -0.41300 -0.02435 0.29850 1.52664 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.345300 1.982971 1.183 0.246 ## Dispersion 0.002584 0.001836 1.407 0.170 ## ## Residual standard error: 0.7434 on 30 degrees of freedom ## Multiple R-squared: 0.06191, Adjusted R-squared: 0.03064 ## F-statistic: 1.98 on 1 and 30 DF, p-value: 0.1697 Multiple Linear Regression - Pitch + Dispersion The model with both Pitch and Dispersion as predictors would be written as such: mod_pitchdisp &lt;- lm(mean_rating ~ Pitch + Dispersion, joined_wide) And gives the following output: summary(mod_pitchdisp) ## ## Call: ## lm(formula = mean_rating ~ Pitch + Dispersion, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.54962 -0.36428 0.04033 0.36327 1.18915 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.444290 1.697362 0.851 0.40179 ## Pitch 0.014855 0.004142 3.586 0.00121 ** ## Dispersion 0.001470 0.001585 0.927 0.36137 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6293 on 29 degrees of freedom ## Multiple R-squared: 0.3501, Adjusted R-squared: 0.3053 ## F-statistic: 7.813 on 2 and 29 DF, p-value: 0.001931 Return to Task 14.5.9 Task 8 A brief explanation: From the models you can see that the Dispersion only model is not actually significant (F(1,30) = 1.98, p = .17) meaning that it is not actually any better than using the model that just predicts the mean values. This is backed up by it only explaining around 3% of the variance. Looking at the multiple linear regression model which contains both pitch and dispersion we can see that it is a significant model ((F(2,29) = 7.81, p = .002) explaining 30.5% of the variance). However looking at the p-values of the coefficients, in the column labelled Pr(&gt;|t|), only pitch is a significant predictor in this model (noting that the p-value for pitch (p = .001) is less than \\(\\alpha\\) = .05, but the p-value for dispersion is not, (p = .361)). In addition, we see that actually the multiple regression model has smaller predictive ability than the pitch alone model. As such, there is an arguement to be made that the pitch alone model is the best model in the current analysis but in future you might want to actually compare the models using the anova(model1, model2) approach. Return to Task 14.5.10 Task 9 Solution Version 1 newdata &lt;- tibble(Pitch = 150, Dispersion = 1100) predict(mod_pitchdisp, newdata) ## 1 ## 5.289819 Solution Version 2 predict(mod_pitchdisp, tibble(Pitch = 150, Dispersion = 1100)) ## 1 ## 5.289819 Solution Version 3 And if you want to bring it out as a single value, say for a write-up, you could do the following This approach may give out a warning about a deprecated function meaning that this wont work in future updates but for now it is ok to use. predict(mod_pitchdisp, tibble(Pitch = 150, Dispersion = 1100)) %&gt;% tidy() %&gt;% pull() %&gt;% round(1) ## Warning: &#39;tidy.numeric&#39; is deprecated. ## See help(&quot;Deprecated&quot;) ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## Please use `tibble()` instead. ## [1] 5.3 or even just: predict(mod_pitchdisp, tibble(Pitch = 150, Dispersion = 1100)) %&gt;% pluck(&quot;1&quot;) %&gt;% round(1) ## [1] 5.3 Return to Task Chapter Complete! "],["combining-anova-and-regression-e-g-ancovas.html", "Lab 15 Combining ANOVA and Regression (e.g. ANCOVAs) 15.1 Overview 15.2 PreClass Activity 15.3 InClass Activity 15.4 Test Yourself 15.5 Solutions to Questions", " Lab 15 Combining ANOVA and Regression (e.g. ANCOVAs) Note: This chapter looks at regression where you have one continuous IV and one categorical IV. More often than not this approach would be called an ANCOVA. However, it can also simply be considered as multitple regression, or the General Linear Model, as really that is what it is all about; just extended to having a mix of continuous and categorical variables. 15.1 Overview Over the last few weeks of the semester we have been really building up our skills on regression and on ANOVAs and now well focus on seeing the link between them. Most places would tell you that they are separate entities but, as you will see from the reading and activities in this lab, they are related. ANOVAs to some degree are just a special type of regression where you have categorical predictors. The question you probably now have is, well, if they are related, cant we merge them and combine categorical and continuous predictors in some fashion? Yes, yes we can! And that is exactly what we are going to do today whilst learning a little bit about screen time and well-being. The goals of this lab are: to gain a conceptual understanding of how ANOVA and regression are interlinked to get practical experience in analysing continuous and categorical variables in one design to consolidate the wrangling skills we have learnt for the past two years 15.2 PreClass Activity In this final PreClass we have two activities. The first is a very short blog by Prof. Dorothy Bishop that helps draw the links between ANOVA and Regression. The second is really the first part of the InClass activity. It is quite a long InClass, which you will be able to cope with, but we have split it across the PreClass and InClass to allow you some time to get some of the more basic wrangling steps out of the way and so you can come to the class and focus on the actual analysis. It would be worth doing most, if not all, of the activity now. 15.2.1 Read Blog Read this short blog by Prof. Dorothy Bishop on combining ANOVA and regression, and how it all fits together. ANOVA, t-tests and regression: different ways of showing the same thing 15.2.2 Activity Background: Smartphone screen time and wellbeing There is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. Well be looking at data from this recent study of English adolescents: Przybylski, A. &amp; Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204215. This was a large-scale study that found support for the Goldilocks hypothesis among adolescents: that there is a just right amount of screen time, such that any amount more or less than this amount is associated with lower well-being. Much like the work you have been doing, this was a huge survey study with data containing responses from over 120,000 participants! Fortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. And the question we want to expand on in this exercise is whether the relationship between screen time and well-being is modulated by partcipants (self-reported) sex. In other words, does screen time have a bigger impact on males or females, or is it the same for both? The dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70. On Przybylski &amp; Weinsteins page for this study on the Open Science Framework, you can find the participant survey, which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen time). Within the same page you can also find the raw data; however, for the purpose of this exercise, you will be using local pre-processed copies of the data found in the accompanying zip file on Moodle or download from here. Przybylski and Weinstein looked at multiple measures of screen time, but again for the interests of this exercise we will be focusing on smartphone use, but do feel free to expand your skills after by looking at different definitions of screen time. Overall, Przybylski and Weinstein suggested that decrements in wellbeing started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, our question is now, does the negative association between hours of use and wellbeing (beyond the one-hour point) differ for boys and girls? Lets think about this in terms of the variables. We have: a continuous\\(^*\\) DV, well-being; a continuous\\(^*\\) predictor, screen time; a categorical predictor, sex. Note: these variables (\\(^*\\)) are technically only quasi-continuous inasmuch as that only discrete values are possible. However, there are a sufficient number of discrete categories in our data that we can treat the data as effectively continuous. Now, in terms of analysis, what we are effectively trying to do is to estimate two slopes relating screen time to well-being, one for girls and one for boys, and then statistically compare these slopes. Sort of like running a correlation for boys, a correlation for girls, and comparing the two. Or alternatively, where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups). But the expressive power of regression allows us to do this all within a single model. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors. We will use an approach called deviation coding which we will look at today later in this lab. Lets Begin! 15.2.3 Loading in the data As always we will need to load in the tidyverse package and load in the data from the accompanying csv files, wellbeing.csv, participant_info.csv, and screen_time.csv. Create a new R Markdown file and put the csv files in the same directory with the Rmd file youve just created. Then load them in as follows (the solution is at the end of the chapter): pinfo stores participant_info.csv wellbeing stores wellbeing.csv screen stores screen_time.csv The Data Take a look at the resulting tibbles pinfo, wellbeing, and screen. The wellbeing tibble has information from the WEMWBS questionnaire; screen has information about screen time use on weekends (variables ending with we) and weekdays (variables ending with wk) for four types of activities: using a computer (variables starting with Comph; Q10 on the survey), playing video games (variables starting with Comp; Q9 on the survey), using a smartphone (variables starting with Smart; Q11 on the survey) and watching TV (variables starting with Watch; Q8 on the survey). If you want more information about these variables, look at the items 8-11 on pages 4-5 of the the PDF version of the survey on the OSF website. note that sex is coded as male = 1, female = 0. also, Serial is common across all datasets and is the participant ID. 15.2.4 Compute the well-being score for each participant Ok, our data is in and we need to create a well-being score for each participant on each item of the WEMWBS. To do this, and to calculate the well-being score for each participant, we simply sum all the items together for that participant. Write code to create a new tibble, called wemwbs, with two variables: Serial, and tot_wellbeing, which is the the total WEMWBS score for each participant. Hint to compute scores Step 1: reshape table from wide to long using pivot_longer(). We suggest creating a new column called var with the labels and a new column called score and gathering all the columns except Serial (e.g. cols = -Serial) Step 2: group_by(); summarise(tot_wellbeing = ) Alternatively, mutate on the sum of all columns, then select the two needed. 15.2.5 Visualising Screen time on all technologies Great, so we have the well-being scores sorted out, we now need to think about the screen time usage and whether it is being used on a weekday or a weekend. As always, to get an idea of the data, it is often very useful to visualize the distributions of variables before proceeding with the analysis. Try recreating this figure based on the data in screen. Note that this will require some tidying of the data in screen: Youll first need to gather the screen tibble into long format and then break apart the column names into two separate columns, one for the name of the variable (Watch, Comp, Comph, and Smart) and the other for part of the week (wk and we). This is going to take using the separate() function which we havent used yet but we think you can manage using the hints below. Next, youll need to alter the values of the variables to reflect the more descriptive text that appears in the plot (e.g., \"Watch\" becomes \"Watching TV\"; \"wk\" becomes \"Weekday\"). This is a recode() issue which you have done a number of times. This is quite a tricky bit of wrangling which we think you are capable of but, do not be put off if you cant quite get it yet. The code is at the end of the chapter for you to use once you have had a shot at it. Figure 15.1: Count of the hours of usage of different types of social media at Weekdays and Weekends Hints on Wrangling Steps 1 and 2 Step 1 pivot_longer() the data in screen into three columns: Serial, var, hours. Do this by gathering all the columns together except Serial (e.g. cols = -Serial) and creating the new columns of var with the label names and hours with the values in it. ?separate() in the console seperate(data, column_name_containing_variables_to_split, c(column_1_to_create,column_2_to_create), character_to_split_by) each variable (category) has an underscore in its name. Use that to split it. I.e. Comph_we will get split into Comph and we if you split by _ Step 2 data %&gt;% mutate(new_variable_name = recode(old_variable_name, wk = Weekday, we = Weekend)) 15.2.6 Visualising the Screen time and Well-being relationship Brilliant, that is truly excellent work and you should be really pleased with yourself. Looking at the figures, it would appear that there is not much difference between screen time use of smartphones in weekend and weekdays so we could maybe collapse that variable together later when we come to analyse it. Overall, people tend to be using all the different technologies for a peak around 3 hours, and then each distribution tails off as you get longer exposure suggesting that there are some that stay online a long time. Video games is the exception where there is a huge peak in the first hour and then a tailing off after that. But first, another visualisation. We should have a look at the relationship between screen time (for the four different technologies) and measures of well-being. This relationship looks like this shown below and the code to recreate this figure is underneath: ## `summarise()` has grouped output by &#39;variable&#39;, &#39;day&#39;. You can override using the `.groups` argument. Figure 15.2: Scatterplot showing the relationship between screen time and mean well-being across four technologies for Weekdays and Weekends At the start we said we were only going to focus on smartphones. So looking at the bottom left of the figure we could suggest that smartphone use of more than 1 hour per day is associated with increasingly negative well-being the longer screen time people have. This looks to be a similar effect for Weekdays and Weekends, though perhaps overall well-being in Weekdays is marginally lower than in Weekends (the line for Weekday is lower on the y-axis than Weekends). This makes some sense as people tend to be happier on Weekends! Sort of makes you wish we had more of them right? Job Done - Activity Complete! That is great work today and it just shows you how far you have come with your wrangling skills over the last couple of years. We will pick up from here in the lab during the week where we will start to look at the relationship in boys and girls. Dont forget to make any notes for yourself that you think will be good to remember - rounding off your Portfolio and bank on skills that you have built up. Any questions or problems, as always post them on the forums or bring them to the lab for discussion. 15.3 InClass Activity Continued: Smartphone screen time and wellbeing We are going to jump straight into this as you will have already started the analysis in the PreClass activity but as a quick recap, there is currently much debate surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. In the PreClass, and continuing today, we have been looking at data from this recent study of English adolescents: Przybylski, A. &amp; Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204215. This was a large-scale study that found support for the Goldilocks hypothesis among adolescents: that there is a just right amount of screen time, such that any amount more or less than this amount is associated with lower well-being. The dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70, and we have been working with a version of some of the available data which can be found in the accompanying zip file on Moodle or download from here Przybylski and Weinstein looked at multiple measures of screen time, but again for the interests of this exercise we will be focusing on smartphone use only, but do feel free to expand your skills after by looking at different definitions of screen time. Overall, Przybylski and Weinstein suggested that decrements in wellbeing started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, our question is now, does the negative association between hours of use and wellbeing (beyond the one-hour point) differ for boys and girls? Lets think about this in terms of the variables. We have: a continuous DV, well-being; a continuous predictor, screen time; a categorical predictor, sex. And to recap in terms of analysis, what we are effectively trying to do is to estimate two slopes relating screen time to well-being, one for girls and one for boys, and then statistically compare these slopes. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care of how you numerically code the categorical predictors. We will use an approach called deviation coding which we will look at today later in this lab. Lets Begin! 15.3.1 Smartphone and well-being for boys and girls Continuing from where we left on in the PreClass, so far we have been matching what the original authors suggested we would find in the data, this drop off in self-reported well-being for longer exposures of smart-phone use (i.e. &gt;1 hour). However, we said we wanted to look at this in terms of males and females, or boys and girls really, so we need to do a bit more wrangling. Also, as above we said there seemed to be only a small difference between Weekday and Weekends so, we will collapse weekday and weekend usage in smartphones. Create a new table, smarttot, that takes the information in screen2 and keeps only the smarthphone usage. Now, create an average smartphone usage for each participant, called tothours, using a group_by and summarise, regardless of whether or not it is the weekend or weekday, i.e. the mean number of hours per day of smartphone use (averaged over weekends/weekdays) for that participant. Now, create a new tibble smart_wb that only includes participants from smarttot who used a smartphone for more than one hour per day each week, and then combine this tibble with the information in the wemwbs tibble and the pinfo tibble. The finished table should look something like this (we are only showing the first 5 rows here): Serial tothours tot_wellbeing sex minority deprived 1000003 2.0 41 0 0 1 1000004 2.5 47 0 0 1 1000005 3.5 32 0 0 1 1000006 2.0 29 0 0 1 1000008 1.5 42 0 0 1 Hints for Wrangling Steps Step 1 filter(Using Smartphone) to keep only smartphone use Step 2 group_by(Participant) %&gt;% summarise(tothours = mean()) Step 3 filter(), inner_join() hours greater than 1 what is the common column to join each time by? Participant? 15.3.2 Visualising and Interpreting the relationship between smartphone use and wellbeing by sex Excellent! Lots of visualisation and wrangling in the PreClass and today but that is what we have been working on and building our skills on up to this point so, we are coping fine! Just a couple more visualisation and wrangles to go before we run the analysis (the easy part!) Using the data in smart_wb create the following figure. You will need to first calculate the mean wellbeing scores for each combination of sex and tothours, and then create a plot that includes separate regression lines for each sex. Next, or if you just want to look at the figure and not create it, make a brief interpretation of the figure. Think about it in terms of who has the overall lower mean wellbeing score and also are both the slopes the same or is one more negative, one more positive, etc. ## `summarise()` has grouped output by &#39;tothours&#39;. You can override using the `.groups` argument. ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 15.3: Scatterplot and slopes for relationships between total hours and mean wellbeing score for boys (cyan) and girls (red) 15.3.3 A side point on mean centering and deviation coding Last bit of wrangling, I promise, before the analysis. Here, we will introduce something that is worth doing to help with our interpretation. You can read up more on this later, and we will cover it in later years more in-depth, but when you have continuous variables in a regression, it is often sensible to transform them by mean centering them which has two very useful outcomes: the intercept of the model now shows the predicted value of \\(Y\\) for the mean value of the predictor variable rather than the predicted value of \\(Y\\) at the zero value of the unscaled variable as it normally would; if there are interactions in the model, any lower-order effects (e.g. main effects) can be interpreted as they would have been, had it been simply an ANOVA. These steps seem rather worthwhile in terms of interpretation and the process is really straightforward. You can mean center a continuous predictor, for example X, simply by subtracting the mean from each value of the predictor: i.e.X_centered = X - mean(X). A second very useful thing to do that aids the interpretation is to convert your categorical variables into what is called deviation coding. Again, we are going to focus more on this in L3 but it is good to hear the term in advance as you will see it from time to time. Again, all this does is to allow you to interpret the categorical predictors as if it were an ANOVA. We are going to do both of these steps, mean centering of our continuous variable and deviation coding of our categorical variable. Here is the code to do it. Copy it and run it but be sure that you understand what it is doing. totothours_c is the mean centered values of tothours sex_c is the deviation coding of the sex column (sex) where male, which was coded as 1, is now coded as .5, and female is now coded as -.5 instead of 0. The ifelse() basically says, if that column you want me to look at, says this (e.g. sex == 1), then I will put a .5, otherwise (or else) I will put a -.5. smart_wb &lt;- smart_wb %&gt;% mutate(tothours_c = tothours - mean(tothours), sex_c = ifelse(sex == 1, .5, -.5)) %&gt;% select(-tothours, -sex) 15.3.4 Estimating model parameters Superb! And now finally, after all that wrangling and visualisation, the models. Finally, we are going to see if there is statistical support for our above interpretation of the Figure 15.3, where we saw that overall, girls have lower well-being and that they are affected more by prolonged smartphone usage than boys are. Just to recap, the previous authors have already looked at smartphone usage and wellbeing but, we want to look at whether it has more of an impact in girls than boys, or boys than girls, or about the same. The multiple regression model, from the general linear model, for this analysis would be written as: \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\) where: \\(Y_i\\) is the wellbeing score for participant \\(i\\); \\(X_{1i}\\) is the mean-centered smartphone use predictor variable for participant \\(i\\); \\(X_{2i}\\) is gender, where we used deviation coding (-.5 = female, .5 = male); \\(X_{3i}\\) is the interaction between smartphone use and gender (\\(= X_{1i} \\times X_{2i}\\)) You have seen multiple regression models before in R and they usually take a format something like, y ~ a + b. The one for this analysis is very similar but with one difference, we need to add the interaction. To do that, instead of saying a + b we do a * b. This will return us the effects of a and b by themselves as well as the interaction of a and b. Just like you would in an ANOVA but here one of the variables is continuous and one is categorical. With that in mind, using the data in smart_wb, use the lm() function to estimate the model for this analysis where we predict tot_wellbeing from mean centered smartphone usage (tothours_c) and the deviation coded sex (sex_c) Hints on model R formulas look like this: y ~ a + b + a:b where a:b means interaction This can be written in short form of y ~ a * b Next, use the summary() function on your model output to view it. The ouput should look as follows: ## ## Call: ## lm(formula = tot_wellbeing ~ tothours_c * sex_c, data = smart_wb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.881 -5.721 0.408 6.237 27.264 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.43724 0.03557 1333.74 &lt;2e-16 *** ## tothours_c -0.54518 0.01847 -29.52 &lt;2e-16 *** ## sex_c 5.13968 0.07113 72.25 &lt;2e-16 *** ## tothours_c:sex_c 0.45205 0.03693 12.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.135 on 71029 degrees of freedom ## Multiple R-squared: 0.09381, Adjusted R-squared: 0.09377 ## F-statistic: 2451 on 3 and 71029 DF, p-value: &lt; 2.2e-16 15.3.5 Final Interpretations Finally, just some quick interpretation questions to round off all our work! To help you, here is some info: The intercept for the male regression line can be calculated by: the Intercept + (the beta of sex_c * .5) The slope of the male regression line can be calculated by: the beta of the tothours_c + (the beta of interaction * .5) The intercept for the female regression line can be calculated by: the Intercept + (the beta of sex_c * -.5) The slope of the female regression line can be calculated by: the beta of the tothours_c + (the beta of interaction * -.5) Look at your model output in the summary() and try to answer the following questions. The solutions are below. The interaction between smartphone use and gender is shown by the variable tothours_c sex_c tothours_c:sex_c, and this interaction was significant nonsignificant at the \\(\\alpha = .05\\) level. To two decimal places, the intercept for male participants is: To two decimal places, the slope for male participants is: To two decimal places, the intercept for female participants is: To two decimal places, the slope for female participants is: As such, given the model of Y = intercept + (slope * X) where Y is wellbeing and X is total hours on smartphone, what would be the predicted wellbeing score for a male and a female who use their smartphones for 8 hours. Give your answer to two decimal places: Male: Female: And finally, what is the most reasonable interpretation of these results? smartphone use harms girls more than boys smartphone use harms boys more than girls there is no evidence for gender differences in the relationship between smartphone use and well-being smartphone use was more negatively associated with wellbeing for girls than for boys Job Done - Activity Complete! One last thing: Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, dont forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become. 15.4 Test Yourself This is a formative assignment meaning that it is purely for you to test your own knowledge, skill development, and learning, and does not count towards an overall grade. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. You will be instructed by the Course Lead on Moodle as to when you should attempt this assignment. Please check the information and schedule on the Level 2 Moodle page. Chapter 15: Regression with categorical and continuous factors Welcome to the final formative assignment of this book where we will look at rounding off the main topics of the quantitative approach we have looked at over the past few months. Over the weeks we have looked at analysing data from categorical factors in ANOVAs as well as continuous factors in regression. However, we have also looked that the General Linear Model and how it shows that ANOVAs and Regression are all part of the same model. If that is true then in principle you should be able to perform analysis with both categorical and continuous factors in the same model. This is what we looked at in the inclass activities of recent chapters and it is how we will round off the book, sending you on your way primed and ready for the next stage of your development! In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester2_Ch15.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here. Background For this assignment, we will be looking at data from the following archival data: Han, C., Wang, H., Fasolt, V., Hahn, A., Holzleitner, I. J., Lao, J., DeBruine, L., Feinberg, D., Jones, B. C. No evidence for correlations between handgrip strength and sexually dimorphic acoustic properties of voices. Available on the Open Science Framework, retrieved from https://osf.io/na6be/ Han and colleagues studied the relationship between the pitch of a persons voice (as measured by fundamental frequency - see Chapter 14) and hand grip strength to test the theory that lower voices signal physical strength. In particular, the study sought to replicate a previous study that found lower voices associated with greater grip strength in male participants but in for female participants. The idea proposed in previous studies is that pitch is a signal of physical strength and people use it as a mate preference signal, though more recent work, including this data, would tend to refute this notion. So in summary, the main relationship of interest is between voice frequency (F0 - pronounced F zero) and hand grip strength (HGS) as modulated by sex (male or female), testing the hypothesis that there is a negative relationship between F0 and HGS in male voices but not in female voices. Note 1: Lower values of F0 are associated with lower pitch voices (deep voices) and higher values of HGS would be associated with a stonger grip. Note 2: The data also contains nationality but we will not look at that today. Note 3: On the original OSF page for this archival data, the data is in excel format. You can actually read in excel data using readxl::read_excel() but we have given you the data in .csv format for consistency and to keep our open practices. Excel is software that most people have to purchase and as such does not fully conform with the ideas of open science. Hence, creating and transferring data as .csv format is always preferrable. Today, make sure to use the .csv format we have been teaching all year. Note 4: Pay particular attention to your spelling when working with the pitch column. Note that it is labelled as F0 (F and the number 0) and not FO (F and the letter O). Do not rename the column. Be exact on your spelling and typing. Before starting lets check: The .csv file is saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Semester2_Ch15.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 15.4.1 Task 1A: Load add-on packages Today we will need only the tidyverse package. Load in this package by putting the appropriate code into the T1A code chunk below. # call in the required library packages 15.4.2 Task 1B: Load the data Using read_csv() replace the NULL in the T1B code chunk to read in the data file with the exact filename you have been given. Store Han_OSF_HGSandVoiceData.csv as a tibble in dat dat &lt;- NULL Be sure to have a look at your data through the viewer or by using commands in your console only, not in your code. 15.4.3 Task 2: Demographics Because the researchers are really excellent the data is already set up in a nice layout for us to quickly create some demographics. In the T2 code chunk below replace the NULL with a code pipeline that calculates the number of male and female participants there are, in the data, as well as the mean age and standard deviation of age for both sex. store the output as a tibble in demogs the resulting tibble will have the following columns in this exact order: sex, n, mean_age, sd_age. there will be four columns (as named above in that order) with 2 rows; i.e. 1 row for each sex of voice. Be exact on spelling and order of column names. By alphabetical order, female data will be in row one. demogs &lt;- NULL 15.4.4 Task 3: Plotting the general relationship of F0 and HGS Great, we will need that data later on but for now lets start looking at the data by plotting a basic relationship between F0 and HGS regardless of sex. Enter code into the T3 code chunk below to exactly replicate the shown figure. you must use ggplot() make sure the background, colors, labels, and x and y dimensions match what is shown below. # replicate the figure shown using a ggplot() approach Figure 15.4: Replicate this Figure 15.4.5 Task 4: Analysing the general relationship of F0 and HGS Hmmm, so the scatterplot illustrates the basic relationship between F0 and HGS but we should test this using a simple linear regression. Using the data in dat, replace the NULL in the T4 code chunk below to run a simple linear regression testing the relationship of F0 (our IV) predicting hand grip strength (our DV). store the output of the regression in mod1 Do nothing to the output of lm() other than store it in the variable named mod1 (note: technically it will store as a list). do not include sex in this model. Just include the stated DV and the IV. You will need to think about the order to make sure F0 is predicting hand grip strength. mod1 &lt;- NULL 15.4.6 Task 5: Interpreting the general relationship of F0 and HGS You should have a look at the output of mod1 but only do this in the console and not in your code. Based on that output and on the scatterplot in Task 4, one of the below statements is a coherent and accurate summary of the above analysis. As illustrated in the scatterplot of Task 4, the model revealed that fundamental frequency does predict hand grip strength, explaining approximately 56.7% of the population variance. The model would suggest that there is a negative relationship between F0 and HGS (b = -.145) supporting the basic hypothesis that as pitch increases, hand grip strength decreases (Adjusted R-squared = .567, F(1, 289.3) = 219, p &lt; .001. As illustrated in the scatterplot of Task 4, the model revealed that fundamental frequency does predict hand grip strength, explaining approximately 56.7% of the population variance. The model would suggest that there is a positive relationship between F0 and HGS (b = .145) supporting the basic hypothesis that as pitch increases, hand grip strength increases (Adjusted R-squared = .567, F(1, 289.3) = 219, p &lt; .001. As illustrated in the scatterplot of Task 4, the model revealed that fundamental frequency does predict hand grip strength, explaining approximately 56.7% of the population variance. The model would suggest that there is a negative relationship between F0 and HGS (b = -.145) supporting the basic hypothesis that as pitch increases, hand grip strength decreases (Adjusted R-squared = .567, F(1, 219) = 289.3, p &lt; .001. As illustrated in the scatterplot of Task 4, the model revealed that fundamental frequency does predict hand grip strength, explaining approximately 56.7% of the population variance. The model would suggest that there is a positive relationship between F0 and HGS (b = .145) supporting the basic hypothesis that as pitch increases, hand grip strength increases (Adjusted R-squared = .567, F(1, 219) = 289.3, p &lt; .001. In the T5 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t5 answer_t5 &lt;- NULL 15.4.7 Task 6: Plotting the modulation of voice/strength relationship by sex Well that is interesting but we really want to look at the relationship between fundamental frequency and hand grip strength separately for male and female participants. Lets start by plotting the relationship between F0 and HGS for males and females separately. Enter code into the T6 code chunk below to represent this relationship for each sex separately. This has to be one figure, meaning only one ggplot call. Two ggplot calls will not be accepted. male and female can be represented on the same plot area or split into two plot areas using any method previously shown. Why not start with your code from Task 3 and simply add an aes call or an additional line to split the male and female data. Your output should show two separate regression lines; one for the male participants and one for the female participants. Make any other changes to your figure that you wish, the only criteria are: that there is only one ggplot call in your code chunk; that each participant is represented by a dot; that voice pitch is on the x-axis; that there are two regression lines, one for each sex. # you must use the stated function 15.4.8 Task 7: Mean centering and Deviation coding the variables. Looking at your new figure this would perhaps suggest that there is not quite as clear a relationship between HGS and F0 as our previous analysis showed. Now that we have split male and female data, the regression lines for both sex look relatively flat and not the sloping relationship we saw earlier. We are going to test the relationship in the next Task but, before doing that, thinking back to the in-class activities of the chapter, we said we needed to do two things to the data first before running a regression that has both continuous and categorical data. Those two steps were mean centering the continuous predictor and deviation code the categorical predictor. In the T7 code chunk below, replace the NULL with a line of code that adds on two new columns to the dataset. The first new column will be the mean centered continuous variable (F0). The second new column will be the deviation coded categorical variable (sex) store the output of this wrangle in the tibble called dat_dev. call the column containing the mean centered continuous variable F0_C. To clarify, that is pronounced as F zero underscore C. Both the F and C should be capital letters. call the column containing the deviation coded categorical variable sex_C. To clarify, that is pronounced as sex underscore C. The C is a capital letter but the s is a lowercase letter. deviation code females as -.5 and males as .5 Keep all other columns and pay strict attention to the order asked for when adding the two new columns; F0_C then sex_C. dat_dev &lt;- NULL 15.4.9 Task 8: Estimate the modulation of voice/strength relationship by sex Ok great, the data is now in the correct format for us to test the hypothesis that there is a significant relationship between fundamental frequency and HGS in male voices but not in female voices. Another way of saying this is that the F0 and HGS relationship is modulated by sex, or that there is an interaction between sex and F0 when predicting HGS. Using the data in dat_dev, replace the NULL in the T8 code chunk below to run a regression model testing the relationship of fundamental frequency and sex (our IVs) on predicting hand grip strength (our DV). store the output of the regression in mod2 Do nothing to the output of lm() other than store it in the variable named mod1 (note: technically it will store as a list). Remember that we want the interaction of fundamental frequency and sex so set the model accordingly. Put fundamental frequency as the first predictor and sex as the second predictor, and do not forget to use the mean centered and deviation coded variables. mod2 &lt;- NULL 15.4.10 Task 9: Interpreting the output 1 Looking at the output of mod2 (remember to do this only in your console and not in the code), one of the below statements is a true synopsis of each of the three effects. The beta coeffecient of fundamental frequency (b = -.016) and the interaction (-.033) are not significant but the coeffecient of sex is significant (b = 13.182). The beta coeffecient of fundamental frequency (b = -.033) and the interaction (-.016) are significant but the coeffecient of sex is not significant (b = 13.182). The beta coeffecient of fundamental frequency (b = -.016) and the interaction (-.033) are significant but the coeffecient of sex is not significant (b = 13.182). The beta coeffecient of fundamental frequency (b = -.033) and the interaction (-.016) are not significant but the coeffecient of sex is significant (b = 13.182). In the T9 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t9 answer_t9 &lt;- NULL 15.4.11 Task 10: Interpreting the output 2 Based on the output of mod2 one of the below statements is a true synopsis of slopes for male and female participants. Do any necessary additional calculations in your console, not in the script. The slope of the male regression line (slope = -.042) is marginally more negative than the slope of the female regression line (slope = -.025). The slope of the male regression line (slope = .042) is marginally more positive than the slope of the female regression line(slope = .025). The slope of the female regression line (slope = -.042) is marginally more negative than the slope of the male regression line (slope = -.025). The slope of the female regression line (slope = .042) is marginally more positive than the slope of the male regression line (slope = .025). In the T10 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t10 answer_t10 &lt;- NULL 15.4.12 Task 11: Interpreting the output 3 Based on the output of all of the above, one of the below statements is a true synopsis of the analyses. An analysis involving 221 participants (male: N = 110, Mean Age = 22.93, SD Age = 3.35; female: N = 111, Mean Age = 23.87, SD Age = 5.14) was conducted to test the hypothesis that there was a negative relationship between fundamental frequency and hand grip strength in males but not in females. A simple linear regression suggested that there was a significant negative relationship between F0 and HGS, when not controlling for sex. However, after controlling for participant sex, there appears to be no significant relationship between fundamental frequency and grip strength. The only significant effect suggests that there is a difference in hand grip strength between males and females but this is not related to fundamental frequency. An analysis involving 221 participants (female: N = 110, Mean Age = 22.93, SD Age = 3.35; male: N = 111, Mean Age = 23.87, SD Age = 5.14) was conducted to test the hypothesis that there was a negative relationship between fundamental frequency and hand grip strength in males but not in females. A simple linear regression suggested that there was no significant negative relationship between F0 and HGS, when not controlling for sex. Likewise, after controlling for participant sex, there appears to be no significant relationship between fundamental frequency and grip strength. The only significant effect suggests that there is a difference in hand grip strength between males and females but this is not related to fundamental frequency. An analysis involving 221 participants (female: N = 110, Mean Age = 22.93, SD Age = 3.35; male: N = 111, Mean Age = 23.87, SD Age = 5.14) was conducted to test the hypothesis that there was a negative relationship between fundamental frequency and hand grip strength in females but not in males. A simple linear regression suggested that there was a significant negative relationship between F0 and HGS, when not controlling for sex. However, after controlling for participant sex, there appears to be no significant relationship between fundamental frequency and grip strength. The only significant effect suggests that there is a difference in hand grip strength between males and females but this is not related to fundamental frequency. An analysis involving 221 participants (female: N = 110, Mean Age = 22.93, SD Age = 3.35; male: N = 111, Mean Age = 23.87, SD Age = 5.14) was conducted to test the hypothesis that there was a negative relationship between fundamental frequency and hand grip strength in males but not in females. A simple linear regression suggested that there was a significant negative relationship between F0 and HGS, when not controlling for sex. However, after controlling for participant sex, there appears to be no significant relationship between fundamental frequency and grip strength. The only significant effect suggests that there is a difference in hand grip strength between males and females but this is not related to fundamental frequency. In the T11 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t11 answer_t11 &lt;- NULL Job Done - Activity Complete! Well done, you are finished! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the forums. 15.5 Solutions to Questions Below you will find the solutions to the questions for the PreClass and InClass activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 15.5.1 PreClass Activities 15.5.1.1 Loading the Data library(&quot;tidyverse&quot;) pinfo &lt;- read_csv(&quot;participant_info.csv&quot;) wellbeing &lt;- read_csv(&quot;wellbeing.csv&quot;) screen &lt;- read_csv(&quot;screen_time.csv&quot;) Return to Task 15.5.1.2 Compute the well-being score for each participant Both of these solutions would produce the same output. Version 1 bit quicker in terms of coding and reduces chance of error by perhaps forgetting to include a specific column wemwbs &lt;- wellbeing %&gt;% pivot_longer(names_to = &quot;var&quot;, values_to = &quot;score&quot;, cols = -Serial) %&gt;% group_by(Serial) %&gt;% summarise(tot_wellbeing = sum(score)) This is how you would do the same above with the gather() function in case there are still some gather() in this chapter by mistake. It does the same task but just slightly different input and can create a slightly different output because it has a different sorting function within it compared to pivot_longer() wemwbs &lt;- wellbeing %&gt;% gather(key = &quot;var&quot;, value = &quot;score&quot;, -Serial) %&gt;% group_by(Serial) %&gt;% summarise(tot_wellbeing = sum(score)) Version 2 this one is a bit slower wemwbs &lt;- wellbeing %&gt;% mutate(tot_wellbeing = WBOptimf + WBUseful + WBRelax + WBIntp + WBEnergy + WBDealpr + WBThkclr + WBGoodme + WBClsep + WBConfid + WBMkmind + WBLoved + WBIntthg + WBCheer) %&gt;% select(Serial, tot_wellbeing) %&gt;% arrange(Serial) Return to Task 15.5.1.3 Visualising Screen time on all technologies ## screen time screen_long &lt;- screen %&gt;% pivot_longer(names_to = &quot;var&quot;, values_to = &quot;hours&quot;, cols = -Serial) %&gt;% separate(var, c(&quot;variable&quot;, &quot;day&quot;), &quot;_&quot;) screen2 &lt;- screen_long %&gt;% mutate(variable = recode(variable, &quot;Watch&quot; = &quot;Watching TV&quot;, &quot;Comp&quot; = &quot;Playing Video Games&quot;, &quot;Comph&quot; = &quot;Using Computers&quot;, &quot;Smart&quot; = &quot;Using Smartphone&quot;), day = recode(day, &quot;wk&quot; = &quot;Weekday&quot;, &quot;we&quot; = &quot;Weekend&quot;)) ggplot(screen2, aes(hours)) + geom_bar() + facet_grid(day ~ variable) Figure 15.5: Count of the hours of usage of different types of social media at Weekdays and Weekends This is how you would do the first step above with the gather() function in case there are still some gather() in this chapter by mistake. It does the same task but just slightly different input and can create a slightly different output because it has a different sorting function within it compared to pivot_longer() screen_long &lt;- screen %&gt;% gather(&quot;var&quot;, &quot;hours&quot;, -Serial) %&gt;% separate(var, c(&quot;variable&quot;, &quot;day&quot;), &quot;_&quot;) Return to Task 15.5.1.4 Visualising the Screen time and Well-being relationship dat_means &lt;- inner_join(wemwbs, screen2, &quot;Serial&quot;) %&gt;% group_by(variable, day, hours) %&gt;% summarise(mean_wellbeing = mean(tot_wellbeing)) ## `summarise()` has grouped output by &#39;variable&#39;, &#39;day&#39;. You can override using the `.groups` argument. ggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) + geom_line() + geom_point() + facet_wrap(~variable, nrow = 2) Figure 15.6: Scatterplot showing the relationship between screen time and mean well-being across four technologies for Weekdays and Weekends Return to Task 15.5.2 InClass Activities 15.5.2.1 Smartphone and well-being for boys and girls Solution Steps 1 to 2 smarttot &lt;- screen2 %&gt;% filter(variable == &quot;Using Smartphone&quot;) %&gt;% group_by(Serial) %&gt;% summarise(tothours = mean(hours)) Solution Step 3 smart_wb &lt;- smarttot %&gt;% filter(tothours &gt; 1) %&gt;% inner_join(wemwbs, &quot;Serial&quot;) %&gt;% inner_join(pinfo, &quot;Serial&quot;) Return to Task 15.5.2.2 Visualise and Interpreting the relationship of smartphone use and wellbeing by sex The Figure smart_wb_gen &lt;- smart_wb %&gt;% group_by(tothours, sex) %&gt;% summarise(mean_wellbeing = mean(tot_wellbeing)) ## `summarise()` has grouped output by &#39;tothours&#39;. You can override using the `.groups` argument. ggplot(smart_wb_gen, aes(tothours, mean_wellbeing, color = factor(sex))) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 15.7: Scatterplot and slopes for relationships between total hours and mean wellbeing score for boys (cyan) and girls (red) A brief Interpretation Girls show lower overall well-being compared to boys. In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat. This suggests that the negative association between well-being and smartphone use is stronger for girls. Return to Task 15.5.2.3 Estimating model parameters This is the chunk we gave in the materials. smart_wb &lt;- smart_wb %&gt;% mutate(tothours_c = tothours - mean(tothours), sex_c = ifelse(sex == 1, .5, -.5)) %&gt;% select(-tothours, -sex) and the model would be specified as: mod &lt;- lm(tot_wellbeing ~ tothours_c * sex_c, smart_wb) or alternatively mod &lt;- lm(tot_wellbeing ~ tothours_c + sex_c + tothours_c:sex_c, smart_wb) and the output called by: summary(mod) ## ## Call: ## lm(formula = tot_wellbeing ~ tothours_c + sex_c + tothours_c:sex_c, ## data = smart_wb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.881 -5.721 0.408 6.237 27.264 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.43724 0.03557 1333.74 &lt;2e-16 *** ## tothours_c -0.54518 0.01847 -29.52 &lt;2e-16 *** ## sex_c 5.13968 0.07113 72.25 &lt;2e-16 *** ## tothours_c:sex_c 0.45205 0.03693 12.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.135 on 71029 degrees of freedom ## Multiple R-squared: 0.09381, Adjusted R-squared: 0.09377 ## F-statistic: 2451 on 3 and 71029 DF, p-value: &lt; 2.2e-16 Return to Task 15.5.2.4 Final Interpretations The interaction between smartphone use and gender is shown by the variable tothours_c:sex_c, and this interaction was significant at the \\(\\alpha = .05\\) level, meaning that there is an significant interaction between sex and hours of smartphone usage on wellbeing To two decimal places, the intercept for male participants is: 50.01 To two decimal places, the slope for male participants is: -0.32 To two decimal places, the intercept for female participants is: 44.87 To two decimal places, the slope for female participants is: -0.77 As such, given the model of Y = intercept + (slope * X) where Y is wellbeing and X is total hours on smartphone, what would be the predicted wellbeing score for a male and a female who use their smartphones for 8 hours. Give your answer to two decimal places: Male: 47.45 Female: 38.71 And finally, the most reasonable interpretation of these results is that smartphone use was more negatively associated with wellbeing for girls than for boys. Return to Task 15.5.3 Test Yourself Activities 15.5.3.1 Task 1A: Load add-on packages The tidyverse would be loaded in as shown below. library(tidyverse) Return to task 15.5.3.2 Task 1B: Load the data The data would be read in as follows. dat &lt;- read_csv(&quot;Han_OSF_HGSandVoiceData.csv&quot;) Remember to check that you have used read_csv() Double check the spelling of the data file. Be sure to have a look at your data through the viewer or by using commands in your console only, not in your code. Return to task 15.5.3.3 Task 2: Demographics This would be carried out using a group_by() and summarise() combination as shown. As always be sure to pay attention to the spelling of variables demogs &lt;- dat %&gt;% group_by(sex) %&gt;% summarise(n = n(), mean_age = mean(age), sd_age = sd(age)) Return to task 15.5.3.4 Task 3: Plotting the general relationship of F0 and HGS The below figure would be appropriate here: ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 15.4: The relationship between fundamental frequency (Hz) and hand grip strength And would be created with the following code: ggplot(dat, aes(F0, HGS)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(x = &quot;Fundamental Frequency (Hz)&quot;, y = &quot;Hand Grip Strength&quot;) + coord_cartesian(ylim = c(0,100)) + theme_classic() Return to task 15.5.3.5 Task 4: Analysing the general relationship of F0 and HGS The appropriate way to run this analysis would be as shown below: HGS is the dependent variable (or the outcome variable) F0 is the independent variable (or the predictor variables The data is stored in dat mod1 &lt;- lm(HGS ~ F0, dat) Note: Be sure to watch spelling and capitalisation of the variables. For example, F0_C is pronounced as F Zero and the F is capitalised. Return to task 15.5.3.6 Task 5: Interpreting the general relationship of F0 and HGS An appropriate summary would be: As illustrated in the scatterplot of Task 4, the model revealed that fundamental frequency does predict hand grip strength, explaining approximately 56.7% of the population variance. The model would suggest that there is a negative relationship between F0 and HGS (b = -.145) supporting the basic hypothesis that as pitch increases, hand grip strength decreases (Adjusted R-squared = .567, F(1, 219) = 289.3, p &lt; .001. As such the answer is: answer_t5 &lt;- 3 Return to task 15.5.3.7 Task 6: Plotting the modulation of voice/strength relationship by sex The below figure would be sufficient for this task: ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 15.8: The relationship between fundamental frequency and hand grip strength by sex And would be created as follows. ggplot(dat, aes(F0, HGS, group = sex)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + labs(x = &quot;Fundamental Frequency (Hz)&quot;, y = &quot;Hand Grip Strength&quot;) + coord_cartesian(ylim = c(0,100)) + theme_classic() Return to task 15.5.3.8 Task 7: Mean centering and Dummy coding the variables. One way to complete this task using a mutate() function as shown below. Be sure however to keep an eye on spelling and capitalisation of variables. For example, F0_C is pronounced as F Zero underscore C and the F and C are both capitalised. dat_dummy &lt;- dat %&gt;% mutate(F0_C = F0 - mean(F0), sex_C = ifelse(sex == &quot;female&quot;, -.5, .5)) Return to task 15.5.3.9 Task 8: Estimate the modulation of voice/strength relationship by sex The appropriate way to run this analysis would be as shown below: HGS is the dependent variable (or the outcome variable) F0_C and sex_C are the independent variables (or the predictor variables) The data is stored in dat_dummy mod2 &lt;- lm(HGS ~ F0_C * sex_C, dat_dummy) Note: Be sure to watch spelling and capitalisation of the variables. For example, F0_C is pronounced as F Zero underscore C and the F and C are both capitalised. Return to task 15.5.3.10 Task 9: Interpreting the output 1 The appropriate synopsis would be: The beta coeffecient of fundamental frequency (b = -.033) and the interaction (-.016) are not significant but the coeffecient of sex is significant (b = 13.182). As such the correct answer is: answer_t9 &lt;- 4 Return to task 15.5.3.11 Task 10: Interpreting the output 2 The appropriate synopsis would be: The slope of the male regression line (slope = -.042) is marginally more negative than the slope of the female regression line (slope = -.025). As such the correct answer is: answer_t10 &lt;- 1 Return to task 15.5.3.12 Task 11: Interpreting the output 3 A good summary of the analysis would be as follows: An analysis involving 221 participants (female: N = 110, Mean Age = 22.93, SD Age = 3.35; male: N = 111, Mean Age = 23.87, SD Age = 5.14) was conducted to test the hypothesis that there was a negative relationship between fundamental frequency and hand grip strength in males but not in females. A simple linear regression suggested that there was a significant negative relationship between F0 and HGS, when not controlling for sex. However, after controlling for participant sex, there appears to be no significant relationship between fundamental frequency and grip strength. The only significant effect suggests that there is a difference in hand grip strength between males and females but this is not related to fundamental frequency. As such the correct answer is: answer_t11 &lt;- 4 Return to task Chapter Complete! "],["reflection-chpts-10-15.html", "Lab 16 Reflection - Chpts 10-15 16.1 Overview 16.2 PreClass Activity 16.3 InClass Activity 16.4 Update Your Notes 16.5 In The End!", " Lab 16 Reflection - Chpts 10-15 16.1 Overview As in Semester 1 we have covered a lot of material in these labs and now would be a good time to stop, recap, and reflect on what we have learned. As such this last chapter is again about looking back at what you have learned, reflecting on your knowledge and skills, resolving issues, and looking at other cool applications of R that have not been covered in this lab series. 16.2 PreClass Activity As we are reflecting on what we have covered so far, your preclass activities this time are: Review the previous chapters and note any issues you have with the elements covered - both in terms of concepts and code. Post these issues on available discussion channels and we can look at them together at our next meeting. 16.3 InClass Activity Like in the PreClass we want to spend sometime reflecting on what we have learnt and as such this InClass is about looking at ideas, concepts, and codes, that you have had issues with and seeing if we can resolve those issues. It would be particularly worthwhile spending sometime looking at aspects of working with the GLM and decomposition matrices as this will make up much of the course next year. We will also look at some other interesting things you can do in R, should you wish to expand your own knowledge and skills. Below is the list we looked at in Semester 1 but, I am sure we have found loads more as the year has gone on, such as: New redoc package, by Ross Noam, for collaborative editing of Word documents (https://noamross.github.io/redoc/) faux for the creation of simulated data (https://debruine.github.io/faux/) (R-faux?) checkpoint for timestamping the version of the packages you are using in your code mydata &lt;- read.delim(\"clipboard\") for creating a dataframe from your copy and paste function on your keyboard. R.Online to compare codes across different versions of R (https://srv.colinfay.me:1001/) Hack Your Data Beautiful (https://psyteachr.github.io/hack-your-data/index.html) - a workshop run by a team of postgraduates in Psychology @ Glasgow that shows a range of excellent and interesting skills and applications. Previous Popping out the Source Window to make working easier - Using Source Windows Analysing Twitter data with the rtweet package (R-rtweet?) Animating plots with the ggganimate package (R-gganimate?) Creating quickfire quizzes with the webex (Barr and DeBruine 2021) Make your own memes using the meme package (R-meme?) The Hex Sticker Memory game and the background behind it Creating interactive plots using ggplot and plotly Even funkier visualtions using the ggforce package - check out facet_zoom() (R-ggforce?) Many diverse fields are now using R and this is a good example: R for Journalists Using the knitr::read_chunk() function to call R script code through R Markdown (Xie 2021) 16.4 Update Your Notes Over the course of this book you have hugely developed your knowledge and skills in a variety of data skills, data management, data wrangling, and data analysis approaches. This is a great achievement and you should be very proud of youself. One thing to note though is that these skills are very much like learning a language or any other skill; the more you practice and the more you use them, they better you will become. As such, now would be an excellent time to review all your notes, check your understanding of the different approaches and sections, see what you know and what you are still uncertain of, and make yourself a plan to bring your notes together, fill in any gaps in your knowledge, and solidify your knowledge and learning. If you have any questions, please post them on the TEAMS channel or speak to a member of staff and dont forget to add any useful information to your Portfolio before you leave it too long and forget. 16.5 In The End! Excellent! That is it. That is all we wanted you to achieve before the end of Level 2! Congratulations on all that you have achieved over the course of this book! As we said before, you are now highly competent, and hopefully confident, at data-wrangling, visualisation, and analysis, as well as interpreting a whole host of different analyses. You are now more than ready for Level 3! Thanks for all your hard work. You have been amazing! References "],["acknolwedgements.html", "Acknolwedgements", " Acknolwedgements This book has been the work of many people, both staff and students within the School of Psychology, University of Glasgow. A special mention however should go to the following people: Stephanie Boyle, Molly Burr, Morgan Daniel, Amalia Gomoiu, Kate Haining, Jesse Klein, Rebecca Lai, Steven McNair, Shannon McNee, Jennifer Murch, Jack Taylor, Jaimie Torrance, Ana Skolaris &amp; Hollie Sneddon. We hugely appreciate all comments and help in creating the material contained within this book. "],["stats-by-hand.html", "Stats by Hand", " Stats by Hand There is an accompanying workbook in development that looks at calculating statistics by hand. If you are interested in it (though it is in development stages so is rough) you can see it here: The Handy Workbook for Stats by Hand "],["references.html", "References", " References "]]
