## PreClass Activity

As in the last chapter, the Preclass activities involve reading and watching. We have written and selected this material to help give you a better understanding of power and how it interacts with effect size, sample size, and alpha. We have also suggested some optional material that you can look at and play with to get a rounder view.

### Reading 

Read the following blog on Power and then the section we have written on **Power and Design**. 

This blog is a fictional conversation between a professor and a student on the importance of power. Grab a coffee and have a read. Don't worry about reading all the additional papers unless you want to; just the blog is fine to get an understanding. What you are trying to understand from this blog is the relationship between sample size and effect sizes, and whether a result from a study is likely to replicate or not based on the power of the original study.

**Blog:**

* <a href="https://pigee.wordpress.com/2016/09/13/the-power-dialogues/" target = "_blank">The Power Dialogues</a> by PIGEE at the University of Illinois.

**Using power to design your study**

To reiterate, power is defined as the probability of correctly rejecting the null hypothesis for a fixed effect size and fixed sample size. As such, power is a key decision when you design your study, under the premis that **the higher the power of your planned study, the better**. 

Two relationships you will learn in this chapter are that:

* for a given sample size and $\alpha$, the power of your study is higher if the effect you are looking for is assumed to be a large effect as opposed to a small effect; large effects are easier to detect. 
* and, for a given effect size and $\alpha$, the power of your study is higher when you increase your sample size. 

From these relationships we see that, because you have little control over the size of the effect you are trying to detect (it lives in the real world which you don't control), you can instead increase the power of your study by increasing the size of your sample (and also reducing sources of noise and measurement error in your study). As such, when planning a study, any good researcher will consider the following four key elements - the **APES**:

* **alpha** - (the false positive rate - Type 1 error) most commonly thought of as the significance level; usually set at $\alpha = .05$
* **power** - the probability of rejecting the null hypothesis for a given effect size and sample size, with $power = .8$ usually cited as the minimum power you should aim for based on the false negative rate being set at $\beta = .2$;
* **effect size** - size of the asssociation or difference you are trying to detect;
* **sample size** - the number of observations (usually, participants, but sometimes also stimuli) in your study.
* **Note:** because power depends on several variables, it is useful to think of power as a **function** with varying value rather than as a single fixed quantity.

Now the cool thing about the APES is that **if you know any three of these elements then you can calculate the fourth**. In reality, the two most common approaches when designing a study would be:

1. to determine the appropriate sample size required to reject your null hypothesis, with high probability, for the effect size that you are interested in. That is, you decide your $\alpha$, $power$, and effect size, and from that you calculate for the sample size required in your study. Generally, **the smaller the assumed effect size, the more participants you will need**, assuming power and alpha are held constant.
2. to determine the smallest effect size you can reliably detect given your sample size. That is, you know everything except the effect size. For example, say you are using an open dataset and you know they have run 100 participants, you can't add any more participants, and you want to know what is the minimum effect size you could detect from this dataset if you set $power$ and $\alpha$ at the field standards. 

Hopefully that gives you an idea of how we use power to determine sample sizes for studies - and that the sample size should not just be pulled out of thin air. Both of these approaches described above are called **a priori power analyses** as you are stating the power level you want **before** (a priori means before) the study. However, you may now be thinking though, if everything is connected, then can we use the effect size from our study and the sample size to determine the power of the study after we have run it? No! Well, you can but it would be wrong to do so. This is actually called **Observed** or **Post-Hoc** power and most papers would discourage you from calculating it on the grounds that the effect size you are using is not the true effect size of the population you are interested in; it is just the effect size of your sample. As such any indication of power from this analysis is misleading. Avoid doing this. You can read more about why, here, in your own time if you like: <a href="http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html" target = "_blank"> Lakens (2014) Observed Power, and what to do if your editor asks for post-hoc power analyses</a>. In short, stick to using only **a priori power analyses** approaches and use them to determine your required sample size or achievable reliable effect size. 

### Watch

You should now also watch this short but nonetheless highly informative video by Daniel Lakens on Power and Sample Size. It will help consolidate the above points. And his shirt is amazing!

**Video:**

* <a href = "https://www.youtube.com/watch?v=Lr-i4Ugoc5M&index=3&list=PLtAL5tCifMi5zG70dslERYcGApAQcvj1s" target = "_blank">Power Analysis and Sample Size Decisions</a> by Daniel Lakens

Remember to make notes about power, effect sizes, and sample sizes, as processing the concepts into your own words will really help you to understand them better.

### Optional

Finally, there are a number of great webpages and blogs that will help you understand the concepts in this chapter. Here are some that we think might be good for you to look at. You don't have to look at all of these to understand this chapter but do come back to them as they will really help you as you progress in becoming a responsinble researcher. We are deliberately giving you a number of options here as for everyone there is that one analogy that will work best for you and that one paper that will make everything click into place. That example will be different from person to person so having a variety of explanations will help.

* A YouTube video by Dan Quintana (University of Oslo) showing how to use the **`pwr`** package to calculate power in t-tests, correlations, and one-way ANOVAs <a href="https://www.youtube.com/watch?v=ZIjOG8LTTh8" target = "_blank">https://www.youtube.com/watch?v=ZIjOG8LTTh8</a>

* A shiny app created by Lisa Debruine (University of Glasgow) on guessing the effect size between two conditions <a href="http://shiny.psy.gla.ac.uk/guess/" target = "_blank">http://shiny.psy.gla.ac.uk/guess/</a>

* A blog by Daniel Lakens (Eindhoven University of Technology) on determining the smallest effect size you are interested in. This is often referred to as the Smallest Effect Size of Interest (SESOI) <a href="http://daniellakens.blogspot.com/2017/05/how-power-analysis-implicitly-reveals.html" target = "_blank">http://daniellakens.blogspot.com/2017/05/how-power-analysis-implicitly-reveals.html</a>

* An interactive webpage by Kristoffer Magnusson (Karolina Instituet, Stockholm) on interpreting Cohen's d effect size <a href="https://rpsychologist.com/d3/cohend/" target = "_blank">https://rpsychologist.com/d3/cohend/</a>

* A shiny app by Hause Lin (University of Toronto) showing the conversion of one effect size into another <a href="http://escal.site/" target = "_blank">http://escal.site/</a>

* A Frontiers in Psychology paper by Daniel Lakens on calculating various effect sizes for t-tests and ANOVAs <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full" target = "_blank">https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full</a>

* A blog by Daniel Lakens on what Type I and Type II errors are acceptable. In short, justify everything <a href="http://daniellakens.blogspot.com/2019/05/justifying-your-alpha-by-minimizing-or.html" target = "_blank">http://daniellakens.blogspot.com/2019/05/justifying-your-alpha-by-minimizing-or.html</a>

* Chapter 9 of Ian Walker's "Research Methods and statistics" which is availble to read online through the University Library. This is a short chapter all about hypothesis testing and power really brining everything from the last few chapters together.

* And don't forget Chapter 3 of "The 7 Deadly Sins of Psychology: A Manifesto for Reforming the Culture of Scientific Practice" is very good to read on the topic of power and unreliable research. The book is available in the University Library or can be bought at all reputable bookshops and online repositories. 

* A really nice paper by Marjan Bakker and colleagues on whether people put power analyses in their ethics proposals. It has a nice introduction on power and the results show how many different ways researchers actually calculate sample sizes <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079" target = "_blank">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0236079</a>

* A paper by Marcus Munafo and colleagues that we have mentioned many times but it might help to read again now as the concepts and ideas will be more famility now. <a href="https://www.nature.com/articles/s41562-016-0021" target = "_blank">https://www.nature.com/articles/s41562-016-0021</a>

* A paper by Schafer and Schwarz (2019) aimed at helping people make meaningful interpretation of effect sizes in Psychology. It also explores differences of commonly found effect sizes in sub-disciplines of Psychology. <a href = "https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/full" target = "_blank">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00813/full</a>

* Finally, a paper by Brysbaert (2019) that shows just how many participants you would need in a variety of common designs in Psychology studies. You will be shocked by the difference between the number of participants needed compared to the number of participants used in published studies  <a href = "https://www.journalofcognition.org/article/10.5334/joc.72/" target = "_blank">https://www.journalofcognition.org/article/10.5334/joc.72/</a>


<span style="font-size: 22px; font-weight: bold; color: var(--blue);">Job Done - Activity Complete!</span>

Hopefully this has given you a good basis to understanding power, sample sizes, alpha, and effect sizes. These are difficult concepts to grasp and it will take a lot of time thinking about them and interacting with them before they really start to sink in.  Hopefully however, if nothing else, the least you come away with is the idea that the number of participants you should run in a study is not an arbitrary decision but is in fact a relationship between the effect size you want to test for and the level of error (Type I or Type II) you are willing to accept.

As always, the best way to understand something is to put it into your own words so don't forget to go back and add any informative points to your Portfolio. Post any questions on the available forums for discussion or ask a member of staff if you are unsure.
