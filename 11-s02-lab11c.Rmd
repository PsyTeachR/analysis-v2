## InClass Activity

We are going to start with a step-by-step example of building a decomposition matrix for an ANOVA (Analysis of Variance) and then ask you to perform the steps yourself on a different dataset. If you feel comfortable with the examples in Chapter 3 of Miller and Haden, feel free to skim worked example and move onto the exercises below. You can find further examples and step-by-step walkthroughs at the end of Miller and Haden (2013), Chapter 3.

**One-factor ANOVA: Worked example**

An ANOVA is a method of analysis for analysing data where you have two or more conditions (levels) for an independent variable (factor), and/or you have more than one independent variable (factors). Thinking back, a t-test is where you normally have two conditions (levels) with one independent variable (factor), right? Well an ANOVA is just an extension of that. For example, in the classic Professor Priming experiments you may have read about, instead of just comparing IQ scores for professors vs. hooligans (t-test), you can compare IQ scores for professors vs. hooligans vs. politicians (ANOVA). Actually, an experiment that has one factor with two levels can be analysed with a t-test or an ANOVA as they are both based on the General Linear Model (GLM).

So let's assume that you have data from a one-factor design with three-levels (i.e. one independent variable with three conditions (Grp1, Grp2, Grp3)). To make this example concrete, let's pretend you are studying how consuming food before an exam affects student performance.  You randomly assign 12 participants to three separate conditions (four participants per group) (We chose a small number of participants to simplify the computations; obviously if you were going to do this study in real life, you'd need **far more** than 12 participants to make this worthwhile). The three conditions are as follow: 

1. no food, glass of water only (Control) 
2. all-you-can-eat buffet (Buffet); 
3. side salad (Salad). 

Your dependent variable is operationalised as the number of questions answered correctly on a difficult exam (100 points possible). The exam is administered right after consuming the meal (or drinking water, for the control group). And just in case you aren't sure, this would be a **one-factor design** because there is a single factor, which we might call "pre-exam consumption". And this factor has three different levels: water, buffet, and salad. It is a **between-subjects design** because there are different people in each group (4 per group). In textbooks you might see this referred to as a one-way between-subjects ANOVA. Yes stats does have numerous names for the same thing - that is why making notes is really important. Depending on training someone might use a different word from you but mean the same thing. You can bridge that gap by knowing the alternative names.

And finally, for our analysis, we want to test whether there is any difference in exam performance across the levels of the factor. We won't complete the analysis today but we will look at setting up our model which we would then take on further to see if there is a difference between groups.

Here's how the exam performance looks like for each of the three groups with each value representing an individual participant's score:

```{r ch11-setup, echo=FALSE}
RNGversion("3.5")
set.seed(500)

## this function creates error values for each condition
err_vec <- function(n, sd) {
  etmp <- as.integer(rnorm(n - 1, mean = 0, sd))
  sample(c(etmp, 0L - as.integer(sum(etmp))))
}

.mu <- 50L
.meal_eff <- c(8L, -6L, -2L)
.etmp <- as.integer(rnorm(11, sd = 8))
.err <- c(replicate(3, err_vec(4, 12)))
.Y <- 50 + rep(.meal_eff, each = 4L) + .err
```

- **Control**: `r paste(.Y[1:4], collapse = ", ")`
- **Buffet**: `r paste(.Y[5:8], collapse = ", ")`
- **Salad**: `r paste(.Y[9:12], collapse = ", ")`

<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>

To make sure you understand the design of this study try to answer the following questions. All the answers are in the information above:

1. Factor is another name for a `r mcq(c("level","condition","control", answer = "variable"))` of the experiment
2. Level is another name for a `r mcq(c("factor", answer = "condition","control", "variable"))` of the experiment
3. In this experiment we have `r mcq(c(answer = "1 factor with 3 levels", "1 level with 3 factors", "3 variables with 1 condition"))`
4. Because each group contains different participants then this is a `r mcq(c("within-subjects design", answer = "between-subjects design", "mixed design"))`
5. The fourth participant in the Control condition scored `r mcq(c(.Y[12], .Y[8], answer = .Y[4], .Y[3]))` on the exam

**Estimating model components**

Great so we understand our experiment! Now, and based on your reading, the General Linear Model (GLM) that we will fit to our data is:

$$Y_{ij} = \mu + A_i + S(A)_{ij}$$

Where:

- $Y_{ij}$ is the observed value for observation $j$ of group $i$ - i.e. a given participant's score ($j$) in a given group ($i$);
- $\mu$ (pronounced "mu") is the population grand mean (estimated by the sample grand mean) - grand just means overall;
- $A_i$ is the deviation of the population mean of group $i$ from the population grand mean - i.e. how different a group's mean is from the overall population mean;
- Now before the last part of the formula, you need to know that the sum of $\mu + A_i$ is known as the **fitted value** or the **typical value** or the **predicted value** of a participant in a condition and is written as: $\hat{Y}_{ij}$. So:

$$\hat{Y}_{ij} = \mu + A_i$$

- The "party hat" that ${Y}_{ij}$ is wearing in this part, i.e. $\hat{Y}_{ij}$, is there to remind us that it is not the actual score of your participant (in a given condition), but an **estimate** of that value. So, when we are working in predicted values (values we haven't actually collected, just predicted) then we stick the "party hat" on the symbol. If we are working in real values (that we have collected) then no party hat!
- Finally, back to the GLM equation, $S(A)_{ij}$ is the error or residual, defined as the observed value ($Y_{ij}$) minus the model prediction ($\hat{Y}_{ij}$), or the actual value minus the predicted value. This can be thought of as how different an indivual observation/participant is from their condition mean. So:

$$S(A)_{ij} = \hat{Y}_{ij} - {Y}_{ij}$$

Now the joy of an analysis such as this is that it can be hard to understand just from the words but makes much more sense when you start to run the numbers - which we will now do. We begin by applying the **estimation equations**. Our estimate of the population grand mean $\mu$, will be based on the grand mean of the sample. We will call this $\hat{\mu}$ (notice once again, the "party hat").

The estimation equations for our model (seen in Table 3.3 on page 18 of Miller and Haden (2013)) are:

- $\hat{\mu} = Y_{..}$

- $\hat{A}_i = Y_{i.} - \hat{\mu}$

- $\widehat{S(A)}_{ij} = Y_{ij} - (\hat{\mu} + \hat{A}_i) = Y_{ij} - \hat{\mu} - \hat{A}_i$

where

- $Y_{..}$ is the mean of all 12 observations in the sample a.k.a. the overall mean or the baseline 

- $Y_{i.}$ is the mean of the 4 observations in group $i$.

- $\hat{A}_i$ is the respective group mean minus the sample mean a.k.a. between-subjects variance 

- $\widehat{S(A)}_{ij}$ is the individual error term for a given participant, or how much they deviate from the group contribution.

Applying these estimation equations to the data above yields the following decomposition matrix:

```{r ch11-dmx-setup, echo = FALSE, results = 'markup'}
.dmx <- tibble(i = rep(1:3, each = 4),
       j = rep(1:4, times = 3),
       Yij = .Y, mu = .mu, Ai = rep(.meal_eff, each = 4),
       err = .err) %>%
  mutate(ID = row_number()) %>%
  select(ID, everything())
```

```{r ch11-dmx-show, echo=FALSE, results = 'asis'}
knitr::kable(.dmx, 
             digits = 3, 
             caption = "Decomposition Matrix of our data")%>%
  kable_styling() %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
<br>
In the above table: 

* the column `ID` can be used to locate individual rows,
* the column `mu` represents the value of $\hat{\mu}$, 
* the columns `Ai` represents the value of $\hat{A}_i$, 
* and the column `err` represents the value of $\widehat{S(A)}_{ij}$.  

Spend a few moments understanding how this table expresses each of the 12 observed values in our example (the $Y_{ij}$s) in terms of the linear model: $Y_{ij} = \mu + A_i + S(A)_{ij}$.  

For example, if the Control group is group $i$ = 1, then for the first participant, $j$ = 1, you would get:

- $Y_{ij} = \mu + A_i + S(A)_{ij}$
- $Y_{ij} = mu + Ai + err$
- $37 = 50 + 8 + -21$

Meaning that the overall mean of the whole sample is **50**. The unique contribution of the control group is **$+8$**, so the predicted value of a member of the control group would be **58**. However the first participant has an error of **$-21$** from the predicted as there actual score is **37**. We will start to understand how the difference (the variance) between conditions and within conditions leads to our analysis later but hopefully you are beginning to understand some of the above.

<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span> 

To make sure you understand the above equations before going on to calculate your own, answer the following questions about the above table which expresses the GLM decomposition matrix, and then check your answers.

1. In which column of the table are the 12 observed values - i.e. the 12 original scores from the participants?
<br>
`r hide("Answer")`
```{block, type ="info"}
The column named `Yij`
```
`r unhide()`
<br>

2. What is the estimated grand mean of this sample? (hint: $\hat{\mu}$)
<br>
`r hide("Answer")`
**The estimate grand mean of the sample is `r .mu`**
`r unhide()`
<br>

3. Which rows of the table contain the data and model estimates for the **Buffet** group if they are group 2?
<br>
`r hide("Answer")`
```{block, type ="info"}
The rows where `i` is equal to 2; in other words, rows 5-8
```
`r unhide()`
<br>

4. What is the value of $\hat{A}_1$ and in what rows does it appear?
<br>
`r hide("Answer")`
**The value would be `r .meal_eff[1]`**

```{block, type ="info"}
This value appears in column `Ai`, rows 1-4, where `i` equals 1.  This can be thought of as the difference between that given group and the sample mean that is applied to all participants within that group. In other words this would be the value of the **typical** participant in that group - as opposed to an individual participant in that group. Note that, conceptually this gives you the effect of your manipulation (food consumption style) and the different `i` allow you do so for each of the different styles you look at. 

```
`r unhide()`
<br>

5. What is the value of $\widehat{S(A)}_{32}$? (hint: this can be read as where i = 3 and j = 2)
<br>
`r hide("Answer")`

**The value would be `r .err[10]`**

```{block, type ="info"}
This value appears in column `err`, where `i` equals 3 and `j` equals 2. This can be thought of the unique difference for that participant from the sample mean and group mean, meaning that we take into consideration that each subject is unique.
```
`r unhide()`
<br>

6. What is the model's prediction for a 'typical' participant in the **Salad** group ($\hat{A}_3$)? (hint: ($\hat{Y}_{ij}$ = mu + Ai)
<br>
`r hide("Answer")`

**The prediction would be $\hat{Y}_{ij} = \hat{\mu} + \hat{A}_3$ = `r .mu` + `r .meal_eff[3]` = `r .mu + .meal_eff[3]`**

```{block, type ="info"}
* A 'typical' participant is one where the residual is 0. 
* The model prediction is also known as the "fitted value" for this group.  
```
`r unhide()`
<br>

7. Where in the table are the differences found between this 'typical' participant prediction and the observed values in the salad group?
<br>
`r hide("Answer")`
```{block, type ="info"}

These are the called the "residuals" ($\widehat{S(A)}_{ij}$) and are found in the `err` column of the table in rows 9-12. As above they are the difference between that specific individual participant and the typical participant for that group.

```
`r unhide()`

### Recreate decomposition matrix from the raw data

So we have shown you where all the parts of the table come from and how to calculate them. Now, for this part, your task is to reproduce the **decomposition matrix** tibble shown above, reproduced here:

```{r ch11-dmx-show2, echo=FALSE, results = 'asis'}
knitr::kable(.dmx, 
             digits = 3, 
             caption = "Decomposition Matrix of our data")%>%
  kable_styling() %>%
  scroll_box(width = "100%", box_css = "border: 0px;")
```
<br>

You will do this by typing the observed values into a tibble, and then writing code to add columns with estimates of the individual components. At the end, your table should look exactly like the one above. You already know how to do all the data-wrangling elements, so today really try to focus on understanding what the values mean. And remember that for each step there is a solution at the end of the chapter.

### Step 1: Create the basic tibble {#Ch11InClassQueT1}

* Create a tibble named `dmx` (short for decomposition matrix). It will eventually contain all of the columns in the one above, but for now, just create the columns `i`, `j`, and `Yij` as they appear above. 
* You already know how to create a tibble (don't forget to load the tidyverse package first). In case you need to refresh your memory, see <a href="https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf" target = "_blank">page 2 of this cheatsheet on data input</a> or refer back to the preclass activities of Chapter 5.
* You should just type in the values for `Yij` but try to use the rep() function for columns `i` (the group) and `j` (the participant).

`r hide("Step 1 Hints")`
```{block, type ="info"}
* You will need some wrangling functions so don't forget to load in tidyverse
* Create a tibble as `dmx <- tibble(i = NA, j = NA, Yij = NA)`
* When using the `rep()` function remember that you can use `each` or `times` as calls in rep: `rep(1:3, each = 4)`
* When typing in numbers the c() function will allow you to put in numbers such as `Column = c(37, 80, 64, 31)`
```
`r unhide()`

### Step 2: Estimate the Grand Mean $\hat{\mu}$ {#Ch11InClassQueT2}

Great, we have our group numbers $i$, our participant numbers $j$, and our participant scores $Yij$. Now we need to start expanding out `dmx`.  First thing we need is the grand mean: $\hat{\mu}$

* In a new tibble called `dmx2`, add a column to the `dmx` tibble, called `mu` representing $\hat{\mu}$.  Remember that you can add a column to a table using `mutate()`.
* $\hat{\mu}$ is the `mean()` of all participants.

`r hide("Step 2 Hints")`
```{block, type ="info"}

* When calculating `mu` keep in mind that each value of `mu` should be the grand mean of the sample; the mean of all participants regardless of group.

`dmx2 <- dmx %>% mutate(mu = ???)`

```
`r unhide()`

### Step 3: Entering the estimates $\hat{A}_1$, $\hat{A}_2$, $\hat{A}_3$ {#Ch11InClassQueT3}

Good! Now we need to add on a column showing the typical effect of being a member of a particular group; the `Ai` column.

* In a new tibble called `dmx3`, add a column to the tibble `dmx2` called `Ai`, with the three estimates for $\hat{A}_i$.  Store the resulting tibble in `dmx3`.
* $\hat{A}_i$ is the difference between the grand mean $\hat{\mu}$ and the mean of individual groups. This means that you will need to group people by the group they belong to.
* Add the `ungroup()` function to the end of your pipeline as you won't need this grouping after.

`r hide("Step 3 Hints")`
```{block, type ="info"}

* To calculate the column $\hat{A}_i$ would be something like:

dmx3 <- dmx2 %>% 
   group_by(something) %>%
   mutate(Ai = something - something) %>%
   ungroup()
```
`r unhide()`

### Step 4: Calculate Residuals $\widehat{S(A)}_{ij}$ {#Ch11InClassQueT4}

Well done, you're almost there!  We just need to add on the final column, `err`, which is called the **residuals** or in other words, the difference between the score of a typical participant for that group (`mu` + `Ai`) and a given individual participant' score (`Yij`).

* In a new tibble called `dmx4` add a column to the `dmx3` tibble called `err` that  contain the residuals - the difference between the observed (Yij) and fitted (typical) values.

`r hide("Step 4 Hints")`
```{block, type ="info"}

For calculating `err` you would use:

$\widehat{S(A)}_{ij} = Y_{ij} - \hat{Y}_{ij}$

where:

$\hat{Y}_{ij} = \hat{\mu} + \hat{A}_i$ 
```
`r unhide()`

### Step 5: Sums of squares {#Ch11InClassQueT5}

Great! Last step for today. Once you have your final dmx, `dmx4`, you can start calculating the sums of squars.  Sums of squares are used in calculations for performing tests on model components, which we will learn more about soon, but you can practice them for now as shown in Section 3.4 of Miller and Haden (2013). The steps are as follows:

* Step1 - Square all the individual values in the columns `Yij`, `mu`, `Ai`, and `err` in `dmx4`, 
* Step2 - Now sum up the squared values for each of these columns.  
* Step3 - Save these in a variable called `sstbl`.

The simplest way to square a column, for example called `x`, would be:

`mutate(squared_x = x^2)`

The `^2` means take `x` to the power of 2.  So typing `3^2` in the console will give you `9` (try it if you're unsure). It also works for columns! Give that a go. If you have done it correctly you should see the below table:

```{r}
.dmx %>%
  mutate(Yij2 = Yij^2,
         mu2 = mu^2,
         Ai2 = Ai^2,
         err2 = err^2) %>%
  select(Yij2, mu2, Ai2, err2) %>%
  summarise(ss_Y = sum(Yij2),
            ss_mu = sum(mu2),
            ss_Ai = sum(Ai2),
            ss_err = sum(err2)) %>%
  knitr::kable(align = "c", caption ="Sums of Squares for this analysis")
```

Where:

* `ss_Y` represents the Sums of Squares of `Yij`. This is referred to as the Sums of Squares total or $SS_{total}$.
* `ss_mu` represents the Sums of Squares of `mu`. This is referred to as the Sums of Squares of the grand mean or $SS_{\mu}$ and sometimes called the intercept.
* `ss_Ai` represents the Sums of Squares of `Ai`. This is referred to as the Sums of Squares of A or $SS_{A}$ and sometimes called $SS_{between}$.
* `ss_err` represents the Sums of Squares of `err`. This is referred to as the Sums of Squares error or $SS_{error}$ and sometimes called $SS_{within}$.
* And the following statement is true:

$$SS_{total} = SS_{\mu} + SS_{A} + SS_{error}$$
<br>

`r hide("Step 5 Hints")`
```{block, type ="info"}

* Mutate on the squared values:

`dmx4 %>% mutate(Yij2 = Yij^2, ...)`

* And sum up using:

`summarise(ss_Y = sum(Yij2), ss_mu = ...)`
```
`r unhide()`
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--blue);">Job Done - Activity Complete!</span>

Well done! How did you do?  If the values in your `dmx4` do not match the table above then you need to go back and look at where it has gone wrong. Alternatively you should look at the solutions at the end of the chapter.  

To recap, what we are doing here is setting up the ANOVA to compare the three groups to see if there is a significant difference between each group.  Doing it this way allows us to get an understanding of where the numbers come from, and it highlights that the analysis is about comparing variance within groups and variance between groups. We will cover this in the next chapter but in short, **if the variance between groups $SS_{A}$ is larger than the variance within groups $SS_{error}$ then it is likely that your experimental manipulation has had an effect.  Conversely, if the variance within groups is larger than the variance between groups then there is likely to be no effect of your experimental manipulation.**

**One last thing:**

Before ending this section, if you have any questions, please post them on the available forums or speak to a member of the team. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. Remember the more you work with knowledge and skills the easier they become.
