## PreClass Activity

A bit of a change of pace in this PreClass Activity. In order to give you a bit more of an understanding of the assumptions of the between-subjects t-test, and a viable alternative to the standard Student's t-test, we ask that you read the following blog (and even the full paper if you have time) and then try out the couple of tasks below.

### Reading

Read the following blog on using **Welch's t-test** for between-subjects designs. 

**Blog:**

* <a href = "https://daniellakens.blogspot.co.uk/2015/01/always-use-welchs-t-test-instead-of.html" target = "_blank">Always use Welch's t-test instead of Student's t-test</a> by Daniel Lakens.

For further reading you can look at the paper that resulted from this blog:

**Paper:**

* Delacre, M., Lakens, D., & Leys, C. (in press) <a href = "https://osf.io/sbp6k/" target = "_blank">Why Psychologists Should by Default Use Welch's t-test Instead of Student's t-test</a>. International Review of Social Psychology.

### Task

1. Copy the script within the blog into an R script and try running it to see the difference between Welch's t-test (the recommended test in the blog) and Student's t-test (the standard test in the field). 

* **Note:** You will need the **`car`** package. This is installed already in the Boyd Orr labs so if doing this in the labs, do not install the package, just call it to the library with library(car). If yuo are using your own machine then you will need to install the **`car`** package.
* **Note:** The code will take a minute or two to run as there is a stage of simulating data (just like we did in Chapter 5) that will take a little time to run.

Don't worry if you don't yet understand all the code. It is highly commented, with each line stating what it does, but it is tricky.  The key thing is to try and run it and to look at the figures that come out of it - particularly the third figure that you see in the blog, the one with the red line on it that compares p-values in the two tests. Look at how many tests (dots) are significant on one test and not the other. We give an explanation of the blog below but it is worth trying it out yourself first.

2. Now try changing the values for n1, n2, sd1 and sd2 at the top of the script to see what effect this has on the Type 1 Error rate (alpha = .05). Again look at the figure with the red line, comparing significance on one test versus significance on the other.  This is what should change depending on the n of each sample and whether the variance is equal or not.

3. Think about the overall point of this blog and which test we should use when conducting a t-test. Once you have thought about this, read the blog below and see if you follow it. We will look at this more again in later chapters and lectures.

**Understanding the Blog and the assumption of variance**

What the blog and paper are trying to help us recognise is that if the two groups you have sampled have equal number of participants and equal variance, then both the Student's t-test and the Welch's t-test will return the same t-value and, therefore, p-value. This means that you would reject the null hypothesis an equal number of times regardless of which test you used. You can see this in the first figure below with significant results shown as blue circles and non-significant resutls show as orange circles. We have used an adapted version of the code from the blog to create this figure - the settings we have used are in the box shown and you can change your code to match them (remember to set the seed to get the same values)

```{r chpt7-match-data, eval = FALSE}
set.seed(1409)

n1 <- 38 
n2 <- 38 
sd1 <- 1.85 
sd2 <- 1.85 

nSims <- 500
```

```{r chpt7-match-figure, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Scatterplot illustrating that with equal number of participants and equal variance, Welch's t-test and Student's t-test find the same results as significant (Blue Circles) and the same results as non-significant (Orange Circles). The horizontal and vertical black lines represent the alpha = .05 for both tests. Dots falling on diagonal red line show tests with the same p-value for Welch's and Student's t-tests. "}
# code adapted from https://daniellakens.blogspot.co.uk/2015/01/always-use-welchs-t-test-instead-of.html

library(car) 
library(tidyverse)

set.seed(1409)

n1 <- 38 #size condition x
n2 <- 38 #size condition y
sd1 <- 1.85 #sd condition x
sd2 <- 1.85 #sd condition y 
m1 <- 0
m2 <- 0
trueD <- (m2-m1)/(sqrt((((n1 - 1)*((sd1^2))) + (n2 - 1)*((sd2^2)))/((n1+n2)-2)))

nSims <- 500 #number of simulated experiments (large numbers might take a while)
p1 <- numeric(nSims) #set up empty container for all simulated Student's t-test p-values 
p2 <- numeric(nSims) #set up empty container for all simulated Welch's t-test p-values
#create variables for dataframe
catx <- rep("x",n1)
caty <- rep("y",n2)
condition <- c(catx,caty)

#run simulations
for(i in 1:nSims){ #for each simulated experiment
  sim_x <- rnorm(n = n1, mean = m1, sd = sd1) #simulate participants condition x
  sim_y <- rnorm(n = n2, mean = m2, sd = sd2) #simulate participants condition y
  #perform Student and Welch t-test
  p1[i] <- t.test(sim_x,sim_y, alternative = "two.sided", var.equal = TRUE)$p.value #perform the t-test and store p-value
  p2[i] <- t.test(sim_x,sim_y, alternative = "two.sided", var.equal = FALSE)$p.value #perform the t-test and store p-value
}

my_dat <- tibble(S = p1, W = p2) %>%
  mutate(Trial = row_number()) %>%
  mutate(Swrong = case_when(W > .05 & S < .05 ~ "A",
                            W > .05 & S > .05 ~ "B",
                            TRUE ~ "C"))

my_dat %>% ggplot(aes(S, W, color = Swrong)) + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red", size = 1) +
  geom_hline(aes(yintercept = .05)) +
  geom_vline(aes(xintercept = .05)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0,0.15), ylim = c(0, 0.15), expand = FALSE) +
  labs(x = "p-values from Student's t-test",
       y = "p-values from Welch's t-test") + 
  guides(color = FALSE) +
  scale_color_manual(values = c("orange","blue")) +
  theme_light()
```

However, and the key point of the blog, if the two groups have unequal variance and/or an unequal number of participants, the two tests start to give different findings. This is shown in the below figure where findings that are significant in both tests are shown in blue, findings that are non-significant in both tests are shown in orange, and findings that are significant using the Student's t-test but non-significant by using Welch's t-test are shown in pink. If we read the blog, especially about how p-values work when there is no actual difference between two groups, then we can come to the conclusion that Welch's t-test is working better than the Student's t-test in this scenario. To put it in ther words, the Student's t-test is finding more tests as significant than it should, and as such has a false positive rate (Type 1 error rate) much higher than the expected $\alpha = .05$. Have a look at the figure and see if you can understand it.


```{r chpt7-miss-match-data, eval = FALSE}
set.seed(1409)

n1 <- 38 
n2 <- 25 
sd1 <- 1.15 
sd2 <- 1.85  

nSims <- 500
```


```{r chpt7-miss-match, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Figure illustrates that with unequal number of participants and/or unequal variance, Welch's t-test and Student's t-test work differently, returning conflciting findings. Findings that are significant in both tests are shown in blue, findings that are non-significant in both tests are shown in orange, and findings that are significant using the Student's t-test but non-significant by using Welch's t-test are shown in pink. The horizontal and vertical black lines represent the alpha = .05 for both tests. Dots falling on diagonal red line show tests with the same p-value for Welch's and Student's t-tests. Dots above (below) the red line shown tests where p-value is smaller (larger) in the Student's t-test."}
# code adapted from https://daniellakens.blogspot.co.uk/2015/01/always-use-welchs-t-test-instead-of.html
set.seed(1409)

n1 <- 38 #size condition x
n2 <- 25 #size condition y
sd1 <- 1.15 #sd condition x
sd2 <- 1.85 #sd condition y 
m1 <- 0
m2 <- 0
trueD <- (m2-m1)/(sqrt((((n1 - 1)*((sd1^2))) + (n2 - 1)*((sd2^2)))/((n1+n2)-2)))

nSims <- 500 #number of simulated experiments (large numbers might take a while)
p1 <- numeric(nSims) #set up empty container for all simulated Student's t-test p-values 
p2 <- numeric(nSims) #set up empty container for all simulated Welch's t-test p-values
#create variables for dataframe
catx <- rep("x",n1)
caty <- rep("y",n2)
condition <- c(catx,caty)

#run simulations
for(i in 1:nSims){ #for each simulated experiment
  sim_x <- rnorm(n = n1, mean = m1, sd = sd1) #simulate participants condition x
  sim_y <- rnorm(n = n2, mean = m2, sd = sd2) #simulate participants condition y
  #perform Student and Welch t-test
  p1[i] <- t.test(sim_x,sim_y, alternative = "two.sided", var.equal = TRUE)$p.value #perform the t-test and store p-value
  p2[i] <- t.test(sim_x,sim_y, alternative = "two.sided", var.equal = FALSE)$p.value #perform the t-test and store p-value
}

my_dat <- tibble(S = p1, W = p2) %>%
  mutate(Trial = row_number()) %>%
  mutate(Swrong = case_when(W > .05 & S < .05 ~ "A",
                            W > .05 & S > .05 ~ "B",
                            TRUE ~ "C"))

my_dat %>% ggplot(aes(S, W, color = Swrong)) + 
  geom_abline(aes(intercept = 0, slope = 1), color = "red", size = 1) +
  geom_hline(aes(yintercept = .05)) +
  geom_vline(aes(xintercept = .05)) +
  geom_point(size = 3) +
  coord_cartesian(xlim = c(0,0.15), ylim = c(0, 0.15), expand = FALSE) +
  labs(x = "p-values from Student's t-test",
       y = "p-values from Welch's t-test") + 
  guides(color = FALSE) +
  scale_color_manual(values = c("pink","orange","blue")) +
  theme_light()
```

So what is the difference between the two tests? The answer relates to the assumption of variance. What is considered the common/standard t-test in the field, Student's t-test, has the assumption of equal variance, whereas Welch's t-test has no assumption of equal variace - it does however have all the other same assumptions as the Student's t-test. What this blog shows is that if the groups have equal variance then both tests return the same finding. However if the assumption of equal variance is violated, i.e. the groups have unequal variance, then Welch's test produce the more accurate finding, based on the data. This is important as often the final decision on whether assumptions are "held" or "violated" is subjective; i.e. it is down to the researcher to fully decide. Nearly all data will show some level of unequal variance (with perfectly equal variance across multiple conditions actually once revealing fraudulent data). Researchers using the Student's t-test regularly have to make a judgement about whether the variance across the two groups is "equal enough". As such, this blog shows that it is always better to run a Welch's t-test to analyse a between-subjects design as a) Welch's t-test does not have the assumption of equal variance, b) Welch's t-test gives more accurate results when variance is not equal, and c) Welch's t-test performs exactly the same as the Student t-test when variance is equal across groups. 

In short, Welch's t-test takes a level of ambiguity (or what may be called a "researcher degree of freedom") out of the analysis and makes the analysis less open to bias or subjectivity. As such, from now on, unless stated otherwise, you should run a Welch's t-test. 

In practice it is very easy to run the Welch's t-test, and you can switch between the tests as shown: 

* to run a Student's t-test you set `var.equal = TRUE`
* to run a Welch's t-test you set `var.equal = FALSE`

For example, if we created some simulated data of two groups (A vs B) with twenty data points in each group:

```{r sim_data}
my_data <- tibble(my_iv = rep(c("A", "B"), each = 20),
                  my_dv = c(rnorm(20,0,1),rnorm(20,0,1)))
```


This would run a Student's t-test:

```{r studentsexp, warning=FALSE}
t.test(my_dv ~ my_iv, data = my_data, var.equal = TRUE)
```

This would run the Welch's test:

```{r welchsexp, warning = FALSE}
t.test(my_dv ~ my_iv, data = my_data, var.equal = FALSE)
```

And two ways to know that you have run the Welch's t-test are:

1. The output says you ran the **Welch Two Sample t-test**
2. The df is likely to have decimal places in the Welch's t-test whereas it will be a whole number in the Student's t-test.

Always run the Welch's test in a between-subjects design when using R!

Don't worry if you don't yet fully understand this blog. We will have some practice on it in coming chapters but in short Welch's t-test is better as it does not require the assumption of equal variance.

Conversely, there is no concern with variance in a within-subjects t-test because, as you will know from lectures, the top half of the equation of the formula (the numerator) is the mean difference between the two conditions, and so it is only one set of values and there is nothing to equate it to. We are going to explore the assumptions of the within-subjects t-test in much more depth next in the InClass Activities!

<span style="font-size: 22px; font-weight: bold; color: var(--blue);">Job Done - Activity Complete!</span>

That's it for the PreClass Activity! This is a bit of a change to the PreClass activities you have done so far, and you will start to see this approach more in the later chapters of this book - you being asked to read blogs, papers, and chapters from other books.  Don't forget though that it is really important to summarise information in your own words to help you really understand it, so you should always be going back and adding informative points to your Portfolio. And, as always, post any questions you have on the available forums or ask a member of staff.
